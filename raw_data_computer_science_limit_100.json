[
        {
            "acceptedDate": "2010-03-01T00:00:00",
            "arxivId": "1003.0219",
            "authors": [
                {
                    "name": "Malioutov, Dmitry"
                },
                {
                    "name": "Sanghavi, Sujay"
                },
                {
                    "name": "Willsky, Alan"
                }
            ],
            "contributors": [
                "Willsky, Alan S.",
                "Malioutov, Dmitry M.",
                "Massachusetts Institute of Technology. Department of Electrical Engineering and Computer Science"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/4425864"
            ],
            "createdDate": "2012-04-13T14:17:48",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 178,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/178",
                    "logo": "https://api.core.ac.uk/data-providers/178/logo"
                }
            ],
            "depositedDate": "2010-04-01T00:00:00",
            "abstract": "Compressed sensing allows perfect recovery of sparse signals (or signals\nsparse in some basis) using only a small number of random measurements.\nExisting results in compressed sensing literature have focused on\ncharacterizing the achievable performance by bounding the number of samples\nrequired for a given level of signal sparsity. However, using these bounds to\nminimize the number of samples requires a-priori knowledge of the sparsity of\nthe unknown signal, or the decay structure for near-sparse signals.\nFurthermore, there are some popular recovery methods for which no such bounds\nare known.\n  In this paper, we investigate an alternative scenario where observations are\navailable in sequence. For any recovery method, this means that there is now a\nsequence of candidate reconstructions. We propose a method to estimate the\nreconstruction error directly from the samples themselves, for every candidate\nin this sequence. This estimate is universal in the sense that it is based only\non the measurement ensemble, and not on the recovery method or any assumed\nlevel of sparsity of the unknown signal. With these estimates, one can now stop\nobservations as soon as there is reasonable certainty of either exact or\nsufficiently accurate reconstruction. They also provide a way to obtain\n\"run-time\" guarantees for recovery methods that otherwise lack a-priori\nperformance bounds.\n  We investigate both continuous (e.g. Gaussian) and discrete (e.g. Bernoulli)\nrandom measurement ensembles, both for exactly sparse and general near-sparse\nsignals, and with both noisy and noiseless measurements.Comment: to appear in IEEE transactions on Special Topics in Signal Processin",
            "documentType": "research",
            "doi": "10.1109/jstsp.2009.2038211",
            "downloadUrl": "http://arxiv.org/abs/1003.0219",
            "fieldOfStudy": "computer science",
            "fullText": "ar\nX\niv\n:1\n00\n3.\n02\n19\nv1\n  [\ncs\n.IT\n]  \n1 M\nar \n20\n10\n1\nSequential Compressed Sensing\nDmitry M. Malioutov, Sujay R. Sanghavi, and Alan S. Willsky, Fellow, IEEE\nAbstract—Compressed sensing allows perfect recovery\nof sparse signals (or signals sparse in some basis) using\nonly a small number of random measurements. Existing\nresults in compressed sensing literature have focused on\ncharacterizing the achievable performance by bounding the\nnumber of samples required for a given level of signal\nsparsity. However, using these bounds to minimize the\nnumber of samples requires a-priori knowledge of the\nsparsity of the unknown signal, or the decay structure for\nnear-sparse signals. Furthermore, there are some popular\nrecovery methods for which no such bounds are known.\nIn this paper, we investigate an alternative scenario\nwhere observations are available in sequence. For any\nrecovery method, this means that there is now a sequence\nof candidate reconstructions. We propose a method to\nestimate the reconstruction error directly from the samples\nthemselves, for every candidate in this sequence. This\nestimate is universal in the sense that it is based only on the\nmeasurement ensemble, and not on the recovery method or\nany assumed level of sparsity of the unknown signal. With\nthese estimates, one can now stop observations as soon as\nthere is reasonable certainty of either exact or sufficiently\naccurate reconstruction. They also provide a way to obtain\n“run-time” guarantees for recovery methods that otherwise\nlack a-priori performance bounds.\nWe investigate both continuous (e.g. Gaussian) and\ndiscrete (e.g. Bernoulli) random measurement ensembles,\nboth for exactly sparse and general near-sparse signals,\nand with both noisy and noiseless measurements.\nIndex Terms—Compressed sensing, sequential measure-\nments, stopping rule.\nI. INTRODUCTION\nIn compressed sensing (CS) [1], [2] a few random\nlinear measurements of a signal are taken, and the signal\nis recovered using the additional knowledge that either\nthe signal or some linear transform of it is sparse.\nThese ideas have generated a lot of excitement in the\nsignal processing and machine learning communities,\nand have been applied to a range of applications such as\nDmitry Malioutov (e-mail: dmal@alum.mit.edu) is at Mi-\ncrosoft Research, Cambridge, UK. Sujay Sanghavi (email: sang-\nhavi@mail.utexas.edu) is with the Electrical and Computer Engineer-\ning department at the University of Texas, Austin. Alan Willsky is\nwith the Department of Electrical Engineering and Computer Science,\nMassachusetts Institute of Technology, Cambridge, MA.\nPREPRINT. The article is to appear in IEEE Transactions on Special\nTopics in Signal Processing. This work was supported by the Army\nResearch Office under Grant W911NF-05-1-0207, and the Air Force\nOffice of Scientific Research under Grant FA9550-04-1-0351.\nmagnetic resonance imaging (MRI) [3], computational\nphotography [4], wireless networks [5], and structure\ndiscovery in biological networks [6].\nThe applications where compressed sensing is most\nbeneficial (e.g. MRI) have a high cost of acquiring each\nadditional sample. If this cost (in terms of time, power,\ne.t.c) is high as compared to the cost of computation,\nthen it is suitable to use sophisticated recovery algo-\nrithms which include the `1-based basis pursuit [7],\ngreedy approaches [8], and even non-convex (`p) or\niterative formulations [9]–[11] to enable recovery from\nfewer measurements.\nWhile some of the recovery methods, especially those\nbased on `1-regularization, have analytically provable\nperformance guarantees [2], [12], others, such as non-\nconvex `p, reweighted `1 [11], and sparse Bayesian\nlearning (SBL) [13] do not, and they have been shown\nempirically to often require even fewer samples than `1-\nbased methods. Furthermore, when guarantees do exist,\nthey have been empirically observed to sometimes be\nhighly pessimistic and may require large dimensions to\nhold with high probability [1], [14]. Another drawback\nis that much of the existing analysis characterizes how\nmany measurements are needed for a signal with a given\nsparsity level. However, as the sparsity level is often not\nknown a-priori, it can be very challenging to use these\nresults in practical settings.\nIn this paper we take an alternative approach and\nwe develop estimates and bounds for the reconstruction\nerror using only the observations, without any a-priori\nassumptions on signal sparsity, or on the reconstruction\nmethod. We consider a scenario where one is able to get\nobservations in sequence, and perform computations in\nbetween observations to decide whether enough samples\nhave been obtained – thus allowing to recover the signal\neither exactly or to a given tolerance from the smallest\npossible number of random observations. This, however,\nrequires a computationally efficient approach to detect\nexactly when enough samples have been received. To get\nan intuition behind our approach – suppose that we first\nattempt to reconstruct the signal while withholding some\navailable observations, akin to cross-validation. The ob-\nservations correspond to a known linear function of the\ntrue signal, so if the reconstructed signal is quite different\nfrom the true signal, then the same linear function\napplied to our recovered signal will result in a value that\nis far from the actual observation, with high probability.\nOur results provide estimates of the reconstruction error\nbased on the statistics of the measurement model. They\ncan thus be used to provide ’run-time’ guarantees even\nfor decoders that are otherwise not amenable to analysis.\nWe first consider the case when noiseless measure-\nments are taken using the random Gaussian (or generic\ncontinuous) ensemble, and we show that simply checking\nfor one-step agreement provides a way to check exactly\nwhen enough samples have been received. Suppose that\nafter receiving M samples yi = a′ix, i = 1, ..,M ,\nwe apply a sparse reconstruction method of our choice,\nand obtain a solution xˆM satisfying all the M measure-\nments. We can use any sparse decoder, including greedy\nmatching pursuit, SBL, `p formulations, and even the\nbrute-force decoder, but we require that the solution at\neach step M satisfies yi = a′ixˆM , for i = 1, ..,M . For\nexample, in the case of basis pursuit, we would solve\nxˆM = argmin ||x||1 s.t. a′ix = yi, i = 1, ..,M. (1)\nNext, we receive one more measurement, and check\nfor one step agreement: i.e. if xˆM+1 = xˆM , then the\ndecoder declares xˆM to be the reconstruction and stops\nrequesting new measurements. In Section III we show\nin Propositions 1 and 2 that this decoder gives exact\nreconstruction with probability one.\nFor some other measurement ensembles, such as ran-\ndom Bernoulli and the ensemble of random rows from\na Fourier basis, the one-step agreement stopping rule no\nlonger has zero probability of error. We modify the rule\nto wait until T subsequent solutions xˆM , ..., xˆM+T all\nagree. In Section IV we show in Proposition 3 that in the\nBernoulli case the probability of making an error using\nthis stopping rule decays exponentially with T , allowing\ntrade-off of error probability and delay.\nIn Sections V and VI we show how the error in\nreconstruction can be estimated from the sequence of\nrecovered solutions. We first present analysis for the\nGaussian measurement ensemble in Proposition 4, and\nthen generalize to any sensing matrices with i.i.d. entries.\nThis enables the decoder to stop once the error is below a\nrequired tolerance – even for signals that are not exactly\nsparse, but in which the energy is largely concentrated\nin a few components, or for measurements which are\ncorrupted by noise.\nFinally, in Section VII we motivate the need for\nefficient solvers in the sequential setting. We consider\nthe basis pursuit sparse solver and show that rather than\nre-solving the problem from scratch after an additional\nmeasurement is received, we could use an augmented\nlinear program that uses the solution at step M to guide\nits search for the new solution. We show empirically\nthat this approach significantly reduces computational\ncomplexity.\nDuring the review process we learned about a very\nrecent analysis in [15] for the cross-validation setting,\nusing the Johnson-Lindenstrauss lemma. We describe\nsimilarities and differences from our work in the discus-\nsion in Section V. Our current paper extends our earlier\nresults presented in [16].\nII. BRIEF OVERVIEW OF COMPRESSED SENSING\nAs there is no dearth of excellent tutorials on com-\npressed sensing [1], [2], [17], in this section we give\nonly a brief outline mainly to set the stage for the rest\nof the paper. At the heart of compressed sensing lies\nthe sparse recovery problem1, which tries to reconstruct\nan unknown sparse signal x from a limited number of\nmeasurements y = Ax, where A ∈ RM×N , M << N .\nMuch of excitement in the field stems from the fact\nthat the hard combinatorial problem of searching for\nsparse solutions in the affine space {x : y = Ax}\nunder certain suitable conditions can be solved exactly\nvia various tractable methods. The most widely known\nmethods include greedy matching pursuit and its variants\n[8], and approaches based on convex optimization, using\n`1 norms as a proxy for sparsity [7]:\nmin ‖x‖1 subject to y = Ax. (2)\nAn early sufficient condition for sparse recovery [18]\nstates that the formulation in (2) recovers the unique\nsparse solution if A is well-posed and x is sparse enough,\ni.e. if ‖x‖0 < 1+1/M(A)2 , where M(A) = maxi6=j |a′iaj |,\nand A has columns ai normalized to 1. However,\nthis simple condition is very pessimistic. Much tighter\nconditions are obtained by considering larger subsets\nof columns of A, e.g. the restricted isometry property\n(RIP) depends on the maximum and minimum singular\nvalues over all M ×K submatrices of A [12]. Namely,\na matrix A satisfies the K-RIP with constant δK if\n(1 − δK)‖x‖22 ≤ ‖Ax‖22 ≤ (1 + δK)‖x‖22 for every x\nwhich has at most K non-zero entries. While enabling\nmuch tighter sufficient conditions for recovery of sparse\nsignals [12], the RIP is very costly (exponential in K)\nto check for a given matrix.\nResults in compressed sensing take advantage of RIP\nby bringing in the theory of random matrices into\nthe picture. In compressed sensing we receive random\n1 The ground-breaking results [18] predating compressed sensing\nwere in context of sparse signal representation where one seeks to\nrepresent a vector y in an overcomplete dictionary A ∈ RM×N ,\nM << N , with coefficients x, i.e., y = Ax.\n2\n15 20 25 30 35 40 45 50\n0\n20\n40\n60\n80\n100\nGaussian\n15 20 25 30 35 40 45 50\n0\n20\n40\n60\n80\n100\nBernoulli\nFig. 1. Histogram of the stopping times distribution for Gaussian and\nBernoulli measurement ensembles: N = 100, and K = 10, and `1\ndecoding.\nmeasurements y = Ψs where the unknown signal of\ninterest s is itself sparse in some basis, i.e. s = Φx, with\nx sparse. Hence the problem reduces to finding sparse\nsolutions satisfying y = ΨΦx = Ax, where A = ΨΦ is\na random matrix.\nA collection of results have been established that RIP\nholds for random matrices of certain size from given\nensembles: Gaussian, Bernoulli, random Fourier rows\n[2], [12], [14]. The general conclusion of these results\nis that the convex `1 formulation can recover (with high\nprobability) a signal x ∈ RN with K non-zeros from\nonly CK log(N) measurements, where C is a constant\ndepending on the random measurement ensemble. This\nis indeed remarkable – as it only requires a logarithmic\ndependence of the number of measurements on N .\nHowever, when each additional measurement is very\ncostly there are several problems with these bounds –\nfirstly, since they are high-probability results independent\nof y, they tend to be conservative, and also the constants\nC are typically generous upper-bounds. Secondly, the\nnumber of measurements depends on the number of\nnon-zero components of x which may not be known\na-priori. Finally, there are successful approaches which\nwe mentioned in Section I for which no such results are\navailable.\nIn Figure 1 we illustrate the drawbacks of using upper\nbounds on the number of measurements. We find the\nminimum number M of random samples which were\nneeded to recover a sparse signal x with N = 100,\nand K = 10 from random Gaussian and Bernoulli\nmeasurements using the `1-formulation in (2), over 500\nrandom trials. We plot a histogram of these numbers, and\nwe see that they exhibit high variance, and so relying on\nconditions that guarantee recovery with high probability\noften means taking many unnecessary samples. This\nmotivates the need for sequential compressed sensing\nscenario that can adaptively minimize the number of\nsamples for each observed y, which we describe next.\nIII. STOPPING RULE IN THE NOISELESS CONTINUOUS\nCASE\nWe now analyze the sequential CS approach for the\ncase when the measurements vectors ai come from a\ncontinuous ensemble (e.g., the i.i.d. Gaussian ensemble),\nhaving the property that with probability 1 a new vector\naM+1 will not be in any lower-dimensional subspace\ndetermined by previous vectors {ai}Mi=1. Suppose that\nthe underlying sparse signal x∗ ∈ RN has K non-\nzero components (we denote the number of non-zero\nentries in x by ‖x‖0). We sequentially receive ran-\ndom measurements yi = a′ix∗, where for concreteness\nai ∼ N (0, I) is a N -vector of i.i.d. Gaussian samples,\nbut the analysis also holds if entries of ai are i.i.d.\nsamples of an arbitrary continuous random variable. At\nstep M we use a sparse solver of our choice to obtain a\nfeasible2 solution xˆM using all the received data. Results\nin compressed sensing [1], [14] indicate that if we use\nbasis pursuit or matching pursuit methods, then after\nreceiving around M ∝ K log(N) measurements we can\nrecover the signal x∗ with high probability. This requires\nthe knowledge of K , which may not be available, and\nonly rough bounds on the scaling constants are known.\nOur approach is different – we compare the solutions at\nstep M and M+1, and if they agree, we declare correct\nrecovery.\nProposition 1: If in the Gaussian (generic continuous)\nmeasurement ensemble it holds that xˆM+1 = xˆM , then\nxˆM = x∗, with probability 1.\nProof. Let y1:M , [y1, ..., yM ]′, and\nAM , [a1, ..., aM ]\n′\n. Suppose that xˆM 6= x∗. We\nhave that y1:M = AM xˆM and y1:M = AMx∗: both x∗\nand xˆM belong to the (N−M)-dimensional affine space\n{x | y1:M = AMx}. The next measurement passes a\nrandom hyperplane yM+1 = a′M+1x∗ through x∗ and\nreduces the dimension of the affine subspace of feasible\nsolutions by 1. In order for xˆM to remain feasible\nat step M + 1, it must hold that yM+1 = a′M+1xˆM .\nSince we also have yM+1 = a′M+1x∗, then xˆM\nremains feasible only if (xˆM − x∗)′aM+1 = 0, i.e.\n2This requirement is essential for the noiseless case (it is relaxed in\nlater sections). For greedy methods such as matching pursuit this means\nthat we allow enough iterations until all the measurements received so\nfar are satisfied perfectly. Noiseless basis pursuit formulations satisfy\nit by construction.\n3\nAMx = y1:M\nxˆ\nM\nx\n∗\nATM+1x = yM+1\nFig. 2. A new constraint is added: a′\nM+1\nx = yM+1. Probability\nthat this hyperplane passing through x∗ also passes through xˆM is\nzero.\nif aM+1 falls in the N − 1 dimensional subspace of\nR\nN corresponding to Null((xˆM − x∗)′). As aM+1 is\nrandom and independent of xˆM and of the previous\nsamples a1, ..., aM , the probability that this happens is 0\n(event with measure zero). See Figure 2 for illustration.\n\u0003\nNote that the proof implies that we can simplify\nthe decoder to checking whether a′M+1xˆM = yM+1,\navoiding the need to solve for xˆM+1 at the last step3.\nMoreover, if using any sparse solver in the continuous\nensemble case the solution xˆM has fewer than M non-\nzero entries, then xˆM = x∗ with probability 1.\nProposition 2: For a Gaussian (continuous) measure-\nment ensemble, if ‖xˆM‖0 < M , then xˆM = x∗ with\nprobability 1.4\nProof. Denote the support of our unknown sparse\nvector x∗ by I, i.e. I = {i | x∗i 6= 0}. We next\ngenerate a random measurement matrix AM . Let A =\nAM to simplify notation. We receive the corresponding\nmeasurements y = Ax∗. Now A is M × N , with\nM < N . The key fact about random matrices with\ni.i.d. entries from a continuous distribution is that any\nM ×M submatrix of A is non-singular with probability\n15. We now argue that with probability 1 after receiving\ny there will not exist another sparse feasible solution\nxˆ 6= x∗, i.e. xˆ with fewer than M non-zero entries\nsatisfying y = Axˆ . We consider all possible sparse\nsupports J ⊂ {1, .., N}, with |J | < M , and show that\n3We thank the anonymous reviewer for this simplification.\n4Note that a random measurement model is essential: for a fixed\nmatrix A if 2K > M then there exist x1 and x2 such that Ax1 =\nAx2 and ‖xi‖0 ≤ K . However, for a fixed x∗ with ‖x∗‖0 < M the\nprobability that it will have ambiguous sparse solutions for a random\nchoice of A is zero.\n5This is easy to see: fix T ⊂ {1, ...,N} with |T | = M . Then\nprobability that ATM ∈ span(AT1 , ...,ATM−1 ) is zero, as ATM\nis a random vector in RM and the remaining columns span a lower-\ndimensional subspace.\n0 5 10 15 20 25 30 35 40\n0\n20\n40\n \n||x\n|| 0\n0 5 10 15 20 25 30 35 40\n0\n5\n10\n \n||x\n|| 1\n0 5 10 15 20 25 30 35 40\n0\n2\n4\n6\n \n||x\n*\n \n−\n \nx||\n2\nM\nFig. 3. Gaussian ensemble example: N = 100, and K = 10. (Top):\n‖xˆM‖0. (Middle): ‖xˆM‖1. (Bottom): ‖x∗ − xˆM‖2.\na feasible solution xˆ 6= x∗ can have this support only\nwith probability 0. There are two cases: I ⊂ J and\nI 6= I ∩ J .\nFirst suppose I ⊂ J , |J | < M , and suppose there\nexists some feasible xˆ supported on J . Then xˆ− x∗ ∈\nNull(A), and support of xˆ−x∗ is a subset of J , hence it\nis smaller than M . But that means that there is a subset of\nfewer than M columns of A that are linearly dependent,\nwhich can only happen with probability zero.\nNow consider the case I 6= I ∩ J . For a fixed I\nwe consider all such possible sets J , with |J | < M .\nFirst fix one such set J . We use the notation\nI\\J = { i ∈ I | i /∈ J}. Note that we have\ny = Ax∗ = AJx∗J +AI\\Jx\n∗\nI\\J . Let y˜ = AI\\Jx\n∗\nI\\J .\nNow since we require xˆ to be feasible, we also\nneed y = Axˆ = AJ xˆJ which would imply that\ny˜ = AJ (xˆJ −x∗J ). This means that the vector y˜ would\nalso have to be in the span of AJ . However, y˜ is a\nrandom vector in RM (determined by x∗ and AI\\J ),\nand span of AJ is an independent random subspace\nof dimension strictly less than M . Hence, the event\nthat y˜ also falls in the span of AJ has measure zero.\nThis means that for a fixed J a distinct sparse solution\ncan only exist with probability 0. Now the number\nof possible subsets J is finite (albeit large), so even\nwhen we take all such supports J , a distinct sparse\nsolution supported on J can only exist with probability\n0. Hence, with probability 1 there is only one solution\nwith ‖x‖0 < M , namely x∗. \u0003\nThis proposition allows to stop making measurements\nwhen a feasible solution has less than M nonzero\nentries – avoiding the need to make the last (M + 1)-st\n4\nmeasurement.\nConsider an example in Figure 3 with N = 100, and\nK = 10. We keep receiving additional measurements\nand solving (1) until we reach one-step agreement,\nxˆM = xˆM+1. The top plot shows that ‖xˆM‖0 increases\nlinearly with M until one step agreement occurs at\nM = 35, at which point it drops to K = 10 and a and\nwe recover the correct sparse solution, xˆM = x∗. The\nmiddle plot shows the monotonic increase in ‖xˆM‖1 (as\nthe feasible set is shrinking with M ). The bottom plot\nshows the error-norm of the solution, ‖xˆM − x∗‖2. On\naverage it tends to go down with more observations, but\nnon-monotonically. After M = 35 the error becomes\nzero. We see that in the ideal conditions of no mea-\nsurement noise, sparse unknown signals and Gaussian\nmeasurement ensembles, the number of measurements\ncan be indeed minimized by a simple stopping rule.\nIV. STOPPING RULE IN THE BERNOULLI CASE\nIn this section we study a simple but popular measure-\nment ensemble that is not one of the generic continuous\nensembles described in the previous section. Suppose\nthat the measurement vectors ai have equiprobable\ni.i.d. Bernoulli entries ±1. A difference emerges from\nthe Gaussian case: the probability that all M × M\nsubmatrices of AM are non-singular is no longer 0.\nThis makes it possible (with non-zero probability) for\nxˆM+1 to agree with xˆM even though xˆM 6= x∗, and\nfor erroneous solutions xˆM to have cardinality less than\nM . We modify the stopping rule to require agreement\nfor several steps - success is declared only when last\nT solutions all agree. We show in proposition 3 that the\nprobability of error decays exponentially with T . We use\nthe following Lemma from [19]:\nLemma 1 (Tao and Vu): Let a be an i.i.d. equiproba-\nble Bernoulli vector with a ∈ {−1, 1}N . Let W be a de-\nterministic d-dimensional subspace of RN , 0 ≤ d < N .\nThen P (a ∈ W ) ≤ 2d−N .\nWe are now ready to establish the following claim:\nProposition 3: Consider the Bernoulli measurement\ncase. If xˆM = xˆM+1 = ... = xˆM+T , then xˆM = x∗\nwith probability greater than or equal to 1− 2−T .\nProof. Suppose that xˆM 6= x∗. Denote the support of\nx∗ by I and the support of xˆM by J . At step M we have\nAMx∗ = AM xˆM . Let W = {a | (xˆM − x∗)′a = 0},\ni.e. the nullspace of (xˆM −x∗)′. Then W is an (N−1)-\ndimensional subspace of RN .\nGiven a new random Bernoulli sample aM+1, the\nvector xˆM can remain feasible at step M + 1 only\nif (xˆM − x∗)′ aM+1 = 0, i.e. if aM+1 falls into W .\nBy Lemma 1, the probability that aM+1 ∈ W is a\nmost 1/2. The same argument applies to all subsequent\nsamples of aM+i for i = 1, .., T , so the probability of\nhaving T -step agreement with an incorrect solution is\nbounded above by 2−T . \u0003\nNote that as in the discussion for the continuous case,\nwe can simply check that a′M+ixˆM = yM+i for i =\n1, ..., T , avoiding the need to solve for xˆM+T .\nWe now pursue an alternative heuristic analysis, more\nakin to Proposition 2. For the Bernoulli case, ‖xˆM‖0 <\nM does not imply xˆM = x∗. However, we believe that\nonce we obtain enough samples so that N221−M \u001c 1\nthen ‖xˆM‖0 < M will imply that xˆM = x∗ with high\nprobability. Since the elements of ai belong to finite set\n{−1, 1}, an M ×M submatrix of AM can be singular\nwith non-zero probability. Surprisingly, characterizing\nthis probability is a very hard question. It is conjectured\n[19] that the dominant source of singularity is the event\nthat two columns or two rows are equal or opposite in\nsign. This leads to the following estimate (here XM is\nM ×M ):6\nP (detXM = 0) = (1 + o(1))M\n221−M . (3)\nHowever the very recent best provable bound on this\nprobability is still rather far: P (detXM = 0) = ((34 +\no(1))M ) [19]. If we assume that the simple estimate\nbased on pairs of columns is accurate, similar analysis\nshows that the probability that a random ±1 M × N\nmatrix with M \u001c N having all M ×M submatrices\nnon-singular is (1 + o(1))N221−M .\nV. NEAR-SPARSE SIGNALS\nIn practical settings, e.g. when taking Fourier and\nwavelet transforms of smooth signals, we may only have\napproximate sparseness: a few values are large, and most\nare very small. In this section we extend our approach to\nthis case; again, and in contrast to existing work, we do\nnot need to assume a specific near-sparse structure, like\npower-law decay, but instead provide bounds that hold\nfor any signal.\nThe exact one-step agreement stopping rule from Sec-\ntion III is vacuous for near-sparse signals, as ‖x∗‖0 = N ,\nand all samples are needed for perfect recovery. We start\nby considering Gaussian measurements, and show that\nwe can gather information about the current reconstruc-\ntion error by obtaining a small number of additional\nmeasurements, and computing the distance between the\ncurrent reconstruction and the affine space determined\n6Probability that two columns are equal or opposite in sign is 21−M ,\nand there are O(M2) pairs of columns.\n5\nxˆ\nM\nx\n∗\nθ\nHM : A\nM\nx = y1:M\nHM+T : A\nM+T\nx = y1:M+T\nFig. 4. Geometry of the analysis for near-sparse signals. The unknown\nreconstruction error is related to d(xˆM ,HM+T ) and the angle θ\nbetween the line from x∗ to xˆM and the affine space HM+T defined\nby the new measurements.\nby these new measurements. The reconstruction error is\nthen equal to an unknown constant times this distance:\n‖x∗ − xˆM‖2 = CT d(xˆM , HM+T ), (4)\nwhere HM+T , {x | yi = a′ix, 1 ≤ i ≤M +T } is the\naffine space determined by all M + T measurements,\nCT is a random variable that we will bound, and\nd(xˆM , HM+T ) denotes the distance from xˆM to HM+T .\nWe characterize E[CT ] and V ar[CT ] – this gives us\na confidence interval on the reconstruction error using\nthe observed distance d(xˆM , HM+T ). We can now stop\ntaking new measurements once the error falls below a\ndesired tolerance. Note that our analysis does not assume\na model of decay, and bounds the reconstruction error by\nobtaining a small number of additional measurements,\nand computing the prediction error. In contrast, some\nrelated results in CS literature assume a power-law\ndecay of entries of x∗ (upon sorting) and show that\nwith roughly O(K logN) samples, xˆM in (1) will have\nsimilar error to that of keeping the K largest entries in\nx∗ [1].\nWe now outline the analysis leading to a bound based\non (4). Consider Figure 4. Let HM = {x : AMx =\ny1:M} be the subspace of feasible solutions after M\nmeasurements. Both x∗ and xˆM lie in HM . The affine\nspace HM+T is contained in HM . Let L = N − M ,\nand θT be the angle between the vector xˆM − x∗ and\nthe affine space HM+T . Both are contained in the L-\ndimensional space HM . Centering around x∗, we see\nthat θT is the angle between a fixed vector in RL and\na random L − T dimensional subspace of RL, and the\nconstant CT in (4) is equal to 1sin(θT ) :\n‖x∗ − xˆM‖2 = d(xˆ\nM , HM+T )\nsin(θT )\n, (5)\nWe next analyze the distribution of θT and hence of CT .\nIn distribution, θT is equivalent to the angle between a\n0 20 40 60 80 100\n0\n2\n4\n6\n8\n10\n \n \nT\n0 20 40 60 80 100\n0\n2\n4\n6\n8\n \n \nT\nsample mean\nmean estimate\nbound on mean\nsample std\nbound on std\nFig. 5. (Top) sample mean, estimate of the mean, and a bound on\nthe mean of CT . (Bottom) sample standard deviation, and a bound on\nthe standard deviation of CT . Sample mean is based on 1000 samples.\nL = 100.\nfixed L− T dimensional subspace, say the one spanned\nby the last L − T coordinates, and an i.i.d. Gaussian\nvector (whose direction falls uniformly on a unit sphere\nin RL). This holds because the distribution of an i.i.d.\nGaussian sample does not get changed after applying an\narbitrary orthogonal transformation. Let H be the span\nof the last L − T coordinate vectors, and h be i.i.d.\nGaussian. Then:\nCT =\n1\nsin(θ)\n=\n√√√√ L∑\ni=1\nh2i /\n√√√√ T∑\ni=1\nh2i . (6)\nUsing the properties of χL, χ2L, and inverse-χ2L distri-\nbutions [20] and Jensen’s inequality, we have an estimate\nof the mean E[CT ] ≈\n√\nL\nT and an upper bound on both\nthe mean and the variance:\nE [CT ] ≤\n√\nL− 2\nT − 2 , (7)\nV ar [CT ] ≤ L− 2\nT − 2 −\nL\nT\n. (8)\nWe describe the analysis in Appendix A. Using these\nbounds in conjunction with the Chebyshev inequality7,\np(|a−E[a]| ≥ kσa) ≤ 1k2 , we have the following result:\nProposition 4: In the Gaussian measurement\nensemble we have: ‖x∗ − xˆM‖2 ≤ C¯kT d(xˆM , HM+T )\n7To improve upon Chebyshev bounds we could directly characterize\nthe cumulative density function of CT – either analytically, or by\nsimple Monte Carlo estimates.\n6\n0 20 40 60 80 100\n−80\n−60\n−40\n−20\n0\n20\nEr\nro\nr (\ndB\n)\n \n \nerror\nerror bound\n0 200 400 600 800 1000\n−60\n−50\n−40\n−30\n−20\n−10\n0\n10\nM\nEr\nro\nr (\ndB\n)\n \n \nerror\nerror bound\nFig. 6. (Top) Error confidence bounds and actual errors for a sparse\nsignal, N = 100, T = 5, K = 10. (Bottom): Error confidence\nbound and actual errors for a signal with power-law decay, N = 1000,\nT = 10.\nwith probability at least 1 − 1k2 , where\nC¯kT =\n√\nL−2\nT−2 + k\n√\nL−2\nT−2 − LT , for any k > 0.\nIn Figure 5 (top) we plot the mean estimate, and our\nbound in (7) for CT and (bottom) the standard deviation\nbound for L = 100 and a range of T . We compare them\nto sample mean and standard deviation of CT based\non 5000 samples. The figure shows that both bounds\nprovide very good approximation for most of the range\nof T > 2, and also that the standard deviation quickly\nfalls off with T , giving tight confidence intervals. In\nFigure 6 we perform numerical experiments with two\nexample signals, a sparse signal, N = 100, K = 10,\nT = 5 (top) and a near-sparse signal with power-law\ndecay, N = 1000, T = 10 (bottom). We use basis pursuit\nto recover the signals as we obtain progressively more\nmeasurements, and we compare our error bounds (via\nChebyshev inequality) to the actual errors. We see that\nthe bounds reliably indicate the reconstruction error –\nafter a small delay of T additional measurements. We\nhave used basis pursuit in the experiments, but we could\nsubstitute any sparse solver instead, for example we\ncould have also computed error estimates for matching\npursuit.\nA. Analysis for More General Ensembles\nTo get the bound in (4) we characterized the distri-\nbution of 1sin(θT ) and used the properties of the Gaus-\nsian measurement ensemble. Analysis of θT for general\nensembles is challenging. We now consider a simpler\nanalysis which provides useful estimates when T << L,\ni.e. the case of main interest for compressed sensing,\nand when the measurement coefficients aij are from\nan i.i.d. zero-mean ensemble. The previous bound for\nthe Gaussian case depended on both M , the number of\nsamples used for the current reconstruction, and T , the\nnumber of extra samples. Now, in the following we give\nestimates and bounds that depend only on T , and in\nthat sense could be weaker for the Gaussian case when\nM is large; they are however more generally applicable\n– in particular we no longer require xˆM to satisfy the\nmeasurements exactly.\nSuppose we have a current reconstruction xˆ, and\nsuppose x∗ is the (unknown) true signal. We now take T\nnew samples yi = a′ix∗, for 1 ≤ i ≤ T . For each of these\nsamples we compute yˆi = a′ixˆM to be the same vector ai\napplied to the current reconstruction. Denote the current\nerror vector by δ = xˆM −x∗, and compute zi = yˆi−yi,\nthe deviations from the actual measurements. Then\nzi = a\n′\niδ, 1 ≤ i ≤ T (9)\nThe new measurements ai are independent of xˆ and\nof x∗, hence of δ. The zi’s are i.i.d. from some (un-\nknown) distribution, which has zero mean and variance\n‖δ‖22 V ar(aij). We can estimate ‖δ‖22 by estimating the\nvariance of the zi’s from the T samples. The quality of\nthe estimate will depend on the exact distribution of aij .\nConsider the case where ai are i.i.d. Gaussian. Then\nzi is Gaussian as well. For simplicity suppose that\nV ar(aij) = 1, then the distribution of zi is i.i.d.\nGaussian with zero-mean and variance ‖δ‖22. Let ZT =∑M+T\ni=M+1 z\n2\ni . Then Z˜T , ZT‖δ‖2 ∼ χ2T , i.e. χ2 random\nvariable with T degrees of freedom. Now to obtain a\nconfidence interval for ‖δ‖22 we use the cumulative χ2T\ndistribution. We pick a confidence level 1−α (for some\nsmall α > 0), and we use the χ2T cumulative distribution\nto find the largest z∗ such that p(Z˜T ≤ z∗) ≤ α.8\nDuring the review process a related analysis in [15]\nwas brought to our attention: the paper considers com-\npressed sensing in a cross-validation scenario, and it\nproposes to estimate the errors in the reconstruction\nfrom a few additional (cross-validation) measurements.\n8 We have that σ2z =\nZT\nz∗\ngives the smallest value of σ2z such that\nprobability of observing ZT is at least α. That is to say, the bound\n‖δ‖2 < ZT\nz∗\nwill hold for at least 1 − α fraction of realizations of\nZT .\n7\n0 0.5 1 1.5 2\n0\n1\n2\n3\n4\nNear−sparse estimates, M small\n \n \n0 0.5 1 1.5 2\n0\n2\n4\n6\nNear−sparse estimates, M large\nEstimated error norm\ne\nm\npi\nric\nal\n h\nist\nog\nra\nm\n \n \nχ2 analysis\nsin(θ) analysis\nχ2 analysis\nsin(θ) analysis\nFig. 7. Comparison of χ2 and sin(θ) analysis. Given a unit-norm\nvector δ, we obtain T additional measurements, and compute our two\nestimates of ‖δ‖. We plot the histogram of the estimates over 5000\ntrials with N = 250, T = 25, and (a) M = 0, (b) M = 200.\nThe paper cleverly uses the Johnson-Lindenstrauss (JL)\nlemma to find out how many random measurements are\nneeded for predicting the error to a desired accuracy. For\nGaussian measurements ensembles our χ2-based analy-\nsis can be seen as a special case (where all the constants\nare computed explicitly since we use the exact sampling\ndistribution of ZT ), but JL lemma also generalizes to\nother ensembles satisfying certain requirements on the\ndecay of the tails [15], [21].\nTo compare our analysis in (5), based on CT , to\nthe one in (9) we note that the latter simply estimates\nthe error ‖δ‖ as ‖ 1√\nT\nA˜δ‖, where A˜ are the new mea-\nsurements9. Now unlike the analysis in (9), in (5) we\nrequire that the solution at step M is feasible (matches\nall the measurements) and instead we compute the error\nof projecting δ onto the null-space of A and adjust it\nby the expected value of 1sin(θ) , i.e. we estimate ‖δ‖\nas\n√\nL\nT ‖A′(AA′)−1Aδ‖, where A includes all M + T\nmeasurements. To compare the quality of the two esti-\nmates we conducted a simulation with N = 250 and\nT = 25, and computed the estimates for random unit-\nnorm vectors δ. We plot the histograms for M = 0 and\nM = 200 over 5000 trials in Figure 7. In the first case\nwith M = 0, we see that both estimates have about the\nsame accuracy (similar error distributions), however as\nM becomes appreciable the approach in (5) becomes\nmore accurate.\n9This is essentially the same estimate as the one based on JL lemma\nin [15], as the expected value of χ2\nT\nis T , hence E[ZT ] = T‖δ‖22.\nVI. NOISY CASE\nNext we consider the sequential version of the noisy\nmeasurement setting, where the observations are cor-\nrupted by additive uncorrelated i.i.d. Gaussian noise with\nvariance σ2n:\nyi = a\n′\nix+ ni, i ∈ {1, ..,M}. (10)\nTo solve this problem one can adapt a variety of sparse\nsolvers which allow inexact solutions xˆM in the se-\nquential setting – for example matching pursuit methods\nwith a fixed number of steps, or the noisy versions\nof basis pursuit. All of these methods have a trade-off\nbetween sparsity of the desired solution and the accuracy\nin representing the measurements. In the case of basis\npursuit denoising a regularization parameter λ balances\nthese two costs:\nxˆM = argmin\n1\n2\n‖y1:M −AMx‖22 + λM‖x‖1. (11)\nFor greedy sparse solvers such as matching pursuit and\nits variants the trade-off is controlled directly by deciding\nhow many columns of A to use to represent y. We are\ninterested in a stopping rule which tells us that xˆ is\nreasonably close to x∗ for any sparse solver and for any\nuser defined choice of the trade-off between sparsity and\nmeasurement likelihood. We do not discuss the question\nof selecting a choice for the trade-off – we refer the\nreaders to [22], [23] and also to [15] for a discussion\nof how this can be done in a cross-validation setting.\nNow, due to the presence of noise, exact agreement will\nnot occur no matter how many samples are taken. We\nconsider a stopping rule similar to the one in Section V.\nIn principle, the analysis in (4) can be extended to the\nnoisy case, but we instead follow the simplified analysis\nin Section V-A.\nWe establish that the reconstruction error can be\nbounded with high probability by obtaining a small\nnumber of additional samples, and seeing how far the\nmeasurements deviate from yˆi = a′ixˆM . With such a\nbound one can stop receiving additional measurements\nonce the change in the solution reaches levels that can\nbe explained due to noise. The deviations zi now include\ncontribution due to noise:\nzi = yˆi − yi = a′i(xˆM − x∗)− ni. (12)\nLet ZT =\n∑\nz2i . Consider the Gaussian measurement\nensemble. Then zi = a′iδ+ni, and Z˜T , ZT‖δ‖2+σ2\nn\n∼ χ2T .\nThe distribution of zi is Gaussian with mean zero and\nvariance ‖δ‖22+σ2n. Now following a similar analysis as\nin previous section we can obtain an estimate of ‖δ‖22+\nσ2n from a sample of ZT , and subtracting σ2n we get an\nestimate of ‖δ‖22.\n8\n0 200 400 600 800 1000\n−30\n−20\n−10\n0\n10\n20\n30\nM\nEr\nro\nr (\ndB\n)\n \n \nerror\nerror bound\nFig. 8. Error estimate in the noisy case: true error and a 90-percent\nconfidence bound (dB scale): N = 1000, T = 10, K = 100.\nWe show an example in Figure 8 where the true error\nappears along with a 90-percent confidence bound. We\nhave N = 1000, K = 100, T = 10 and σn = 0.01. We\nuse basis pursuit denoising (12) as our choice for sparse\nsolver, and we set λM ∝\n√\nM log(N) motivated by the\nuniversal rule for wavelet denoising [22] to account for\nnoise added with additional measurements. The bound\nclearly shows where the sparse signal has been recovered\nup to the noise floor (the signal is sparse with K = 100\nnon-zero elements).\nVII. EFFICIENT SEQUENTIAL SOLUTION\nThe main motivation for the sequential approach is to\nreduce the number of measurements to as few as possi-\nble. Yet, we would also like to keep the computational\ncomplexity of the sequential approach low. We focus\non the `1-based formulations here, and show that there\nis some potential of using ”memory” in the sequential\nsetting for reducing the computational complexity. For\nthe static setting there exists a great variety of approaches\nto solve both noiseless and noisy basis pursuit (i.e.\nbasis pursuit denoising) in various forms, e.g. [23]–\n[25]. However, instead of re-solving the linear program\n(1) after each new sample, we would like to use the\nsolution to the previous problem to guide the current\nproblem. It is known that interior point methods are\nnot well-suited to take advantage of such “warm-starts”\n[23]. Some methods are able to use warm-starts in\nthe context of following the solution path in (11) as\na function of λ [23], [26], [27]. In that context the\nsolution path xˆ(λ) is continuous (nearby values of λ give\nnearby solutions) enabling warm-starts. However, once\na new measurement ai is received, this in general makes\nthe previous solution infeasible, and can dramatically\nchange the optimal solution, making warm-starts more\nchallenging10.\nWe now investigate a linear programming approach\nfor warm-starts using the simplex method to accomplish\nthis in the noiseless case (a similar strategy can be used\nwith the Dantzig decoder [1] for the noisy case). We can\nnot use the solution xˆM directly as a starting point for\nthe new problem at step M + 1, because in general it\nwill not be feasible. In the Gaussian measurement case,\nunless xˆM = x∗, the new constraint a′M+1xˆM = yM+1\nwill be violated. One way to handle this is through a dual\nformulation11, but we instead use an augmented primal\nformulation [29].\nFirst, to model (1) as a linear program we use the stan-\ndard trick: define x+i = max(xi, 0), x\n−\ni = max(−xi, 0),\nand x = x+ − x−. This gives a linear program in\nstandard form:\nmin1′x+ + 1′x− (13)\ny1:M =\n[\nAM −AM ] [ x+\nx\n−\n]\n, and x+,x− ≥ 0\nNext we need to add an extra constraint yM+1 =\na′M+1x\n+ − a′M+1x−. Suppose that a′M+1xˆM > yM+1.\nWe add an extra slack variable z to the linear program,\nand a high positive cost Q on z. This gives the following\nlinear program:\nmin 1′x+ + 1′x− +Qz (14)\ny1:M =\n[\nAM −AM ] [ x+\nx\n−\n]\n, and x+,x− ≥ 0\nyM+1 = a\n′\nM+1x\n+ − a′M+1x− − z, and z ≥ 0\nNow using xˆM and z = a′M+1(xˆM )+ −\na′M+1(xˆ\nM )−− yM+1 yields a basic feasible solution to\nthis augmented problem. By selecting Q large enough,12\nz will be removed from the optimal basis (i.e. z is set to\n0), and the solutions to this problem and the (M +1)-th\nsequential problem are the same.\nWe test the approach on an example with N = 200,\nK = 10, and 100 trials. In Figure 9 we plot the number\nof iterations of the simplex method required to solve the\nproblem (1) at step M from scratch (LP1) and using\nthe formulation in (14) (LP2). To solve (13) we first\nhave to find a basic feasible solution, BFS, (phase 1)\nand then move from it to the optimal BFS. An important\nadvantage of (14) is that we start right away with a BFS,\n10In related work, [28] proposed to use Row-action methods for\ncompressed sensing, which rely on a quadratic programming for-\nmulation equivalent to (1) and can take advantage of sequential\nmeasurements.\n11If at step M the optimal dual solution is p, then a feasible solution\nat step M+1 is [p; 0]. However, it may not be a basic feasible solution.\n12E.g. the big-M approach [29] suggests treating Q as an undeter-\nmined value, and assumes that Q dominates when compared to any\nother value.\n9\n0 5 10 15 20 25 30\n0\n50\n100\nM\nn\nu\nm\n it\ner\n.\n \n \nLP1\nLP2\nFig. 9. A comparison of the number of simplex iterations when\nsolving (1) from scratch (LP1) and using the solution at step M − 1\n(LP2). We plot the average number of iterations vs. M , over 100 trials.\nso phase 1 is not required. The figure illustrates that for\nlarge M the approach LP2 is significantly faster.\nWe note that recently a very appealing approach\nfor sequential solution in the noisy setting has been\nproposed based on the homotopy continuation idea [30],\n[31], where a homotopy (a continuous transition) is\nconstructed from the problem at step M to the problem\nat step M+1 and the piecewise-smooth path is followed.\nThe efficiency of the approach depends on the number\nof break-points in this piecewise-smooth path, but the\nsimulations results in the papers are very promising. We\nalso note that [30] proposes an approach to select the\ntrade-off in the noisy case, using cross-validation ideas.\nVIII. CONCLUSION AND DISCUSSION\nThis paper presents a formulation for compressed\nsensing in which the decoder receives samples sequen-\ntially, and can perform computations in between samples.\nWe showed how the decoder can estimate the error in\nthe current reconstruction; this enables stopping once the\nerror is within a required tolerance. Our results hold for\nany decoding algorithm, since they only depend on the\ndistribution of the measurement vectors. This enables\n“run-time” performance guarantees in situations where a-\npriori guarantees may not be available, e.g. if the sparsity\nlevel of the signal is not known, or for recovery methods\nfor which such guarantees have not been established.\nWe have studied a number of scenarios including\nnoiseless, noisy, sparse and near sparse, and involving\nGaussian and Bernoulli measurements, and demonstrated\nthat the sequential approach is practical, flexible and\nhas wide applicability. A very interesting problem is to\nboth extend the results to other measurement ensembles,\ne.g. for sparse ensembles, and moreover, to go beyond\nresults for particular ensembles and develop a general\ntheory of sequential compressed sensing. Furthermore, in\nmany important applications the sparse signal of interest\nmay also be evolving with time during the measurement\nprocess. Sequential CS with a notion of ’time of a\nmeasurement’ is a natural candidate setting in which to\nexplore this important extension to the CS literature.\nWe also remark that there is a closely related problem\nof recovering low-rank matrices from a small number\nof random measurements [32], [33], where instead of\nsearching for sparse signals one looks for matrices with\nlow-rank. This problem admits a convex ’nuclear-norm’\nrelaxation (much akin to `1 relaxation of sparsity). Some\nof our results can be directly extended to this setting\n– for example if in the Gaussian measurement case\nwith no noise there is one-step agreement, then the\nrecovered low-rank matrix is the true low-rank solution\nwith probability one.\nFinally we comment on an important question [6], [34]\nof whether it is possible to do better than simply using\nrandom measurements – using e.g. experiment design or\nactive learning techniques. In [6] the authors propose\nto find a multivariate Gaussian approximation to the\nposterior p(x |y) where p(y | x) ∝ exp( 1σ2 ‖y−Ax‖2),\nand p(x) ∝ exp(−λ‖x‖1). Note that MAP estimation in\nthis model xˆ = argmaxx p(x | y) is equivalent to the\nformulation in (11), but does not provide uncertainties.\nUsing the Bayesian formalism it is possible to do ex-\nperiment design, i.e. to select the next measurement to\nmaximally reduce the expected uncertainty. This is a very\nexciting development, and although much more complex\nthan the sequential approach presented here, may reduce\nthe number of required samples even further.\nAPPENDIX A\nDERIVATION OF THE DISTRIBUTION FOR 1sin θ\nConsider E[sin2(θ)] = E[\n(∑T\ni=1 h\n2\ni\n)\n/‖h‖22]. Since∑\ni E[\nh2\ni\n‖h‖2\n2\n] = 1, and each hi is i.i.d., we have\nE[\nh2\ni\n‖h‖2\n2\n] = 1L . In fact E[\nh2\ni\n‖h‖2\n2\n] follows a Dirichlet\ndistribution. Therefore, E[sin2(θ)] = TL .\nUsing Jensen’s inequality with the convex function√\n1/x, x > 0, we have E[1/ sin(θ)] ≥\n√\nL\nT .\nNow, E[ 1sin2(θ) ] =\nL−2\nT−2 (for T > 2). This is true\nbecause E[ 1\nsin2(θ)\n] = E\n(∑L\ni=1 h\n2\ni\n)\n/\n(∑T\ni=1 h\n2\ni\n)\n=\n1 + E\n(∑L\ni=T+1 h\n2\ni /\n∑T\ni=1 h\n2\ni\n)\n= 1 + (L − T ) 1T−2 .\nThe second term is a product of a χ2 random variable\nwith (L − T ) degrees of freedom and an independent\ninverse-χ2 distribution with T degrees of freedom:\nE[\n∑L\ni=T+1 h\n2\ni ] = L− T , and E[ 1(∑Ti=1 h2i ) ] =\n1\nT−2 , see\n[20]. Now 1 + (L − T )/(T − 2) = (L− 2)/(T − 2).\nFinally, using Jensen’s inequality with the concave\nfunction\n√\nx, E[ 1sin(θ) ] ≤\n√\nT−2\nL−2 .\n10\nREFERENCES\n[1] E. J. Candes, “Compressive sampling,” in Proc. Int. Congress of\nMath., 2006, Madrid, Spain.\n[2] D. Donoho, “Compressed sensing,” IEEE Trans. on Information\nTheory, vol. 52, no. 4, pp. 1289–1306, Apr. 2006.\n[3] M. Lustig, D. L. Donoho, and J. M. Pauly, “Sparse MRI:\nThe application of compressed sensing for rapid MR imaging,”\nMagnetic Resonance in Medicine, vol. 58, no. 6, pp. 1182–1195,\nDec. 2007.\n[4] M. F. Duarte, M. A. Davenport, D. Takhar, J. N. Laska, T. Sun,\nK. F. Kelly, and R. Baraniuk, “Single pixel imaging via com-\npressive sampling,” IEEE Signal Processing Magazine, vol. 25,\nno. 2, pp. 83–91, Mar. 2008.\n[5] W. Bajwa, J. Haupt, A. Sayeed, and R. Nowak, “Compressive\nwireless sensing,” in Int. Conf. on Information Processing in\nSensor Networks (IPSN), April 2006.\n[6] F. Steinke, M. Seeger, and K. Tsuda, “Experimental design for\nefficient identification of gene regulatory networks using sparse\nBayesian models,” BMC Systems Biology, vol. 1, no. 51, 2007.\n[7] S. S. Chen, D. L. Donoho, and M. A. Saunders, “Atomic\ndecomposition by basis pursuit,” SIAM J. Scientific Computing,\nvol. 20, no. 1, pp. 33–61, 1998.\n[8] J. Tropp, “Greed is good: Algorithmic results for sparse approx-\nimation,” IEEE Trans. Info. Theory, vol. 50, no. 10, pp. 2231–\n2242, Oct. 2004.\n[9] R. Chartrand and W. Yin, “Iteratively reweighted algorithms for\ncompressive sensing,” in ICASSP, 2008.\n[10] M. Cetin, D. M. Malioutov, and A. S. Willsky, “A variational\ntechnique for source localization based on a sparse signal recon-\nstruction perspective,” in ICASSP, 2002.\n[11] E. J. Candes, M. Wakin, and S. Boyd, “Enhancing sparsity by\nreweighted l1 minimization,” 2007, technical report, California\nInstitute of Technology.\n[12] E. Cands, J. Romberg, and T. Tao, “Robust uncertainty principles:\nExact signal reconstruction from highly incomplete frequency\ninformation,” IEEE Trans. on Information Theory, vol. 52, no. 2,\npp. 489–509, Feb. 2006.\n[13] D. Wipf and B. D. Rao, “Sparse bayesian learning for basis\nselection,” IEEE Transactions on Signal Processing, vol. 52,\nno. 8, 2004.\n[14] M. Rudelson and R. Vershynin, “Sparse reconstruction by convex\nrelaxation: Fourier and Gaussian measurements,” in CISS 2006,\n2006.\n[15] R. Ward, “Compressed sensing with cross validation,” to appear\nin IEEE Transactions on Information Theory, 2009.\n[16] D. M. Malioutov, S. R. Sanghavi, and A. S. Willsky, “Com-\npressed sensing with sequential observations,” in ICASSP, 2008.\n[17] R. Baraniuk, “Compressive sensing,” IEEE Signal Processing\nMagazine, vol. 24, no. 4, pp. 118–121, Jul. 2007.\n[18] D. L. Donoho and X. Huo, “Uncertainty principles and ideal\natomic decomposition,” IEEE Trans. on Information Theory,\nvol. 47, no. 7, pp. 2845–2862, Nov. 2001.\n[19] T. Tao and V. Vu, “On the singularity probability of random\nBernoulli matrices,” Journal Amer. Math. Soc., vol. 20, pp. 603–\n628, 2007.\n[20] S. Kotz, N. Balakrishnan, and N. L. Johnson, Continuous Multi-\nvariate Distributions. Wiley and Sons, 2000.\n[21] S. Dasgupta and A. Gupta, “An elementary proof of a theorem of\nJohnson and Lindenstrauss,” Random Structures and Algorithms,\n22(1):60-65, vol. 22, no. 1, pp. 60–65, 2003.\n[22] I. M. J. D. L. Donoho, “Ideal spatial adaptation by wavelet\nshrinkage,” Biometrika, vol. 81, no. 3, pp. 425–455, 1994.\n[23] M. Figueiredo, R. Nowak, and S. Wright, “Gradient projection\nfor sparse reconstruction: application to compressed sensing and\nother inverse problems,” IEEE Journal of Selected Topics in\nSignal Processing, vol. 1, no. 4, pp. 586–598, 2007.\n[24] S. J. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky,\n“A method for large-scale l1-regularized least squares,” IEEE\nJournal on Selected Topics in Signal Processing, vol. 4, no. 1,\npp. 606–617, Dec. 2007.\n[25] I. Daubechies, M. De Friese, and C. De Mol, “An iterative\nthresholding algorithm for linear inverse problems with a sparsity\nconstraint,” Comm. in Pure and Applied Math., vol. 57, pp. 1413–\n1457, 2004.\n[26] M. R. Osborne, B. Presnell, and B. A. Turlach, “A new approach\nto variable selection in least squares problems,” IMA Journal of\nNumerical Analysis, vol. 20, no. 3, pp. 389–403, 2000.\n[27] D. M. Malioutov, M. Cetin, and A. S. Willsky, “Homotopy\ncontinuation for sparse signal representation,” in ICASSP, 2005.\n[28] S. Sra and J. A. Tropp, “Row-action methods for compressed\nsensing,” in ICASSP, vol. 3, 2006, pp. 868–871.\n[29] D. Bertsimas and J. N. Tsitsiklis, Introduction to linear optimiza-\ntion. Athena Scientific, 1997.\n[30] P. J. Garrigues and L. El Ghaoui, “An homotopy algorithm\nfor the Lasso with online observations,” in Neural Information\nProcessing Systems (NIPS), Dec. 2008.\n[31] M. S. Asif and J. Romberg, “Streaming measurements in com-\npressive sensing: L1 filtering,” in Proc. of 42nd Asilomar Con-\nference on Signals, Systems and Computers, Oct. 2008.\n[32] B. Recht, M. Fazel, and P. A. Parrilo, “Guaranteed minimum\nrank solutions to linear matrix equations via nuclear norm mini-\nmization,” submitted to SIAM Review, 2007.\n[33] R. Keshavan, A. Montanari, and S. Oh, “Learning low rank\nmatrices from O(n) entries,” in Allerton Conference., Oct. 2008.\n[34] Y. Weiss, H. S. Chang, and W. T. Freeman, “Learning compressed\nsensing,” in Allerton Conference, Sep. 2007.\n11\n",
            "id": 687974,
            "identifiers": [
                {
                    "identifier": "4425864",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:dspace.mit.edu:1721.1/61416",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "oai:arxiv.org:1003.0219",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "2109339",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1109/jstsp.2009.2038211",
                    "type": "DOI"
                },
                {
                    "identifier": "1003.0219",
                    "type": "ARXIV_ID"
                }
            ],
            "title": "Sequential Compressed Sensing",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:arxiv.org:1003.0219",
                "oai:dspace.mit.edu:1721.1/61416"
            ],
            "publishedDate": "2009-10-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1003.0219"
            ],
            "updatedDate": "2022-01-03T00:02:25",
            "yearPublished": 2009,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "1932-4553"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1003.0219"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/687974"
                }
            ]
        },
        {
            "acceptedDate": "2011-09-12T00:00:00",
            "arxivId": "1102.4423",
            "authors": [
                {
                    "name": "Biely, Martin"
                },
                {
                    "name": "Robinson, Peter"
                },
                {
                    "name": "Schmid, Ulrich"
                }
            ],
            "contributors": [
                "Martin"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/193011844"
            ],
            "createdDate": "2012-04-13T14:18:40",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2011-05-01T00:00:00",
            "abstract": "In this paper we consider the k-set agreement problem in distributed\nmessage-passing systems using a round-based approach: Both synchrony of\ncommunication and failures are captured just by means of the messages that\narrive within a round, resulting in round-by-round communication graphs that\ncan be characterized by simple communication predicates. We introduce the weak\ncommunication predicate PSources(k) and show that it is tight for k-set\nagreement, in the following sense: We (i) prove that there is no algorithm for\nsolving (k-1)-set agreement in systems characterized by PSources(k), and (ii)\npresent a novel distributed algorithm that achieves k-set agreement in runs\nwhere PSources(k) holds. Our algorithm uses local approximations of the stable\nskeleton graph, which reflects the underlying perpetual synchrony of a run. We\nprove that this approximation is correct in all runs, regardless of the\ncommunication predicate, and show that graph-theoretic properties of the stable\nskeleton graph can be used to solve k-set agreement if PSources(k) holds.Comment: to appear in 16th IEEE Workshop on Dependable Parallel, Distributed\n  and Network-Centric System",
            "documentType": "research",
            "doi": "10.1109/ipdps.2011.301",
            "downloadUrl": "http://arxiv.org/abs/1102.4423",
            "fieldOfStudy": "computer science",
            "fullText": "ar\nX\niv\n:1\n10\n2.\n44\n23\nv1\n  [\ncs\n.D\nC]\n  2\n2 F\neb\n 20\n11\nSolving k-Set Agreement with\nStable Skeleton Graphs\nMartin Biely∗, Peter Robinson‡, and Ulrich Schmid†\n∗ EPFL, Switzerland, biely@ecs.tuwien.ac.at\n† ECS Group, Technische Universita¨t Wien, Austria, s@ecs.tuwien.ac.at\n‡ Division of Mathematical Sciences, Nanyang Technological University, Singapore, peter.robinson@ntu.edu.sg\nAbstract—In this paper1 we consider the k-set agreement\nproblem in distributed message-passing systems using a round-\nbased approach: Both synchrony of communication and failures\nare captured just by means of the messages that arrive within\na round, resulting in round-by-round communication graphs\nthat can be characterized by simple communication predicates.\nWe introduce the weak communication predicate Psrcs(k) and\nshow that it is tight for k-set agreement, in the following sense:\nWe (i) prove that there is no algorithm for solving (k−1)-set\nagreement in systems characterized byPsrcs(k), and (ii) present a\nnovel distributed algorithm that achieves k-set agreement in runs\nwherePsrcs(k) holds. Our algorithm uses local approximations of\nthe stable skeleton graph, which reflects the underlying perpetual\nsynchrony of a run. We prove that this approximation is correct\nin all runs, regardless of the communication predicate, and show\nthat graph-theoretic properties of the stable skeleton graph can\nbe used to solve k-set agreement if Psrcs(k) holds.\nI. INTRODUCTION\nThe quest of finding minimal synchrony requirements for\ncircumventing the impossibility of distributed agreement prob-\nlems like consensus [9] has always been a very active research\ntopic in distributed computing. Since the exact solvability\nborder of consensus has been researched exhaustively, see e.g.,\n[2], [6], [12], the attention has shifted to weaker agreement\nproblems, in particular, k-set agreement [1], [11], [14], which\nallows the processes in a distributed system to agree on at most\nk different values. For k > 1, the problem itself is possibly\nnot as interesting as consensus (k = 1) from a practical point\nof view, except for partitionable systems that need to reach\nconsensus in every partition. In any case, k-set agreement is\nhighly relevant from a theoretical perspective, as it allows to\nstudy what level of agreement can be achieved in a fault-\ntolerant distributed system. This question is definitely relevant\nin practice, e.g., for name-space reduction (renaming) and\nsimilar problems.\nOne way to model synchrony requirements is through the\nuse of round models. Round-based distributed algorithms\nexecute in a sequence of communication-closed rounds, which\nconsist of message exchanges and processing steps. The\nclassic partially synchronous models of Dwork et. al. [7]\nwere probably the first to allow some messages not to arrive\n1Peter Robinson has been supported by the Austrian Science\nFoundation (FWF) project P20529 and Nanyang Technological\nUniversity grant M58110000.\nwithin a round due to asynchrony (i.e., non-timeliness), rather\nthan solely due to failures. The seminal work by Santoro\nand Widmayer [15], [16] unified the treatment of asynchrony\nand failures by considering synchronous processes that only\nsuffer from “end-to-end communication failures”. This idea\nalso underlies the Round-by-Round failure detector (RRFD)\napproach by Gafni [10], which assumes a local RRFD that tells\nwhether a process shall wait for a round message from some\nother process or not. The actual reason why a receiver process\ndoes not get a message from the sender process is considered\nirrelevant here. The Heard-Of (HO) model [3], [4] integrates\nthis unified treatment of failures and asynchrony of [15], [16]\nwith a flexible way of describing guarantees about commu-\nnication. The basic entity of this model are communication-\nclosed rounds and HO predicates, which specify conditions on\nthe collection of heard-of sets: For each round r and process\np, HO(p, r) denotes the set of processes that p hears of (i.e.,\nreceives a message from) in round r.\nIn this paper, we will use properties of communication\ngraphs for studying k-set agreement in message passing\nsystems with very weak synchrony requirements. In k-set\nagreement, correct processes must output a single value based\non values proposed locally, with no more than k different\nvalues being output system-wide.\nDetailed contributions: We introduce an algorithm for\nk-set agreement, which exploits a natural correspondence be-\ntween communication predicates and round-by-round “timely\ncommunication” graphs Gr in a run; Gr contains an edge\n(q → p) when process p hears of q in round r. Our\nalgorithm incorporates a generic method for approximating\nthe stable skeleton G∩∞, which is the intersection of all Gr\nand reflects the underlying perpetual synchrony of a run. We\nalso introduce the class of communication predicates Psrcs(k),\nwhich guarantees that at least two processes in every subset of\nk+1 processes hear from a common process, in every round.\nUsing the graph-theoretic properties of G∩∞ guaranteed by\nthe predicate Psrcs(k), we show that our algorithm solves k-\nset agreement in all runs where Psrcs(k) holds. Moreover, we\nalso show that Psrcs(k) is “tight” for k-set agreement, as it is\ntoo weak for solving k − 1-set agreement.\nII. COMPUTING MODEL AND PROBLEM DEFINITION\nWe consider distributed computations of a set of processes\nΠ communicating by message passing. Moreover, we consider\nthat the computation is organized in an infinite sequence of\ncommunication-closed [8] rounds; that is, any message sent in\na round can be received only in that round. As in the models\nof Gafni[10] and Charron-Bost and Schiper [4], we will\nexpress assumptions about the synchrony and the reliability of\ncommunication in a system by a predicate that characterizes\nthe set of edges in the communication graph of each round.\nIntuitively speaking, there is an edge from process p to q in\nthe communication graph of round r is q received p’s round r\nmessage. We will in fact name a system by its predicate, that\nis, in a system P the collections of communication graphs\nof each run of an algorithm in that system will must fulfill\npredicate P .\nWe now formally define computations in our round model.\nAs in the aforementioned models, an algorithm is composed\nof two functions: The sending function determines, for each\nprocess p and round r > 0, the message p broadcasts in round\nr based on the p’s state at the beginning of round r. The\ntransition function determines, for each p and round r and the\nvector of messages received in r, the state at the end of round\nr, i.e., at the beginning of round r + 1. Clearly, a run of an\nalgorithm is completely determined by the initial states of the\nprocesses and the sequence of communication graphs.\nFor each round r, we denote the communication graph by\nGr = 〈V,Er〉, where each node of the set V is associated\nwith one process from Π, and where Er is the set of directed\ntimely edges for round r. There is an edge from p to q, denoted\nas (p → q), if and only if q receives p’s round r message\n(in round r).2 To simplify the presentation, we will denote a\nprocess and the associated node in the communication graph\nby the same symbols. However, as we differentiate between V\nand Π, we will always be able to resolve possible ambiguities\nby stating from which set a node or process is taken. We will\nwrite p ∈ Gr and (p → q) ∈ Gr instead of p ∈ V resp.\n(p→ q) ∈ Er.\nWe are primarily interested in the round r skeleton G∩ r of\nGr, which we define as the subgraph consisting of the edges\nthat have been timely in all rounds up to round r. Formally,\nG∩ r := 〈V,E∩ r〉 where E∩ r :=\n⋂\n0<r′6r E\nr\n. The crucial\nproperty of E∩ r is that once an edge is untimely in some\nround r, it cannot be in G∩ r′ , for any r′ > r. That is, ∀r >\n0: E∩ r ⊇ E∩ r+1, which implies the subgraph relation\n∀r > 0: G∩ r ⊇ G∩ r+1. (1)\nWe are particularly interested in the stable skeleton of a run,\nwhich we define as the intersection3 over all rounds, i.e.,\nG∩∞ :=\n⋂\nr∈N+ G\n∩ r\n. (2)\nConsidering that a run α consists of infinitely many rounds,\nwhereas our system consists of only a finite number of\n2Since we consider communication-closed rounds, a message sent in round\nr cannot be received in any later round.\n3For simplicity, we set G ∩G′ := 〈V ∩ V ′, E ∩E′〉.\nprocesses, it follows that the number of possible distinct stable\nskeletons must also be finite. Consequently, the subgraph\nproperty (1) implies that there is some round rST when G∩∞\nhas stabilized, i.e., ∀r > rST : G∩ r = G∩∞.\nAs mentioned in the introduction, our algorithm will solve\nk-set agreement by approximating the stable skeleton of a\nrun. The first step in this effort is to use the locally avail-\nable information about the communication graph, which is\ncaptured by the notion of timely neighbourhoods. The timely\nneighborhood of p, denoted as PT (p, r), is the set of processes\nthat process p has perceived as perpetually timely until round\nr. In other words, p has received a message from every\nprocess in PT (p, r) in every round up to and including r,\ni.e., PT (p, r) := {q | (q → p) ∈ G∩ r} . Analogously to (1)\nand (2), we have\nPT (p, r) ⊇ PT (p, r + 1) (3)\nand define\nPT (p) :=\n⋂\nr>0\nPT (p, r). (4)\nWe will make heavy use of the standard graph-theoretic\nnotion of a strongly connected component of G∩ r. Note that\nwe implicitly assume that strongly connected components\nare always nonempty and maximal. We use the superscript\nnotation Cr when talking about a strongly connected compo-\nnent of G∩ r. Moreover, we write Crp to denote the (unique)\nstrongly connected component of G∩ r that contains process p\nin round r. The strongly connected component C∞p ⊆ G∩∞\nthat contains p in a run is defined analogously to (2) as\nC∞p :=\n⋂\nr>0\nCrp .\nNote that when p and q are strongly connected in G∩ r, then\nthey are also strongly connected in all G∩ r′ , for 0 < r′ 6 r.\nFrom property (1) of G∩ r, we immediately have\n∀r > 0: Crp ⊇ C\nr+1\np . (5)\nWe will also use directed paths in G∩ r, where we assume that\nall nodes on a path are distinct.\nLet Cr ⊆ G∩ r be a strongly connected component. If Cr\nhas no incoming edges from any q ∈ G∩ r \\ Cr, we say Cr is\na root component in round r. Formally,\n∀p ∈ Cr ∀q ∈ G∩ r : (q → p) ∈ G∩ r ⇒ q ∈ Cr.\nFigure 1b shows a graph with 2 root components {p3, p4, p5}\nand {p1, p2}.\nRegarding the relation to the existing round-by-round mod-\nels, we shortly recall what their predicates are based on: In\nthe Heard-Of model [4], for each round r and each process p,\nthe set HO(p, r) contains those processes that p hears from,\ni.e., receives a message from, in round r. In the case of the\nRound-by-Round Fault Detectors [10], the output of p’s fault\ndetector in round r is referred to by D(p, r). In each round r,\nprocess p waits until it receives a message from every process\nthat is not contained in D(p, r). While it is possible that p also\nreceives a round r message from a process in D(p, r), we will\nconsider that this is never the case. From this it is evident that\nwe have the following correspondence between our skeleton\ngraphs and the HO/RbR model:\n(p→ q) ∈ E∩ r ⇐⇒\n{\n∀r′ 6 r : p ∈ HO(q, r′)\n∀r′ 6 r : p 6∈ D(q, r′)\n(6)\nThus a process can determine its timely neighbourhood in\nthe two models as follows:\nPT (p, r) =\n{⋂\n0<r′6rHO(p, r\n′)\nΠ \\\n(⋃\n0<r′6rD(p, r\n′)\n) (7)\nAs in the HO-model, we model a crashed processes by\nan “internally correct” process that no other process receives\nmessages from after it has crashed [4, Sec. 2.2]. This mod-\nelling allows us to require that all processes decide. For a\nmore detailed discussion on the relation between models where\ncrashed processes actually stop and the HO-model, we refer\nto [13].\nA. k-Set Agreement\nThe k-set agreement problem was introduced in [5]. Every\nprocess p starts with a proposal value v and must eventually\nand irrevocably decide on some value adhering to the follow-\ning three constraints:\nk-Agreement: Processes must decide on at most k different\nvalues.\nValidity: If a process decides on v, then v was pro-\nposed by some process.\nTermination: Every process must eventually decide.\nNote that the k-set agreement problem was shown to be im-\npossible in the asynchronous system model (see [1], [11], [14])\nif f > k processes can crash. Recalling the correspondence\nbetween crashed processes and process that no one hears of,\nit is not surprising that this impossibility also holds for the\nsystem Ptrue :: TRUE, where all runs are admissible.\nIII. A TIGHT COMMUNICATION PREDICATE FOR k-SET\nAGREEMENT\nIn this section, we introduce a predicate that, together with\nAlgorithm 1 in Section IV, is sufficient for solving k-set\nagreement.\nFor a run α, predicate Psrcs(k) requires that in every set S\nof k + 1 processes, there are two processes q, q′ that receive\ntimely messages from the same common process p, in every\nround. We say that p is a 2-source and q, q′ are timely receivers\nof p in α.\nPsrc(p, S) :: ∃q, q\n′ ∈ S, q 6= q′ : p ∈ (PT (q) ∩ PT (q′))\nPsrcs(k) :: ∀S, |S| = k + 1 ∃p ∈ Π: Psrc(p, S) (8)\nNote that p is not required to be distinct from q and q′:\nPsrcs(k) still holds if p = q, i.e., p always perceives itself\nin a timely fashion. Regarding communication graphs, this\npredicate ensures that any induced sub-graph S of G∩∞ with\nk + 1 nodes contains distinct nodes q and q′, such that, for\nsome node p, edges (p→ q) and (p→ q′) exist (one of which\nmay be a self-loop). Figure 1b shows the stable skeleton graph\nin a run where Psrcs(k) holds for k = 3.\nAt a first glance, it might appear that the perpetual nature\nof Psrcs(k) is an unnecessarily strong restriction. To see why\nsome (possibly weak) perpetual synchrony is necessary, con-\nsider the predicate ♦Psrcs(k) that satisfies (8) just eventually,\nand suppose that there is an algorithm A that solves k-\nset agreement in system ♦Psrcs(k). Due to its “eventual”\nnature, ♦Psrcs(k) allows runs where every process forms a root\ncomponent by itself, i.e., hears from no other process, for a\nfinite number of rounds. Moreover, for any k, the (infinite) run,\nwhere a single process forms a root component forever and\nthus has to decide on its own input value, is admissible. Using\na simple indistinguishability argument, it is easy to show that\nprocesses decide on n different values.\nThe following result will be instrumental in Section IV,\nwhere we show how to solve k-set agreement with Psrcs(k).\nNote that Theorem 1 is independent of the algorithm em-\nployed.\nTheorem 1: There are at most k root components in any\nrun that is admissible in system Psrcs(k).\nProof: Assume by contradiction that there is a run α of\nsome algorithm A that is admissible in system Psrcs(k), where\nthere is a set of ℓ > k + 1 disjoint root components R ={\nC∞p1 , . . . , C\n∞\npℓ\n}\ncontaining processes p1, . . . , pk+1, . . . , pℓ. Let\nr be the round where every strongly connected root component\nC∞pi ∈ R has stabilized, i.e., ∀i : C\nr\npi\n= C∞pi . That is, any two\ndistinct root components in R must already be disjoint from\nround r on. Since α satisfies Psrcs(k) and ℓ > k+1, there must\nbe a 2-source p such that, for two distinct processes pi, pj ∈\n{p1, . . . , pk+1}, it holds that p ∈ (PT (pi) ∩ PT (pj)) . By (6),\nit follows that the edges ei = (p→ pi) and ej = (p→ pj) are\nin G∩ r. Considering that Crpi and C\nr\npj\nare root components by\nassumption, i.e., do not have incoming edges, it must be that\nei ∈ Crpi and ej ∈ C\nr\npj\n, and therefore p ∈ Crpi ∩ C\nr\npj\n. This,\nhowever, contradicts the fact that Crpi and C\nr\npj\nare disjoint,\nwhich completes our proof.\nA. Impossibility of (k−1)-Set Agreement\nWe will now show that Psrcs(k) does not allow to solve\n(k−1)-set agreement. More specifically, we will prove this\nby assuming the existence of such an algorithm A, and then\nconstruct a run fulfilling Psrcs(k) where processes decide on\nk (instead of k − 1) different values.\nTheorem 2: Consider any k such that 1 < k < n. There\nis no algorithm A that solves (k−1)-set agreement in system\nPsrcs(k).\nProof: Assume for the sake of a contradiction that such\nan algorithm A exists. Suppose that all processes start with\npairwise distinct input values. Consider the run α and a fixed\nset L of k − 1 processes that only hear from themselves,\nformally speaking, ∀p ∈ L : PT (p) = {p} . Moreover, there\nis one process s such that every process not in L only hears\nfrom itself and s, i.e.,\n∀p ∈ Π \\ L : PT (p) = {p, s} .\nSince, by validity and termination, processes eventually have\nto decide on some input value and processes in L∪{s} cannot\nlearn any other process’ input value, they have to decide on\ntheir own value. Thus, we have k different decision values, as\nwe have assumed a unique input value for each process, and\ntherefore a violation of (k−1)-agreement.\nWhat remains to be shown is that this run α actually fulfills\nPsrcs(k). Recall equation (8), i.e., the definition of Psrcs, and\nconsider for any set S of size k+ 1 the set P = S \\L. Since\n|S \\L| > 2, the set P contains at least two distinct processes\nthat permanently hear from s (one of which may be s). That\nis, process s is the required 2-source for any set S of k + 1\nprocesses.\nIV. APPROXIMATING THE STABLE SKELETON GRAPH AND\nSOLVING k-SET AGREEMENT\nIn this section, we present and analyze an algorithm that\nsolves k-set agreement with predicate Psrcs(k). Algorithm 1\nemploys a generic approximation of the stable skeleton graph\nof the run, which works as follows:\nFirst, every process p keeps track of the processes it has\nperceived as timely until round r in the set PTp, updated in\nLine 9. Lemma 3 will show that PTp satisfies the definition of\nPT (p, r), for all rounds r. In addition, every process p locally\nmaintains an approximation graph Gp of the stable skeleton,\ndenoted Grp for round r, which is broadcast in every round.\nIf a process q receives such a graph Grp from some process p\nin its timely neighborhood PT (q, r), it adds the information\ncontained in Grp to its own local approximation Grq . Note that,\nin contrast to the stable skeleton graph G∩ r, the approximation\ngraph Gp is actually a weighted directed graph. The edge\nlabels of Gp correspond to the round number when a particular\nedge was added by some process, i.e., the edge (q′ r→ q) is in\nGp if, and only if, q′ ∈ PT (q, r) (cf. Lemma 3(b)). To prevent\noutdated information from remaining in the approximation\ngraph permanently, every process p purges all edges in Grp that\nwere initially added more than n−1 rounds ago. Figures 1c-1h\nshow this approximation mechanism at work.\nFor k-set agreement, process p only considers proposal\nvalues for its estimated decision value xp that were sent by\nprocesses in its current timely neighborhood, i.e., in PTp. This\nensures that p and q will have a common estimated decision\nvalue xp = xq in round n, if they are in the same strongly\nconnected component (cf. Lemma 14). To determine when to\nterminate, p analyzes its approximation graph in every round\nr > n and decides if Grp is a strongly connected graph.\nWhy is this decision safe with respect to the agreement\nproperty? Using our graph approximation results, we will show\nin Lemma 15 that any strongly connected approximation graph\ncontains at least one root component in the stable skeleton\ngraph. Furthermore, if two processes decide on different\nAlgorithm 1 Approximating the stable skeleton graph and\nsolving k-set agreement with Psrcs(k)\nVariables and Initialization:\n1: PTp ∈ 2\nΠ initially Π\n2: xp ∈ N initially vp // Estimated decision value\n3: Gp := 〈Vp, Ep〉 initially 〈{p} , ∅〉 // weighted digraph\n4: decidedp ∈ {0, 1} initially 0 // is 1 iff p has decided\nRound r: sending function Srp:\n5: if decidedp = 1 then\n6: send (decide, xp, Gp) to all processes\n7: else\n8: send (prop, xp, Gp) to all processes\nRound r: transition function T rp :\n9: update PTp\n10: if received (decide, xq, ) from q ∈ PTp and decidedp = 0\nthen\n11: xp ← xq\n12: decide on xp\n13: decidedp ← 1\n14: // Approximate stable skeleton graph:\n15: Gp ← 〈{p} , ∅〉\n16: for q ∈ PTp do\n17: add directed edge (q r→ p) to Ep\n18: Vp ← Vp ∪ Vq\n19: for every pair of nodes (pi, pj) ∈ Vp × Vp do\n20: Ri,j ← {re | ∃q ∈ PTp : (pi\nre→ pj) ∈ Eq}\n21: if Ri,j 6= ∅ then\n22: rmax ← max(Ri,j)\n23: Ep ← Ep ∪ {(pi\nrmax→ pj)}\n24: discard all (pi\nre→ pj) from Ep where re 6 r − n\n25: discard pi 6= p from Vp if p is unreachable from pi in Gp\n26: if decidedp = 0 then\n27: xp ← min {xq | q ∈ PTp}\n28: if r > n and Gp is strongly connected then\n29: decide on xp\n30: decidedp ← 1\nvalues, it follows that their approximated graphs in the rounds\nof their respective decision are disjoint. Since Theorem 1\nconfirms that there are at most k root components in any run\nwhere Psrcs(k) holds, there can be in fact at most k different\ndecision values.\nA. Approximation of the Stable Skeleton Graph\nThroughout our analysis, we denote the value of variable\nvar of process p at the end of round r as varrp. When we use\nthe subgraph relation (⊆) between graphs Crp and Grp, we mean\nthe standard subgraph relation between Crp and the unweighted\nversion of Grp. We first state some obvious facts that follow\ndirectly from the code of the algorithm:\nObservation 1: For any round r > 0 it holds that p ∈ Grp\nand that no edge (q′ s→ q) ∈ Grp has s 6 r − n.\nNote that, after the initial assignment, p only updates\nvariable PTp in Line 9, which is equivalent to (7). From\nthis and the inspection of Lines 15 and 17, Lemma 3 follows\nimmediately:\np1 p2p3\np4 p5 p6\n(a) G∩ 2\np1 p2p3\np4 p5 p6\n(b) G∩∞\np1 p2p3\np4 p5 p6\n1\n1\n(c) G1p6\np1 p2p3\np4 p5 p6\n2\n2\n1\n1\n(d) G2p6\np1 p2p3\np4 p5 p6\n3\n2\n1\n1\n(e) G3p6\np1 p2p3\np4 p5 p6\n4\n3\n2\n2\n1\n1\n1\n(f) G4p6\np1 p2p3\np4 p5 p6\n5\n4\n3\n2\n2\n(g) G5p6\np1 p2p3\np4 p5 p6\n6\n5\n4\n3\n(h) G6p6\nFig. 1: A system of 6 processes where Psrcs(3) holds. The stable skeleton graph for round 2 is depicted in Figure 1a; 1b shows\nthe stable skeleton graph for the entire run. For simplicity, we omit self-loops, i.e., ∀pi : pi ∈ PT (pi). Figures 1c-1h show\nprocess p6’s approximation of G∩∞ during rounds 1 to 6.\nLemma 3: It holds that q ∈ PT (p, r) if, and only if, all of\nthe following are true:\n(a) q ∈ PT rp ,\n(b) p adds a directed edge q r→ p to Grp by executing Line 17\nin round r, and\n(c) for any r′ 6= r, there is no other edge q r\n′\n→ p in Grp.\nThe following lemma shows that the approximation graph\nGpℓ+1 accurately reflects the timely neighborhood of a process.\nThat is, if p1 is connected to pℓ+1 through a path of length\nℓ, then pℓ+1 will add the timely neighborhood information of\np1 to its approximated graph by round ℓ.\nLemma 4: Suppose that there exists a directed path\nΓ = (p1 → . . .→ pℓ+1)\nin G∩ r for round r > n, where Γ has length ℓ 6 n− 1. Then,\n∀q ∈ PT (p1, r − ℓ) it holds that\n(a) edge (q rq→ p1) is in Grpℓ+1 where r > rq > r − ℓ, and(b) Grpℓ+1 contains no other edges from q to p1.\nProof: Consider an arbitrary q ∈ PT (p1, r−ℓ). The proof\nproceeds by induction over the edges of path Γ indexed by k.\nThat is, we show that for all k, with 0 6 k 6 ℓ, it holds that\nthere is an edge e = (q rk→ p1) in Gr−ℓ+kp1+k where r − ℓ+ k >\nrk > r − ℓ.\nFor the base case (k = 0), we have to show that the edge e\nis in Gr−ℓp1 , but this already follows from q ∈ PT (p1, r − ℓ),\nby Lemma 3.\nFor the induction step, we assume that the statement holds\nfor some k < ℓ and then show that it holds for k+1 as well.\nIn round r − ℓ + (k + 1) process p1+k broadcasts its current\ngraph estimate, i.e., Gr−ℓ+kp1+k to all. We know that p1+(k+1)\nwill receive this message since (p1+k → p1+(k+1)) is in the\npath Γ ⊆ G∩ r, which means that\np1+k ∈ PT (p1+(k+1), r − ℓ+ (k + 1)).\nBy the induction hypothesis, the edge (q rk→ p1) is in Gr−ℓ+kp1+k\nand therefore will be among the edges that p1+(k+1) considers\nin Line 20. This in turn implies that p1+(k+1) will add an edge\nq\nrk+1\n→ p1 to its graph Gr−ℓ+(k+1)p1+(k+1) in Line 23, whereby rk+1\nis calculated in Line 22 such that rk+1 > rk . Moreover, by\ninduction hypothesis we have rk > r − ℓ > r − n, which\nensures that the edge will not be discarded in Line 24. Since\nthe code following the for-loop in Line 19 is executed exactly\nonce for every edge, no other edge q r\n′\n→ p1 is added to\nG\nr−ℓ+(k+1)\np1+(k+1) . This completes the proof our lemma.\nThe next lemma shows that the approximation graph of\ncorrectly (over)estimates the strongly connected component\nfrom round n on:\nLemma 5: Let r > n and consider the strongly connected\ncomponent Crp containing p in G∩ r. Then, it holds that Grp ⊇\nCrp .\nProof: Consider any edge (q′ → q) ∈ Crp . Since Crp is\nstrongly connected, there is a directed path between any pair of\nprocesses in Crp , in particular there is a path of length ℓ 6 n−1\nfrom q to p. By the definition of Crp we know that q always\nperceives q′ as timely in all rounds up to round r, which means\nthat q′ ∈ PT (q, r − ℓ). Then, by applying Lemma 4, we get\nthat the edge (q′ r\n′\n→ q) is in Grp, for some r′ > r − ℓ, which\nshows that Crp is a subgraph of Grp.\nLemma 3 showed that the timely neighborhood is eventually\nin the approximated graph. We now show that our approxima-\ntion contains only valid information:\nLemma 6: Let r > 1 and suppose that there is an edge\ne = (q′\ns\n→ q) in the approximated stable skeleton graph Grp\nof process p. Then it holds that q′ ∈ PT (q, s).\nProof: Note that processes only add edges to their ap-\nproximation graphs in Line 17 or in Line 23. If an edge is\nadded via Line 23, then this edge has previously been added\nby another process by executing Line 17. Therefore, every\nedge must have been added by some process via Line 17. In\ncase of e, this process can only be q. By Lemma 3 this happens\nin round s and q′ ∈ PT (q, s).\nThe following Lemma 7 is in some sense the converse result\nof Lemma 5, as it states that the approximated graph must\napproach Crp from below, if it is strongly connected:\nLemma 7: Let r > 1 and consider the strongly connected\ncomponent Crp . If the approximated skeleton graph Gr+n−1p is\nstrongly connected, then Crp ⊇ Gr+n−1p .\nProof: Consider any edge\ne = (q′\nr′\n→ q) ∈ Gr+n−1p .\nBy Lemma 6, we know that q′ ∈ PT (q, r′). It follows by\nthe subset property (3) that q′ ∈ PT (q, r), as Observation 1\nimplies\nr′ > (r + n− 1)− n = r − 1.\nTherefore, there is an edge (q′ → q) in G∩ r.\nIt follows that Gr+n−1p is isomorphic to a (not necessarily\nmaximal) strongly connected component Sr in G∩ r. Because\nCrp and Sr both contain p, their intersection is nonempty, i.e.,\nCrp ⊇ G\nr+n−1\np .\nAs a final result about the approximated skeleton graph, we\nshow that once the approximation Gp is strongly connected in\nround r > n, it is closed w.r.t. strongly connected components.\nThis means that Gp can be partitioned into disjoint strongly\nconnected components in G∩∞.\nTheorem 8: Suppose that R > n. If the approximated\nskeleton graph GRp is strongly connected, then it contains the\nstrongly connected component C∞q of every q ∈ GRp .\nProof: Consider any q ∈ GRp and its strongly connected\ncomponent C∞q . From (5) and Lemma 7 it follows that\nq ∈ GRp ⊆ C\nR−n+1\np ⊆ C\n1\np ,\ni.e., q ∈ C1p ∩ C1q . Moreover, due to the well-known fact that\ntwo maximal strongly connected components in a digraph are\neither disjoint or equivalent, we get that C1q = C1p .\nNow suppose the theorem does not hold. Then there exists\nsome q′ ∈ C∞q such that q′ 6∈ GRp . Due to Lemma 5, q′ cannot\nbe contained in CRp , but due to (5), q′ ∈ CRq ⊇ C∞q . Therefore,\nCRq 6= C\nR\np , and thus CRq ∩ CRp = ∅. Since GRp is strongly\nconnected and contains q, it also contains a path\nΓ = (q = pℓ → · · · → p0 = p),\nsuch that\n∀i, 0 6 i < ℓ : pi+1 ∈ PT (pi, R− i).\nLet j be the minimal index i such that pj ∈ CRq , and let\nΓj = (pj → · · · → p0) be the path remaining from pj .\nAs both q′ and pj are in CRq , there is a path Γ′ in CRq . Let\nk be the length of this path. Moreover, by applying Lemma 4,\nwe get that GR−jpj contains the outgoing edge e of q\n′ on this\npath, labeled with some round\nr′ > R− j − k. (9)\nBut then, by the definition of Γ, it follows that when GRp\ncontains pj — which it does — then it must also contain q′,\nunless some process pi (i < j) removed e from its set of\nedges in line 24 in round R− i because r′ 6 R− i−n. Since\nround R at process p(= p0) is the latest round when this can\noccur, we get that r′ 6 R− n, and thus, by (9),\nR− j − k 6 r′ 6 R− n, i.e., j + k > n. (10)\nLet ∆ be the subgraph obtained by concatenating paths Γ′\nand Γj . By construction, Γj and Γ′ only share node pj , and\nthus ∆ is a (simple) path and must have length j+k 6 n−1, as\nno path can exceed length n−1. This contradicts (10) and thus\ncompletes the proof that q′ is in GRp . The proof showing that\nall edges of C∞q are in GRp proceeds analogously, by assuming\nthat some edge in C∞q ending in q′ is not in GRp .\nB. k-Set Agreement\nIn this section, we will show that Algorithm 1 not only\napproximates the stable skeleton graph, but also solves k-set\nagreement. Our previous results allow us to immediately prove\nthe validity and the termination properties.\nLemma 9 (Validity): If a process decides on v, then v was\nthe initial value of some process.\nProof: Observe that the decision value xp of any process p\nis initially set to its proposal value vp, which is then broadcast.\nOn all subsequent updates of xp in Line 27, a value xq that\nwas sent by some process q (which originated from some vq′ )\nis assigned, therefore validity holds.\nLemma 10: Every process decides at most once in any run.\nProof: Observe that no process executes Line 29 and\nLine 12 in the same run. This is guaranteed by the fact that\nprocess p cannot pass the if-conditions in Line 10 or in Line 26\nafter decidedp is set to 1, which happens whenever p decides.\nLemma 11 (Termination): Every process decides exactly\nonce.\nProof: Lemma 10 shows that every process decides at\nmost once. We will now show that every process decides at\nleast once. First, we will show that there is a root component\nin every round. Consider the strongly connected components\nthat partition the set of nodes of the stable skeleton graph G∩ r\nin some round r. Such a set always exists, since the strongly\nconnected components form equivalence classes of nodes. It\nis well known that the contraction of the strongly connected\ncomponents is a directed acyclic graph, which reveals that\nthere is at least one node Cr in the contracted graph that has\nno incoming edges. Clearly, Cr satisfies the definition of a\nroot component in G∩ r. Therefore, there is a nonempty set\nRr of strongly connected components all of which are root\ncomponents in round r.\nLet r > 1 be the earliest round where G∩ r is stable for at\nleast n − 1 rounds, i.e., ∀r′ ∈ [r, r + n − 1] : G∩ r′ = G∩ r.\nNote that property (1) implies that r exists. Now, consider any\nroot component Rr ∈ Rr: Clearly, since every process is in\nexactly one strongly connected component, we have\n∀p ∈ Rr : Crp = R\nr = Rr+n−1 = Cr+n−1p . (11)\nWe will now show that the approximated skeleton graph of\nsuch a process p is in fact exactly the strongly connected\ncomponent of p. Consider any p ∈ Rr(= Cr+n−1p ). First, since\n(r+n−1) > n, Lemma 5 and (11) imply that Rr ⊆ Gr+n−1p .\nWe will now show that Rr ⊇ Gr+n−1p , which proves that these\ngraphs are equal: Since Gr+n−1p is connected by construction,\nit is sufficient to show that every edge in Gr+n−1p is also in Rr.\nAssume in contradiction that there is an edge e = (q′ r\n′\n→ q)\nin Gr+n−1p such that q ∈ Rr but q′ /∈ Rr ; note that the\nother way round (q′ ∈ Rr but q /∈ Rr) is impossible by\nconstruction. Using Lemma 6 we know that q′ ∈ PT (q, r′),\nand Observation 1 implies that r′ > (r+ n− 1)− n = r− 1,\ni.e., r′ > r. Then, by definition, we have that e ∈ G∩ r, i.e.,\ne is an incoming edge of Rr, contradicting the assumption\nthat Rr is a root component. We can therefore conclude that\nRr = Gr+n−1p .\nBy assumption, Rr is a root component, which tells us that\nGr+n−1p is strongly connected, i.e., p will pass the if-condition\nin Line 28 in round r+n−1 and decide. Recall the contracted\nstable skeleton graph of round r+ n− 1. Since every path in\nthis graph is rooted at some node corresponding to a root\ncomponent in the set Rr. Thus, all processes that are not in\na root component will receive a decision message by round\nr + 2n− 1 and also decide, which completes our proof.\nIn the remainder of this section we will prove that Algo-\nrithm 1 satisfies the k-agreement property. We will start out\nwith some basic invariants on decision estimates.\nObservation 2 (Monotonicity): In any run of Algorithm 1\nit holds that ∀r > 0: xrp > xr+1p .\nLemma 12: If process p does not decide in Line 12, we\nhave that ∀r > n− 1: xrp = xr+1p .\nProof: Suppose that there is an r > n−1 such that p sets\nxr+1p ← xq and xrp 6= xq . This can only occur in Line 27, if the\nprocess does not decide in Line 12. From Observation 2 and\nvalidity (cf. Lemma 9), we know that p did not previously\nreceive xq and that xq is the initial value of some distinct\nprocess q. Since processes forward their estimated decision\nvalue in every round, (3) implies that the shortest path from\nq to p (along which xp has been propagated to p) in G∩ r+1\nhas length r + 1. However, this is impossible as r + 1 > n\nand the longest possible path has length n− 1.\nLemma 13: Suppose that some process p decides on xp in\nround r by executing line 12. Then some process q 6= p has\ndecided on xp in round r′ < r by executing Line 29.\nProof: Every process decides either in Line 29 or in\nLine 12, but not both (Lemma 10). Since p decided in Line 12\nit must have received a (decide, xq, ) message from some\ndistinct process q. If q decided in Line 29 we are done;\notherwise q decided in Line 12 in round r− 1, we can repeat\nthe same argument for q. After at most n − 1 iterations, we\narrive at some process that must have decided using Line 29.\nLemma 14: Let Cnp be the strongly connected component of\nprocess p in round n. Then, it holds that ∀q ∈ Cnp : xnq = xnp .\nProof: First, observe that due to Lemma 13 and the fact\nthat no process can pass the check in Line 28 before round n,\nno process can decide before round n. Therefore, processes\ncan update their estimate values until at least round n.\nSuppose that there are processes p, q ∈ Cnp , such that xnp 6=\nxnq . In particular we assume without loss of generality, that xnq\nis minimal among all round n estimation values of processes\nin Crp , i.e., xnp > xnq .\nLet rq be the round where q first sets xq to the value\nxnq . By Observation 2 it follows that q does not update xq\nanymore before round n. Since Algorithm 1 satisfies validity\n(Lemma 9), we know that there is some process s that is\nthe source of this value, i.e., s initially proposed xrq . By the\ncode of the algorithm we know that in round r process p only\nconsiders values in Line 27 that were sent by some process\nin PT (p, r). This implies that there is a sequence of pairwise\ndistinct processes s = q1, . . . , qℓ = q, such that\n∀i, (1 6 i < ℓ) : qi ∈ PT (qi+1, i). (12)\nClearly, rq = ℓ − 1. Let j 6 ℓ be such that qj ∈ Cnp and j is\nminimal, let Γq be the path in G∩ 1 induced by the sequence\ns up to qj . Moreover, since qj ∈ Cnp , there is a path Γp in Cnp\nfrom qj to p. Since Cnp ⊆ G∩ 1, Γp is a path in G∩ 1 as well.\nLet Γ be the path in G∩ 1 obtained by appending Γp to Γq . By\nconstruction Γ is simple, and therefore its length is bounded\nby n−1. Moreover, the initial value of s was propagated along\nthis path — over Γq by construction and over Γp, because xnq is\nminimal in Cnp . This leads to process p assigning this value to\nxp in some round rp 6 n−1, which contradicts the assumption\nthat xnp > xnq .\nLemma 15 (k-Agreement): Processes decide on at most k\ndistinct values.\nProof: For the sake of a contradiction, assume that there is\na set of ℓ > k processes D = {p1, . . . , pℓ} in a run α where\npi decides on x∞i = x\nri\ni\n4 in round ri > n and ∀pi, pj ∈\nD : x∞pi 6= x\n∞\npj\n. By virtue of Lemma 13, we can assume that\nevery pi has decided by executing Line 29. Considering that\nno process decides before round n, applying Lemma 12 yields\nthat\n∀r > n ∀pi, pj ∈ D : x\nr\npi\n6= xrpj . (13)\nNote that the approximated skeleton graphs Gripi and G\nrj\npj are\nstrongly connected in round ri resp. rj , otherwise the pro-\ncesses could not have passed the if-condition before Line 29.\nWe will first show that the different decision values of pi\nand pj imply that their approximated skeleton graphs in rounds\nri resp. rj are disjoint. Lemma 7 reveals that these skeleton\ngraphs are contained within the respective strongly connected\ncomponents of an earlier round, i.e.,\nCri−n+1pi ⊇ G\nri\npi\nand Crj−n+1pj ⊇ G\nrj\npj\n.\nIf these strongly connected components of pi and pj are\ndisjoint, then so are the approximated skeleton graphs and\n4Note that x∞p denotes p’s final “estimate”, i.e., the actual decision value\nof process p.\nwe are done. Therefore, assume in contradiction that\nI = Cri−n+1pi ∩ C\nrj−n+1\npj\n6= ∅.\nWe will now prove that one of these components contains\nthe other. Without loss of generality, suppose that ri 6 rj and\nconsider any node p ∈ I ⊆ Crj−n+1pj . Clearly, p is strongly\nconnected to every node in Crj−n+1pj . Let Z be the induced\nsubgraph of Crj−n+1pj in the skeleton graph G∩ ri−n+1. By the\nsubgraph property (5) and since ri 6 rj , it follows that Z =\nC\nrj−n+1\npj , and hence Z ∩Cri−n+1pi 6= ∅. By the fact that p ∈ I ,\nwe know that p ∈ Cri−n+1pi . That is, in the skeleton graph\nG∩ ri−n+1, process p is strongly connected to all nodes in\nCri−n+1pi and Z . But since the strongly connected component\nCri−n+1pi is maximal, we actually have\nCri−n+1pi ⊇ Z = C\nrj−n+1\npj\n,\nwhich means that pj ∈ Cri−n+1pi . Then, Lemma 14 readily\nimplies that ∀q ∈ Cri−n+1pi it holds that x\nn\npi\n= xnq and, in\nparticular, xnpi = x\nn\npj\n, which contradicts (13). We can there-\nfore conclude that the intersection of the strongly connected\ncomponents, and therefore, by Lemma 7, also the intersection\nof Grip and G\nrj\npj is indeed empty, i.e.,\n∀pi, pj ∈ D : (G\nri\npi\n∩Grjpj ) = ∅. (14)\nBy Theorem 8 it follows that each of the strongly connected\napproximated skeleton graphs Gripi can be partitioned into a set\nDi of strongly connected components in G∩∞. By Theorem 1,\nat most k of the sets Di can contain a root component. Note\nthat (14) implies that no strongly connected component is\nin two distinct sets Di, Dj . For the sake of a contradiction,\nassume that (w.l.o.g.) the set Dℓ corresponding to Grℓpℓ does\nnot contain a root component. Now consider the contracted\ngraph of G∩∞ where the nodes are the strongly connected\ncomponents. Since the contracted graph is acyclic, it follows\nthat there exists a path Γ in the (non-contracted) graph G∩∞\nthat ends at process pℓ ∈ Dℓ, and is rooted at some process\nq ∈ C∞q where C∞q is a root component and thus by assumption\nnot in Dℓ. However, by the subgraph property (1), we know\nthat the path Γ is also in G∩ rℓ . But then Lemma 4 implies that\nq ∈ Gripi , and Theorem 8 shows that C\n∞\nq ∈ Dℓ, i.e., one of the\ncomponents in Dℓ in fact is a root component. This provides\nthe required contradiction.\nTheorem 16: Algorithm 1 solves k-set agreement in system\nPsrcs(k).\nProof: Lemma 15 implies k-agreement. Termination is\nguaranteed by Lemma 11 and Lemma 9 shows that validity\nholds.\nV. DISCUSSION AND FUTURE WORK\nWe have introduced the notion of communication graphs\nand presented an algorithm that approximates the stable skele-\nton of a run. The algorithm is based on exchanging local\napproximations of the stable skeleton, hence has a worst-case\nmessage bit complexity that is polynomially in n. We have\nalso introduced a class of communication predicates Psrcs(k)\nand proved that using this approximation one can solve k-\nset agreement in a system that guarantees Psrcs(k). Note that\nthe algorithm actually solves consensus in sufficiently well-\nbehaved runs.\nThe one-to-one correspondence between the (at most) k root\ncomponents of the stable skeleton graph and distinct decision\nvalues shows that these communication graphs are a promising\nnew tool for studying the underlying synchrony in a system.\nSince our algorithm yields a correct approximation atop of\nany communication predicate, part of our future work will\nbe devoted to finding a graph-theoretic characterization of the\nweakest synchrony requirements for different agreement prob-\nlems and further exploring the duality between communication\npredicates and graph-theoretic properties.\nREFERENCES\n[1] E. Borowsky and E. Gafni. Generalized FLP impossibility result for\nt-resilient asynchronous computations. In STOC ’93: Proceedings of\nthe twenty-fifth annual ACM symposium on Theory of computing, pages\n91–100, New York, NY, USA, 1993. ACM.\n[2] T. D. Chandra, V. Hadzilacos, and S. Toueg. The weakest failure detector\nfor solving consensus. Journal of the ACM, 43(4):685–722, June 1996.\n[3] B. Charron-Bost and A. Schiper. Improving fast Paxos: being optimistic\nwith no overhead. In 12th IEEE Pacific Rim International Symposium on\nDependable Computing (PRDC 2006), pages 287–295. IEEE Computer\nSociety, 2006.\n[4] B. Charron-Bost and A. Schiper. The Heard-Of model: computing\nin distributed systems with benign faults. Distributed Computing,\n22(1):49–71, Apr. 2009.\n[5] S. Chaudhuri. More choices allow more faults: set consensus problems\nin totally asynchronous systems. Inf. Comput., 105(1):132–158, 1993.\n[6] D. Dolev, C. Dwork, and L. Stockmeyer. On the minimal synchronism\nneeded for distributed consensus. Journal of the ACM, 34(1):77–97, Jan.\n1987.\n[7] C. Dwork, N. Lynch, and L. Stockmeyer. Consensus in the presence of\npartial synchrony. Journal of the ACM, 35(2):288–323, Apr. 1988.\n[8] T. Elrad and N. Francez. Decomposition of distributed programs\ninto communication-closed layers. Science of Computer Programming,\n2(3):155–173, 1982.\n[9] M. J. Fischer, N. A. Lynch, and M. S. Paterson. Impossibility of\ndistributed consensus with one faulty process. Journal of the ACM,\n32(2):374–382, Apr. 1985.\n[10] E. Gafni. Round-by-round fault detectors (extended abstract): unifying\nsynchrony and asynchrony. In Proceedings of the Seventeenth Annual\nACM Symposium on Principles of Distributed Computing, pages 143–\n152, Puerto Vallarta, Mexico, 1998. ACM Press.\n[11] M. Herlihy and N. Shavit. The asynchronous computability theorem for\nt-resilient tasks. In STOC ’93: Proceedings of the twenty-fifth annual\nACM symposium on Theory of computing, pages 111–120, New York,\nNY, USA, 1993. ACM.\n[12] M. Hutle, D. Malkhi, U. Schmid, and L. Zhou. Chasing the weakest sys-\ntem model for implementing omega and consensus. IEEE Transactions\non Dependable and Secure Computing, 6(4):269–281, 2009.\n[13] M. Hutle and A. Schiper. Communication predicates: A high-level\nabstraction for coping with transient and dynamic faults. In 37th\nAnnual IEEE/IFIP International Conference on Dependable Systems and\nNetworks (DSN’07), pages 92–101, 2007.\n[14] M. Saks and F. Zaharoglou. Wait-free k-set agreement is impossible:\nThe topology of public knowledge. SIAM J. Comput., 29(5):1449–1483,\n2000.\n[15] N. Santoro and P. Widmayer. Time is not a healer. In Proc. 6th Annual\nSymposium on Theor. Aspects of Computer Science (STACS’89), LNCS\n349, pages 304–313, Paderborn, Germany, Feb. 1989. Springer-Verlag.\n[16] N. Santoro and P. Widmayer. Agreement in synchronous networks with\nubiquitous faults. Theor. Comput. Sci., 384(2-3):232–249, 2007.\n",
            "id": 744308,
            "identifiers": [
                {
                    "identifier": "1571000703",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "10.1109/ipdps.2011.301",
                    "type": "DOI"
                },
                {
                    "identifier": "1102.4423",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "2179236",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "193011844",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:arxiv.org:1102.4423",
                    "type": "OAI_ID"
                }
            ],
            "title": "Solving k-Set Agreement with Stable Skeleton Graphs",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "1571000703",
            "oaiIds": [
                "oai:arxiv.org:1102.4423"
            ],
            "publishedDate": "2011-02-22T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1102.4423"
            ],
            "updatedDate": "2021-07-22T00:26:38",
            "yearPublished": 2011,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1102.4423"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/744308"
                }
            ]
        },
        {
            "acceptedDate": "2011-10-06T00:00:00",
            "arxivId": "1105.6163",
            "authors": [
                {
                    "name": "Prabhakaran, Manoj M."
                },
                {
                    "name": "Prabhakaran, Vinod M."
                }
            ],
            "contributors": [
                "The Pennsylvania State University CiteSeerX Archives"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/186876656",
                "https://api.core.ac.uk/v3/outputs/104477499"
            ],
            "createdDate": "2012-04-13T14:18:55",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 145,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/145",
                    "logo": "https://api.core.ac.uk/data-providers/145/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2011-07-01T00:00:00",
            "abstract": "We presented assisted common information as a generalization of\nG\\'acs-K\\\"orner (GK) common information at ISIT 2010. The motivation for our\nformulation was to improve upperbounds on the efficiency of protocols for\nsecure two-party sampling (which is a form of secure multi-party computation).\nOur upperbound was based on a monotonicity property of a rate-region (called\nthe assisted residual information region) associated with the assisted common\ninformation formulation. In this note we present further results. We explore\nthe connection of assisted common information with the Gray-Wyner system. We\nshow that the assisted residual information region and the Gray-Wyner region\nare connected by a simple relationship: the assisted residual information\nregion is the increasing hull of the Gray-Wyner region under an affine map.\nSeveral known relationships between GK common information and Gray-Wyner system\nfall out as consequences of this. Quantities which arise in other source coding\ncontexts acquire new interpretations. In previous work we showed that assisted\ncommon information can be used to derive upperbounds on the rate at which a\npair of parties can {\\em securely sample} correlated random variables, given\ncorrelated random variables from another distribution. Here we present an\nexample where the bound derived using assisted common information is much\nbetter than previously known bounds, and in fact is tight. This example\nconsiders correlated random variables defined in terms of standard variants of\noblivious transfer, and is interesting on its own as it answers a natural\nquestion about these cryptographic primitives.Comment: 8 pages, 3 figures, 1 appendix; to be presented at the IEEE\n  International Symposium on Information Theory, 201",
            "documentType": "research",
            "doi": "10.1109/isit.2011.6034098",
            "downloadUrl": "http://arxiv.org/abs/1105.6163",
            "fieldOfStudy": "computer science",
            "fullText": "Assisted Common Information: Further Results\nVinod M. Prabhakaran\nE´cole Polytechnique Fe´de´rale de Lausanne\nSwitzerland\nManoj M. Prabhakaran\nUniversity of Illinois, Urbana-Champaign\nUrbana, IL 61801\nAbstract—We presented assisted common information as a\ngeneralization of Ga´cs-Ko¨rner (GK) common information at\nISIT 2010. The motivation for our formulation was to improve\nupperbounds on the efficiency of protocols for secure two-party\nsampling (which is a form of secure multi-party computation).\nOur upperbound was based on a monotonicity property of a rate-\nregion (called the assisted residual information region) associated\nwith the assisted common information formulation.\nIn this note we present further results. We explore the\nconnection of assisted common information with the Gray-Wyner\nsystem. We show that the assisted residual information region and\nthe Gray-Wyner region are connected by a simple relationship:\nthe assisted residual information region is the increasing hull\nof the Gray-Wyner region under an affine map. Several known\nrelationships between GK common information and Gray-Wyner\nsystem fall out as consequences of this. Quantities which arise in\nother source coding contexts acquire new interpretations.\nIn previous work we showed that assisted common information\ncan be used to derive upperbounds on the rate at which a pair of\nparties can securely sample correlated random variables, given\ncorrelated random variables from another distribution. Here we\npresent an example where the bound derived using assisted\ncommon information is much better than previously known\nbounds, and in fact is tight. This example considers correlated\nrandom variables defined in terms of standard variants of\noblivious transfer, and is interesting on its own as it answers\na natural question about these cryptographic primitives.\nI. INTRODUCTION\nIf U, V,W are independent random variables, a natural\nmeasure of “common information” of X = (U, V ) and Y =\n(U,W ) is H(U). Observers of either X or Y may produce the\ncommon part U and conditioned on this common part, there\nis no residual information, i.e., I(X;Y |U) = 0. Ga´cs-Ko¨rner\n(GK) common information [5], [16] is a generalization of this\nto arbitrary X,Y . Two observers see Xn = (X1, X2, . . . , Xn)\nand Y n = (Y1, Y2, . . . , Yn), resp., where (Xi, Yi) are indepen-\ndent draws of (X,Y ). The observers produce W1 = f1(Xn)\nand W2 = f2(Xn) which have an asymptotically vanishing\nprobability of not matching. GK common information is the\nlargest entropy rate (normalized by n) of such a common\nrandom variable. It was however shown that this value is the\nlargest H(U) for which the random variables can be written\nas X = (U, V ) and Y = (U,W ) (where U, V,W may be\ndependent), i.e., the definition captures only an explicit form\nof common information in a single instance of X,Y .\nAt ISIT 2010 we presented a generalization of GK common\ninformation [13]. In our setup (see Figure 1), an omniscient\ngenie (who has access to the X and Y sequences) assists\nthe users in generating the common random variables by\nsending them messages over rate-limited noiseless links. A\nthree-dimensional trade-off region which characterizes the\ntrade-off between the rates of the two noiseless links and\nthe resulting residual information (defined as the conditional\nmutual information between the source sequences conditioned\non the common random variable normalized by the length of\nthe sequence) was derived. We call this the assisted residual\ninformation region. When the links have zero rates, we recover\nGK common information.\nOur motivation for this generalization was an application\nto cryptography. Distributed dependent random variables are\nan important resource in the cryptographic task of secure\nmulti-party computation. A fundamental problem here is for\ntwo parties to securely generate a certain pair of random\nvariables, given another pair of random variables, by means of\na protocol. Our main result there was that the assisted residual\ndependency region of the views of two parties engaged in\nsuch a protocol can only monotonically expand and not shrink\nwhich immediately leads to upperbounds on the efficiency\nwith which a target pair of random variables can be generated\nfrom another pair. This work generalized previous work on\nmonotones [17]. These works are in the same vein as [1], [4],\n[15], [10], [9], [7], [2], [14] which employ information theory\nto derive bounds on efficiency in cryptography.\nIn the first part of this paper we explore connections be-\ntween the assisted common information system and the Gray-\nWyner source coding system of [6]. In the Gray-Wyner system,\na pair of sources is decomposed into three components: one\npublic and two private. Using the public and one of the private\ncomponents, one of the pair of sources must be recoverable,\nwhile the other source must be recoverable using the other\nprivate component and the public component. Gray-Wyner\nregion is a three-dimensional region which characterizes the\ntrade-offs between the rates at which the three components\ncan be encoded.\nWe show that the assisted residual information region and\nthe Gray-Wyner region are connected by a simple relationship:\nthe assisted residual information region is the increasing hull1\nof the Gray-Wyner region under an affine map. Several known\nrelationships between GK common information and Gray-\nWyner system fall out as consequences of this. This also leads\n1Increasing hull i(S) of a set S ⊆ Rd is the set of all s ∈ Rd such that\nthere is a s′ ∈ S such that s ≥ s′, where the inequality is component-wise.\nar\nX\niv\n:1\n10\n5.\n61\n63\nv1\n  [\ncs\n.IT\n]  \n31\n M\nay\n 20\n11\nUser 1 User 2\ngenie\nX1 , X2 , . . . , Xn Y1 , Y2 , . . . , Yn\nW2 W1\nR2R1\nFig. 1: Setup for assisted common information system. The\nusers generate W1 and W2 which are required to agree with\nhigh probability. A genie assists the users by sending separate\nmessages to them over rate-limited noiseless links. When the\ngenie is absent the setup reduces to the one for Ga´cs-Ko¨rner\ncommon information.\nX1 , X2 , . . . , Xn\nY1 , Y2 , . . . , Yn\nDefinition 1.2: The Gray-Wyner region RGW(X,Y )\nis the set of all achievable rate 3-tuples.\nWe write RGW when the random variables are clear from\nthe context. A simple lower-bound to RGW is\nLGW = {(RA, RB, RC) : RA +RC ≥ H(X), RB +RC\n≥ H(Y ), RA +RB +RC ≥ H(X,Y )}\nThe Gray-Wyner region was characterized in [?].\nProposition 1.3:\nRGW = i\n ￿\npU|X,Y ∈P\n{(H(X|U),H(Y |U), I(X,Y ;U))}\n\nThe Gray-Wyner system generalizes the setup for\nWyner’s common information which is defined as the\nsmallest RC such that the outputs of the encoder taken\ntogether is an asymptotically efficient representation of\n(X,Y ), i.e., when RA + RB + RC = H(X,Y ). Using\nthe above proposition we have\nProposition 1.4:\nCWyner = inf{RC : (RA, RB, RC) ∈ RGW,\nRA +RB +RC = H(X,Y )}\n= inf\npU|X,Y ∈P:X−U−Y\nI(X,Y ;U)\nC. Known connections\nThe following connections between the two systems\nare known:\n• Ga´cs-Ko¨rner-Witsenhausen common information\ncan be obtained from the Gray-Wyner region [5,\nProblem 4.29, pg. 404].\nCGKW = sup{RC : RA +RC = H(X), RB +RC\n= H(Y ), (RA, RB, RC) ∈ RGW} (8)\nAlternatively [?],\nCGKW = sup{R : R ≤ I(X;Y ),\n{RC = R} ∩ LGW ⊆ RGW} (9)\n• Wyner’s common information can be obtained from\nthe Ga´cs-Ko¨rner-Witsenhausen system [?, Corollary\n2.3].\nCWyner = I(X;Y ) + inf\n(R1,R2,0)∈RGKW\nR1 +R2.\n(10)\nII. RELATIONSHIP BETWEEN\nGA´CS-KO¨RNER-WITSENHAUSEN AND GRAY-WYNER\nSYSTEMS\nTheorem 2.1: Let R￿GW be the image of RGW under\nthe affine map f defined below.\nf(s) =\n 1 0 10 1 1\n1 1 1\n s−\n H(X)H(Y )\nH(X,Y )\n , s ∈ R+3.\nThen\nRGKW = i\n￿R￿GW￿ .\nThus, the assisted residual information region RGKW\nis the increasing hull of the Gray-Wyner region RGW\nunder an affine map. However, it must be noted that the\nGray-Wyner region itself does not possess the monoticity\nproperty of RGKW which leads to Theorem 3.1 and is\ntherefore less-suited for the cryptographic application\nwhich motivated [?]. The two points we noted in Sec-\ntion I-C fall out of Theorem 2.1.\nCorollary 2.2:\nCGKW = sup{RC : RA +RC = H(X), RB +RC\n= H(Y ), (RA, RB, RC) ∈ RGW} (8)\n= sup{R : R ≤ I(X;Y ),\n{RC = R} ∩ LGW ⊆ RGW} (9)\nCorollary 2.3:\nCWyner = I(X;Y ) + inf\n(R1,R2,0)∈RGKW\nR1 +R2. (10)\nAnalogous to the definition of RRD-0, we define the\naxes intercepts on the other two axes.\nR1−0 = inf{R1 : (R1, 0, 0) ∈ RGKW}\nR2−0 = inf{R2 : (0, R2, 0) ∈ RGKW}\nR1−0 (resp., R2−0) is the rate at which the genie must\ncommunicate when it has a link to only the user who\nreceives X (resp. Y ) source so that the users can produce\na common random variable conditioned on which the\nsources are independent2. Using Proposition 1.2 we can\n2Though the definition allows for zero-rate communication to the\nother user and a zero-rate, but non-zero residual conditional mutual\ninformation, it can be shown from the expression for these rates in\n(11)-(12) that there is a scheme which achieves exact conditional\nindependence and requires no communication to the other user.\nDefinition 1.2: The Gray-Wyner region RGW(X,Y )\nis the set of all achievable rate 3-tuples.\nWe write RGW when the random variables are clear from\nthe context. A simple lower-bound to RGW is\nLGW = {(RA, RB, RC) : RA +RC ≥ (X), RB +RC\n≥ H(Y ), RA +RB +RC ≥ H(X,Y )}\nThe Gray-Wyner region was characterized in [?].\nProposition 1.3:\nRGW = i\n ￿\npU|X,Y ∈P\n{(H(X|U),H(Y |U), I(X,Y ;U))}\n\nThe Gray-Wyner system generalizes the setup for\nWyner’s common information which is defined as the\nsmallest RC such that the outputs of the encoder taken\ntogether is an asymptotically efficient representation of\n(X,Y ), i.e., when RA + RB + RC = H(X,Y ). Using\nthe above proposition we have\nProposition 1.4:\nCWyner = inf{RC : (RA, RB, RC) ∈ RGW,\nRA +RB +RC = H(X,Y )}\n= inf\npU|X,Y ∈P:X−U−Y\nI(X,Y ;U)\nC. Known connections\nThe following connections between the two systems\nare known:\n• Ga´cs-Ko¨rner-Witsenhausen common information\ncan be obtained from the Gray-Wyner region [5,\nProblem 4.29, pg. 404].\nCGKW = sup{RC : RA +RC = H(X), RB +RC\n= H(Y ), (RA, RB, RC) ∈ RGW} (8)\nAlternatively [?],\nCGKW = sup{R : R ≤ I(X;Y ),\n{RC = R} ∩ LGW ⊆ RGW} (9)\n• Wyner’s common information can be obtained from\nthe Ga´cs-Ko¨rner-Witsenhausen system [?, Corollary\n2.3].\nCWyner = I(X;Y ) + inf\n(R1,R2,0)∈RGKW\nR1 +R2.\n(10)\nII. RELATIONSHIP BETWEEN\nGA´CS-KO¨RNER-WITSENHAUSEN AND GRAY-WYNER\nSYSTEMS\nTheorem 2.1: Let R￿GW be the image of RGW under\nthe affine map f defined below.\nf(s) =\n 1 0 10 1 1\n1 1 1\n s−\n H(X)H(Y )\nH(X,Y )\n , s ∈ R+3.\nThen\nRGKW = i\n￿R￿GW￿ .\nThus, the assisted residual information region RGKW\nis the increasing hull of the Gray-Wyner region RGW\nunder an affine map. However, it must be noted that the\nGray-Wyner region itself does not possess the monoticity\nproperty of RGKW which leads to Theorem 3.1 and is\ntherefore less-suited for the cryptographic application\nwhich motivated [?]. The two points we noted in Sec-\ntion I-C fall out of Theorem 2.1.\nCorollary 2.2:\nCGKW = sup{RC : RA +RC = H(X), RB +RC\n= H( ), (RA, RB, RC) ∈ GW} (8)\n= sup{R : R ≤ I(X;Y ),\n{RC = R} ∩ LGW ⊆ RGW} (9)\nCorollary 2.3:\nCWyner = I(X;Y ) + inf\n(R1,R2,0)∈RGKW\nR1 +R2. (10)\nAnalogous to the definition of RRD-0, we define the\naxes intercepts on the other two axes.\nR1−0 = inf{R1 : (R1, 0, 0) ∈ RGKW}\nR2−0 = f{R2 : (0, R2, 0) ∈ RGKW}\nR1−0 (resp., R2−0) is the rate at which the genie must\ncommunicat when it has a link to only the user who\nreceives X (resp. Y ) source so that the users can produce\na common random variable conditioned on which the\nsources are independent2. Using Proposition 1.2 we can\n2Though the definition allows for zero-rate communication to the\nother user and a zero-rate, but non-zero residual conditional mutual\ninformation, it can be shown from the expression for these rates in\n(11)-(12) that there is a scheme which achieves exact conditional\nindependence and requires no communication to the other user.\nDefinition 1.2: The Gray-Wyner regio RGW(X,Y )\nis the set of all achievable rate 3-tuples.\nWe write RGW when the random variables are clear from\nthe context. A simple lower-bound to RGW is\nLGW = {(RA, RB, RC) : RA +RC ≥ H(X), RB +RC\n≥ H(Y ), RA +RB +RC ≥ H(X,Y )}\nThe Gray-Wyner region was characterized in [?].\nProposition 1.3:\nRGW = i\n ￿\npU|X,Y ∈P\n{(H(X|U),H(Y |U), I(X,Y ;U))}\n\nThe Gray-Wyner syste generalizes the setup for\nWyner’s common information which is defined as the\nsmallest RC such that the outputs of the encoder taken\ntogether is an asymptotically efficient representation of\n(X,Y ), i.e., when RA + RB + RC = H(X,Y ). Using\nthe above proposition we have\nProposition 1.4:\nCWyner = inf{RC : (RA, RB, RC) ∈ RGW,\nRA +RB +RC = H(X,Y )}\n= inf\npU|X,Y ∈P:X−U−Y\nI(X,Y ;U)\nC. Known connections\nThe following connections between the two s st ms\nare known:\n• Ga´cs-Ko¨rner-Witsenhausen common information\ncan be obtained from the Gray-Wyner region [5,\nProblem 4.29, pg. 404].\nCGKW = sup{RC : RA +RC = H(X), RB +RC\n= H(Y ), (RA, RB, RC) ∈ RGW} (8)\nAlternatively [?],\nCGKW = sup{R : R ≤ I(X;Y ),\n{RC = R} ∩ LGW ⊆ RGW} (9)\n• Wyner’s common information can be obtained from\nthe Ga´cs-Ko¨rner-Witsenhausen system [?, Corollary\n2.3].\nCWyner = I(X;Y ) + inf\n(R1,R2,0)∈RGKW\nR1 +R2.\n(10)\nII. RELATIONSHIP BETWEEN\nGA´CS-KO¨RNER-WITSENHAUSEN AND GRAY-WYNER\nSYSTEMS\nTheorem 2.1: Let ￿GW be the image of RGW under\nthe affine map f defined below.\nf(s) =\n 1 0 10 1 1\n1 1 1\n s−\n H(X)H(Y )\nH(X,Y )\n , s ∈ R+3.\nThen\nRGKW = i\n￿R￿GW￿ .\nThus, the assisted residual information region RGKW\nis the increasing hull of the Gray-Wyner region RGW\nunder an affine map. However, it must be noted that the\nGray-Wyner region itself does not possess the monoticity\nproperty of RGKW which leads to Theorem 3.1 and is\ntherefore less-suited for the cryptographic application\nwhich motivated [?]. The two points we noted in Sec-\ntion I-C fall out of Theorem 2.1.\nCorollary 2.2:\nCGKW = sup{RC : RA +RC = H(X), RB +RC\n= H(Y ), (RA, RB, RC) ∈ RGW} (8)\n= sup{R : R ≤ I(X;Y ),\n{RC = R} ∩ LGW ⊆ RGW} (9)\nCorollary 2.3:\nCWyner = I(X;Y ) + inf\n(R1,R2,0)∈RGKW\nR1 +R2. (10)\nAnalogous to the definition of RRD-0, we define the\naxes intercepts on the other two axes.\n= inf{R1 : (R1, 0, 0) ∈ GKW}\nR2−0 = inf{R2 : (0, R2, 0) ∈ RGKW}\nR1−0 (resp., R2−0) is the rate at which the genie must\ncommunicate when it has a link to only the user who\nreceives X (resp. Y ) source so that the users can produce\na common random variable conditioned n which the\nsources are independent2. Using Proposition 1.2 we can\n2Though the d finition llows fo zero-rate communicati n to the\nother user and a zero-rate, but non-zero residual conditional mutual\ninformation, it can be shown from the expression for these rates in\n(11)-(1 ) that there is a scheme which achieves exact conditional\nindependence and requires no communication to the other user.\nimproved on the upperbounds that could be derived\nfrom previous results. These pairs were contrived to\nhighlight the shortcomings of prior work. Her we give\nyet another example where the upperbound from our\nresult strictly improves on prior work, but is further\ninteresting for two reasons: firstly, the new example is\nbased on natural correlated random variables that are\nwidely studied (namely, variants of oblivious transfer),\nand secondly the new upperbound we can prove actually\nmatches an easy lowerbound and is therefore tight.\nA. A New Example\nWe now discuss the n w example where our uppe -\nbound is not only strictly better t an the previously est\navailable upperbound, but is also tight.\nExample 3.1: Let SA,1, SA,2, SB,1, SB,2 ∈ {0, 1}L\nand CA, CB ∈ {1, 2} be six independent random vari-\nables all of which are uniformly distributed over their\nalphabets. Consider a pair of random variables X,Y\ndefined as X = (CA, SA,1, SA,2, SB,CA) nd Y =\n(CB, SB,1, SB,2, SA,CB). Notice that these are in fact\na pair of independent string-oblivious transfers (string-\nOT’s) [?]citatio (string length L) in opposite directions.\nLet U, V be a pair of random variables whose joint\ndistribution is the same as that of X,Y , but with L = 1.\nIn other words, U, V are a pair of independent bit-OT’s\nin opposite directions. The goal is to characterize the\nefficiency with whic we may securely gen rate ind -\npendent instances of U, V from independent instances\nof X,Y for L > 1.\nIt is easy to see that RGKW(X,Y ) intersects the co-\nordinate axes at (1+L, 0, 0), (0, 1+L, 0), and (0, 0, 2L).\nFrom, these we can immediately obtain the upperbound\nof [18] on the efficie cy, namely (1 +L)/2. Notice that\nthis is dependent on L and would suggest that (several)\nlong string-OT pairs can be turned into several (more)\nbit-OT pairs. However, as we show below, the efficiency\nof conversion is just 1, i.e., the best one can do is to turn\neach pair of string-OT’s into a pair of bit-OT’s.\nWe will show that inf{R1 + R2 : (R1, R2, 0) ∈\nRGKW(U, V )} = 2. But, (1, 1, 0) ∈ RGKW(X,Y ). This\ncan be seen by setting Q = (CA, CB, SA,CB , SB,CA)\nfor which (R1, R2, RRD) = (1, 1, 0). Thus, inf{R1 +\nR2 : (R1, R2, 0) ∈ RGK (X,Y )} ≤ 2. Hence, from\nTheorem 3.1, we may conclude that the efficiency of\nconversion we are after is 1.\nIt only remains to ch racterize inf{R1 + R2 :\n(R1, R2, 0) ∈ RGKW(U, V )}. The following lemma\nwhose proof is delegated to the appendix for want of\nspace provides the required characterization.\nLemma 3.2:\ninf{R1 +R2 : (R1, R2, 0) ∈ RGKW(U, V )} = 2.\nACKNOWLEDGEMENTS\nThe authors would like to gratefully acknowledge\ndiscussions with Venkat Anantharam, Pe´ter Ga´cs, and\nYoung-H n Ki . The example in Section III-A is based\non a uggestion by Ju¨rg Wullschleger.\nXˆ1, Xˆ2, . . . , Xˆn\nYˆ1, Yˆ2, . . . , Yˆn\nAPPENDIX\nProof of Proposition 1.1:\nGKW common information CGKW is defined as the\nsupremum of the set of R such that for every ￿ > 0\nthere are maps g1 : X n → Z, and g2 : Yn → Z for a\nufficiently large n which satisfy\nPr (g1(X\nn) ￿= g2(Y n)) ≤ ￿, (17)\n1\nn\nH(g1(X\nn))) ≥ R− ￿. (18)\nAn altern tive defintion which allows for a genie with\nzero-rate links to the users is given below. It is easy to\nsee that this can only lead to a larg r value. But as we\nwill show, the defin tions are in fact equivalent.\nLet C￿GKW be the s premum of the set of R such\nthat for every ￿ > 0 th re are maps fk : X n × Yn →\n{1, . . . , 2n￿}, (k = 1, 2), g1 : X n × {1, . . . , 2n￿} → Z,\nand g2 : Yn × {1, . . . , 2n￿} → Z for a sufficiently large\nn which satisfy (1) and\n1\nn\nH(g1(X\nn, f1(X\nn, Y n))) ≥ − ￿.\nClearly, C￿GKW ≥ CGKW. We first show\nC￿GKW = I(X;Y )−RRD-0. (19)\nLet U = g1(Xn(f1(Xn, Y n))). Then\nI(Xn;Y n|U) +H(U)\n= I(Xn;Y n|U) + I(Xn, Y n;U)\n= H(Xn, Y n)−H(Xn|Y n, U)−H(Y n|Xn, U)\n= I(Xn;Y n) + I(Xn;U |Y n) + I(Y n;U |Xn)\n≥ nI(X;Y ).\nTherefor , if the maps satisfy (2), then\nH(U) ≥ nI(X; )− I(Xn;Y n|U)\n≥ nI(X;Y )− n(RRD + ￿)\n= n(I(X;Y )−RRD − ￿)\nimproved on the upperbounds that could be derived\nfrom previous results. These pairs were contrived to\nhighlight the shortcomings of prior work. Here we give\nyet another xample where the upperbound from our\nresult strictly improves on prior work, but is further\ninteresting for two reasons: firstly, the new example is\nbased on natural correlated random variables that are\nwidely studied (namely, variants of oblivious transfe ),\nand sec ndly the new u perb und we can prove actually\nmatches an easy lowerbound and is therefore tight.\nA. A New Example\nWe now discuss the new example where our upper-\nbound is not only strictly better than the previously best\navailable upperbound, but is also ti ht.\nExample 3.1: Let SA,1, SA,2, SB,1, SB,2 ∈ {0, 1}L\nand CA, CB ∈ {1, 2} be six independent random vari-\nables all of which are uniformly distributed over their\nalphabets. Consider a pair of random variables X,Y\ndefined as X = (CA, SA,1, SA,2, SB,CA) and Y =\n(CB, SB,1, SB,2, SA,CB). Notice that these are in fact\na pair f independ nt stri g-oblivious transfers (string-\nOT’s) [?]c tation (string length L) in opposite directions.\nLet U, V be a pair of random variables whose joint\ndistribution is the same as that of X,Y , but with L = 1.\nIn other words, U, V are a pair of independent bit-OT’s\nin opposite directions. The goal is to characterize the\nefficiency with which we may securely generate inde-\npendent instances of , V from independent instanc s\nof X,Y for L > 1.\nIt is easy to see that RGKW(X,Y ) intersects the co-\nordinate axes at (1+L, 0, 0), (0, 1+L, 0), and (0, 0, 2L).\nFrom, these we can immediately obtain the upperbound\nof [18] on the efficiency, namely (1 +L)/2. Notice that\nthis is dependent on L and would suggest that (several)\nlong string-OT pairs can be turned into several (more)\nbit-OT pairs. However, as we show below, the efficiency\nof conversion is just 1, i.e., the best one can do is to turn\neach pair of string-OT’s into a pair of bit-OT’s.\nWe will show that inf{R1 + R2 : (R1, R2, 0) ∈\nRGKW(U, V )} = 2. But, (1, 1, 0) ∈ RGKW(X,Y ). This\ncan be seen by setting Q = (CA, CB, SA,CB , SB,CA)\nfor which (R1, R2, RRD) = (1, 1, 0). Thus, inf{R1 +\nR2 : (R1, R2, 0) ∈ RGKW(X,Y )} ≤ 2. Hence, from\nTheorem 3.1, we may conclude that the efficiency of\nconversion we are after is 1.\nIt only remains to characterize inf{R1 + R2 :\n(R1, R2, 0) ∈ RGKW(U, V )}. The following lemma\nwhose proof is delegated to the appendix for want of\nspace provides the required characterization.\nLemma 3.2:\ninf{R1 +R2 : (R1, R2, 0) ∈ RGKW(U, V )} = 2.\nACKNOWLEDGEMEN S\nThe authors would like to gratefully acknowledge\ndiscussions with Venkat Anantharam, Pe´ter Ga´cs, and\nYoung-Han Kim. The example in Section III-A is based\non a suggestion by Ju¨rg Wullschleger.\nXˆ1, Xˆ2, . . . , Xˆn\nYˆ1, Yˆ2, . . . , Yˆn\nAPPENDIX\nProof of Proposition 1.1:\nGKW common information CGKW is defined as the\nsupremum of the set of R such that for every ￿ > 0\nthere are maps g1 : X n → Z, and g2 : Yn → Z for a\nsufficiently large n which satisfy\nPr (g1(X\nn) ￿= g2(Y n)) ≤ ￿, (17)\n1\nn\nH(g1(X\nn))) ≥ R− ￿. (18)\nAn alternative definti n whic all ws for a genie with\nzero-rate links to the users is given below. It is easy to\nsee that this can only lead to a larger value. But as we\nwill show, the definitions are in fact equivalent.\nLet C￿GKW be the supremum of the set of R such\nthat for every ￿ > 0 there are maps fk : X n × Yn →\n{1, . . . , 2n￿}, (k = 1, 2), g1 : X n × {1, . . . , 2n￿} → Z,\nand g2 : Yn × {1, . . . , 2n￿} → Z for a sufficiently large\nn which satisfy (1) and\n1\nn\nH(g1(X\nn, f1(X\nn, n))) ≥ R− ￿.\nClearly, C￿GKW ≥ CGKW. We first show\nC￿GKW = I(X;Y )−RRD-0. (19)\nLet U g1(Xn(f1(Xn, Y n))). Then\nI(Xn;Y n|U) +H(U)\n= I(Xn;Y n|U) + I(Xn, Y n;U)\n= H(Xn, n)−H(Xn|Y n, U)−H(Y n|Xn, U)\n= I(Xn;Y n) + I(Xn;U |Y n) + I(Y n;U |Xn)\n≥ nI(X;Y ).\nTherefore, if the maps satisfy (2), then\nH(U) ≥ nI(X;Y )− I(Xn;Y n|U)\n≥ nI(X;Y )− n(RRD + ￿)\n= n(I(X;Y )−RRD − ￿)\nEncoder\nDecoder 1\nDecoder 2\nFig. 2: Setup for Gray-Wyner (GW) system.\nto alternative interpretations (in terms of the assisted common\ninformation system) to quantities which arise naturally in\ncertain other source coding co t xts. How ver, it must be\nnoted that the Gray-Wyner region itself does not posse s\nthe monotonicity property which makes it less-suited for the\ncryptographic application which motivated [13].\nThe second half of the paper is a sequel to the cryptographic\napplication in [13]. Th re sh wed an example where our\nupperbound (on the efficiency with which a pair of random\nvariables can b securely generated from another pair) strictly\nimproved upon bounds from previ us results. That example\nwas contrived to highlight the shortcomings of prior work.\nHere we giv yet another example wh re the upperbound\nfrom our result strictly improves on the p ior work, b t is\nfurther interesting for two reasons: firstly, the new example i\nbased on natural correlated random variables that are widely\nstudied (namely, variants of oblivious tra sfer), and sec ndly\nthe new upperbound we can prove actually matches an easy\nlowerbound and is therefore tight.\nII. PRELIMINARIES\nA. Assisted Common Information System\nWe presented the following generalization of GK common\ninformation at ISIT, 2010 [13]. We call it the assisted common\ninformation system.\nConsider Figure 1. For a pair of random variables (X,Y ),\nwe say that a rate pair (R1, R2) enables a residual information\nrate RRD if for every \u000f > 0, there is a large enough\ninteger n and (deterministic) functions fk : Xn × Yn →\n{1, . . . , 2n(Rk+\u000f)}, (k = 1, 2), g1 : Xn×{1, . . . , 2n(R1+\u000f)} →\nZ, and g2 : Yn × {1, . . . , 2n(R2+\u000f)} → Z (where Z is the set\nof integers) such that\nPr (g1(X\nn, f1(X\nn, Y n)) 6= g2(Y n, f2(Xn, Y n))) ≤ \u000f, (1)\n1\nn\nI(Xn;Y n|g1(Xn, f1(Xn, Y n))) ≤ RRD + \u000f. (2)\nDefinition 2.1: We define the assisted residual information\nregion2 RACI(X,Y ) of a pair of random variables (X,Y ) with\njoint distribution pX,Y as the set of all (r1, r2, rRD) ∈ R+3 for\nwhich there is a (R1, R2, RRD) such that r1 ≥ R1, r2 ≥ R2,\nrRD ≥ RRD, and (R1, R2) enables the residual information\nrate RRD. In other words,\nRACI(X,Y ) 4= i ({(R1, R2, RRD) : (R1, R2) enables RRD}) ,\nwhere i (S) denotes the increasing hull of S ⊆ R+3: i (S) =\n{s ∈ R+3 : s ≥ s′ component-wise for some s′ ∈ S}.\nWe will write ACI when the random variables involved are\nobvious from the context.\nWhen the two rates from the genie are zero, we recover\nGa´cs-Ko¨rner common information, CGK [5], [16]. Let RRD-0\n4\n=\ninf{R D : (0, 0, RRD) ∈ RACI(X,Y )}. Then we have the\nfollowing proposition.\nProposition 2.1:\nCGK(X,Y ) = I(X;Y )−RRD-0. (3)\nF rther\nRRD-0 = inf\npU|XY :I(X;U |Y )=I(Y ;U |X)=0\nI(X;Y |U) (4)\nwhich gives\nCGK(X,Y ) = sup\npU|XY :I(X;U |Y )=I(Y ;U |X)=0\nH(U). (5)\nMoreover, CGK(X,Y ) = 0 unless there are X ′, Y ′, U ′ such\nthat X = ( ′, U ′), Y = (Y ′, U ′), in which case CGK =\nmaxU ′:X=(X′,U ′),Y=(Y ′,U ′)H(U\n′).\nThe proof of this proposition and all other results are\navail ble in t e appendix. The proof of (4) relies on the\nfollowing character zation of RACI which was proved in [13].\nLet P be the set f all marginal p.m.f’s pU |X,Y such that the\nrdinality f lphab t U of U is |X ||Y|+ 2.\nPropositi n 2.2:\nRACI(X,Y ) =\ni\n ⋃\npU|X,Y ∈P\n{(I(Y ;U |X), I(X;U |Y ), I(X;Y |U))}\n\n2We may also define an analogous assisted common information region by\nreplacing the definition in (2) by\n1\nn\nI(Xn, Y n; g1(X\nn, f1(X\nn, Y n))) ≥ RCI − \u000f.\nSee [13] for this and its connection to the above definition. In effect, the\ndefinitions are equivalent as we discuss there. We work with assisted residual\ninformation region since it has a simple monotonicity property (Theorem 4.1)\nwhich makes it appealing for deriving bounds for secure two-party sampling.\nB. Gray-Wyner system\nThe Gray-Wyner system is shown in Figure 2. It is a source\ncoding problem formulated as follows: We say that a rate 3-\ntuple (RA, RB, RC) is achievable if for every \u000f > 0, there is a\nlarge enough integer n and (deterministic) encoder functions\nfA : Xn × Yn → {1, . . . , 2n(RA+\u000f)}, fB : Xn × Yn →\n{1, . . . , 2n(RB+\u000f)}, fC : Xn × Yn → {1, . . . , 2n(RC+\u000f)}, and\n(deterministic) decoder functions gAC : {1, . . . , 2n(RA+\u000f)} ×\n{1, . . . , 2n(RC+\u000f)} → Xn, and gBC : {1, . . . , 2n(RB+\u000f)} ×\n{1, . . . , 2n(RC+\u000f)} → Yn such that\nPr (gAC(fA(X\nn, Y n), fC(X\nn, Y n)) 6= Xn)) ≤ \u000f, (6)\nPr (gBC(fB(X\nn, Y n), fC(X\nn, Y n)) 6= Y n)) ≤ \u000f. (7)\nDefinition 2.2: The Gray-Wyner region RGW(X,Y ) is the\nset of all achievable rate 3-tuples.\nWe write RGW when the random variables are clear from the\ncontext. A simple lower-bound to RGW(X,Y ) is\nLGW(X,Y ) = {(RA, RB, RC) : RA +RC ≥ H(X), RB +RC\n≥ H(Y ), RA +RB +RC ≥ H(X,Y )}\n(8)\nThe Gray-Wyner region was characterized in [6].\nProposition 2.3 ([6]):\nRGW(X,Y ) =\ni\n ⋃\npU|X,Y ∈P\n{(H(X|U), H(Y |U), I(X,Y ;U))}\n\nThe Gray-Wyner system generalizes the setup for Wyner’s\ncommon information [19] which is defined as the smallest\nRC such that the outputs of the encoder taken together is an\nasymptotically efficient representation of (X,Y ), i.e., when\nRA +RB +RC = H(X,Y ). Using the above proposition we\nhave\nProposition 2.4:\nCWyner(X,Y ) = inf{RC : (RA, RB, RC) ∈ RGW(X,Y ),\nRA +RB +RC = H(X,Y )}\n= inf\npU|X,Y ∈P:X−U−Y\nI(X,Y ;U)\nC. Known connections\nThe following connections between the two systems are\nknown:\n• Ga´cs-Ko¨rner common information can be obtained from\nthe Gray-Wyner region [3, Problem 4.28, pg. 404].\nCGK(X,Y ) = sup{RC : RA +RC = H(X), RB +RC\n= H(Y ), (RA, RB, RC) ∈ RGW} (9)\nAlternatively [11],\nCGK(X,Y ) = sup{R : R ≤ I(X;Y ),\n{RC = R} ∩ LGW ⊆ RGW}\n(10)\n• Wyner’s common information can be obtained from the\nGa´cs-Ko¨rner system [13, Corollary 2.3].\nCWyner(X,Y ) = I(X;Y ) + inf\n(R1,R2,0)∈RACI\nR1 +R2.\n(11)\nIII. RELATIONSHIP BETWEEN ASSISTED COMMON\nINFORMATION AND GRAY-WYNER SYSTEMS\nTheorem 3.1: LetR′GW(X,Y ) be the image ofRGW(X,Y )\nunder the affine map fX,Y defined below.\nfX,Y\n RARB\nRC\n 4=\n RA +RC −H(X)RB +RC −H(Y )\nRA +RB +RC −H(X,Y )\n .\nThen\nRACI(X,Y ) = i (R′GW(X,Y )) .\nThus, the assisted residual information region RACI(X,Y )\nis the increasing hull of the Gray-Wyner region RGW(X,Y )\nunder an affine map fX,Y . The map, in fact, computes the\ngap of RGW(X,Y ) to the simple lower bound LGW(X,Y ) of\n(8) under a coordinate transformation. The first coordinate of\nR′GW is indeed the gap between the (sum) rate at which the\nfirst decoder in the Gray-Wyner system receives data and the\nminimum possible rate at which it may receive data so that\nit can losslessly reproduce Xn. The second coordinate has\na similar interpretation with respect to the second decoder.\nThe third coordinate is the gap between the rate at which the\nencoder sends data and the minimum possible rate at which it\nmay transmit to allow both decoders to losslessly reproduce\ntheir respective sources.\nIt must, however, be noted that the Gray-Wyner region\nitself does not possess the monotonicity property of RACI\nwhich leads to Theorem 4.1 and is therefore less-suited for\nthe cryptographic application which motivated [13].\nThe two points we noted in Section II-C fall out of Theo-\nrem 3.1.\nCorollary 3.2:\nCGK(X,Y ) = sup{RC : RA +RC = H(X), RB +RC\n= H(Y ), (RA, RB, RC) ∈ RGW(X,Y )} (9)\n= sup{R : R ≤ I(X;Y ),\n{RC = R} ∩ LGW(X,Y ) ⊆ RGW(X,Y )}\n(10)\nCorollary 3.3:\nCWyner(X,Y ) = I(X;Y ) + inf\n(R1,R2,0)∈RACI(X,Y )\nR1 +R2.\n(11)\nAnalogous to the definition of RRD-0, we define the axes\nintercepts on the other two axes.\nR1−0\n4\n= inf{R1 : (R1, 0, 0) ∈ RACI}\nR2−0\n4\n= inf{R2 : (0, R2, 0) ∈ RACI}\nR1−0 (resp., R2−0) is the rate at which the genie must commu-\nnicate when it has a link to only the user who receives X (resp.\nY ) source so that the users can produce a common random\nvariable conditioned on which the sources are independent3.\nUsing Proposition 2.2 we can show that\nR1−0 = inf\npU|X,Y ∈P:I(X;U |Y )=I(X;Y |U)=0\nI(Y ;U |X), (12)\nR2−0 = inf\npU|X,Y ∈P:I(Y ;U |X)=I(X;Y |U)=0\nI(X;U |Y ). (13)\nThese quantities were identified in [17] and shown to posses\na monotonic property in the context of secure two-party\nsampling (a result which [13] generalized).\nAs we will show below, this pair of quantities is closely\nrelated to a pair which has been identified elsewhere in the\ncontext of lossless coding with side-information [12] and the\nGray-Wyner system [11]. Let (following the notation of [11])\nG(Y → X)\n= inf{RC : (H(X|Y ), H(Y )−RC, RC) ∈ RGW(X,Y )},\nG(X → Y )\n= inf{RC : (H(X)−RC, H(Y |X), RC) ∈ RGW(X,Y )}.\nIt has been shown [12], [11] that G(Y → X) is the\nsmallest rate at which side-information Y may be coded and\nsent to a decoder which is interested in recovering X with\nasymptotically vanishing probability of error if the decoder\nreceives X coded and sent at a rate of only H(X|Y ) (which\nis the minimum possible rate which will allow such recovery).\nFurther, [11] arrives at the maximum of G(Y → X) and\nG(X → Y ) as a dual to the alternative definition of CGK\nin (10) from the Gray-Wyner system.\nWe have the following relationship between the two pairs\nof quantities.\nCorollary 3.4:\nG(Y → X) = I(X;Y ) +R1−0, (14)\nG(X → Y ) = I(X;Y ) +R2−0. (15)\nFurther,\ninf{R : R ≥ I(X;Y ), (RC = R) ∩ LGW(X,Y ) ⊆ RGW(X,Y )}\n= max(G(Y → X), G(X → Y )) (16)\n= I(X;Y ) + max(R1−0, R2−0). (17)\nIV. CRYPTOGRAPHIC APPLICATION\nThe cryptographic problem we consider is of 2-party secure\nsampling: Alice and Bob should sample correlated random\nvariables (U, V ) (Alice getting U and Bob getting V ), such\nthat Alice’s view during the sampling protocol reveals nothing\nmore to her about Bob’s outcome V than what her own\noutcome U reveals to her, and similarly Bob’s view reveals\nnothing more about Alice’s outcome than is revealed by his\n3Though the definition allows for zero-rate communication to the other\nuser and a zero-rate (but non-zero) residual conditional mutual information,\nit can be shown from the expression for these rates in (12)-(13) that there\nis a scheme which achieves exact conditional independence and requires no\ncommunication to the other user.\nown outcome. This is an important special case of secure\nmulti-party computation, a central problem in modern cryp-\ntography.\nHowever, it is well-known (see for instance [18] and ref-\nerences therein) that very few distributions can be sampled\nfrom in this way, unless the computation is aided by a set\nup — some correlated random variables that are given to the\nparties at the beginning of the protocol. The set up itself will\nbe from some distribution (X,Y ) (Alice gets X and Bob gets\nY ) which is different from the desired distribution (U, V ).\nThe fundamental question then is, which set ups (X,Y ) can\nbe used to securely sample which distributions (U, V ), and\nhow efficiently.\nWe restrict ourselves to the setting of honest-but-curious\nplayers. In this case, the requirements on a protocol Π for\nsecurely sampling (U, V ) given a set up (X,Y ) can be stated\nas follows, in terms of the outputs and the views of the parties\nfrom the protocol:4\n(ΠoutAlice(X,Y ),Π\nout\nBob(X,Y )) = (U, V )\nΠviewAlice(X,Y )↔ ΠoutAlice(X,Y )↔ ΠoutBob(X,Y )\nΠoutAlice(X,Y )↔ ΠoutBob(X,Y )↔ ΠviewBob (X,Y )\nThese three conditions correspond to correctness, security\nagainst a curious Alice and security against a curious Bob,\nrespectively.\nIn [13], we showed that the region RACI can be used as\na measure of cryptographic complexity of correlated random\nvariables (a smaller region RACI corresponding to a higher\ncomplexity), in that the rate at which a pair (U, V ) can be\nsecurely sampled given a set up (X,Y ) can be upperbounded\nby the ratio of their complexity measures. More formally, there\nwe presented the following result. (For completeness, a proof\nis provided in the appendix.)\nTheorem 4.1 ([13]): If n1 independent copies of a pair\nof correlated random variables (U, V ) can be securely\nrealized from n2 independent copies of a pair of cor-\nrelated random variables (X,Y ), then n1RACI(X,Y ) ⊆\nn2RACI(U, V ) (where multiplication by n refers to n-times\nrepeated Minkowski sum).\nIn [13] we gave an instance of pairs (U, V ) and (X,Y )\nsuch that the upperbound on the rate at which instances of\n(U, V ) can be securely sampled from instances of (X,Y )\nthat is implied by the above result strictly improved on the\nupperbounds that could be derived from previous results.\nThese pairs were contrived to highlight the shortcomings of\nprior work. Here we give yet another example where the\nupperbound from our result strictly improves on prior work,\nbut is further interesting for two reasons: firstly, the new\nexample is based on natural correlated random variables that\nare widely studied (namely, variants of oblivious transfer), and\nsecondly, the new upperbound we can prove actually matches\nan easy lowerbound and is therefore tight.\n4Here we state the conditions for “perfect security,” but our definitions\nand results generalize to the setting of “statistical security,” where a small\nstatistical error is allowed.\nA. A New Example\nWe now discuss the new example where our upperbound\nis not only strictly better than the previously best available\nupperbound, but is also tight.\nExample 4.1: Let SA,1, SA,2, SB,1, SB,2 ∈ {0, 1}L and\nCA, CB ∈ {1, 2} be six independent random variables all\nof which are uniformly distributed over their alphabets. Con-\nsider a pair of random variables X,Y defined as X =\n(CA, SA,1, SA,2, SB,CA) and Y = (CB , SB,1, SB,2, SA,CB ).\nNotice that these are in fact a pair of independent string-\noblivious transfers (string-OT’s) of string length L in opposite\ndirections. Let U, V be a pair of random variables whose joint\ndistribution is the same as that of X,Y , but with L = 1.\nIn other words, U, V are a pair of independent bit-OT’s in\nopposite directions. The goal is to characterize the efficiency\nwith which we may securely generate independent instances\nof U, V from independent instances of X,Y for L > 1. Here\nefficiency is the supremum of n2/n1 over secure sampling\nschemes which produce n2 independent copies of (U, V ) from\nn1 independent copies of (X,Y ).\nIt is easy to see that RACI(X,Y ) intersects the co-ordinate\naxes at (1 +L, 0, 0), (0, 1 +L, 0), and (0, 0, 2L). From, these\nwe can immediately obtain the upperbound of [17] on the\nefficiency, namely (1 + L)/2. Notice that this is dependent\non L and would suggest that (several) long string-OT pairs\ncan be turned into several (more) bit-OT pairs. However, as\nwe show below, the efficiency of conversion is just 1, i.e., the\nbest one can do is to turn each pair of string-OT’s into a pair\nof bit-OT’s.\nWe will show that inf{R1 + R2 : (R1, R2, 0) ∈\nRACI(U, V )} = 2. But, (1, 1, 0) ∈ RACI(X,Y ). This can\nbe seen by setting Q = (CA, CB , SA,CB , SB,CA) for which\n(R1, R2, RRD) = (1, 1, 0). Thus, inf{R1 +R2 : (R1, R2, 0) ∈\nRACI(X,Y )} ≤ 2. Hence, from Theorem 4.1, we may con-\nclude that the efficiency of conversion we are after is 1.\nIt only remains to characterize inf{R1 +R2 : (R1, R2, 0) ∈\nRACI(U, V )}. The following lemma, which is proved in the\nappendix, provides the required characterization.\nLemma 4.2:\ninf{R1 +R2 : (R1, R2, 0) ∈ RACI(U, V )} = 2.\nACKNOWLEDGEMENTS\nThe authors would like to gratefully acknowledge discus-\nsions with Venkat Anantharam, Pe´ter Ga´cs, and Young-Han\nKim. The example in Section IV-A is based on a suggestion\nby Ju¨rg Wullschleger.\nREFERENCES\n[1] D. Beaver, “Correlated pseudorandomness and the complexity of private\ncomputations,” in Proc. 28th STOC, pp. 479–488. ACM, 1996.\n[2] I. Csisza´r and R. Ahlswede, “On oblivious transfer capacity,” in Proc.\nInternational Symposium on Information Theory (ISIT), pp. 2061–2064,\n2007.\n[3] I. Csisza´r and J. Ko¨rner, Information Theory: Coding Theorems for\nDiscrete Memorless Systems, Akade´miai Kiado´, Budapest, 1981.\n[4] Y. Dodis and S. Micali, “Lower bounds for oblivious transfer reductions,”\nin Jacques Stern, editor, EUROCRYPT, vol. 1592 of Lecture Notes in\nComputer Science, pp. 42–55. Springer, 1999.\n[5] P. Ga´cs and J. Ko¨rner, “Common information is far less than mutual\ninformation,” Problems of Control and Information Theory, 2(2):119–\n162, 1973.\n[6] R. M. Gray and A. D. Wyner, “Source coding for a simple network,” Bell\nSystem Technical Journal, vol. 53, pp. 16811721, 1974.\n[7] H. Imai, K. Morozov, and A. C. A. Nascimento, “On the oblivious transfer\ncapacity of the erasure channel,” in Proc. International Symposium on\nInformation Theory (ISIT), pp. 1428–1431, 2006.\n[8] H. Imai, K. Morozov, and A. C. A. Nascimento, “Efficient oblivious\ntransfer protocols achieving a non-zero rate from any non-trivial noisy\ncorrelation,” in International Conference on Information Theoretic\nSecurity (ICITS), 2007.\n[9] H. Imai, K. Morozov, A. C. A. Nascimento, and A. Winter, “Efficient\nprotocols achieving the commitment capacity of noisy correlations,” in\nInternational Symposium on Information Theory (ISIT), pp. 1432–1436,\n2006.\n[10] H. Imai, J. Mu¨ller-Quade, A. C. A. Nascimento, and A. Winter,\n“Rates for bit commitment and coin tossing from noisy correlation,” in\nInternational Symposium on Information Theory (ISIT), pp. 45–, 2004.\n[11] S. Kamath and V. Anantharam, “A new dual to the Ga´cs-Ko¨rner common\ninformation defined via the Gray-Wyner system,” in Proc. 48th Allerton\nConf. on Communication, Control, and Computing, pp. 1340–1346, 2010.\n[12] D. Marco and M. Effros, “On lossless coding with coded side infor-\nmation,” IEEE Transactions on Information Theory, vol. 55, no. 7, pp.\n3284–3296, 2009.\n[13] V. M. Prabhakaran and M. Prabhakaran, “Assisted common informa-\ntion,” in International Symposium on Information Theory (ISIT), pp.\n2602-2606, 2010. Extended draft available at http://arxiv.org/abs/1002.\n1916.\n[14] S. Winkler and J. Wullschleger. “Statistical impossibility results\nfor oblivious transfer reductions,” Cryptology ePrint Archive, Report\n2009/508, 2009. http://eprint.iacr.org/.\n[15] A. Winter, A. C. A. Nascimento, and H. Imai. “Commitment capacity\nof discrete memoryless channels,” In Kenneth G. Paterson, editor, IMA\nInt. Conf., vol. 2898 of Lecture Notes in Computer Science, pp. 35–51.\nSpringer, 2003.\n[16] H. S. Witsenhausen, “On sequences of pairs of dependent random\nvariables,” SIAM Journal of Applied Mathematics, 28:100–113, 1975.\n[17] S. Wolf and J. Wullschleger. “New monotones and lower bounds in\nunconditional two-party computation,” IEEE Transactions on Information\nTheory, 54(6):2792–2797, 2008.\n[18] J. Wullschleger. Oblivious-Transfer Amplification. Ph.D. thesis, Swiss\nFederal Institute of Technology, Zu¨rich. http://arxiv.org/abs/cs/0608076.\n[19] A. D. Wyner, “The common information of two dependent random\nvariables,” IEEE Transactions on Information Theory, 21(2),163–179,\n1975.\nAPPENDIX\nProof of Proposition 2.1:\nGK common information CGK is defined as the supremum\nof the set of R such that for every \u000f > 0 there are maps\ng1 : Xn → Z, and g2 : Yn → Z for a sufficiently large n\nwhich satisfy\nPr (g1(X\nn) 6= g2(Y n)) ≤ \u000f, (18)\n1\nn\nH(g1(X\nn))) ≥ R− \u000f. (19)\nAn alternative defintion which allows for a genie with zero-\nrate links to the users is given below. It is easy to see that this\ncan only lead to a larger value. But as we will show, the\ndefinitions are in fact equivalent.\nLet C′GK be the supremum of the set of R such that for\nevery \u000f > 0 there are maps fk : Xn × Yn → {1, . . . , 2n\u000f},\n(k = 1, 2), g1 : Xn × {1, . . . , 2n\u000f} → Z, and g2 : Yn ×\n{1, . . . , 2n\u000f} → Z for a sufficiently large n which satisfy (1)\nand\n1\nn\nH(g1(X\nn, f1(X\nn, Y n))) ≥ R− \u000f.\nI(Y ;U |X) = I(X,Y ;U)− I(X;U) = H(X|U) + I(X,Y ;U)−H(X), (20)\nI(X;U |Y ) = I(X,Y ;U)− I(Y ;U) = H(Y |U) + I(X,Y ;U)−H(Y ), and (21)\nI(X;Y |U) = H(X|U) +H(Y |U)−H(X,Y |U) = H(X|U) +H(Y |U) + I(X,Y ;U)−H(X,Y ). (22)\nClearly, C′GK ≥ CGK. We first show\nC′GK = I(X;Y )−RRD-0. (20)\nLet U = g1(Xn(f1(Xn, Y n))). Then\nI(Xn;Y n|U) +H(U)\n= I(Xn;Y n|U) + I(Xn, Y n;U)\n= H(Xn, Y n)−H(Xn|Y n, U)−H(Y n|Xn, U)\n= I(Xn;Y n) + I(Xn;U |Y n) + I(Y n;U |Xn)\n≥ nI(X;Y ).\nTherefore, if the maps satisfy (2), then\nH(U) ≥ nI(X;Y )− I(Xn;Y n|U)\n≥ nI(X;Y )− n(RRD + \u000f)\n= n(I(X;Y )−RRD − \u000f)\nwhich implies (20).\nWith CGK replaced by C′GK, we can prove (4)-(5) as follows:\n(4) follows from Proposition 2.2; (4) and (3) imply (5).\nSee [13, section II.B] for a proof from (5) of the explicit\ncharacterization stated at the end of the proposition. Since\nthis explicit form can be achieved without any communication\nfrom the genie, it follows that C′GK = CGK.\nProof of Theorem 3.1:\nIt is easy to prove the above theorem from the single-letter\nexpressions for the regions in propositions 2.2 and 2.3 by\nmaking use of the mutual information equalities (20)-(22) at\nthe top of the page.\nProof of Corollary 3.2:\nsup{RC : RA +RC = H(X),\nRB +RC = H(Y ), (RA, RB, RC) ∈ RGW}\n(a)\n= sup{R : (0, 0, I(X;Y )−R) ∈ R′GW}\n(b)\n= sup{R : (0, 0, I(X;Y )−R) ∈ RACI},\nwhere (a) follows from the definition R′GW = f(RGW). The\n≤ direction of (b) follows directly from Theorem 3.1. But <\ncannot hold since if (0, 0, I(X;Y )−R) ∈ RACI, then there is\na R′ ≥ R such that (0, 0, I(X;Y )−R′) ∈ R′GW. Finally, (c)\nfollows from Proposition 2.1.\nTo arrive at the alternative form, we verify the equivalence\nof the two forms.\n{R : R ≤ I(X;Y ), {RC = R} ∩ LGW ⊆ RGW}\n= {RC : RA +RC = H(X),\nRB +RC = H(Y ), (RA, RB, RC) ∈ RGW}.\n⊆: if R ≤ I(X;Y ), then (H(X)−R,H(Y )−R,R) ∈ {RC =\nR} ∩ LGW.\n⊇: Let s = (H(X)−RC, H(Y )−RC, RC) ∈ RGW. Then (a)\nRC ≤ I(X;Y ) since s ∈ LGW, and (b) if s′ = (rA, rB, RC) ∈\nLGW, then since rA ≥ H(X)−RC and rB ≥ H(Y )−RC, we\nhave s′ ≥ s (component-wise) which implies that s′ ∈ RGW\nfrom the definition of the GW system.\nProof of Corollary 3.3:\nCWyner = inf{RC : (RA, RB, RC) ∈ RGW,\nRA +RB +RC = H(X,Y )}\n(a)\n= inf{R1 +R2 + I(X;Y ) : (R1, R2, 0) ∈ R′GW}\n(b)\n= inf{R1 +R2 + I(X;Y ) : (R1, R2, 0) ∈ RACI},\nwhere (a) follows from the definition R′GW = f(RGW); (b)\nfollows from Theorem 3.1: ≥ direction follows directly from\nthe theorem. But > cannot hold, since by the theorem, if\n(R1, R2, 0) ∈ RACI then there exists (R′1, R′2, 0) ∈ R′GW such\nthat R′1 ≤ R1 and R′2 ≤ R2.\nProof of Corollary 3.4:\nG(Y → X)\n= inf{RC : (H(X|Y ), H(Y )−RC, RC) ∈ RGW},\n(a)\n= inf{R : (R− I(X;Y ), 0, 0) ∈ R′GW}\n(b)\n= inf{R : (R− I(X;Y ), 0, 0) ∈ RACI}\n(c)\n= I(X;Y ) +R1−0,\nwhere (a) follows from R′GW = f(RGW). (b) is a consequence\nof Theorem 3.1: And (c) follows from the definition of R1−0.\nSimilarly we get (15). The equality (16) is proved in [11]\nwhich along with (14)-(15) implies (17).\nProof of Theorem 4.1: The theorem is in fact corollary\n3.2 of [13] which follows immediately from Theorem 3.1 of\n[13] and the following lemma:\nLemma A.1: Let the pair of random variables (X1, Y1) be\nindependent of the pair (X2, Y2). If X = (X1, X2) and Y =\n(Y1, Y2), then\nRACI(X,Y ) = RACI(X1, Y1) +RACI(X2, Y2).\nFor completeness, we give a proof of Theorem 3.1 of [13]\nbelow since the proof was not provided there. This also\ncontains a proof of Lemma A.1 (see (d) below). Please refer\n[13] for notation and a statement of the theorem being proved\nbelow.\nWe show that under each step of a secure protocol, RACI\ncan only grow.\n(a) Local computation cannot shrink it: For all random vari-\nables with X−Y −Z, we have RACI(X,Y Z) ⊇ RACI(X,Y )\nand RACI(XY ,Z) ⊇ RACI(X,Y ).\nThe first set inclusion follows from the fact that for the joint\np.m.f. pX,Y,Z,Q = pX,Y pZ|Y pQ|X,Y\nI(X;Y,Z|Q) = I(X;Y |Q)\nI(Q;Y, Z|X) = I(Q;Y |X)\nI(X;Q|Y, Z) = I(X;Q|Y ).\n(b) Communication cannot shrink it: For all random vari-\nables (X,Y ) and functions f over the support of X (resp,\nY ), we have RACI(X, (Y, f(X))) ⊇ RACI(X,Y ) (resp,\nRACI((X, f(Y )), Y ) ⊇ RACI(X,Y )).\nThe first set inclusion follows from the following facts for\nthe joint p.m.f pX,Y,Z,Q = pX,Y pZ|Y pQ|X,Y :\nI(X;Y, f(X)|Q, f(X)) = I(X;Y |Q, f(X))\n≤ I(X;Y |Q)\nI(X;Q, f(X)|Y, f(X)) = I(X;Q|Y, f(X))\n≤ I(X;Q|Y )\nI(Y ;Q, f(X)|X) = I(Y ;Q|X)\n(c) Securely derived outputs do not have a smaller region:\nFor all random variables X,U, V, Y such that X−U −V and\nU − V − Y , we have RACI(U, V ) ⊇ RACI((X,U), (Y, V )).\nThis follows from the following facts for (dependent) ran-\ndom variables X,Y, U, V,Q which satisfy the Markov chains\nX − U − V and U − V − Y :\nI(X,U ;Y, V |Q) ≥ I(U ;V |Q),\nI(X,U ;Q|Y, V ) = I(X,U ;Q,Y |V )− I(X,U ;Y |V )\n(a)\n= (I(U ;Q,Y |V ) + I(X;Q,Y |U, V ))\n− I(X;Y |U, V )\n≥ I(U ;Q|V ),\nand similarly\nI(Y, V ;Q|X,U) ≥ I(V ;Q|U),\nwhere we used U − V − Y to obtain equality (a).\n(d) Regions of independent pairs add up: If (X,Y ) is\nindependent of (U, V ), we have RACI((X,U), (Y, V )) =\nRACI(X,Y ) +RACI(U, V ). This follows easily from the fol-\nlowing facts:\nFor the joint p.m.f. pX,Y pU,V pQ1|X,Y pQ2|U,V , we have\nI(X,U ;Y, V |Q1, Q2) = I(X;Y |Q1) + I(U, V |Q2)\nI(X,U ;Q1, Q2|Y, V ) = I(X;Q1|Y ) + I(U ;Q2|V )\nI(Y, V ;Q1, Q2|X,U) = I(Y ;Q1|X) + I(V ;Q2|U)\nAnd, for the joint p.m.f. pX,Y pU,V pQ|X,Y,U,V , we have\nI(X,U ;Y, V |Q) ≥ I(X;Y |Q) + I(U ;V |Q)\nI(X,U ;Q|Y, V ) ≥ I(X;Q|Y ) + I(U ;Q|V )\nI(Y, V ;Q|X,U) ≥ I(Y ;Q|X) + I(V ;Q|U)\nProof of Lemma 4.2:\nBy Lemma A.1, we need only characterize the inf{R1+R2 :\n(R1, R2, 0) ∈ RACI} of one of the pair of independent\nbit-OT’s. Let us denote one bit-OT by A,B: where A =\n(S1, S2) ∈ {0, 1}2 uniformly distributed over its alphabet and\nB = (C, SC), where C ∈ {1, 2} is independent of A and\nuniformly distrbuted over its alphabet. By Proposition 2.2,\ninf{R1 +R2 : (R1, R2, 0) ∈ RACI(A,B)}\n= inf\npQ|A,B∈P:I(A;B|Q)=0\nI(B;Q|A) + I(A;Q|B)\n= H(A|B) +H(B|A)\n− sup\npQ|A,B∈P:I(A;B|Q)=0\nH(A|Q,B) +H(B|Q,A).\nWe show below that the sup term is 1. Since H(A|B) +\nH(B|A) = 2, this will allow us to conclude that the smallest\nsum-rate of RRD(0) of A,B is 1. Invoking the lemma above,\nthe corresponding smallest sum-rate for U, V is then 2 as\nrequired.\nTo show that the sup term is 1, notice that the only valid\nchoices of pQ|A,B are such that I(A;B|Q) = 0. This means\nthat the resulting pA,B|Q(., .|q) must belong to one of eight\npossible classes shown in Figure 3b (for any q with non-zero\nprobability pQ(q); we may assume that all q’s have non-zero\nprobability without loss of generality). Recall that there is a\ncardinality bound on Q; let us denote the alphabet of Q by\n{q1, q2, . . . , qN}, where N is the cardinality bound.\nWe will first show that there is no loss of generality in\nassuming that no more than one of the qi’s is such that its\npA,B|Q(., .|qi) belongs to the same class (and hence we may\ntake N = 8). Suppose, q1 and q2 belong to the same class,\nsay class 1, with parameters p1 and p2 respectively. Then, if\nwe denote the binary entropy function by H2(.), we have\nH(A|Q,B) +H(B|Q,A)\n=\nN∑\nk=1\npQ(qk) (H(A|B,Q = qk) +H(B|A,Q = qk))\n= pQ(q1)H2(p1) + pQ(q2)H2(p2)\n+\nN∑\nk=3\npQ(qk) (H(A|B,Q = qk) +H(B|A,Q = qk))\n≤ (pQ(q1) + pQ(q2))H2\n(\npQ(q1)p1 + pQ(q2)p2\npQ(q1) + pQ(q2)\n)\n+\nN∑\nk=3\npQ(qk) (H(A|B,Q = qk) +H(B|A,Q = qk)) ,\nwhere the inequality (Jensen’s) follows from the concavity\nof the binary entropy function. Thus, we can define a Q′ of\nalphabet size N − 1 where letters q1, q2 are replaced by q0\nsuch that pQ′(q0) = pQ(q1) + pQ(q2), and pA,B|Q′=q0 is in\nclass 1 with parameter pQ(q1)p1+pQ(q2)p2pQ(q1)+pQ(q2) , while maintaining\nfor i = 3, . . . , N , pQ′(qi) = pQ(qi) and pA,B|Q′(a, b|qi) =\npA,B|Q(a, b|qi). (It is easy to verify (a) that this gives a valid\n\u0002\u0001\n1/8\n00\n01\n11\n10\n10\n21\n11\n20\n(a)\np1\np2\np3\np4\np5 1 - p6\np7\np8\n1 - p1\n1 - p2\n1 - p4\n1 - p3\n1 - p5\np6 1 - p7\n1 - p8\npAB|Q(.,.|qi) pAB|Q(.,.|qii) pAB|Q(.,.|qiii) pAB|Q(.,.|qiv)\npAB|Q(.,.|qv) pAB|Q(.,.|qvi) pAB|Q(.,.|qvii) pAB|Q(.,.|qviii)\n(b)\nFig. 3: (a) Joint p.m.f. of A,B. Each solid line represents a probablity mass of 1/8. (b) Eight possible classes that pA,B|Q(., .|q)\nmay belong to for a pQ|A,B which satisfies I(A;B|Q) = 0.\njoint p.m.f. for pA,B,Q′ , (b) that the induced pA,B is the same\nas the original, and (c) that the induced pQ′|A,B satisfies the\ncondition I(A;B|Q′) = 0.) Then, the above inequality states\nthat\nH(A|Q,B) +H(B|Q,A) ≤ H(A,Q′, B) +H(B|Q′, A)\nproving our claim.\nThus, without loss of generality, we may assume that N = 8\nand pA,B|Q(., .|qi) belongs to class i. Notice that\npQ|A,B(q1|00, 10) + pQ|A,B(q5|00, 10) = 1,\npQ|A,B(q2|01, 10) + pQ|A,B(q5|01, 10) = 1,\npQ|A,B(q2|01, 21) + pQ|A,B(q6|01, 21) = 1,\npQ|A,B(q3|11, 21) + pQ|A,B(q6|11, 21) = 1,\npQ|A,B(q3, 11, 11) + pQ|A,B(q7|11, 11) = 1,\npQ|A,B(q4|10, 11) + pQ|A,B(q7|10, 11) = 1,\npQ|A,B(q4|10, 20) + pQ|A,B(q8|10, 20) = 1,\npQ|A,B(q1|00, 20) + pQ|A,B(q8|00, 20) = 1.\nLet us define\np˜1\n4\n= pQ|A,B(q1|00, 10), p˜5 4= pQ|A,B(q5|01, 10),\np˜2\n4\n= pQ|A,B(q2|01, 21), p˜6 4= pQ|A,B(q6|11, 21),\np˜3\n4\n= pQ|A,B(q3|11, 11), p˜7 4= pQ|A,B(q7|10, 11),\np˜4\n4\n= pQ|A,B(q4|10, 20), p˜8 4= pQ|A,B(q8|00, 20).\nLet us evaluate H(B|Q,A) in terms of the above parame-\nters. Notice that H(B|Q = qi, A) = 0 for i = 5, . . . , 8. Hence\nH(B|Q,A)\n=\n∑\n(q,a)∈{(1,00),(2,01),\n(3,11),(4,10)}\npQ,A(q, a)H(B|Q = q,A = a)\n=\np˜1 + (1− p˜8)\n8\nH2\n(\np˜1\np˜1 + (1− p˜8)\n)\n+\np˜2 + (1− p˜5)\n8\nH2\n(\np˜2\np˜2 + (1− p˜5)\n)\n+\np˜3 + (1− p˜6)\n8\nH2\n(\np˜3\np˜3 + (1− p˜6)\n)\n+\np˜4 + (1− p˜7)\n8\nH2\n(\np˜4\np˜4 + (1− p˜7)\n)\n≤ 4 +\n∑4\ni=1 p˜i −\n∑8\nj=5 p˜j\n8\n,\nwhere the inequality follows from the fact that binary entropy\nfunction is upperbounded by 1. Similary, we can get\nH(A|Q,B) ≤ 4 +\n∑8\nj=5 p˜j −\n∑4\ni=1 p˜i\n8\n.\nCombining, we obtain the desired\nH(B|Q,A) +H(A|Q,B) ≤ 1.\n",
            "id": 760223,
            "identifiers": [
                {
                    "identifier": "186876656",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "1105.6163",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "2198747",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:citeseerx.psu:10.1.1.767.5178",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "oai:arxiv.org:1105.6163",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "10.1109/isit.2011.6034098",
                    "type": "DOI"
                },
                {
                    "identifier": "104477499",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2142437065",
                    "type": "MAG_ID"
                }
            ],
            "title": "Assisted Common Information: Further Results",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2142437065",
            "oaiIds": [
                "oai:arxiv.org:1105.6163",
                "oai:citeseerx.psu:10.1.1.767.5178"
            ],
            "publishedDate": "2011-01-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1105.6163",
                "http://arxiv.org/pdf/1105.6163.pdf"
            ],
            "updatedDate": "2021-07-22T01:01:09",
            "yearPublished": 2011,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1105.6163"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/760223"
                }
            ]
        },
        {
            "acceptedDate": "2009-03-17T00:00:00",
            "arxivId": "0904.1110",
            "authors": [
                {
                    "name": "A.C. Yao"
                },
                {
                    "name": "B. Blanchet"
                },
                {
                    "name": "B. Blanchet"
                },
                {
                    "name": "D. Nowak"
                },
                {
                    "name": "D.E. Knuth"
                },
                {
                    "name": "G. Gonthier"
                },
                {
                    "name": "L. Blum"
                },
                {
                    "name": "N. Ramsey"
                },
                {
                    "name": "P. Lafourcade"
                },
                {
                    "name": "R. Affeldt"
                },
                {
                    "name": "S. Goldwasser"
                },
                {
                    "name": "U.V. Vazirani"
                }
            ],
            "contributors": [
                "Pil Joong",
                "David"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/186812205"
            ],
            "createdDate": "2012-04-13T14:17:05",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2009-01-01T00:00:00",
            "abstract": "Cryptographic primitives are fundamental for information security: they are\nused as basic components for cryptographic protocols or public-key\ncryptosystems. In many cases, their security proofs consist in showing that\nthey are reducible to computationally hard problems. Those reductions can be\nsubtle and tedious, and thus not easily checkable. On top of the proof\nassistant Coq, we had implemented in previous work a toolbox for writing and\nchecking game-based security proofs of cryptographic primitives. In this paper\nwe describe its extension with number-theoretic capabilities so that it is now\npossible to write and check arithmetic-based cryptographic primitives in our\ntoolbox. We illustrate our work by machine checking the game-based proofs of\nunpredictability of the pseudo-random bit generator of Blum, Blum and Shub, and\nsemantic security of the public-key cryptographic scheme of Goldwasser and\nMicali.Comment: 13 page",
            "documentType": "research",
            "doi": "10.1007/978-3-642-00730-9_23",
            "downloadUrl": "http://arxiv.org/abs/0904.1110",
            "fieldOfStudy": "computer science",
            "fullText": "ar\nX\niv\n:0\n90\n4.\n11\n10\nv1\n  [\ncs\n.C\nR]\n  7\n A\npr\n 20\n09\nOn Formal Verification of\nArithmetic-Based Cryptographic Primitives⋆\nDavid Nowak\nResearch Center for Information Security, AIST, Japan\nAbstract. Cryptographic primitives are fundamental for information security: they are used as\nbasic components for cryptographic protocols or public-key cryptosystems. In many cases, their\nsecurity proofs consist in showing that they are reducible to computationally hard problems. Those\nreductions can be subtle and tedious, and thus not easily checkable. On top of the proof assis-\ntant Coq, we had implemented in previous work a toolbox for writing and checking game-based\nsecurity proofs of cryptographic primitives. In this paper we describe its extension with number-\ntheoretic capabilities so that it is now possible to write and check arithmetic-based cryptographic\nprimitives in our toolbox. We illustrate our work by machine checking the game-based proofs of\nunpredictability of the pseudo-random bit generator of Blum, Blum and Shub, and semantic secu-\nrity of the public-key cryptographic scheme of Goldwasser and Micali.\nKeywords: machine formalization, cryptographic primitives, CSPRBG, semantic security\n1 Introduction\nCryptographic primitives are fundamental components for information security. In many cases,\ntheir security proofs consist in showing that they are reducible to computationally hard prob-\nlems. Those reductions can be subtle and tedious, and thus not easily checkable. Bellare and\nRogaway even claim in [4] that:\n“many proofs in cryptography have become essentially unverifiable. Our field may be\napproaching a crisis of rigor.”\nAs a remedy, they, and also Shoup [16], advocate game-based security proofs. This is a method-\nology for writing proofs which makes them easier to read and check. Halevi goes further by\nadvocating the need for a software which can deal with the mundane parts of writing and\nchecking game-based proofs [10].\nIn the game-based approach, a security property is modeled as a probabilistic program which\nimplements a game to be solved by the attacker. The attacker itself is modeled as an external\nprobabilistic procedure interfaced with the game. The goal is then to prove that any attacker\nhas at most a negligible advantage over a random player. An attacker is assumed to be efficient\ni.e., it is modeled as a probabilistic polynomial-time (PPT) algorithm.\nRelated Work. There are tools such as ProVerif [5], CryptoVerif [6] or the prototype implementa-\ntion of [12] which can make automatic proofs of cryptographic protocols or generic cryptographic\nschemes. However those tools assume that some secure cryptographic primitives are given. The\nsecurity of those primitives cannot be proved automatically. Nevertheless their security proofs\ncan be checked by a computer.\nThe game-based proof of the PRP/PRF switching lemma has been formalized in the proof\nassistant Coq [1]. Although it is not by itself a cryptographic primitive, this lemma is fundamen-\ntal in proving security of some cryptographic schemes. The proof has been made in the random\n⋆ This paper is an extended version of [14].\noracle model. The machine formalization in [1] is a so-called deep embedding: games are syn-\ntactic objects; and game transformations are syntactic manipulations which can be automated\nin the language of the proof assistant. The main advantages of this approach are that one can\nprove completeness of decision procedures, if any, and get smaller proof terms. However those\nadvantages are not exploited in [1]. Moreover this is at the cost of developing a huge machinery\nfor syntactic manipulations. Two other deep embeddings are currently being developed [2,3].\nPrevious Work. In cryptographers’ papers, the formal semantics of games is either left implicit\nor, at best, informally explained in English. It is not enough for machine formalization. In\nprevious work, we have (1) proposed a formal semantics for games, (2) implemented it in\nthe proof assistant Coq, and (3) used it to prove the semantic security of the ElGamal and\nHashed ElGamal public-key cryptographic schemes [13]. Our machine formalization is a so-\ncalled shallow embedding: games are probability distributions (as advocated by Shoup [16]).\nGame transformations can still be automated by going through the metalanguage of the proof\nassistant. Compared to [1], We have been very careful in making our design choices such that our\nimplementation remains light. This is an important design issue in formal verification because\nformal proofs grow quickly in size when one tackles real-world use-cases.\nOur toolbox comes in two layers. The first layer extends the standard library of Coq with\nmathematical notions and their properties that are fundamental in cryptography but not avail-\nable in the standard library of Coq. This consists of a library for probability distributions,\nbitstrings, and a small library for elementary group theory. On top of this, the second layer\nconsists of formal versions of security definitions and hard problems, and basic game trans-\nformations which can be composed to reduce the security of cryptographic primitives to hard\nproblems.\nBy using our toolbox, one is forced to exhibit all the steps in his or her game-based proof\nand thus cannot hide assumptions or make proofs by intimidation such as “Trivial” or “The\nreader may easily supply the details”. In spite of the required level of detail, proofs remain\nhuman readable and human checkable.\nOur contributions. We have extended our toolbox in order to be able to deal with cryptographic\nprimitives based on number theory. For that purpose we have added to the first layer a library\nof definitions and lemmas for integers modulo n. In particular we have formalized the notions of\nLegendre and Jacobi symbol, Blum primes, and their properties which are of fundamental use\nin cryptography. This is already by itself a contribution to the theorem proving community.1\nWe have also considerably extended and generalized our library on elementary group theory.2\nThen we have used our extension to the first layer in adding to the second layer the security\nnotion of unpredictability, the quadratic residuosity assumption, and number-theoretic game\ntransformations.\nFinally, we have used our extensions to machine check the proof of unpredictability of the\npseudo-random bit generator of Blum, Blum and Shub [7]. We have also machine checked the\nproof of semantic security of the public-key cryptographic scheme of Goldwasser and Micali\n[8]. Our security proofs of those two primitives are based on the intractability of the quadratic\nresiduosity problem. To the best of our knowledge, this is the first time that security proofs\nof cryptographic primitives based on number theory are machine checked. This is also the first\ntime that a proof of unpredictability is machine checked. None of the above mentioned related\n1 Tools such as Mathematica can deal with formal computations involving Legendre and Jacobi symbols, but\ncannot be used to make formal proofs. In particular they do not allow reasoning by induction.\n2 There are more advanced library on group theory, such as [9], but none of them are available with the version\nof Coq that we are using (8.2beta3).\n2\nwork could be used in their current state to formalize such proofs because they are missing\ncomponents for number theory.\nOutline. We introduce the proof assistant Coq in Sect. 2. In Sect. 3, we give formal meaning\nto games in terms of distributions. We give in Sect. 4 the number-theoretic facts that we use\nSect. 5. In Sect. 5.1 we apply our work to the proof of unpredictability of the Blum-Blum-Shub\ngenerator, and in Sect. 5.2 we apply it to the proof of semantic security of the Goldwasser-Micali\nscheme. Finally, we briefly describe our implementation in Sect. 6 before concluding in Sect. 7.\n2 The Coq proof assistant\nCoq is a proof assistant developed at INRIA since 1984.3 It is based on a kernel which takes\na mathematical statement S and proof term p as input and check whether p is a correct proof\nof S. On top of this kernel there are: a tactic language which allows to build proof terms in an\nincremental way; and decision procedures for decidable fragments such as Presburger arithmetic\nor propositional logic.\nCoq is goal-directed. This means that if we are trying to prove that a formula Q (the goal)\nis true, and we have an already proved theorem stating that P1 & P2 implies Q, then we can\napply this theorem. Coq will replace the goal Q by two subgoals P1 and P2. Proofs by induction\nare also possible. We proceed this way until we finally reach goals that are either axioms or are\ntrue by definition. On the way, Coq builds incrementally a proof term to be checked later by\nthe kernel.\nThe kernel is the only critical part: if a bug outside the kernel causes a wrong proof term to\nbe built, it will be rejected by the kernel.\nIn order to be closer to mathematical practice, Coq also provides mechanisms for introducing\nnotations, or for inferring implicit parameters and subset coercions. It also comes with a standard\nlibrary of definitions and lemmas, for instance on elementary arithmetic, analysis or polymorphic\nlists.\n3 Games\nWe denote a game by a finite probability distribution (from now on, we will abbreviate this\nterm as distribution). A distribution δ over a set S is defined as a finite multiset4 of ordered\npairs from S × R such that ∑(a,p)∈δ p = 1. We use the symbols {| and |} as delimiters of\nmutisets not to confuse them with sets. We write p · {|(a1, p1), . . . , (an, pn)|} for the multiset\n{|(a1, p · p1), . . . , (an, p · pn)|}.\nFor convenience, we introduce some notations (cf. Fig. 1) for writing distributions so that\nthey will look like probabilistic programs:\n– We write return a for the distribution with only the element a of probability 1.\n– We write x⇐ δ; ϕ(x) for the distribution built by picking at random a value x according\nto the distribution δ and then computing the distribution ϕ(x).\n– We write x\nR← {a1, . . . , an}; ϕ(x) for the distribution built by picking at random a value x\nin the set {a1, . . . , an} and then computing the distribution ϕ(x).\n– We abbreviate this last case by x← a; ϕ(x) when the set is a singleton {a}.\n3 http://coq.inria.fr/\n4 A multiset (a.k.a. a bag) is a generalization of a set: a member of a multiset may be member more that once.\nFor example, the multisets {|1, 2, 2|} and {|1, 2|} are different; and the union of {|1, 2, 2, 3|} and {|1, 4, 4|} is equal\nto {|1, 1, 2, 2, 3, 4, 4|}.\n3\nreturn a = {|(a, 1)|}\nx⇐ δ;\nϕ(x)\n=\n[\n(a,p)∈δ\np · ϕ(a)\nx\nR\n← {a1, . . . , an};\nϕ(x)\n=\nx⇐ {|(a1,\n1\nn\n), . . . , (an,\n1\nn\n)|};\nϕ(x)\nx← a;\nϕ(x)\n= x\nR\n← {a};\nϕ(x)\nFig. 1. Notations for distributions\nx← a;\nϕ(x)\n= ϕ(a)\nx⇐ δ;\nreturn x\n= δ\ny ⇐ (\nx⇐ δ;\nϕ(x)\n);\nψ(y)\n=\nx⇐ δ;\ny ⇐ ϕ(x);\nψ(y)\nFig. 2. Monad laws\nDistributions have a monadic structure [15] and thus satisfy the monad laws (cf. Fig. 2).\nThose laws state that our notations for distributions behave well. The first one simply states\nthat the occurences of a deterministically assigned variable x can be replaced by their definition\n(constant propagation). The second one is a kind of η-reduction. The third one allows to simplify\nnested sequences.\nWe write Pr\nP (δ) for the probability that P holds of an element picked at random in\nthe distribution δ. Its value is ∑\n(a,p)∈δ s.t. P (a)\np\nWe define a notion of indistinguishability for distributions. Security definitions, hard problems\nand game transformations will all be defined with this relation. Two distributions δ1 and δ2 are\nindistinguishable modulo ǫ w.r.t. a predicate P , written δ1 ≡Pǫ δ2, iff:∣∣∣PrP (δ1)\n − PrP (δ2)\n∣∣∣ ≤ ǫ\n≡Pǫ is reflexive and symmetric. If δ1 ≡Pǫ δ2 and δ2 ≡Pǫ′ δ3, then δ1 ≡Pǫ+ǫ′ δ3. If δ1 ≡Pǫ δ2 and\nǫ ≤ ǫ′, then δ1 ≡Pǫ′ δ2. We write ≡ǫ instead of ≡Pǫ when P is the predicate on booleans such\nthat P (b) holds iff b is equal to true.\nLemma 3.1. If f : S → T is a bijection then, for all ϕ and P ,\nx\nR← S;\nϕ(f(x))\n≡P0 x\nR← T ;\nϕ(x)\nIt is also true when f is a surjective N -to-one function.\nThis kind of transformation of one game into another one is at the crux of the security proofs\nwe are dealing with.\n4 Some elementary number theory\nLet n be a positive number. We write Zn for the set of integers modulo n. The multiplicative\ngroup of Zn is written Z\n∗\nn and consists of the subset of integers modulo n which are coprime\n4\nwith n. An integer x ∈ Z∗n is a quadratic residue modulo n iff there exists a y ∈ Z∗n such that\ny2 ≡ x (mod n). Such a y is called a square root of x modulo n. We write QRn for the set\nof quadratic residues modulo n, and QNRn for its complement i.e., the set of quadratic non-\nresidues modulo n. We write Z∗n(+1) (respectively, QNRn(+1)) for the subset of integers in Z\n∗\nn\n(respectively, QNRn) with Jacobi symbol equal to 1.\nThe quadratic residuosity problem is the following: given an odd composite integer n, decide\nwhether or not an x ∈ Z∗n is a quadratic residue modulo n.\nLet n be the product of two distinct odd primes p and q. The quadratic residuosity assump-\ntion (QRA) states that the above problem is intractable. In our framework, this can be stated\nas:\nAssumption 4.1 (QRA). For every attacker A′, there exists a negligible ǫ such that\nx\nR← Z∗n(+1);\nb̂⇐ A′(n, x);\nb← b̂ = qr(x);\nreturn b\n≡ǫ b\nR← {true, false};\nreturn b\nIn the left-side game, an x is picked at random in the set Z∗n(+1); this x is passed with n to the\nattacker A′; the attacker returns its guess b̂ for the quadratic residuosity (modulo n) of x; this\nguess is compared with the true quadratic residuosity (modulo n) qr(x) of x; and the result b\nof this comparison is returned. In the rigth-side game, the result is random. QRA states that\nthe advantage ǫ of any attacker over a random player is negligible.\nNote that the fact that A′ is a randomized algorithm is modeled by the attacker returning\na distribution in which b̂ is picked.\nIn the security proofs, we will need the following well-known mathematical facts (remember\nthat n is the product of two distinct odd primes p and q):\nFact I. The function which maps an x ∈ Z∗n to x2 ∈ QRn is a surjective four-to-one function.\nFact II. For any y ∈ QNRn(+1), the function which maps an x ∈ QRn to y · x ∈ QNRn(+1)\nis a bijection.\nFact III. |QRn| = |QNRn(+1)|\nFact IV. Z∗n(+1) = QRn ∪QNRn(+1)\nLet n be a Blum integer i.e., the product of two distinct prime numbers p and q, each\ncongruent to 3 modulo 4. In this case, any x ∈ QRn has a unique square root in QRn which we\ndenote by\n√\nx and is called the principal square root of x. And we get the following additional\nfacts [7]:\nFact V. The function which maps an x ∈ QRn to x2 ∈ QRn is a permutation.\nFact VI. The function which maps an x ∈ Z∗n(+1) to x2 ∈ QRn is a surjective two-to-one\nfunction.\nFact VII. For all x ∈ QRn,\n√\nx2 = x\nFact VIII. For all x ∈ Z∗n(+1), x ∈ QRn ⇔ parity(x) = parity(\n√\nx2)\n5 Applications\nIn this section, we apply our work to the proofs of unpredictability of the Blum-Blum-Shub\ngenerator, and to the proof of semantic security of the Goldwasser-Micali scheme.\n5\nbbs(len ∈ N, seed ∈ Z∗n) =\nbbs rec(len, seed2)\nbbs rec(len ∈ N, x ∈ QRn) =\nmatch len with\n|0⇒ []\n|len′ + 1⇒ parity(x) :: bbs rec(len′, x2)\nend\nFig. 3. The Blum-Blum-Shub generator\n5.1 The Blum-Blum-Shub pseudorandom bit generator\nThe security of many cryptographic systems depends upon a cryptographic primitive for the\ngeneration of unpredictable sequences of bits. They are used to generate keys, nonces or salts.\nIdeally, those sequences of bits should be random, that is, generated by successive flips of a fair\ncoin. In practice, one uses a pseudorandom bit generator (PRBG) which, given a short seed,\ngenerates a long sequence of bits that appears random. For the purpose of simulation, one only\nrequires of a PRBG that it passes certain statistical tests (cf. Chapter 3 of [11]). This is not\nenough for cryptography. A PRBG is cryptographically secure iff it passes all polynomial-time\nstatistical tests: roughly speaking, no polynomial-time algorithm can distinguish between an\noutput sequence of the generator and a truly random sequence.\nIn [7], the Blum-Blum-Shub generator (BBS) is proved left-unpredictable (under the quadratic\nresiduosity assumption). It was proved by Yao in [18] that this is equivalent to stating that BBS\npasses all polynomial-time statistical tests. It is shown in [17] that BBS is still secure under the\nweaker assumption that n is hard to factorize. The same authors also show that, for sufficiently\nlarge n, more than one bit can be extracted at each iteration of the algorithm. However, in this\npaper, we stick to the original proof of [7].\nLet n = p · q be a Blum integer. The BBS generator is defined by the function bbs given in\nFig. 3 which takes as input a length and a seed, and returns a pseudorandom sequence of bits\nof the required length.\nIn our framework, one can state the left-unpredictability of bbs by the following definition.\nDefinition 5.1 (Left-unpredictability). bbs is left-unpredictable iff for all length len, for\nevery attacker A, there exists a negligible ǫ such that\nseed\nR← Z∗n;\n[b0, . . . , blen]← bbs(len + 1, seed);\nb̂0 ⇐ A([b1, . . . , blen]);\nb← b̂0 = b0;\nreturn b\n≡ǫ b\nR← {true, false};\nreturn b\nIn the left-side game, a seed is picked at random in the set Z∗n; the function bbs is then used to\ncompute a pseudorandom sequence of bits [b0, . . . , blen] of length len + 1; this sequence minus\nits first bit b0 is passed to the attacker A; the attacker returns its guess b̂0 for the value of the\nbit b0; this guess is compared with b0; and the result b of this comparison is returned. bbs is\nleft-unpredictable if the advantage ǫ of any attacker over a random player is negligible.\nBefore proving that bbs is left-unpredictable, we show that it can be reduced to the problem\nof finding the parity of a random quadratic residue modulo n.\nLemma 5.2. If, for every attacker A′, there exists a negligible ǫ such that\nx\nR← QRn;\nb̂⇐ A′(n, x);\nb← b̂ = parity(√x);\nreturn b\n≡ǫ b\nR← {true, false};\nreturn b\n6\nthen bbs is left-unpredictable.\nIn the left-side game, x is picked at random in the set QRn; this x is passed with n to\nthe attacker A′; the attacker returns its guess b̂ for the parity of\n√\nx; this guess is compared\nwith the true parity of\n√\nx; and the result b of this comparison is returned. The above lemma\nstates that if the advantage ǫ of any attacker over a random player is negligible, then bbs is\nleft-unpredictable.\nProof (of Lemma 5.2). We proceed by rewriting the left-side game of the left-unpredictability\nspecification (Def. 5.1).\nBBS1. We unfold the definition of bbs:\nseed\nR← Z∗n;\n[b0, . . . , blen]← bbs rec(len+ 1, seed2);\nb̂0 ⇐ A([b1, . . . , blen]);\nb← b̂0 = b0;\nreturn b\nBBS2. Because of Fact I, we can rewrite the game as:\nx\nR← QRn;\n[b0, . . . , blen]← bbs rec(len+ 1, x);\nb̂0 ⇐ A([b1, . . . , blen]);\nb← b̂0 = b0;\nreturn b\nBBS3. x is a quadratic residue, we can thus replace x with\n√\nx2 (according to Fact VII).\nx\nR← QRn;\n[b0, . . . , blen]← bbs rec(len + 1,\n√\nx2);\nb̂0 ⇐ A([b1, . . . , blen]);\nb← b̂0 = b0;\nreturn b\nBBS4. Because of Fact V, we can rewrite the game as:\nx\nR← QRn;\n([b0, . . . , blen])← bbs rec(len + 1,\n√\nx);\nb̂0 ⇐ A([b1, . . . , blen]);\nb← b̂0 = b0;\nreturn b\nBBS5. By unfolding one step of bbs rec, we get:\nx\nR← QRn;\n[b1, . . . , blen]← bbs rec(len, x);\nb̂0 ⇐ A([b1, . . . , blen]);\nb← b̂0 = parity(\n√\nx);\nreturn b\n7\nBBS6. We have reduced the game to the left-side one of the hypothesis where the attacker\nA′(n, x) is instantiated by:\n[b1, . . . , blen]← bbs rec(len, x);\nb̂0 ⇐ A([b1, . . . , blen]);\nreturn b̂0 ⊓⊔\nUsing the above lemma, we can now prove that bbs is left-unpredictable.\nTheorem 5.3. bbs is left-unpredictable (under the quadratic residuosity assumption).\nProof. By Lemma 5.2, we only need to prove that for every attacker A:\nx\nR← QRn;\nb̂⇐ A(n, x);\nb← b̂ = parity(√x);\nreturn b\n≡ǫ b\nR← {true, false};\nreturn b\nWe proceed by rewriting the left-side game.\nBBS7. Because of Fact VI, we can rewrite the game as:\nx\nR← Z∗n(+1);\nb̂⇐ A(n, x2);\nb← b̂ = parity(\n√\nx2);\nreturn b\nBBS8. By Fact VIII, we can replace the equality test b̂ = parity(\n√\nx2) by b̂⊕parity(x)⊕1 = qr(x)\n(where ⊕ is the notation for the exclusive-or XOR).\nx\nR← Z∗n(+1);\nb̂⇐ A(n, x2);\nb← b̂⊕ parity(x)⊕ 1 = qr(x);\nreturn b\nBBS9. We have reduced the game to the left-sided one of QRA (Assumption 4.1) where the\nattacker A′(n, x) is instantiated by:\nb̂⇐ A(n, x2);\nreturn b̂⊕ parity(x)⊕ 1 ⊓⊔\n5.2 The Goldwasser-Micali public-key cryptographic scheme\nThe Goldwasser-Micali public-key cryptographic scheme (GM) was the first probabilistic one\nwhich was provably secure. More precisely it is semantically secure under the quadratic residu-\nosity assumption [8]. For defining GM , we need a number n which is the product of two distinct\nprime numbers p and q, and a y ∈ QNRn(+1). It is then defined by the three functions given in\nFig. 4 where\n(\nc\np\n)\ndenotes the Legendre symbol of c.\nIn our framework, one can state the semantic security of the above scheme by the following\ndefinition.\n8\nkeygen() =\npk ← (n, y);\nsk ← (p, q);\nreturn (pk, sk)\nencrypt((n, y) ∈ Z× Z∗n, b ∈ {0, 1}) =\nx\nR\n← Z∗n;\nc← (if b = 1 then y · x2 else x2);\nreturn c\ndecrypt((p, q) ∈ Z× Z, c ∈ Z∗n) =\ne←\n“\nc\np\n”\n;\nm← (if e = 1 then 0 else 1);\nreturn m\nFig. 4. The Goldwasser-Micali scheme\nDefinition 5.4. GM is semantically secure iff, for every attacker (A1, A2), there exists a neg-\nligible ǫ such that\n(pk, sk)⇐ keygen();\n(m1,m2)⇐ A1(pk);\ni\nR← {1, 2};\nc⇐ encrypt(pk,mi);\nı̂⇐ A2(pk, (m1,m2), c);\nreturn ı̂ = i\n≡ǫ b\nR← {true, false};\nreturn b\nIn the left-side game, a pair (pk, sk) of public and secret keys is generated; the public key pk\nis passed to the attacker A1 which returns two messages m1 and m2; one of them is picked at\nrandom and encrypted with the secret key sk; the obtained cyphertext c is then passed with\nthe public key pk and the pair of picked messages (m1,m2) to the attacker A2; the attacker\nreturns its guess for the picked message; whether the attacker is right or not is returned as a\nresult. A scheme is semantically secure if the advantage ǫ of any attacker over a random player\nis negligible.\nTheorem 5.5. The scheme of Goldwasser and Micali is semantically secure (under the quadratic\nresiduosity assumption).\nProof. We proceed by rewriting the left-side game of the semantic-security specification (Def. 5.4).\nGM1. We unfold definitions of keygen and encrypt:\n(m1,m2)⇐ A1(n, y);\ni\nR← {1, 2};\nx\nR← Z∗n;\nı̂⇐ A2((n, y), (m1,m2), if mi = 1 then y · x2 else x2);\nreturn ı̂ = i\nGM2. Because of Fact I, we can rewrite the game as:\n(m1,m2)⇐ A1(n, y);\ni\nR← {1, 2};\nx\nR← QRn;\nı̂⇐ A2((n, y), (m1,m2), if mi = 1 then y · x else x);\nreturn ı̂ = i\n9\nGM3. Because of Fact II, we can rewrite the game as:\n(m1,m2)⇐ A1(n, y);\ni\nR← {1, 2};\nx\nR← QRn;\nz\nR← QNRn(+1);\nı̂⇐ A2((n, y), (m1,m2), if mi = 1 then z else x);\nreturn ı̂ = i\nNote that this transformation is only valid because the result of the game does not\ndepend on the relation between y · x and x. Indeed, if mi = 1 then only y · x is used\nwhile x can be ignored, and vice versa.\nNow we consider the different cases for the messages m1 and m2 chosen by the attacker.\n(i) (m1,m2) = (0, 0):\nGM4. We can rewrite the game as:\nx\nR← QRn;\nz\nR← QNRn(+1);\nı̂⇐ A2((n, y), (0, 0), x);\ni\nR← {1, 2};\nreturn ı̂ = i\ni can be picked randomly after the calls to the attacker. Therefore ı̂ does not\ndepend on i. Our goal is proved.\n(ii) (m1,m2) = (1, 1): This is similar to the previous case except that A2 is given z instead of\nx.\n(iii) (m1,m2) = (0, 1):\nGM5. We can rewrite the game GM3 as:\ni\nR← {1, 2};\nif i = 1 then\nx\nR← QRn;\nı̂⇐ A2((n, y), (0, 1), x);\nreturn ı̂ = i\nelse\nz\nR← QNRn(+1);\nı̂⇐ A2((n, y), (0, 1), z);\nreturn ı̂ = i\n10\nGM6. By definition of qr, we can rewrite the game as:\ni\nR← {1, 2};\nif i = 1 then\nx\nR← QRn;\nı̂⇐ A2((n, y), (0, 1), x);\nb̂← ı̂ = 1;\nreturn b̂ = qr(x)\nelse\nz\nR← QNRn(+1);\nı̂⇐ A2((n, y), (0, 1), z);\nb̂← ı̂ = 1;\nreturn b̂ = qr(z)\nGM7. Because of Fact III, we can rewrite the game as:\nx\nR← QRn ∪QNRn(+1);\nı̂⇐ A2((n, y), (0, 1), x);\nb̂← ı̂ = 1;\nreturn b̂ = qr(x)\nGM8. Because of Fact IV, we can rewrite the game as:\nx\nR← Z∗n(+1);\nı̂⇐ A2((n, y), (0, 1), x);\nb̂← ı̂ = 1;\nreturn b̂ = qr(x)\nGM9. We have reduced the game to the left-side one of QRA (Assumption 4.1) where\nthe attacker A′(n, x) is instantiated by:\nı̂⇐ A2((n, y), (0, 1), x);\nb̂← î = 1;\nreturn b̂\n(iv) (m1,m2) = (1, 0): This case is similar to the previous one. ⊓⊔\n6 Implementation\nWe have extended our toolbox with a module which contains number-theoretic lemmas on\nLegendre and Jacobi symbols, and on Blum integers. Based on those lemmas, we have proved\narithmetic-based game transformations in the module dedicated to transformations. We have\nadded the definition of unpredictability and the quadratic residuosity assumption in the appro-\npriate modules. We have then used those extensions to make the formal security proofs of the\nBlum-Blum-Shub generator and the Goldwasser-Micali scheme.\n11\nFuture work. Two standard mathematical results remain to be proved in the proof assistant\nCoq. The first one is Fermat’s little theorem. Although there are proofs of this theorem in\nthe contributions of Coq, they are not compatible with its standard library. The second one is\nthe fact that if p is a prime number then the group Z∗p is cyclic. Although those theorems are\northogonal to our work, it would be nice to have them machine checked, if only for the sake of\ncompleteness. For the moment, we have added them as axioms.\nWe neither compute exact nor asymptotic running time. This is orthogonal to the verification\nof game transformations. In the examples we dealt with, the algorithm A′ we built from the\nattacker A at the end of Lemma 5.2, Theorem 5.3 and Theorem 5.5 are trivially PPT and thus\nvalid attackers. However this is not checked by the current implementation.\n7 Conclusions\nWe have extended our toolbox with number-theoretic capabilities. It is thus now possible to\nuse this toolbox for machine-checking game-based proofs of arithmetic-based cryptographic\nprimitives. We have shown usability of our implementation by applying it to the proof of\nunpredictability of the Blum-Blum-Shub generator and the proof of semantic security of the\nGoldwasser-Micali scheme. This is the first time that a proof of unpredictability is machine\nchecked. Machine formalization has forced us to make clear all details in those proofs that are\nusually either left to the reader or roughly explained in English. In spite of this level of details,\nwe claim that our proofs remain human readable and are mechanically human checkable without\nappealing too much to intuition.\nAcknowledgements\nWe are grateful to Fre´de´rique Oggier and Nicolas Perrin for fruitful discussions.\nReferences\n1. R. Affeldt, M. Tanaka and N. Marti. Formal proof of provable security by game-playing in a proof assistant.\nIn Proceedings of the 1st International Conference on Provable Security (ProvSec 2007), volume 4784 of\nLecture Notes in Computer Science, pages 151–168. Springer, 2007.\n2. M. Backes, M. Berg and D. Unruh. A formal language for cryptographic pseudocode. 4th Workshop on\nFormal and Computational Cryptography (FCC 2008).\n3. G. Barthe, B. Gre´goire, R. Janvier, F. Olmedo and S. Z. Be´guelin. Formal certification of code-based\ncryptographic proofs. 4th Workshop on Formal and Computational Cryptography (FCC 2008).\n4. M. Bellare and P. Rogaway. Code-based game-playing proofs and the security of triple encryption. Cryptology\nePrint Archive, Report 2004/331, 2004.\n5. B. Blanchet. An efficient cryptographic protocol verifier based on Prolog rules. In Proceedings of the 14th\nIEEE Computer Security Foundations Workshop (CSFW-14), pages 82-96. IEEE Computer Society, 2001.\n6. B. Blanchet and D. Pointcheval. Automated security proofs with sequences of games. In Proceedings of\nCRYPTO 2006, volume 4117 of Lecture Notes in Computer Science, pages 537–554. Springer, 2006.\n7. L. Blum, M. Blum and M. Shub. A simple unpredictable pseudo random number generator. In SIAM\nJournal on Computing, 15(2):364–383. Society for Industrial and Applied Mathematics, 1986. An earlier\nversion appeared in Proceedings of Crypto 82.\n8. S. Goldwasser and S. Micali. Probabilistic encryption. In Journal of Computer and System Sciences (JCSS),\n28(2):270–299. Academic Press, 1984. An earlier version appeared in proceedings of STOC’82.\n9. G. Gonthier, A. Mahboubi, L. Rideau, E. Tassi and L. The´ry. A modular formalisation of finite group theory.\nIn Proceedings of the 20th International Conference on Theorem Proving in Higher Order Logics (TPHOLs\n2007), volume 4732 of Lecture Notes in Computer Science, pages 86–101. Springer, 2007.\n10. S. Halevi. A plausible approach to computer-aided cryptographic proofs. Cryptology ePrint Archive, Report\n2005/181, 2005.\n11. D.E. Knuth. Volume 2 of The Art of Computer Programming – Seminumerical Algorithms. Addison-Wesley,\n1969.\n12\n12. P. Lafourcade, Y. Lakhnech, C. Ene, J. Courant and M. Daubignard. Towards automated proofs of asymmet-\nric encryption schemes in the random oracle model. To appear in Proceedings of the 2008 ACM Conference\non Computer and Communications Security. ACM, 2008.\n13. D. Nowak. A framework for game-based security proofs. In Proceedings of the 9th International Conference\non Information and Communications Security (ICICS 2007), volume 4861 of Lecture Notes in Computer\nScience, pages 319–333. Springer, 2007. An extended version is available as Cryptology ePrint Archive,\nReport 2007/199.\n14. D. Nowak. On formal verification of arithmetic-based cryptographic primitives. In Proceedings of the 11th\nInternational Conference on Information Security and Cryptology (ICISC 2008), volume 5461 of Lecture\nNotes in Computer Science, pages 368–382. Springer, 2008.\n15. N. Ramsey and A. Pfeffer. Stochastic lambda calculus and monads of probability distributions. In Proceedings\nof the 29th ACM Symposium on the Principles of Programming Languages (POPL 2002), pages 154–165.\nACM, 2002.\n16. V. Shoup. Sequences of games: a tool for taming complexity in security proofs. Cryptology ePrint Archive,\nReport 2004/332, 2004.\n17. U.V. Vazirani and V.V. Vazirani. Efficient and secure pseudo-random number generation. In Proceedings of\nthe IEEE 25th Annual Symposium on Foundations of Computer Science (FOCS’84), pages 458–463. IEEE,\n1984.\n18. A.C. Yao. Theory and applications of trapdoor functions. In Proceedings of the IEEE 23rd Annual Symposium\non Foundations of Computer Science (FOCS’82), pages 80–91. IEEE, 1982.\n13\n",
            "id": 640990,
            "identifiers": [
                {
                    "identifier": "2201750573",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "186812205",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2051089",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "0904.1110",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "oai:arxiv.org:0904.1110",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "10.1007/978-3-642-00730-9_23",
                    "type": "DOI"
                }
            ],
            "title": "On formal verification of arithmetic-based cryptographic primitives",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2201750573",
            "oaiIds": [
                "oai:arxiv.org:0904.1110"
            ],
            "publishedDate": "2009-01-01T00:00:00",
            "publisher": "'Springer Science and Business Media LLC'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/0904.1110"
            ],
            "updatedDate": "2021-07-20T19:41:44",
            "yearPublished": 2009,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "0302-9743"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/0904.1110"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/640990"
                }
            ]
        },
        {
            "acceptedDate": "2009-07-02T00:00:00",
            "arxivId": "0906.3924",
            "authors": [
                {
                    "name": "Kovács, László"
                },
                {
                    "name": "Mátételki, Péter"
                },
                {
                    "name": "Pataki, Balázs"
                }
            ],
            "contributors": [
                "ter Beek, Maurice H."
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/26544234",
                "https://api.core.ac.uk/v3/outputs/48290301"
            ],
            "createdDate": "2012-04-13T14:17:15",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 2211,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/2211",
                    "logo": "https://api.core.ac.uk/data-providers/2211/logo"
                },
                {
                    "id": 645,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/645",
                    "logo": "https://api.core.ac.uk/data-providers/645/logo"
                }
            ],
            "depositedDate": "2009-06-26T00:00:00",
            "abstract": "Location- and context-aware services are emerging technologies in mobile and\ndesktop environments, however, most of them are difficult to use and do not\nseem to be beneficial enough. Our research focuses on designing and creating a\nservice-oriented framework that helps location- and context-aware,\nclient-service type application development and use. Location information is\ncombined with other contexts such as the users' history, preferences and\ndisabilities. The framework also handles the spatial model of the environment\n(e.g. map of a room or a building) as a context. The framework is built on a\nsemantic backend where the ontologies are represented using the OWL description\nlanguage. The use of ontologies enables the framework to run inference tasks\nand to easily adapt to new context types. The framework contains a\ncompatibility layer for positioning devices, which hides the technical\ndifferences of positioning technologies and enables the combination of location\ndata of various sources",
            "documentType": "research",
            "doi": "10.4204/eptcs.2.2",
            "downloadUrl": "https://core.ac.uk/download/48290301.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "M.H. ter Beek (Ed.): Young Researchers Workshop\non Service-Oriented Computing 2009 (YR-SOC’09).\nEPTCS 2, 2009, pp. 15–26, doi:10.4204/EPTCS.2.2\nc© L. Kova´cs & P. Ma´te´telki & B. Pataki\nService-oriented Context-aware Framework\nLa´szlo´ Kova´cs Pe´ter Ma´te´telki Bala´zs Pataki\nMTA SZTAKI\nComputer and Automation Research Institute of the Hungarian Academy of Sciences\nDepartment of Distributed Systems\n1111 Budapest, Kende u. 13-17, Hungary\n{laszlo.kovacs,peter.matetelki,balazs.pataki}@sztaki.hu\nLocation- and context-aware services are emerging technologies in mobile and desktop environ-\nments, however, most of them are difficult to use and do not seem to be beneficial enough. Our\nresearch focuses on designing and creating a service-oriented framework that helps location- and\ncontext-aware, client-service type application development and use. Location information is com-\nbined with other contexts such as the users’ history, preferences and disabilities. The framework also\nhandles the spatial model of the environment (e.g. map of a room or a building) as a context. The\nframework is built on a semantic backend where the ontologies are represented using the OWL de-\nscription language. The use of ontologies enables the framework to run inference tasks and to easily\nadapt to new context types. The framework contains a compatibility layer for positioning devices,\nwhich hides the technical differences of positioning technologies and enables the combination of\nlocation data of various sources.\n1 Introduction\nIn the past years mobile and desktop computer application developers realized the need to apply location-\nand context data so as to help users with such awareness information. However, location data has mainly\nspread in the field of mobile applications while context-sensitivity is used in desktop applications. The\nreal advantage can result from the combination of these: to create location- and context-aware services\nand applications. Comparing a simple location-aware audio-guide to a context-sensitive and location-\naware audio guide in a museum (that is capable of ranking the showpieces and change the granularity of\nthe presented information according to the users preference and behavior) or a simple file- and application\nmanager on a laptop computer to a location-aware one (that displays different folders and opens specific\napplications in case the user resides at home or at work) we can feel the benefit of the additional location-\nand context information.\nApart from navigation, location-based services for mobile devices have not managed to result in such\na breakthrough, like e.g. short messaging service. Two major shortcomings for the current location-based\nservices have been indentified. On one hand, using only location as a context does not seem to be enough\nto satisfy the users’ augmenting needs. On the other hand, using a certain location-aware service often\nrequires the application of a specific device that belongs to a specific positioning technology. In our\nresearch we designed and created a framework for context-sensitive service and application development\nthat aims to solve the abovementioned problems, meaning that the framework makes it easy to use and\ndevelop smart location- and context-aware services. It even goes beyond this goal by offering a semantic\nbasis for the information and by integrating the geometry of the environment.\nIn this paper we first present our motivation and goals. The following chapter gives an overview of\nthe designed system and we also get into the details about certain important concepts and building blocks\nof the framework. The last chapter summarizes our work.\n16 Service-oriented Context-aware Framework\nFrom the point of mobile applications, considering characteristics of the users other than their loca-\ntion can enrich services. This additional information can include the users’ schedule, tasks, presence,\npreferences, age, mental and physical state (e.g. disabilities), etc. A system can be even more circum-\nspect by not only using the users’ characteristics but also their circumstances, e.g. the users’ environment\nat the current location such as streets, buildings, rooms and objects. Accordingly, the model of the en-\nvironment needs to be taken into account. The geographical model of the physical space should also\ndescribe the interrelations of the objects in the physical space [13]. The ability of inferencing on the\nenvironmental model is necessary in case of requests that rely on the user’s real surroundings like in case\nof a navigation application or museum guide. By being aware of the characteristics and circumstances\nof the users in addition to their location, the answer to their questions can become more appropriate and\ncan specifically cover their needs.\nThe major problem when using a certain location-based service is that users are required to apply a\nspecific positioning technology, and own a specific type of device. This prevents the smooth spread of\nlocation-based services and even makes it impossible to use them with future, not-yet existing positioning\ntechniques. A positioning generalization layer is needed for the location-aware services to hide the\ndifferences of the numerous positioning technologies. A subsystem of this kind, a location-manager\nlayer that joins the diverse positioning subsystems enables users and developers to create and use the\npositioning services irrespective of the type of the positioning devices and technologies. In addition,\nfor users having multiple positioning devices, location data generated by these sources can be combined\nby the location-master layer so the precision and trustworthiness of the localization and the coverage of\nthe positioning service can be improved. Users can also switch any time from one positioning device to\nanother without interrupting the positioning service.\nAs users need intelligent services, ones that are able to answer questions that are closer to human\nway of thinking, much can be done to achieve this goal: more information can be used about the users\nthemselves and their environment and more intelligent answering engines can be applied. The more con-\ntext parameters are considered for information retrieval, the more beneficial and satisfactory the services\nwill be for the users [18, 13]. A system of this kind accumulates much information and interrelations,\nand contains a remarkable knowledge base. To insure information reuse, machine-processability and\ndata consistency, the data must have semantic interpretation.\nThis kind of a context-aware system uses many aspects (characteristics and circumstances) of the user\nand much information about the environment. As there is no limit for the number of types of contexts\nthat can be helpful and applicable for the system, it is important to have a flexible data model and a\nclearly understandable dataset at all times. Creating rules and running automatic consistency checking\ncan facilitate this task. Well-defined relations between the data classes and instances can be used to\nreveal hidden connections in the database. Using ontologies as the base of the context-aware system can\nsatisfy all the above requirements and wishes. It also makes it possible to perform inference tasks on the\ndata.\nOur framework was designed and developed in line with the above principles so as to solve the men-\ntioned problems. To build such an infrastructure, two levels are involved [23]: we created the conceptual\ncontext model that describes the concepts and their interrelations, and a SOA style architecture that de-\nfines the modules, their hierarchy and connections. By providing the appropriate semantic context-based\nservices it can help to solve more general and more realistic challenges. Such a system with a seman-\ntic background, with the ability to take into account both the location and diverse context information\nabout the user and the ability to use the environmental model for geometric calculations, is capable of\nresponding to high abstraction level questions that are closer to human thinking, people’s everyday needs.\nAlthough, the framework’s primary target is to support application and services in the world of\nL. Kova´cs & P. Ma´te´telki & B. Pataki 17\nmobile technologies, our goal was to create a system that can be at hand for both mobile and desktop-\nstyle environments.\n2 Related Work\nOne possible method to create a context-aware system [1, 12] is to combine an existing information\nsystem (to access information about the users) with an awareness system (that helps to distribute context\ninformation among users’ devices). The HIPPIE [18] information system can support users with location-\nbased services during the preparation, execution and evaluation of a museum or fair visit. It takes into\naccount the users’ positions and characteristics. It is a well-designed system for single-user context\nsupport, however, is not capable of handling user interactions. Following the above idea, the authors of\nAwareness in Context-Aware Information Systems [13] combine HIPPIE with an event based awareness\nenvironment, ENI [19], which is able to distribute context information to users. They outline a complex\nsystem to support awareness for nomadic users about other users residing in a similar electronic or spatial\ncontext. The combined context-aware information system, featuring user tracking in the physical world\nand electronic space, user modeling and an event-based awareness environment, is able to support a\njoint experience for local visitors and remote electronic visitors at an exhibition. Although, this project\ntakes into account the physical location, environment and other aspects of the user, it lacks the semantic\ndescription of the information, the ability to execute inference tasks and also a sophisticated positioning\nmanager layer that we feel necessary to create an advanced and flexible location- and context-aware\nframework.\nMoreover, semantic-rich approaches also exist to design a context-aware system, for example the\nOCCA [23] that proposes a new semantic rich context-modeling concept for collaborative environments.\nOCCA uses OWL [25] to describe its base ontology that the architecture builds on. In the TEAM [20]\nproject, a similar recent EU project, an ontology-based contextual framework [15] is described to cap-\nture, access and share software developers experiences, the information that usually gets lost among\ndistributed teams. Compared to our project the main difference is the moment when context fusion is\nachieved: this system combines contexts asynchronously while sensing the users local environment to\nfind out his activity. On the contrary, our system combines the contexts on-demand at the time a certain\nintelligent service is invoked by a user. Thanks to the semantic-rich model, it provides the possibility\nto infer on the knowledge base of contexts, but as this work focuses on contexts of the collaborative\nenvironments, especially on internal contexts to describe humans and tasks, integrating and using the\ngeometry-based model of the environment are not possible within this approach.\nThe literature describes other similar ontologies, such as CONON [24], CoBrA-Ont [5] and context\nontology [21], all described in OWL. These ontologies represent the ubiquitous computing domain and\nconsider concepts like location, time, people and devices. The advantage of these ontologies over OCCA\nis that they consider the physical context of the users. The main focus of these projects [24, 21] differs\nfrom ours as they aim to create ontologies for context-representation, but do not intend to build a frame-\nwork for creating location- and context-aware services or applications based on the semantic backend.\nCoBrA [6] is an advanced complex architecture to support context-aware systems, which takes into ac-\ncount many aspects of the user and environment. In addition to its functionality our framework provides\nthe geometric environmental calculations and a positioning generalization layer.\nDu and Wang [8] have developed a system for facilitating context-aware application development\nfor mobile devices. Although, the basic idea may seem parallel to ours, the focus points are different.\nTheir project focuses on providing a development environment with code generation tools to create\n18 Service-oriented Context-aware Framework\napplications, while our research focuses on creating a framework providing location- and context-aware\nservices for developers to create applications.\nDevaraju el al. [7] have worked on a context gathering framework to support context-aware mobile\nsolutions that deals with the heterogeneous data provided by sensors. This research describes in more\ndetail what we call “Context Middleware” and “Contexts” tiers in our architecture but omits the higher-\nlevel logical layer, our “Core” of the framework.\nIn 2006, when this research started the solution to combine the various positioning technologies\ncorresponding to a single user was a novel one. Nowadays, we can find some applications, e.g. Google\nMaps [11] using the Google My Location API [10] that benefit from a similar idea. Although, Google\nMaps can use various positioning technologies to find the users’ current location, it does not combine the\ninformation from the different sources.\nIn environments where high precision localization is needed but not possible, an image or video\nof the local environment can help positioning. Inspired by the ideas [2] presented in MOBVIS [22], a\nrecent EU project, low-precision location information can be combined with object recognition of the\nlocal visual information and result in high-precision localization.\nThe above evaluation of related projects show that, although, the low level details of our concept\nof a location- and context-aware framework are known to researchers, there has not been a project that\ncombines all the essential necessities - the geometric model of the environment, the location informa-\ntion together with other contexts of the user and the semantic representation of all the above informa-\ntion – all of which we believe to be necessary to create smart semantic-rich context-, location- and\nenvironment-aware services. In addition to the previous combination of system components, we also\nprovide a positioning generalization and arbitration module to support users carrying and using several\ntypes of positioning devices to increase the coverage and accuracy of the location-aware services.\n3 The Framework\nFigure 1 shows the architecture of the framework.\nFollowing the paradigm of service-orientation, the functions of the system are separated into distinct\nunits that are accessible as services over the network.\nContexts can be found in the lowest tier in the framework that contains the hardware and software\ncomponents for context perception. The collected context values are forwarded to the Framework core.\nIntelligent contexts (e.g. location) can perform the framework’s commands or reply its requests, e.g. to\nsend context information continuously, on demand or in bursts. In the context-aware system we encounter\nseveral types of contexts, either simple or complex ones. The contexts in this tier may be inhomogeneous,\nrequiring different interpretation and handling. The following list shows examples of possible contexts:\n• Presence: is the user available at the given moment? Is a certain context of the user available at\nthe moment?\n• Time: it can be local time (hours and minutes) or the current season, decade, etc.\n• Means of transportation: on foot, by car, using public transport\n• Social issues or status: is the user a child, a pensioner, a worker?\n• Role: can be the users social role (grandfather) or the role held in an office (manager)\n• Operating system: the contexts of the device used are also contexts of the user\nL. Kova´cs & P. Ma´te´telki & B. Pataki 19\nFigure 1: The components of the framework.\n20 Service-oriented Context-aware Framework\n• Expertise or qualifications: people having different qualifications require different information, so\nthis defines the style or granularity of the information\n• Coordinates, velocity: for navigation applications for drivers and excursionists\n• Traffic: dynamic traffic information for vehicle-oriented navigation\n• Environmental model: countries, cities, streets, houses and even furniture\n• Activity: what files are open/edited by the user, what is his current schedule is, where is he cur-\nrently walking/driving, etc.\n• Preferences, taste: “above 300HP”, “Greek mythology” or “cheapest route” are all types of pref-\nerences. This category can involve a very wide variety of information.\n• History: a list of the past actions of the users, similar to detailed activity log in [13]. In case the\nusers rate the system’s answers according to their satisfaction a recommender system can enhance\nthe framework to be an adaptive, learning one.\nThe location context is a complex subsystem managed by the Location Middleware. It contains all\nthe different, non-compatible positioning technologies and masks the differences so the framework can\nintegrate any kind of positioning system via the Location Middleware. This module enables users to\nprofit from the simultaneous or alternating usage of their different positioning devices. In an area cov-\nered with RFID readers and GSM signal, high precision global positioning is impossible with the existing\ntools. However, by using a simple mobile phone and an RFID tag our system is able to determine the\nglobal position with high accuracy although neither the GSM nor the RFID technology is capable of\ndoing this by itself. Using this module it is possible to achieve an increased precision in localization by\nautomatically combining the sources of different positioning devices. Another benefit of this module is\nthe possibility to detect the users’ location in changing conditions, for users switching from one posi-\ntioning device to another (e.g. caused by signal loss). This is useful in scenarios where both indoor and\noutdoor positioning is needed. Outside the user can be positioned by his GPS while getting indoors (and\nlosing the GPS signal) the positioning continues as the GSM or with other indoor positioning systems\nthat become accessible.\nOur system was designed in a way that is able to represent location information acquired from var-\nious positioning sources. This ensures the compatibility with existing and future positioning solutions.\nWithin our project two scientific groups are researching alternative positioning technologies and building\nprototype systems. One group for RFID and the other one for WLAN based positioning. We cooperated\nwith these teams to inspect their solutions and to integrate the positioning solutions into the framework.\nFor the position data input we provide a Web Service that accepts location data input using a fixed pa-\nrameter set. Apart from raw coordinate data, location data also includes the coordinate system, precision\nand probability information. So as to accommodate to the model of the framework, each team developed\nan extension to their system (Ext RFID and Ext WLAN according to the previous sections) that sends\nthe position information to the Web Service.\nApart from the location, other contexts can serve as input of the system, e.g. time, user preferences,\ndisabilities and history. Every context has its ontology model in the framework. The storage and query-\ning of the heterogeneous contexts are executed through the Context Middleware using Web Service\ninterfaces.\nHistory is a special type of context. It does not provide fresh information but stores old information\nas historic data for latter use as time-stamped context-states. It’s a repository-like context for storing\ndata to provide querying possibilities later on. The information found in the History can be used by a\nrecommender system.\nL. Kova´cs & P. Ma´te´telki & B. Pataki 21\nThe goal of the Context Middleware tier between the Framework core and the contexts is to mask\nthe differences between the different context types and allow unified context handling and querying\nfor the Framework core. This module transforms the heterogeneous context data into a homogeneous,\nsemantic representation enabling the Framework core to operate independently from the structure of the\ndifferent contexts and ensuring that the system can easily integrate future contexts. It also forwards the\nframework’s requests to the intelligent, interactive contexts. The Context Middleware contains a database\nthat matches the context-devices with their owner (a user in the system) and with the supported contexts.\nThis database also stores other device- and context related information. As the Context Middleware\nhides the low-level devices from the upper tiers, it enables the Framework core to work with high-level\nuser-assigned context data so it does not have to deal with low-level devices.\nThe Framework core between the contexts and the applications realizes the main functionality. This\nis the core of the framework. It accepts requests from the Applications, evaluates them according to the\ninformation provided by the contexts. The semantics of the context data is contained by the ontology.\nThe main module of the core is the State Manager that can be queried for the actual state or history of\nthe contexts. The response is provided to the applications by the Application2Core interface.\nThe Context2Core Web Service interface accepts asynchronously incoming context data from the\nContext Middleware. It can also work as a command-generator: in case the State Manager is in need\nof fresh data input it requests context data from the Context Middleware in burst or single input mode\nthrough this interface. The gathered context-information is forwarded to the State Manager for further\nprocessing and storage.\nThe ontologies provide the semantic and logical model of the contexts. Every context has a map-\nping and description in the ontology. Employing semantic-aware components like ontologies in a com-\nplex system is beneficial in a number of ways. Ontology is used to describe vocabularies, machine-\ninterpretable definitions of concepts and relations [17] between the individuals of the domain: it makes\nsense of the things. In machine-to-machine interactions, just like in human interactions, it is important\nthat each party shares the same understanding of the structure and meaning of the exchanged information.\nThis is one of the main reasons to develop ontology-based data models [17, 24] in addition to enabling\nthe reuse of the knowledge and enabling the use of reasoning engines to infer on the data. Ontologies\nhave been used in numerous other research projects [24, 6, 21] to represent contextual information.\nIn our ontology the so-called UserContext class is the main class for the miscellaneous context in-\nformation. All context data must correspond to one exact user and must have a timestamp. As described\nbefore, many kinds of context information can be possibly added to the system. As an example disabil-\nities, preferences and location were defined. Our Coordinates class must have exactly one coordinate\nsystem from the CoordinateSystemType instances. As depicted on figure 2, a two dimensional coordi-\nnate must have, in addition to the previous constraint, probability and precision values in the X and Y\naxes, coordinate values in X and Y directions but must not have a Z coordinate.\nTo describe the geometric shapes we use the W3C Geospatial Vocabulary [14], created by the W3C\nGeospatial Incubator Group in 2007. This ontology describes the location and geometric properties of\nresources as “geographic features”. Our complex geometry shapes are composed of multiple geographic\nfeatures. The ontology handler helps to manage existing ontologies and to define and integrate new on-\ntologies to the framework. It also functions as a browsing and querying interface for the stored instances\ninside the ontology.\nThe State Manager represents the state of the world from the point of view of the framework: it\ncontains fresh and historical context-information about users and the spatial model of their environment.\nThis world-model is continuously updated by the contexts. The great majority of the requests from\nthe applications and services arrive to the State Manager that processes and answers them. So as to\n22 Service-oriented Context-aware Framework\nFigure 2: The definition of Coordinates2D class.\nanswer complex requests it divides them into more simple ones and forwards them to the Ontology- and\nGeometry managers. Gathering and combining the responses produce the final result. To enrich the high\nabstraction level functionality and the novelty of the framework, the mutual matching of the geometry\nobjects to the instances stored in the ontology is very important. The Geometry manager module of\nthe State manager uses the geometry object definitions stored in the ontology and the visually portrayed\ngeometry objects can be associated to the corresponding ontology instances.\nThe world model, such as the spatial description and all context information about the users, is\nsemantic in our framework. The Ontology manager stores and queries the context data in the ontology.\nIt contains an inference engine that allows executing complex SPARQL queries, revealing consistency\nproblems and running other inference tasks on the ontology.\nThe ability of making calculations and inference in finite time on the context-data was a high priority\nwhen selecting the ontology description language, OWL DL [3]. As the inferencing solutions for de-\nscription logic languages are not appropriate to calculate spatial relationships (defined by coordinates),\na separate module is created in the framework dealing with spatial calculus: the Geometry manager. As\npart of the State manager this module executes tasks related to spatial calculations. The spatial model of\nthe environment can be represented in 2D or 3D. The system also provides the possibility of importing\nthe models and worlds described in 3D modeling languages (e.g. VRML) to the internal representation.\nThe Geometry manager is capable of working with local and global coordinates.\nBased on the users actions in the past the integrated Recommender Systems can make recommenda-\ntions and create predictions. These can be used to auto-personalize the system for the users.\nApplication2Core interface is the Web Service interface used to communicate with applications.\nThe applications are not part of the framework; they are the interfaces towards the users. They can\nuse and benefit from the simple and complex services offered by the framework through its Web Services.\nApplications may be as simple as a user tracking software or may be complex, intelligent applications\nusing a complex service of the framework. A fat-client style complex application can use the services\nof the framework, can recompose or combine them and create new services and functionality. We can\nimagine applications running on mobile devices, desktop or notebook PC-s or even web applications.\nBecause the framework is designed as a generic context-support system, is can meet the needs of many\nsorts of applications.\nL. Kova´cs & P. Ma´te´telki & B. Pataki 23\n4 Sample Services and Utilization Examples\nThe current version of the framework uses the relational data store of the Context Middleware for storing\ndevice related information. For storing and searching user-contexts it uses the ontology based persistence\nmodules in the Framework core. It also has a spatial reasoning module. Combining all these modules\nmakes the framework able to respond to complex questions where location, context (e.g. history, prefer-\nences, disabilities) and the geometry of the environment all play an important role. We developed some\nsample services to demonstrate the available functionality of the framework to reveal some possible\nusages.\nA service where location and geometry play the most important role is the “Can user A see user B?”\nservice. Positions of both users are queried from the framework and the geometry (currently in 2D) is\nexamined for objects between the users. For a query to this service a user is given a textual answer and a\ngraphical answer with the map of the scene and the users. In case there is an object between the users, it\nis highlighted with red. The textual answer also gives information about which geometry object (which\ngeography feature and which spatial object) is between the users.\nVisibility itself is an interesting question; a detailed and accurate model and a geometry engine are\nneeded to execute this kind of calculation. This service also proves that by combining a few contexts\n(the location and the environment) interesting new services can be created. In our current sample service\nwe could also benefit from the users’ preferences or disabilities to personalize the visualization; e.g. for\npeople with low vision the system could generate a high contrast image.\nWe also created a simple service that can be used to acquire a user’s location. This service has\nmultiple purposes; on one hand, it provides an easy solution for monitoring the user’s real-time location\nusing the framework. On the other hand, service developers can use this service to create third-party\nlocation-based services based on the framework, yet those are not implemented in the framework. To\ndemonstrate the use of this service, a dynamic webpage that shows a user’s alternating location in real\ntime was developed. The page also shows the old positions and differentiates their visualizations from\nthe most recent location. For the real-time visualization we use the DWR (Direct Web Remoting) [9]\nReverse Ajax technique.\nAlso, some base-level services were created for determining the users’ distance from a given point, a\ngeometric shape or complex shape in addition to services calculating the containment-relations of points,\nshapes and complex shapes.\nAs an example for complex services, we developed a distance-sensitive service that detects whenever\na user’s proximity – in relation to a point, shape or complex shape – falls below a threshold. The service\ncan perform different operations, such as opening a file or sending a (email, IM, SMS) message. This ser-\nvice can facilitate a conference by automatically opening the lecturer’s presentation who approaches the\nlectern. It can also act as an alarm-system by automatically sending an on- or offline warning (IM, email,\nSMS) in case someone gets too close to a certain location or detects theft of (an RFID tag equipped)\nobject.\nThe Location Middleware can be used for tracking a user’s position that has multiple positioning\ndevices. It works when the technological circumstances of the positioning change, e.g. switch from GPS\nto RFID. The use-case of tracking such a user with the aid of our system is as follows. A user’s position\nis tracked by his positioning devices such as GPS, cell phone (GSM or 3G) and an RFID device. The\nlocation-data is forwarded to the Location Middleware that makes the positioning technology transparent\nfor the upper level systems. The navigation system requests the actual position of the user from the\nframework, so the technological details and actual devices remain transparent to the navigation system.\nWhile the user drives to work the framework receives and combines the GPS and GSM based position\n24 Service-oriented Context-aware Framework\ninformation. As the GPS coordinates are of higher accuracy, its role will be more important during the\ncalculation. While the user drives in a tunnel (where he loses the GPS signal) the position can be still\ndetermined using his mobile phone. As soon as he gets into the coverage of a WLAN (in addition to\nthe GSM), the system will combine the different position information and create a result that is even\nmore accurate and trustworthy. If the user enters a building that is not GPS covered but is equipped with\nWLAN and RFID positioning systems his position may be determined even in 3D. Neither the user nor\nthe developer has to worry about the positioning technologies and devices as the Location Middleware\nmasks them from the application.\nThe framework supports context-aware services that depend on multiple circumstances of the user.\nTherefore, not only the location but also other important contexts of the user is used by the system such\nas the disabilities and preferences to be able to provide personalized answers for the users’ queries. In\ncase a user has low vision the user interface of a service on his mobile device can display high contrast\ncolors. In case he has mobility impairment a service helping the orientation in a museum will offer the\nelevator in place of the stairs. In case the user prefers the ancient aged show pieces to the new aged\nones the system will give more detailed information about the preferred ones during guidance. These\nexamples show that the benefit of using multiple user contexts helps to fine tune the service.\nAs an example for showing how to use the geometry of the environment of a user we can think of an\nintelligent museum guide. In case a visitor loses his child in the museum he can ask the intelligent guide\nhow to find him. Beyond telling the room number the framework using the spatial model of the museum\ncan show the route to get to the desired location. The route can be simply drawn on a graphical map and\nsent to the users device but as the system knows the meaning (semantics) of the geometry objects it can\nguide the user by telling “. . . enter the door on your right and turn right . . . ” and so on.\nObjects can have disqualifications for certain disabilities: an elevator meets the needs of a handi-\ncapped person but all the stairs are disqualified for this kind of disability according to the semantics of\nthe geometry model. As soon as the disability context is also combined with the geometry and location\ncontexts for the route planning, the framework can avoid making wrong suggestions such as using the\nstairs while being in a wheelchair. In case a museum visitor is blear-eyed and can see clearly only 1\nmeter away, the system will send the information about a showpiece to his handheld device instead of\ndisplaying it on the fixed monitor, which he would be unable to read.\nIn a museum environment, the use of the recommender system enables the framework to suggest\nunvisited showpieces for a user. Suggestions can be made based on the user’s profile or based on similar\ntracks of other users.\n5 Summary\nConsidering the present deficiencies and future expectations of location-based services a system for\ncontext- and location-aware services was designed. The framework generalizes the location-based ser-\nvices to context-based services; its aim is to assist context-aware services and applications in mobile or\ndesktop environments. The framework supports location-based services, and also implements context\nassistance in general where users’ characteristics (e.g. disabilities and preferences) and circumstances\n(e.g. the environment and history of past actions) are also part of the context. Context modeling is\nachieved by using the OWL description language, we use ontologies to define and store all context data.\nThis semantic description of the data allows the system to perform reasoning and automatic consistency\nchecking in the knowledge base, makes the data machine-processable and also provides us a flexible and\nexpandable data model.\nL. Kova´cs & P. Ma´te´telki & B. Pataki 25\nOur framework also offers a subsystem masking the different positioning technologies from the users\nand developers. Users can use location-based services independent of what kind of positioning device\nthey own, developers can create location-based services independent from what kind of positioning de-\nvice produces the location-data input. This subsystem is able to combine the location information pro-\nvided by different positioning sources to spread the positioning coverage and to help achieve a higher\npositioning availability and precision.\nThe development of the framework is currently under process. The core parts have been created: the\nontology itself, the Ontology manager and Ontology manager of the State Manager, the Recommender\nsystem and the Web Service interfaces to communicate with the applications and the contexts. The\nContext Middleware relational database and web interface have also been developed. To accumulate real\ncontext-data we have integrated our system with both the RFID and WLAN based positioning solutions.\nFor testing, the sample services described in the previous sections have been used. There are two major\ndirections to continue the development and evaluation of the system. So as to improve the functionality\nwe can integrate new contexts by creating new contexts or finding new context providers or we can build\nnew applications to combine the existing contexts. To specify what contexts are relevant for specific\napplication types a sociological survey would help further research.\nThe framework can provide a good opportunity in several fields in the IT sector: data providers who\npossess context data, a large amount of information about the users, can combine the various raw data\nto increase their knowledge base and to obtain additional information. Developers may also realize that\ncombining the different contexts of the users can result in interesting applications. These and other new\npossibilities may become accessible via the use of the framework. With such context-aware services\naccessibility issues in general can be assisted. Examples may vary since not only physical disability can\nbe taken into account. Also, the system detecting poor mobile phone signal reception can switch to VoIP\ncommunication, and the system being aware that the user is carrying heavy luggage can advice to take\nthe escalator in place of the stairs.\nAcknowledgments\nThis work has been supported by the BREIN project [4] and the MIK project [16]. BREIN is partly\nfunded by the European Commission under contract FP6-034556. MIK is funded by the National Office\nfor Research and Technology in Hungary.\nReferences\n[1] G.D. Abowd, A.K. Dey, P.J. Brown, N. Davies, M. Smith & P. Steggles, (1999): Towards a Better Under-\nstanding of Context and Context-Awareness. In Lecture Notes In Computer Science Vol. 1707, pp. 304-307.\n[2] K. Amlacher & L. Paletta (2008): An Attentive Machine Interface Using Geo-Contextual Awareness for\nMobile Vision Tasks. In: Proc. European Conference on Artificial Intelligence, pp. 601-605.\n[3] S. Bechhofer, F. van Harmelen, J. Hendler, I. Horrocks, D. McGuinness, P. Patel-Schneider & L. Stein\n(2004): Web Ontology Language (OWL) Reference, W3C Recommendation. At http://www.w3.org/TR/\nowl-ref/\n[4] BREIN. At http://www.gridsforbusiness.eu Accessed on 2009/01/08.\n[5] H. Chen & T. Finin (2003): An Ontology for a Context Aware Pervasive Computing Environments. In: IJCAI\nWorkshop on Ontologies and Distributed Systems.\n[6] Context Broker Architecture. Papers listed at: http://cobra.umbc.edu/paper.html\n26 Service-oriented Context-aware Framework\n[7] A. Devaraju, S. Hoh & M. Hartley (2007): A context gathering framework for context-aware mobile solutions.\nIn Proceedings of the 4th international Conference on Mobile Technology, Applications, and Systems and\nthe 1st international Symposium on Computer Human interaction in Mobile Technology, pp. 39-46.\n[8] W. Du & L. Wang (2008): Context-aware application programming for mobile devices. In Proceedings of\nthe 2008 C3S2E Conference, pp. 215-227.\n[9] DWR. At http://directwebremoting.org/ Accesssed on 2008/10/21.\n[10] Google Geolocation API. At http://code.google.com/apis/gears/api_geolocation.html Ac-\ncessed on 2009/03/16\n[11] Google Maps. At urlhttp://maps.google.com Accessed on 2009/01/14\n[12] T. Gross (2001): Towards Ubiquitous Awareness: The PRAVTA Prototype. In Ninth Euromicro Workshop on\nParallel and Distributed Processing, pp. 139-146.\n[13] T. Gross & M. Specht (2001): Awareness in Context-Aware Information Systems.\n[14] J. Lieberman, R. Singh & C. Goad: W3C Geospatial Vocabulary. At http://www.w3.org/2005/\nIncubator/geo/XGR-geo/ Accessed on 2009/02/23\n[15] W. Maalej & H.J. Happel (2008): A Lightweight Approach for Knowledge Sharing in Distributed Software\nTeams. In: Lecture Notes in Computer Science.\n[16] MIK http://mik.bme.hu/main.htm Accessed on 2009/02/11.\n[17] N.F. Noy & D.L. McGuiness (2001): Ontology Development 101: A Guide to Creating\nYour First Ontology. At http://protege.stanford.edu/publications/ontology_development/\nontology101-noy-mcguinness.html\n[18] R. Oppermann & M. Specht (2000): A Context-Sensitive Nomadic Exhibition Guide. In Second Symposium\non Handheld and Ubiquituous Computing, Springer, pp. 127-142.\n[19] W. Prinz (1999): NESSIE: An Awareness Environment for Cooperative Settings. In Proceedings of the Sitxth\nEuropean Conference on Computer-Supported Cooperative Work, pp. 391-410.\n[20] Tightening knowledge sharing in distributed software communities by applying semantic technologies At\nhttp://www.team-project.eu/ Accesssed on 2009/05/11.\n[21] V. Vieira, P. Tedesco & A.C. Salgado (2005): Towards an Ontology for Context Representation in Groupware.\nIn: Lecture Notes in Computer Science.\n[22] Vision Technologies and Intelligent Maps for Mobile Attentive Interfaces in Urban Scenarios At http://\nwww.mobvis.org/ Accesssed on 2009/05/05.\n[23] G. Wang, J. Jiang & M. Shi (2007): Modeling Contexts in Collaborative Environment: A New Approach. In\nLecture Notes in Computer Science Vol. 4402, pp. 23-32.\n[24] X.H. Wang, D.Q. Zhang, T. Gu & H.K. Pung (2004): Ontology Based Context Modeling and Reasoning using\nOWL. In: Workshop Proceedings of the 2nd IEEE Conference on Pervasive Computing and Communications.\n[25] Web Ontology Language. At http://www.w3.org/TR/owl-features/Accessed on 2009/03/13\n",
            "id": 651303,
            "identifiers": [
                {
                    "identifier": "2063788",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "48290301",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:doaj.org/article:77c1343803dd457ebc53dc1c4fb5bd25",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "0906.3924",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "10.4204/eptcs.2.2",
                    "type": "DOI"
                },
                {
                    "identifier": "2024787663",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "26544234",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:eprints.sztaki.hu:5303",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "oai:arxiv.org:0906.3924",
                    "type": "OAI_ID"
                }
            ],
            "title": "Service-oriented Context-aware Framework",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2024787663",
            "oaiIds": [
                "oai:arxiv.org:0906.3924",
                "oai:doaj.org/article:77c1343803dd457ebc53dc1c4fb5bd25",
                "oai:eprints.sztaki.hu:5303"
            ],
            "publishedDate": "2009-06-01T00:00:00",
            "publisher": "'Open Publishing Association'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/0906.3924",
                "http://doaj.org/search?source=%7B%22query%22%3A%7B%22bool%22%3A%7B%22must%22%3A%5B%7B%22term%22%3A%7B%22id%22%3A%2277c1343803dd457ebc53dc1c4fb5bd25%22%7D%7D%5D%7D%7D%7D"
            ],
            "updatedDate": "2021-05-22T12:34:50",
            "yearPublished": 2009,
            "journals": [
                {
                    "title": "Electronic Proceedings in Theoretical Computer Science",
                    "identifiers": [
                        "2075-2180"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/48290301.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/48290301"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/48290301/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/48290301/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/651303"
                }
            ]
        },
        {
            "acceptedDate": "2011-09-15T00:00:00",
            "arxivId": "1110.3397",
            "authors": [
                {
                    "name": "Ahnert, Karsten"
                },
                {
                    "name": "Mulansky, Mario"
                }
            ],
            "contributors": [
                "Karsten"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/445268173"
            ],
            "createdDate": "2012-04-13T14:19:17",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2011-01-01T00:00:00",
            "abstract": "Many physical, biological or chemical systems are modeled by ordinary\ndifferential equations (ODEs) and finding their solution is an every-day-task\nfor many scientists. Here, we introduce a new C++ library dedicated to find\nnumerical solutions of initial value problems of ODEs: odeint (www.odeint.com).\nodeint is implemented in a highly generic way and provides extensive\ninteroperability at top performance. For example, due to it's modular design it\ncan be easily parallized with OpenMP and even runs on CUDA GPUs. Despite that,\nit provides a convenient interface that allows for a simple and easy usage.Comment: 4 pages, 1 figur",
            "documentType": "research",
            "doi": "10.1063/1.3637934",
            "downloadUrl": "http://arxiv.org/abs/1110.3397",
            "fieldOfStudy": "computer science",
            "fullText": "Odeint – Solving ordinary differential equations in C++\nKarsten Ahnert1 and Mario Mulansky1\n1Department of Physics and Astronomy, University of Potsdam\nPACS numbers: 02.60.Lj, MSC 68-04\nI. INTRODUCTION\nOrdinary differential equations (ODEs) play a crucial role in many scientific disciplines. For example the Newtonian\nand Hamiltonian mechanics are completely formulated in terms of ODEs. Other important applications can be found in\nbiology (population dynamics or neuroscience), statistical physics and molecular dynamics or in nonlinear sciences [1].\nFurthermore, ODEs are used in numerical simulations to solve partial differential equations (PDEs), for example by\ndiscretizing the spatial coordinates.\nAn analytic solution of an ODE can only be found in very rare cases such that numerical methods have to be\nemployed. Solving ODEs numerically [2, 3] has a long tradition and has become popular by the rise of computers and\nits spreading in form of micro computers which are now present in our daily live. The most famous solvers for ODEs\nare surely the well-known explicit Runge-Kutta solvers which are easy to implement and can easily be applied to a\nwide range of problems. They come with step-size control and some algorithms also possess dense output functionality.\nAnother class of solvers are implicit solvers which are important for stiff problems, hence ODEs with two or more\ndifferent scales of the independent variable.\nIn mathematical terms, an ordinary differential equation is defined as\n~˙x = ~f(~x, t) . (1)\nHere and in the following the time t is used as the independent variable. The state of the ODE is ~x which is a vector\nfield and ~˙x denotes its time derivative. An initial value problem (IVP) of an ODE is to find a solution given an initial\nvalue ~x0(t0). Nearly all methods for solving ODEs work iteratively, that is they start with ~x0(t0) and iteratively\ncreate a sequence ~x(ti) where every ~x(ti) is obtained from previously calculated values of ~x.\nIn this paper we introduce odeint [4] – a C++ library for solving the IVP of ODEs. Of course, there exist many\nof such libraries for different languages. Particular examples are the GNU Scientific Library (gsl) [5] or the routines\nfrom the Numerical Recipes (NR) [6]. These two libraries are written in C and C++ and are widely used in the\nscientific community. Other popular examples are Apache.Math [7] for Java, the ode* functions MATLAB [8] or\nscipy.integrate.odeint [9] for Python.\nII. REQUIREMENTS\nThe main goal of odeint is to provide a modern and fast C++ library for solving the initial value problem (IVP) of\nODEs. Furthermore, emphasis is put on the following points:\nContainer independence – The state type of the ODE should be parametrized by the user of odeint. It should\nbe possible to use odeint with the most common types in C++ such as vector< double > or array< double , N\n>. It must also be possible to work in the complex domain simply by using array< complex< double > , N > as\nrepresentation for the state. This is already a major improvement over most of the existing libraries. Furthermore, it\nmust be possible to use odeint with exotic state types like matrices, complex networks, and vectors or arrays living\non modern GPUs.\nOperation independence – It must be possible to change the way numerical operations are performed. This way\none can use odeint with SIMD (Single instruction multiple data) operations and arbitrary precision types.\nHigh performance – Odeint should be fast and its performance must be at least comparable to standard software\non ODEs like gsl and NR.\nGenerality – The design of odeint must be generic, such that it is possible to implement arbitrary solvers within\nthe framework and its interfaces. It must support the classical Runge-Kutta steppers, implicit methods and solvers\nfor stiff systems, as well as symplectic solvers and multistep methods. Step size control must be implement and it\nshould use dense output functionality if the solver under consideration supports it.\nar\nX\niv\n:1\n11\n0.\n33\n97\nv1\n  [\ncs\n.M\nS]\n  1\n5 O\nct \n20\n11\n2FIG. 1: Brief overview over the structure of odeint.\nIII. LIBRARY STRUCTURE AND DESIGN\nOdeint is an open source library. It is available via subversion or by direct download [4]. Odeint lives within the\nboost ecosystem [10] – a collection of state-of-the-art C++ libraries. The boost libraries are well know within the\nC++ community for their high quality. Several C++ standard libraries have been implemented and released here.\nAt the moment odeint is under development, therefore it is not an official boost library. It is planned to bring odeint\nto a level that it can be accepted as a full boost library.\nThe design of odeint is based on generic programming and functional programming using the advantages of the\nC++ template system [11, 12]. Generic programming allows one to use static polymorphism also known as compile-\ntime polymorphism to create the basic parts of the library. This has the advantage that all parts are known during\ncompilation and the C++ compiler can use mighty optimization techniques to build fast run-time machine code. Run-\ntime polymorphism is not sufficient here since it always results in direct function calls which can not be optimized.\nOdeint is a header-only C++-library. So, all one has to do to use odeint is to include the appropriate headers. The\noverall structure of odeint is shown in Fig. 1. It consists of four basic parts: one part defines the steppers, a second\none defines integrate functions which use the steppers to compute a numerical solution of the ODE. In the third\npart different algebras are introduced which are responsible for performing the basic operations in the most common\nsteppers while a fourth one defines utility and helper functions, for example functions for resizing, copying, etc.\nIn classical object-oriented languages one uses interfaces or abstract classes to define the methods and functionality\na special class has to fulfill [13]. In generic programming this is not possible. Here, one defines concepts which are\nnot written within the source code but which are defined elsewhere, for example in the documentation. A concept is\na convention how a class can be used, e.g. which methods it must provide and which types it defines. In odeint four\nbasic stepper concepts are defined.\nThe most general concept is the Stepper concept which corresponds to the basic interface one expects on a solver\nfor ODEs. Any stepper fulfilling this concept has to have a method do step( ode , x , t , dt ) which performs\none iteration of the ODE defined by ode with the current state x at the time t with a step size dt. This method\ntransforms the state of the ODE in-place, meaning that the state of the ODE is updated inside the same container.\nThe concept also defines an out-of-place do step method. Furthermore, it must define the types to represent the\nstate, the time, the derivative of the state, the basic value type and a function returning the order of the stepper.\nThe ODE enters the stepper as a template parameter of do step. It can be a function pointer or a functor, hence a\nclass with a public operator(). Furthermore, the function signature should be in most cases ode( x , dxdt , t ).\nBesides the stepper concept an ErrorStepper concept is available which only differs from the stepper concept by\nthe do step method. Here, this method also fills a state type containing the error made during one step do step(\node , x , t , dt , xerr ). This error might be used by an appropriate step size controller. For such controllers\nan own concept exist – the ControlledStepper concept. It defines a method try step( ode , x , t , dt ) which\nwill try to perform a controlled step. If this trial has been successful t is increased, dt is adapted, and the state x\nis set to its new value. Furthermore, this method will return an enum indicating whether the current step has been\naccepted and the step size is unchanged or increased or if the step has been rejected. In odeint several controlled\nsteppers exist, for example one which has an error stepper as a parameter and several specialized controlled steppers\nfor specific steppers. Finally, the DenseOutputStepper concept defines steppers with dense output functionality.\nHere, the do step-method has the signature do step( ode ), hence a dense output stepper controls the state and\nthe step size internally.\nSome steppers fulfill more than one of the above concepts. For further specialization several sub concepts have\nbeen defined which provide nicer interfaces to the stepper. For example the classical explicit steppers and the explicit\n3TABLE I: Overview over the steppers implemented in odeint.\nMethod Class name Stepper concept Notes\nExplicit euler explicit euler S\nRunge-Kutta 4 explicit rk4 S\nRunge-Kutta Cash-Karp explicit error rk54 ck SE\nDormand-Prince 5 explicit error dopri5 SE Can be used with\ndense output\nImplicit Euler implicit euler S\nRosenbrock 4 rosenbrock4 SE Provides a separate controller\nand dense output\nSymplectic Euler symplectic euler S\nDefault controller controlled error stepper C Works with explicit error steppers\nDefault dense output explicit dense output D Works with Dormand-Prince 5\nListing 1: Examplatory usage of odeint.\ntypedef std::tr1::array < double , 3 > state_type;\nvoid lorenz( const state_type &x , state_type &dxdt , double t ) {\ndxdt [0] = 10.0 * ( x[1] - x[0] );\ndxdt [1] = 28.0 * x[0] - x[1] - x[0] * x[2];\ndxdt [2] = - 8.0 / 3.0 * x[2] + x[0] * x[1];\n}\ninline void write_state ( const state_type &x , double t ) {\nstd::cout << t;\nfor( size_t i=0 ; i<x.size() ; ++i ) std::cout << \"\\t\" << x[i];\nstd::cout << \"\\n\";\n}\nexplicit_rk4 < state_type > rk4;\nstate_type x = {{ 10.0 , 10.0 , 10.0 }};\nfor( size_t i=0 ; i <1000 ; ++i )\nrk4.do_step( lorenz , x , 0.0 , 0.01 );\nexplicit_error_dopri5 < state_type > dopri5;\nintegrate_const( dopri5 , lorenz , x , 0.0 , 1000.0 , 1.0 , write_state );\nFSAL-steppers (first same as last) have their own concepts. At the moment some standard steppers have been\nimplemented, see Table I. For future versions of odeint it is planned to implement several other schemes.\nThe steppers can be used inside the integrate functions which allow an easy generation of the numerical solution\nof an ODE; integrate const generates a solution with constant step size and integrate adaptive iterates the\nsolution with step size control. All integrate functions take full advantage of the specific stepper type. For example, if\na dense-output stepper is used with integrate const it performs the largest possible steps and evaluates the solution\nwith the help of the dense output functionality.\nTo access the state of the current numerical solution of the ODE every integrate function accepts an additional\nargument – an observer. This observer has to be a function pointer or a functor and it can be used to write the\nstate of the ODE or to do some statistical analysis. An example how the integrate functions and the observers play\ntogether is shown in Listing 1. It is also possible to compose the observer from functional programming libraries, like\nBoost.Lambda.\nAs stated above one of the main requirements of odeint is its container independence. For this reason an algebra\nconcept has been introduced which is used by most of the steppers (but not by all). It defines how basic operations\non a state type are performed. Steppers taking advantage of the algebra concept are parametrized by the algebra.\nThis gives very interesting possibilities for high performance computing using modern GPUs and is one of the major\nadvantages of odeint. For example, it is possible to perform parameter studies of a given ODE on a GPU where a\n4whole ensemble of ODEs with different parameters is iterated in parallel. Or it is possible to study large lattices of\ncoupled ODEs or discretized PDEs on a GPU. Examples how one can use odeint with GPUs and CUDA are shown\nin the documentation of odeint.\nOther examples where one can use the advantages of the algebras within odeint are exotic state types. For example,\nit is possible to create algebras which work with the containers from the GSL. Another interesting use case are ODEs\ndefined on regular n-dimensional lattices where the underlying state type is a container with n-indices. Again, in\nthis case one has to specialize the algebra and one can use all routines from odeint. ODEs on complex networks and\ngraphs are also possible to solve with odeint as far as an appropriate algebra has been implemented.\nIV. CONCLUSION\nIn this article odeint – a high level C++ library for solving ordinary differential equations has been introduced. Its\nmain advantages over existing solutions is its performance and its container independence making this library feasible\nfor many use cases ranging from educational purposes to high-performance computing. During the development of\nodeint many advanced programming techniques like template meta programming, expression templates and functional\nprogramming have been used. For example, a new technique for implementing the explicit Runge-Kutta steppers\nhas been developed as well as a Taylor stepper of arbitrary order, where the first k derivatives of the ODE are\nevaluated using expression templates and auto differentiation. It is planned to continuously expand odeint with\nnew implementations of existing steppers. Furthermore, odeint may serve as a playground for research on numerical\nsolution of ordinary differential equations.\n[1] E. Ott. Chaos in Dynamical Systems. Cambridge Univ. Press, 2nd edition, Cambridge, 2002.\n[2] Ernst Hairer, Syvert P. Nørsett, and Gerhard Wanner. Solving Ordinary Differential Equations I: Nonstiff Problems.\nSpringer, Berlin, 2nd ed. 1993. corr. 3rd printing. 1993. corr. 3rd. edition, 2009.\n[3] Ernst Hairer and Gerhard Wanner. Solving Ordinary Differential Equations II: Stiff and Differential-Algebraic Problems.\nSpringer, Berlin, 2nd ed. 1996. 2nd printing. edition, 2010.\n[4] http://www.karstenahnert.com/software/odeint/ (6. May 2011).\n[5] http://www.gnu.org/software/gsl/ (6. May 2011).\n[6] S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. Numerical Recipes in C: The Art of Scientific Computing. Cambridge\nUniversity Press, Cambridge, 2nd edition, 1993.\n[7] http://commons.apache.org/math/ (6. May 2011).\n[8] http://www.mathworks.de/ (10. May 2011).\n[9] http://www.scipy.org/SciPyPackages/Integrate (6. May 2011).\n[10] http://www.boost.org (6. May 2011).\n[11] Andrei Alexandrescu. Modern C++ Design, Generic Programming and Design Patterns Applied. Addison-Wesley Long-\nman, Amsterdam, 2001.\n[12] Nicolai M. Josuttis and David Vandevoorde. C++ Templates: The Complete Guide. Addison-Wesley Longman, Amster-\ndam, 12. edition, 2002.\n[13] E. Gamma, R. Helm, R. Johnson, and J. Vlissides. Design Patterns: Elements of Reusable Object-Oriented Software.\nAddison-Wesley, 1995.\n",
            "id": 787195,
            "identifiers": [
                {
                    "identifier": "10.1063/1.3637934",
                    "type": "DOI"
                },
                {
                    "identifier": "445268173",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:arxiv.org:1110.3397",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "2049797245",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "2228026",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "1110.3397",
                    "type": "ARXIV_ID"
                }
            ],
            "title": "Odeint - Solving ordinary differential equations in C++",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2049797245",
            "oaiIds": [
                "oai:arxiv.org:1110.3397"
            ],
            "publishedDate": "2011-01-01T00:00:00",
            "publisher": "'AIP Publishing'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1110.3397"
            ],
            "updatedDate": "2021-07-20T15:54:27",
            "yearPublished": 2011,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1110.3397"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/787195"
                }
            ]
        },
        {
            "acceptedDate": "2011-11-07T00:00:00",
            "arxivId": "1110.6483",
            "authors": [
                {
                    "name": "Balas, V. E."
                },
                {
                    "name": "Motoc, I. M."
                },
                {
                    "name": "Popescu-Bodorin, N."
                }
            ],
            "contributors": [
                "N."
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/188704222"
            ],
            "createdDate": "2012-04-13T14:19:19",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2011-09-01T00:00:00",
            "abstract": "The main topic discussed in this paper is how to use intelligence for\nbiometric decision defuzzification. A neural training model is proposed and\ntested here as a possible solution for dealing with natural fuzzification that\nappears between the intra- and inter-class distribution of scores computed\nduring iris recognition tests. It is shown here that the use of proposed neural\nnetwork support leads to an improvement in the artificial perception of the\nseparation between the intra- and inter-class score distributions by moving\nthem away from each other.Comment: 6 pages, 5 figures, Proc. 5th IEEE Int. Symp. on Computational\n  Intelligence and Intelligent Informatics (Floriana, Malta, September 15-17),\n  ISBN: 978-1-4577-1861-8 (electronic), 978-1-4577-1860-1 (print",
            "documentType": "research",
            "doi": "10.1109/isciii.2011.6069760",
            "downloadUrl": "http://arxiv.org/abs/1110.6483",
            "fieldOfStudy": "computer science",
            "fullText": "Iris Codes Classification Using Discriminant and \nWitness Directions \n \nN. Popescu-Bodorin\n*\n, Member, V.E. Balas\n**\n, Senior Member, and I.M. Motoc\n*\n, Student Member, IEEE \n*Artificial Intelligence & Computational Logic Lab., Math. & Comp. Sci. Dept., „Spiru Haret‟ University, Bucharest, Romania \n**Faculty of Engineering, „Aurel Vlaicu‟ University, Arad, Romania \nbodorin@ieee.org, balas@drbalas.ro, motoc@irisbiometrics.org \n \nAbstract- The main topic discussed in this paper is how to use \nintelligence for biometric decision defuzzification. A neural \ntraining model is proposed and tested here as a possible solution \nfor dealing with natural fuzzification that appears between the \nintra- and inter-class distributions of scores computed during \niris recognition tests. It is shown here that the use of proposed \nneural network support leads to an improvement in the artificial \nperception of the separation between the intra- and inter-class \nscore distributions by moving them away from each other. \nI. INTRODUCTION \nThe relation between fuzziness and intelligence is an open \nproblem these days. Fuzzy instruments are usually being used \nto attempt intelligent problem solving in conditions of incerti-\ntude / imprecision and this is also the case discussed here. The \nmain topic of this paper is how to use intelligence in order to \nachieve biometric decision defuzzification. A neural training \nmodel is proposed here as a possible solution for dealing with \nnatural fuzzification that appears between the intra- and the \ninter-class score distributions computed during iris \nrecognition tests. Are the sets of iris codes somehow separa-\nble in a neural perspective? Are the genuine and imposter \npairs two separable classes in some space? Is there a neural \nnetwork structure able to decrease the degree of confusion \nbetween inter- and intra-class distributions of scores? It is \nshown here that using neural-network support leads to an im-\nprovement in the artificial perception of the separation \nbetween intra- and inter-class distributions of scores by \n„moving‟ the two score distributions away from each other. \nUsually in biometric identification / verification, the \nseparation between intra- and inter-class distributions of \nscores is vague (Fig. 1 in [3]). Even when working on an ideal \niris image database [9] this fuzzification is inherent. There are \nfour main categories of factors leading to the fuzzification of \nthe two score distributions: firstly, the acquisition and \nsegmentation conditions, secondly, the feature encoding and \nfeature matching conditions, thirdly, the different posture of \nthe eye relative to the camera, and last but not least, the fact \nthat the laws of radial iris movement are, in fact, unknown, \nand therefore, successful matching of two samples taken for \nthe same iris is far from being guaranteed when pupil is \ncaptured at very different dilations in those two samples. \nEach time when the recognition system negotiates between \nspeed and accuracy, if a degree of imprecision is accepted as \na counterbalance for gaining speed processing, fuzzification \nof the two classes of scores is guaranteed. The same situation \noccurs when the system is not endowed with suitable methods \nenabling successful recognition of the same iris captured in \ndifferent acquisition conditions. \nThe fuzzification between intra- and inter-class scores is \nusually (and paradoxically) expressed through a crisp \nconcept, namely the Equal Error Rate (EER, [2], [3]). The \nexistence of such a crisp point was not confirmed in our \npreviously undertaken iris recognition tests ([3] - [5]). Indeed, \nit can be seen in the mentioned references (especially in Fig. 2 \nand Fig. 3 from [3]) that EER point varies from one \nrecognition test to another and, in fact, the experimental \nmeasurement corresponding to the theoretical concept of EER \nis a fuzzy EER interval - a collection of recognition thresh-\nolds for which it is very hard (or simply impossible) to say for \nsure if they are recognition scores rather than rejection scores \nor vice-versa. It can be said that in the fuzzy EER interval, the \nrecognition and rejection are fuzzy (vague / imprecise / \nalmost / quasi) equal probable. In the terms proposed and \ndiscussed in [10], the fuzzy EER interval (f-EER) is the \nf-geometry corresponding to the crisp (but theoretical) \nprototype EER. In terms of logic [6], f-EER corresponds to a \nthird logical state „u‟ (unprecisated and uncertain) of the \nbiometric system, different from 0 (which encodes an \nimposter pair of samples) and 1 (which encodes a genuine \npair of samples). It is shown in [5] (see Theorem 2 in [5]) that \nthe logic of such a system is induced by a Boolean algebra of \nmodulo 8 integers. Here in this paper we will further show \nthat despite being unprecisated and uncertain, the fuzzy EER \ninterval (f-EER) is not unprecisable. Defuzzification of f-EER \nwill be achieved here by using an adequate neural network \nsupport. In short, the theoretical crisp prototype EER is \nfuzzified into f-EER by compressing uint8 (8-bit unsigned \ninteger) iris images as binary codes (which are therefore \nimperfect and incomplete pieces of information, weakened \naliases of the original uint8 codes in a space of binary \nmatrices). This operation will be partially reversed by using \nneural network support in order to recover digital identities as \nneural memories from the available iris codes. \nA. Terminology \nIn this section we aim to clarify the difference between an iris \ncode and a digital identity. An iris code is a binary matrix that \nfollows to be recognized (accepted or rejected) as being \nrepresentative for an identity which is a symbolic or numeric\ndata structure associated to a person. In the simplest case, an \nidentity is a label - even it is encoded in a numeric vocabulary \n(like auto-number ID fields). In a little bit more complex \nscenario, a digital identity is a numeric data structure obtained \nby detecting and extracting common features in a set of sam-\nples taken for the same individual, discriminant features be-\ntween the sets of samples taken for different individuals, and \nby encoding all of these common and discriminant features in \na numerical space. Hence a digital identity is a memory that \ncan be trained with iris codes in order to recognize them, or in \nother words, the digital identity is a recognizer object, \nwhereas the iris code is a recognized object. \nA digital identity encodes more entropy than an iris code. \nAs a matrix, it may share the same dimension with the iris \ncode, but if this is the case, then the type of its components \nwill be different (all components having longer binary repre-\nsentation). On the other hand, in a multi-enrollment scenario, \nthe enrolled iris codes together define a digital identity (which \ntherefore contains the same type of components as the iris \ncodes contain, but has bigger dimension, [3]). \nIn the example that follows to be given in this paper, the \ndigital identities will be double matrices (trained neuronal \nmemories) of the same dimension as the iris codes. A \ndifferent approach involving neuro-evolutionary trained \nmemories (digital identities) can be found in [5]. \nB. Other Neural Approaches to Iris Recognition \nThe successfully neural approaches to iris recognition are \npretty rare indeed. It is somehow explicable because, even \nthis subject is not present at all in scientific publications, AI \ncommunity in general share a point of view according to \nwhich it is very hard to predict where a training procedure \ndeviates from learning features (learning a concept) to \nlearning specific data (memorizing specific instances of a \nconcept). We don‟t share this view. \nOn the other hand, why would or should somebody try such \na complicate solution for such a simple problem? Actually, \nthe neural networks are simple enough, much simpler than the \ncurrent state of affairs in the field of iris recognition. For \nexample, it was shown recently [5], [6] that the artificial \nunderstanding of iris recognition couldn‟t be binary and \nlogically consistent simultaneously if the imposter and \ngenuine score distributions collide into each other, whereas a \nfuzzy 3-valent disambiguated model of iris recognition [6] \ncan guarantee logical consistency. The utility of a neural \nnetwork is to deconfuse the two score distributions. \nII. HAMMING DISTANCE - A VECTORIAL \nPERSPECTIVE ON IRIS RECOGNITION \nIt is not the first time when we say that iris recognition as a \nfield of applied science is still in its childhood. Investigations \nof iris recognition as a problem of logic [6] and artificial \nintelligence [4], [5] are indeed very recent topics. On the \nother hand, in [6] we have shown how important it is the \nperspective from which iris recognition is viewed and \npracticed. It is illustrated there (Fig. 1.d in [6]) that improving \niris recognition theory and practice depends on searching, \nidentifying and accepting new perspectives over this domain. \nAnother argument sustaining our points of view will be \nfurther formulated, proved and explained in this paper: the \npresent way of using Hamming distance for iris code \ncomparisons is a pure vectorial manner of understanding iris \ncodes. Hence, similarity of two matrices is decided using \nvectorial means only. It is not just the fact that representing \nirides as iris codes is a uint8-to-binary lossy compression but \nthese codes are further compared only as vectors, without \ntaking into account any matrix-type means and properties. In \nthis context could we (or should we) be still surprised about \nthe fact that the imposter and the genuine score distributions \n(Fig. 1 in [3]) usually collide into each other? \nA. Hamming distance - a pure vectorial manner of \nunderstanding and practicing iris code comparisons \nLet us consider the simplest case of two unmasked iris codes \nIC1, IC2 of the same dimension w h. Their Hamming \nsimilarity score (the complement of Hamming distance \nrelative to the unitary score) is: \n            \n                    \n   \n        (1) \nLet us reshape the iris codes IC1, IC2 as the vectors IC1(:), \nIC2(:) of length ℓ = w∙h, and denote C1,2 = (IC1 == IC2), which \nis also a vector and will be further referred to as a comparison \ncode. The expression of Hamming similarity score becomes: \n            \n              \n \n  \n         \n \n      (2) \nand further, if the notation C1,2 is simplified to C, then: \n      \n   \n   \n  \n      \n      \n  \n  \n  \n  \n  \n  \n              (3) \nwhere: D and W are further referred to as the discriminant \ndirection and the witness direction, respectively (in this \nparticular case they are both trivial and equal to the diagonal \nof the unit hypercube in ℝℓ), CD and WD are the orthogonal \nprojections of C and W onto D, respectively, and „.‟ signifies \nthe scalar product of two vectors. Hence we‟ve proved the \nfollowing theorem: \nTheorem 1 (N. Popescu-Bodorin): \nIn the binary iris code space {0, 1}\nℓ\n, the Hamming distance is \na purely vectorial feature computable through orthogonal \nprojection of the comparison code onto the main diagonal of \nthe unit hypercube in ℝℓ. \nB. The problem \nNow, we are able to see how a crisp (consistent) theory of iris \nrecognition written in terms of discriminant and witness di-\nrections would look like. Let us denote as PC a conjunction of \nprerequisite conditions (relative to the image acquisition and \nprocessing at all levels from eye image to the iris code) ex-\npressed in binary logic, and let               a family of nontrivial \ndiscriminant directions that need to be established for k \nenrolled identities, C = I   G - the partitioning of the \ncomparison code space in imposter comparison codes (I ) and \ngenuine comparison codes (G ), W - the trivial witness \ndirection, S - a similarity score computed as: \n      \n   \n   \n                                        (4) \nfor each pair (ICi, Di) which represents the same identity. \nRegardless the fact that the similarity score could be \ncomputed otherwise than in (4), a consistent theory of iris \nrecognition would then say that: \n                                          (5) \nor in other words, if the prerequisite conditions are fulfilled, \nthen the maximum imposter similarity score should be \nsmaller than the minimum genuine similarity score, whereas a \ncomfortable theory of iris recognition would say that: \n                                          (6) \nor in other words, a safety band can be fitted between the \nimposter and the genuine similarity score distributions (i.e. \nthe biometric system can be described by the fuzzy 3-valent \nmodel of iris recognition, [5], [6]). \nThe main goal of this paper is showing that discriminant \ndirections and an iris recognition theory of type (5) can be \nlearned heuristically by using neural network support. \nIII. PROPOSED METHODOLOGY \nKnowing irides and recognizing irides are two very different \nthings. It must be stated clearly if our approach here is \nintended to announce and describe new iris recognition results \nor to formulate new points of view about irides and iris codes. \nIn order to improve previously iris recognition approaches or \nto formulate new iris recognition methodologies, it is neces-\nsary to find new knowledge about irides and iris codes. One \nof our hypotheses is that the confusion between the genuine \nand imposter score distributions is motivated by the relative \nposition of some iris codes in the iris code space. All iris \ncodes extracted from samples taken for the same eye of the \nsame person are viewed here as clusters in the iris code space, \nnamely personal clusters. The separation between the person-\nal clusters is fuzzy, or otherwise the genuine and imposter \nscore distributions should not collide into each other. In this \ncontext, defuzzification between the two score distributions \nshould be achievable if each personal cluster would be \nendowed with a suitable discriminant charged centroid, \nstrong enough to alter the space in its immediate proximity by \ndiscriminately attracting the members of the personal cluster \nand repulsing non-members away from the personal cluster. \nThis paper shows that it is possible to learn such special \ncentroids as digital identities, using neural network support. \nA.  Iris Segmentation and Encoding \nSince neural network training is known to by very sensitive to \nnoise, the segmentation procedure used here is designed to \nminimize the noise presence in the extracted samples by \n \n \n \nFig. 1. Iris segmentation: a quarter is selected from each iris sample. \navoiding the chances that eyelids and eyelashes to escape \nundetected and unfiltered. On the other hand, neural network \ntraining is known to be more expensive when the iris codes \ngrow bigger. For all of these reasons, we choose to work with \njust a quarter of the actual iris segment, as illustrated in Fig. 1. \nIn this way, iris code dimension and noise presence are both \nkept to a minimum, whereas signal-to-noise ratio is \nmaximized. Circular Fuzzy Iris Segmentation (CFIS2, [3]) is \nused in order to extract the iris segment as a circular ring. A \nquarter of this circular ring (Fig. 1) is unwrapped and further \nencoded as a binary iris code using Haar-Hilbert encoder [3]. \nB. The Learning Algorithm and Neural Network Structure  \nA well suited neural network for learning discriminant \ndirections is a recurrent neural network which must be \ncompatible with the following space requirements and with \nthe learning algorithm described below. System memory is \ndesigned to hold: the current recognition threshold t, the \nsafety band sb, the numbers of identities k, stopping flag stop \nthe discriminant directions               which are currently \ntrained, the iris codes IC on which the current discriminant \ndirection is trained on, and other calibration variables as \nlearning rates r and b. \nHeuristic Blind Training of Discriminant Directions – HBTDD \n(N. Popescu-Bodorin, V.E. Balas) \n1. Initialize sb, t, k, stop; \n2. Initialize all              as random vectors of binary digits \n3. Until stop do: stop = 1 and: \n4.    For each comparison code Cj,i \n5.       If S(Cj,i) is not on the correct side of the safety band: \n6.          stop = 0; \n7.          or Cj,i   G and then: \n8.             Dj = Dj + r ∙ Cj,i - r ∙       and sb = sb - b; \n9.          or Cj,i   I and then: \n10.           Dj = Dj - r ∙ Cj,i + r ∙       and sb = sb + b; \n11.     EndIf; \n12.  EndFor; \n13. EndUntil; \n14. END; \nIn the above heuristic procedure,       denotes the binary \ncomplement of Cj,i. HBTDD procedure stops only if all \ndiscriminant directions               are trained i.e. they produce \nsimilarity scores positioned correctly for each comparison \ncode and outside the safety band. \nThe search is blind in HBTDD because there is no strategy \nfor choosing initial discriminant directions and, most \nimportant, the entire procedure ignores the results of any \nTuring test [8] of iris recognition. Hence, the training of \ndiscriminant directions is made by following biometric \ndecisions given by an imprecise software agent (Fig. 1 in [3]), \nnot the accurate biometric decision given by a human agent \n(Fig. 1.a in [6]). This is why it is considered here that HBTDD \nworks much better than initially expected. Even it is a worst \ncase scenario, an uninformed search in which no specific \nknowledge about digital identities was used, HBTDD ensures \nartificial learning of the discriminant directions and artificial \nunderstanding of an iris recognition theory of type (5). \nC. Numerical results \nThe numerical results presented in Fig. 2 and Fig. 3 illustrate \nan incipient state of training the discriminant directions \nobtained through HBTDD after 4 iterations, with a safety \nband of 0.01 in width. It proves that iris code classification \nbased on Hamming similarity can be considered a particular \ncase of iris codes classification using discriminant and \nwitness directions. \nIt can be seen in Fig. 2 that discriminant directions are weak \ndigital identities which diminish the confusion between \nimposter and genuine score distributions. Still, separation \nbetween the two classes of scores is weak illustrating that \nlearning fuzzified prototypes (Fig. 1 in [3]) may reduce the \nerror of classifying imposter and genuine comparison codes \nbut the performances obtained by learning crisp prototypes \n(Fig. 1.a in [6]) are far much better, [5]. However, as it can be \nseen in Section V of this paper, the information that the \nclasses of comparison codes are separable in a neural perspec-\ntive by using discriminant and witness directions can be \nexploited in the Intelligent Iris Verifier architecture ([4], [5]). \nD. Geometric Interpretation \nIt can be seen in Fig. 2 that for any enrolled iris code, the \nensemble formed by the corresponding discriminant and \nwitness directions act like a lens through which the iris code \nsee its farthest friend (the farthest iris code from the same \npersonal cluster, with which it forms the lowest scored \ngenuine pair) as being closer to him than the nearest enemy \n(the nearest iris code from any different personal cluster, with \nwhich it forms the highest scored imposter pair).  \nFor all enrolled iris codes the difference between the lowest \ngenuine similarity score and the highest imposter similarity \nscore is at least 0.03. Hence, HBTDD is a reliable solution for \nany personal-use application of iris recognition being able to \ndeliver a kind of nearest-neighbor based biometric decisions \nwhich are safer than those based on Hamming \ndistance/similarity. \nThe recognition function describing a biometric decisional \nmodel based on discriminant and witness directions is defined \nas: \n                                        (7) \n                \n   \n   \n                   (8) \n \nFig. 2. Statistics of all-to-all comparisons. \n \nFig. 3. For each sample, the farthest friend is closer than the nearest enemy. \nFor each enrolled identity D and for each comparison code let \nus consider the vector: \n                   \n \n   \n       \n \n   \n     (9) \nIf an iris recognition theory of type (5) is learned through \ndiscriminant and witness directions, the correspondence: \n                                             (10) \nmaps all imposter comparison codes into a hypersphere \nwithin the unit hypersphere in ℝℓ and all genuine comparison \ncodes in between the two hyperspheres. \nIV. IRIS RECOGNITION FORMAL THEORIES \nLet us recall that PC is a conjunction of prerequisite condi-\ntions regarding image acquisition and all image processing \nsteps that must be undertaken in order to generate iris codes, \nC is a comparison code, S(C) is a similarity score, I and G are \nthe sets of imposter and genuine comparison codes respec-\ntively, and ⨂ denotes logical exclusive disjunction. Above in \nthis paper there are two variants of formal iris recognition \ntheories that have been discussed already, namely: \n                                         (11) \n                                         (12) \nLet us see now how a formal iris recognition theory should \nlook like if the recognition wouldn‟t be made by an artificial \nagent, being made by a human agent instead. We recall that \nthe geometry which illustrates the biometric decisions given \nby a human agent (Fig. 1.a, in [6]) is a crisp geometry. Hence, \nthe corresponding formal theory of iris recognition is no \nlonger a matter of nuance, degree, incertitude and imprecision \nregarding the genuine and imposter scores and their \ndistributions, but a crisp theory encodable in binary logic: \n                    ⨂                   (13) \nHence, it makes sense trying to implement software agents \nenabled to mimic the crisp theory (13), as close as possible, \nthrough a fuzzy iris recognition theory: \n              ⨂         ⨂             (14) \nwhere   ,   ,    are the fuzzy values of truth within a fuzzy \n3-valent disambiguated logical model of iris recognition [6], \nand: \n                                             (15) \ni.e. the volume of ambiguous comparison codes is negligible \nrelative to the other two volumes of imposter and genuine \ncomparison codes, respectively. \nThe list of qualitative conditions imposed while building the \ntheory (14) can be extended with fuzzy rules like „the safety \nband must by as wider as possible‟, or „the volume of ambig-\nuous comparison codes must be negligible relative to the \nnumber of iris codes sampled for a single eye‟, or regarding \nthe nature and the properties of discriminant directions. \nWe saw that above, the discriminator directions are weak \ndigital identities when their training relies on a fuzzy \nprototype recognition function and stationary learning rules. \nTheir weakness is reflected in the width of the safety band. \nHence, it is desirable to train robust digital identities able to \nensure a wide safety band, a wide gap between the safe \nimposter and the safe genuine scores. Therefore, our personal \nlist of challenges [4] increases with one: \n(C.7.2.) Find evolutionary methods for encoding enrolled iris \ncodes through robust digital identities determined as trained \ndiscriminant directions enabled to give a good approximation \nfor the crisp recognition prototype function previously \ndetermined in a Turing test of iris recognition (N. Popescu-\nBodorin). \nV. IIV BALANCED SYSTEM \nThe results obtained by training discriminant directions (as \nrobust digital identities) on IIV infrastructure [5] are \nillustrated in Fig. 4 and Fig. 5. The new recognition system \nobtained in this way is an IIV Balanced System based on \nlearning robust discriminant and witness directions. It is „bal-\nanced‟ because the values of genuine and imposter absolute \nsafety rates (77.64% vs. 85.19%) are more balanced than the \nvalues obtained in the previous IIV simulations [5], [4]. \nThe fuzzy 3-valent disambiguated model [6] corresponding \nto the IIV Balanced System is shown in Fig. 4. \nThe eye image database [9] was split into two parts: the \ntraining set – containing 5 samples per iris, and the test set \nwhich contains 15 samples per iris or less (14) in the cases of \nfailed segmentation (there are 3 cases of failed segmentation \nbetween all 1000 images of the database). Then robust \ndiscriminant directions have been learned as double matrices \non IIV infrastructure from binary iris codes of dimension \n64 64 extracted as in Fig. 1. \nFig. 4 and Fig. 5 together illustrate what we call intelligent, \nconsistent and logically argued / motivated biometric safety. \nThe difference between how different people understand the \nconcepts of „statistically motivated biometric safety‟ and \n„logically motivated biometric safety‟ is illustrated by a \ncomparison between the results presented in Fig. 4 and Fig. 5 \nand the results presented in Fig. 1 from [3], or in Fig. 9.a - \nFig. 9.f and Table 6 from [2] for the same image database [9], \nor in Fig. 10 from [1] and in Fig. 4 from [3]. \nIf a biometric system for personal use is detached from IIV \nBalanced System and endowed with nearest neighbor based \nbiometric decisional support, its safety band would be really \nwide having 0.4 in width (see Fig. 5). \nIt can be seen in Fig. 4 that the IIV Balanced System proves \nto have a crisp understanding of what it means to be a genuine \npair (or a genuine comparison code) for 85.19% of all \ngenuine cases (which are scored with crisp unitary recogni-\ntion score), a crisp understanding of what it means to be an \nimposter pair (or an imposter comparison code) for 77.64% of \nall imposter cases (scored with crisp null recognition score), a \nfuzzy understanding of what it means to be a genuine pair for \n14.81% of all genuine cases, (with fuzzy unitary recognition \nscore), a fuzzy understanding of what it means to be an \nimposter pair for 22.36% of all imposter cases (scored with \nfuzzy zero recognition score), a global f-consistent ([6], [10]) \nand complete understanding of iris recognition (being able to \ngive the correct biometric decision for each enrollable pair), a \nhuge safety band (a huge f-EER interval; see the statistics of \nall-to-all comparisons in Fig. 4) reflecting the artificial \nconsistent understanding of three concepts: „genuine‟, \n„imposter‟ and „unenrollable‟ (unsafe / uncertain) pair, i.e. the \nartificial fuzzy 3-valent disambiguated understanding of iris \nrecognition. \nThe difference between the safety bands obtained through \nHBTDD and IIV Balanced System is explicable in two ways: \nfirstly, the training tools are more performant in the second \ncase, and secondly, the learned prototype is accurate only in \nthe second case. This illustrates two things: the importance of \nTuring tests in artificial intelligence (it is the only way of \ncorrectly encoding human intelligence in numerical data) and \nthe importance of the informed search. In other words, an \ninformed search (an advanced artificial intelligence tool) \ntargeting a correct prototype will always overcome a blind \nsearch targeting an incorrectly chosen prototype. \nVI. CONCLUSION AND FUTURE WORK \nIn a perfect world, perfect processing tools would exist \nensuring that the crisp human perception of iris recognition is \nperfectly replicable as a crisp artificial perception. In reality, \nthe crisp human perception of iris recognition is expressed as \na set of Horn clauses. The world being far from perfect and \nthe processing tools being imprecise enough, the artificial \nperception of the iris recognition fuzzifies the Horn clauses \ninto fuzzy if-then Sugeno rules [7]. It is a special kind of \nlossy compression that we would call semantic compression \nand which was previously named, less suggestive, as \nfuzzification. Two actual distinct concepts - namely „genuine‟ \nand „imposter‟, represented in a space which is large enough \nto hold them distinct, are „mirrored‟ into a smaller space (are \ncompressed, fuzzified) where their images are no longer \ndistinct. Hence the artificially perceived concepts (mirrored \nconcepts) lost an important part of their original meaning, \nespecially the meaning of their distinct individuality. It is \nclear now why we said that, in our case, fuzzification means a \nlossy compression of meaning. \nThis paper showed that, in some conditions, by using tools \nof artificial intelligence, the memory of distinct individuality \ncan be partially reconstructed / recovered / rediscovered \n(trained) from a number of compressed samples which taken \ntogether as a whole (not individually) host a hidden and \napparently lost meaning of the original data. \nThe reconstruction achieved with IIV Balanced System has \nso much quality that the recovered memory of distinct \nindividuality allows a wide and comfortable safety band \ninbetween the numerical representations of the artificial \nperceived concepts of „genuine‟ and „imposter‟ comparisons. \nHence, we came up to this point where we showed that \n„state of the art‟ in iris recognition means separating the \nimposter and genuine score distributions with a wide safety \nband, not just with the so called recognition threshold. Still, \nour future works in this field depend almost exclusively on \nthe socio-economical acceptability of this simple and evident \ntruth. \nACKNOWLEDGMENT \nThe authors would like to thank Professor Donald Monro \n(Dept. of Electronic and Electrical Engineering, University of \nBath, UK) for granting the access to the Bath University Iris \nImage Database. \nREFERENCES \n[1] J.G. Daugman, “How Iris Recognition Works,” IEEE Trans. on cir-\ncuits and Systems for Video Technology, Vol. 14, No. 1, January 2004. \n[2] P. Grother, E. Tabassi, G. Quinn, W. Salamon, “Interagency report \n7629: IREX I - Performance of iris recognition algorithms on standard \nimages,” N.I.S.T., October 2009. \n[3] N. Popescu-Bodorin, V.E. Balas, “Comparing Haar-Hilbert and Log-\nGabor based iris encoders on Bath Iris Image Database,” Proc. 4th \nInternational Workshop on Soft Computing Applications, pp. 191-196, \nIEEE Press, July 2010. \n \nFig. 4. IIV Balanced System - Statistics of all-to-all comparisons and its \nfuzzy 3-valent disambiguated logical model. \n \nFig. 5. For each sample of the database [9], the farthest friend is closer than \nthe nearest enemy - the minimum genuine score is much greater than the \nmaximum imposter. The safety band has almost 0.3 in width. \n[4] N. Popescu-Bodorin, V.E. Balas, “Learning Iris Biometric Digital \nIdentities for Secure Authentication. A Neural-Evolutionary \nPerspective Pioneering Intelligent Iris Identification,” Recent Advances \nin Intelligent Engineering Systems, Springer Verlag, in press 2011. \n[5] N. Popescu-Bodorin, V.E. Balas, “Exploratory Simulation of an \nIntelligent Iris Verifier Distributed System,” Proc. 6th IEEE \nInternational Symposium on Applied Computational Intelligence and \nInformatics, pp. 259-262, IEEE Press, June 2011. \n[6] N. Popescu-Bodorin, V.E. Balas, I.M. Motoc, “8-Valent Fuzzy Logic \nfor Iris Recognition and Biometry,” 5th Int. Symp. on Computational \nIntelligence and Intelligent Informatics, Sep. 2011, Floriana, Malta. \n[7] M. Sugeno, T. Yasukawa, “A Fuzzy-Logic-Based Approach to \nQualitative Modeling,” IEEE Trans. on Fuzzy Systems, Vol. 1, No. 1, \nFebruary 1993. \n[8] A.M. Turing, “Computing machinery and intelligence,” Mind, 59, 433- \n460, 1950. \n[9] University of Bath Iris Image Database, September 2009, \nhttp://www.bath.ac.uk/elec-eng/research/sipg/irisweb/ \n[10] L.A. Zadeh, “Toward extended fuzzy logic A first step,” Fuzzy Sets \nand Systems, 160(2009), pp. 3175-3181. \n",
            "id": 786553,
            "identifiers": [
                {
                    "identifier": "2231112",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1109/isciii.2011.6069760",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:arxiv.org:1110.6483",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "188704222",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "1110.6483",
                    "type": "ARXIV_ID"
                }
            ],
            "title": "Iris Codes Classification Using Discriminant and Witness Directions",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:arxiv.org:1110.6483"
            ],
            "publishedDate": "2011-11-08T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1110.6483"
            ],
            "updatedDate": "2021-07-22T01:39:08",
            "yearPublished": 2011,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1110.6483"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/786553"
                }
            ]
        },
        {
            "acceptedDate": "2011-10-31T00:00:00",
            "arxivId": "1111.0090",
            "authors": [
                {
                    "name": "Alan J. Martin"
                },
                {
                    "name": "Alan J. Martin"
                },
                {
                    "name": "Alberto Momigliano"
                },
                {
                    "name": "Alberto Momigliano"
                },
                {
                    "name": "Amy Felty"
                },
                {
                    "name": "Amy P. Felty"
                },
                {
                    "name": "Andrei Popescu"
                },
                {
                    "name": "Andrew Gacek"
                },
                {
                    "name": "Andrew M. Pitts"
                },
                {
                    "name": "Brigitte Pientka"
                },
                {
                    "name": "Christian Urban"
                },
                {
                    "name": "Frank Pfenning"
                },
                {
                    "name": "Gopalan Nadathur"
                },
                {
                    "name": "Herman Geuvers"
                },
                {
                    "name": "Joëlle Despeyroux"
                },
                {
                    "name": "Michael Norrish"
                },
                {
                    "name": "Murdoch Gabbay"
                },
                {
                    "name": "N. G. de Bruijn"
                },
                {
                    "name": "Simon Ambler"
                },
                {
                    "name": "Tobias Nipkow"
                },
                {
                    "name": "Venanzio Capretta"
                },
                {
                    "name": "Venanzio Capretta"
                }
            ],
            "contributors": [
                "The Pennsylvania State University CiteSeerX Archives"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/190533515",
                "https://api.core.ac.uk/v3/outputs/25769629"
            ],
            "createdDate": "2012-04-13T14:19:20",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 145,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/145",
                    "logo": "https://api.core.ac.uk/data-providers/145/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 645,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/645",
                    "logo": "https://api.core.ac.uk/data-providers/645/logo"
                }
            ],
            "depositedDate": "2011-10-31T00:00:00",
            "abstract": "Hybrid is a formal theory implemented in Isabelle/HOL that provides an\ninterface for representing and reasoning about object languages using\nhigher-order abstract syntax (HOAS). This interface is built around an HOAS\nvariable-binding operator that is constructed definitionally from a de Bruijn\nindex representation. In this paper we make a variety of improvements to\nHybrid, culminating in an abstract interface that on one hand makes Hybrid a\nmore mathematically satisfactory theory, and on the other hand has important\npractical benefits. We start with a modification of Hybrid's type of terms that\nbetter hides its implementation in terms of de Bruijn indices, by excluding at\nthe type level terms with dangling indices. We present an improved set of\ndefinitions, and a series of new lemmas that provide a complete\ncharacterization of Hybrid's primitives in terms of properties stated at the\nHOAS level. Benefits of this new package include a new proof of adequacy and\nimprovements to reasoning about object logics. Such proofs are carried out at\nthe higher level with no involvement of the lower level de Bruijn syntax.Comment: In Proceedings LFMTP 2011, arXiv:1110.668",
            "documentType": "research",
            "doi": "10.4204/eptcs.71.6",
            "downloadUrl": "http://arxiv.org/abs/1111.0090",
            "fieldOfStudy": "computer science",
            "fullText": "H. Geuvers, G. Nadathur (Eds): LFMTP 2011\nEPTCS 71, 2011, pp. 76–90, doi:10.4204/EPTCS.71.6\nc© Alan J. Martin & Amy P. Felty\nThis work is licensed under the\nCreative Commons Attribution License.\nAn Improved Implementation and\nAbstract Interface for Hybrid\nAlan J. Martin\nDepartment of Mathematics and Statistics,\nUniversity of Ottawa, Canada\namart045@site.uottawa.ca\nAmy P. Felty\nSchool of Electrical Engineering and Computer Science and\nDepartment of Mathematics and Statistics,\nUniversity of Ottawa, Canada\nafelty@site.uottawa.ca\nHybrid is a formal theory implemented in Isabelle/HOL that provides an interface for representing\nand reasoning about object languages using higher-order abstract syntax (HOAS). This interface is\nbuilt around an HOAS variable-binding operator that is constructed definitionally from a de Bruijn\nindex representation. In this paper we make a variety of improvements to Hybrid, culminating in\nan abstract interface that on one hand makes Hybrid a more mathematically satisfactory theory, and\non the other hand has important practical benefits. We start with a modification of Hybrid’s type of\nterms that better hides its implementation in terms of de Bruijn indices, by excluding at the type level\nterms with dangling indices. We present an improved set of definitions, and a series of new lemmas\nthat provide a complete characterization of Hybrid’s primitives in terms of properties stated at the\nHOAS level. Benefits of this new package include a new proof of adequacy and improvements to\nreasoning about object logics. Such proofs are carried out at the higher level with no involvement of\nthe lower level de Bruijn syntax.\n1 Introduction\nHybrid is a system developed to specify and reason about logics, programming languages, and other\nformal systems expressed in higher-order abstract syntax (HOAS). It is implemented as a formal theory\nin Isabelle/HOL [15]. By providing HOAS in a modern proof assistant, Hybrid automatically gains\nthe latter’s capabilities for meta-theoretical reasoning. This approach is intended to provide advantages\nin flexibility and proof automation, in contrast to systems that directly implement logical frameworks,\nwhich must build their own meta-reasoning layers from the ground up. Building a system such as Hybrid\nwithin a general purpose theorem prover poses a variety of challenges. Our goal in this work is to improve\nthe implementation and interface of Hybrid’s basic theory, bringing it to a point where its potential\nadvantages can be more fully realized.\nUsing HOAS, binding constructs in the represented language (the object logic or OL) are encoded\nusing the binding constructs provided by an underlying λ -calculus or function space of the meta-logic,\nthus representing the arguments of these constructs as functions of the meta-level. Isabelle/HOL im-\nplements an extension of higher-order logic, where the function types are “too large” for HOAS in two\nsenses. First, they contain elements with irreducible occurrences of logical constants, which do not rep-\nresent syntax. Second, the function space τ ⇒ τ has larger cardinality than τ , so a variable-binding\noperator represented as a functional Φ of type (τ⇒ τ)⇒ τ cannot be injective. This makes it unsuitable\nfor syntax, for we cannot uniquely recover the argument F from a term of the form Φ(F). Our work\nbuilds directly on the original Hybrid system [1], whose solution to both problems is to use only a subset\nof the funtion type, identified by a predicate called abstr. It builds a type expr of terms with an HOAS\nvariable-binding operator definitionally in terms of a de Bruijn index representation.\nAlan J. Martin & Amy P. Felty 77\nIn earlier work joint with Alberto Momigliano, we gave a system presentation of Hybrid [14], which\nbuilt on the original Hybrid and serves as a starting point for the work presented here. In this paper, we\nfill in many details that could not be described in a short system description, as well as make signifi-\ncant further improvements, allowing us to complete a characterization of Hybrid’s type expr in terms of\nproperties stated at the HOAS level. In the new Hybrid, the type expr, its constructors, and these prop-\nerties form an abstract interface that allows users to reason at the higher level with no involvement of\nthe lower level implementation details. This interface was motivated by and is illustrated by a new proof\nof representational adequacy for Hybrid [12, Sect. 3.4] that does not make any reference to de Bruijn\nsyntax.\nWe start in Sect. 2 by giving an abstract view of Hybrid that motivates and explains the interface.\nSections 3–7 fill in many of the details of its implementation. The type dB implementing the de Bruijn\nindex representation is defined in Sect. 3, along with a predicate level to keep track of dangling indices.\nThe original Hybrid [1] used a datatype corresponding to our dB directly as expr. Section 4 defines the\nnew version of expr, which excludes at the type level terms with dangling indices. This simplifies the\nrepresentation of object languages by eliminating the need to carry a predicate for this purpose (called\nproper in [1]) along with Hybrid terms in meta-theoretic reasoning. Section 5 defines Hybrid’s variable\nbinding operator LAM and the abstr predicate. These definitions support a stronger injectivity prop-\nerty, presented in Sect. 6 with only one abstr premise rather than two. This property was also proved\nin [14]; the results here generalize and simplify these definitions as well as simplify other related Hybrid\ninternals. (In particular, we eliminate the need for the auxiliary function dB fn defined in [14] using the\nfunction package first introduced in Isabelle/HOL 2007, and we eliminate some other auxiliary functions\nby using a more systematic treatment of level.)\nIn Sect. 7, we formally prove that a version of abstr for two-argument functions (as described in [13])\nis equivalent to a conjunction of one-argument abstr conditions on “slices” of the function (fixing one\nargument). We use this result to prove a case-distinction lemma for functions satisfying abstr, and a\nlemma that enables compositional proof of abstr conditions at the HOAS level, without conversion to de\nBruijn indices as required in [1]. These two lemmas represent important new results that complete the\nabstract interface for Hybrid.\nIn Sect. 8, we discuss related work as well as ongoing work with Hybrid.\nThe Isabelle/HOL 2011 theory file for the present version of Hybrid is available online at:\nhttp://hybrid.dsi.unimi.it/download/Hybrid.thy\nand a more thorough presentation can be found in the first author’s Ph.D. thesis [11, 12]. In addition\nto the results described here, this theory file also replaces tactic-style proofs of the original version of\nHybrid with Isar proofs. This style of proof is both more readable and more robust against changes to the\nunderlying proof assistant. It also includes rewrite rules for Isabelle’s simplifier to convert automatically\nbetween HOAS at type expr and de Bruijn indices at type dB. With the improvements allowing users to\nwork exclusively at the HOAS level, this is no longer needed, and only included for illustrative purposes.\n2 An Abstract View of Hybrid\nWe use a pretty-printed version of Isabelle/HOL concrete syntax in this and the following sections. A\ndouble colon :: separates a term from its type, and the arrow⇒ is used in function types. We stick to the\nusual logical symbols for connectives and quantifiers (¬, ∧, ∨, −→, ∀, ∃). Free variables (upper-case)\nare implicitly universally quantified (from the outside). The sign ≡ (Isabelle meta-equality) is used for\nequality by definition, and =⇒ for Isabelle meta-level implication. In the notation qP1; . . . ; Pn y =⇒\n78 An Improved Implementation and Abstract Interface for Hybrid\nP, the square brackets are used to group premises to abbreviate nested implications; in its expanded form,\nit is P1 =⇒ . . . =⇒ Pn =⇒ P. Similarly,\n[\nt1, . . . , tn\n] ⇒ t abbreviates the type t1 ⇒ · · · ⇒ tn ⇒\nt. The keyword datatype introduces a new datatype, while function introduces a recursively defined\nfunction. We freely use infix notations, often without explicit declarations. Other syntax is intrduced as\nit appears.\nIsabelle/HOL already has extensive support for first-order abstract syntax, in the form of its datatype\npackage. Hybrid may be viewed as an attempt to approximate a datatype definition that is not well-\nformed because of its higher-order features:\ndatatype expr = CON con\n∣∣ VAR var ∣∣ APP expr expr (notation (s $$ t))∣∣ LAM (expr ⇒ expr) (notation (LAM x. B))\nwhere CON represents constants, from an OL-specific type con (typically a trivial datatype); VAR may\nbe used to represent free variables, from a countably infinite type var (actually a synonym for nat);\nAPP represents pairing, which is sufficient to encode list- or tree-structured syntax; and LAM represents\nvariable binding in HOAS style, using the bound variable of an Isabelle/HOL λ -abstraction to represent\na bound variable of the object language.1\nIt should be noted that Hybrid only approximates one such pseudo-datatype, not the datatype pack-\nage with its ability to define multiple types for first-order abstract syntax. That is, Hybrid is untyped, so\npredicates rather than types must be used to distinguish different kinds of OL terms encoded into expr.\nThe problem with the above definition is LAM, whose argument type includes a negative occurrence\nof expr (underlined above). This is essential for HOAS, but it is not permitted in a datatype definition\n[16, Sect. 2.6], and it will require modifications to some of the properties expected for a constructor of a\ndatatype; we will return to this issue later.\nHybrid does provide a type expr with operators CON, VAR, APP, and LAM of the appropriate types.\nThis type and the latter three operators can be used directly as a representation of the untyped λ -calculus.\nWhen encoding OLs in general, however, it is usual to represent each OL construct as a list built using\n$$ and headed by a CON term identifying the particular construct. To illustrate this idea, we take the\nuntyped λ -calculus as our OL with its usual named-variable syntax, using capital letters for variables\n(Vi, i ∈ N) and λ -abstraction (Λ) to avoid confusion with Isabelle’s λ operator. In this form, an object\nlanguage term (Λ V1.Λ V2.(V1 V2) V3), for example, can be represented as\nc lam $$ (LAM x. c lam $$ (LAM y. c app $$ (c app $$ x $$ y) $$ VAR 3)),\nwhere c lam = CON c1 and c app = CON c2 for distinct constants c1,c2 :: con. We may use Isabelle’s\nability to define abbreviations and infix notations to recover a reasonable concrete syntax:\nfn x y. (x $ y) $ VAR 3.\nNote that although de Bruijn indices do not appear in such terms, numbers can appear as arguments to\nHybrid’s VAR operator, which is included to allow a representation of free variables that is distinct from\nbound variables.\nWe now turn to the properties required of expr and its operators to function as HOAS. We motivate\nthe requirements by considering adequacy, an important meta-theoretic property. This can take several\nforms, but the proof presented in [12] uses bijectivity of a set-theoretic semantics on a λ -calculus-like\nsubset of the Isabelle/HOL terms of type expr, called the syntactic terms:\ns ::= x | CON a | VAR n | s1 $$ s2 | LAM x.s\n1While APP and LAM were inspired by the untyped λ -calculus, in Hybrid they are used only as syntax, without built-in\nnotions of β -conversion, normal forms, etc.\nAlan J. Martin & Amy P. Felty 79\nwhere s (with possible subscripts) stands for a syntactic term, x for a variable of type expr, a for a constant\nof type con, and n for a natural-number constant. Note that s is an informal mathematically defined set;\nit is not a formal Isabelle/HOL definition.\nHowever, open terms present a complication. Suppose we have a theory where the semantics is\nbijective on closed syntactic terms, which it maps to a set S. Then it will map open terms with n free\nvariables to functions from the Cartesian power Sn to S. But there are many such functions that do not\ncorrespond to syntactic terms; for example, the function S→ S corresponding to the Isabelle/HOL term\nλ x. if (∃ a. x = CON a) then (x $$ x) else x\nof type (expr⇒ expr). Indeed, there are a countable infinity of syntactic terms, while the set of functions\nfrom Sn to S is uncountable for n≥ 1.\nThus, Hybrid must define a subset of the function space to be used as its representation for open syn-\ntactic terms. This is done using a predicate abstr :: ((expr ⇒ expr) ⇒ bool). The functions satisfying\nabstr will be those of the form (λx.s) where s is a syntactic term with (at most) one free variable x; we\ncall these the syntactic functions.2 (Syntactic terms with more than one free variable can be handled one\nvariable at a time.)\nIn the first-order case, three properties hold of a type defined using Isabelle/HOL’s datatype: dis-\ntinctness of the datatype constructors, injectivity of each constructor, and an induction principle. In the\ncase of Hybrid, distinctness of all the operators and injectivity of the first-order operators (i.e., all except\nLAM) are straightforward to achieve, e.g.:\n∀ (c :: con) (S :: expr ⇒ expr). CON c 6= LAM S\n∀ (s t s′ t′ :: expr). (s $$ t = s′ $$ t′) −→ (s = s′) ∧ (t = t′).\n(These properties are used as rewrite rules for Isabelle’s simplifier, to reduce equalities of Hybrid terms\nwith known operators on both sides; typically this results in equalities where one side is just an Isabelle/\nHOL variable, which can then be eliminated by substitution.3)\nInjectivity of LAM must be restricted to functions satisfying abstr; indeed, it can be proven in\nIsabelle/HOL that no injective function from (expr ⇒ expr) to expr exists, by formalizing Cantor’s\ndiagonal argument. As mentioned earlier, our improved version requires an abstr condition for only one\nside of the equality: q\nabstr S ∨ abstr T; LAM S = LAM T y =⇒ S = T.\nRequiring only a single condition reduces the need for explicit abstr conditions in object-language en-\ncodings, because they can be transported across equalities of LAM terms. It is achieved by adding to\nthe type expr an additional constant ERR, and defining LAM to take the value ERR on functions not\nsatisfying abstr. (The constant ERR will sometimes appear as an additional case alongside the operators\nof Hybrid, in lemmas that impose an abstr condition for the LAM case. We also include it among the\nsyntactic terms.)\nSince abstr appears as a premise of injectivity—and it would in any case be needed to state proper-\nties of open syntactic terms—we must also include properties sufficient to characterize it. While Hybrid\n2Previous work called such functions abstractions [1] – thus the predicate name abstr; and called functions not satisfying\nabstr exotic terms [1, 5].\n3Indeed, most use of Hybrid’s lemmas in object-language work is automated using Isabelle’s simplifier and classical rea-\nsoner, and as a result, direct references to Hybrid’s lemmas may be rare.\n80 An Improved Implementation and Abstract Interface for Hybrid\nproves a number of lemmas regarding abstr for convenience and proof automation, the desired charac-\nterization can be given in a single statement:\nabstr Y ≡ (Y = (λ x. x))\n∨ (∃ a. Y = (λ x. CON a))\n∨ (∃ n. Y = (λ x. VAR n))\n∨ (∃ S T. Y = (λ x. S x $$ T x) ∧ abstr S ∧ abstr T)\n∨ (∃W. Y = (λ x. LAM y. W x y) ∧ abstr W)\n∨ (Y = (λ x. ERR))\nOnce again the LAM case complicates matters: the underlined occurrence of (abstr W) applies\nabstr to a function W ::\n(\n[expr, expr] ⇒ expr). This should be possible by using type classes to give a\npolymorphic definition for abstr, but that is future work. The present version of Hybrid instead replaces\n(abstr W) with (∀ y. abstr (λ x. W x y)) ∧ (∀ x. abstr (λ y. W x y)).\nAs for induction, it can take several forms. First, a kind of size induction on expr is available,\nsimilar to size induction for types defined by Isabelle/HOL’s datatype package. This induction has limited\napplicability in the higher-order setting, although it was used in the proof of adequacy [12]. We also retain\nan induction principle from the original version of Hybrid [1] where the first-order induction cases are\nstandard, while the LAM case is:\n∀ S :: (expr ⇒ expr). abstr S ∧ (∀ n. P (S (VAR n))) −→ P (LAM x. S x).\nA common form of induction used in many case studies involves some form of structural induction\non the encoding of the inference rules of an OL. For this kind of reasoning, a two-level approach is\nadopted, similar in spirit to other systems such as Twelf [18] and Abella [10]. An intermediate layer\nbetween the meta-logic (Isabelle/HOL) and the OL, called a specification logic, is defined inductively\nin Isabelle/HOL. This middle layer allows succinct and direct encodings of object logic inference rules,\nwhich are also defined as inductive definitions. Successful applications of this kind of induction can be\nfound in [8, 12], for example.\nFinally, Hybrid aims to build expr and its operators definitionally in Isabelle/HOL. While the de-\nscription above is an informal but reasonably complete specification of Hybrid, it is not directly usable\nas a definition because it is circular: the arguments of LAM and abstr may themselves contain LAM,\nand injectivity of LAM depends on abstr. It could be formalized as an axiomatic theory, leaving consis-\ntency as a meta-theoretical problem; but instead, Hybrid is built definitionally in terms of a first-order\nrepresentation of variable binding based on de Bruijn indices. The definitions and lemmas involved in\nachieving this are the subject of the next sections.\n3 De Bruijn syntax\nThe Hybrid theory defines the type expr in terms of an Isabelle/HOL datatype dB, which represents\nabstract syntax using a nameless first-order representation of bound variables called de Bruijn indices [2].\nThis approach differs from the original version of Hybrid [1], which used a datatype corresponding to\nour dB directly as expr; the significance of this difference will be explained in Sections 4 and 6. However,\nthe datatype itself is very similar, and this section follows [1] closely.\nDefinition 1\ntypes\nvar = nat\nbnd = nat\nAlan J. Martin & Amy P. Felty 81\ndatatype a dB =\nCON′ a\n∣∣ VAR′ var ∣∣ APP′ (a dB) (a dB) (notation (s $$′ t))∣∣ ERR′ ∣∣ BND′ bnd ∣∣ ABS′ (a dB)\nThe constructors CON′, VAR′, and APP′ correspond to the operators CON, VAR, and APP on type\nexpr, which were discussed in Sect. 2 and will be defined later. The one significant difference is that the\nargument of CON′ is a type parameter a, rather than a particular type con. This will actually be true for\nCON as well, and it allows Hybrid to be defined as an OL-independent Isabelle/HOL theory, and later\nused with OL-specific constants. (We will frequently omit this type parameter, except where it occurs in\nformal definitions or it is instantiated.)\nThe other three constructors (ERR′, BND′, and ABS′) will all be used in the definition of LAM. The\nconstant ERR′ will be a placeholder for LAM applied to a non-syntactic function; it was not present\nin [1], and its significance will be explained later. The constructor ABS′ functions as a nameless binder,\nwhile (BND′ i) represents the variable implicitly bound by the (i + 1)th enclosing ABS′ node. If there\nare not enough ABS′ nodes, then it is called a dangling index.\nAs an example, consider the term\nABS′ (ABS′ (BND′ 2 $$′ BND′ 1 $$′ BND′ 0) $$′ BND′ 0).\nThe underlined occurrences of (BND′ 1) and (BND′ 0) both refer to the variable bound by the outer\nABS′ (also underlined), while the other occurrence of (BND′ 0) refers to the variable bound by the inner\nABS′. (BND′ 2) is a dangling index, because there are only 2 enclosing ABS′ nodes.\nTo keep track of dangling indices, we define a predicate level ::\n[\nbnd, dB\n] ⇒ bool such that (level i t)\nis true if enclosing the term t in i or more ABS′ nodes would result in a term without dangling indices.\n(We omit the formal definition, which is straightforward.) A term with no dangling indices is called\nproper, and we may define an abbreviation (proper t) = (level 0 t). These notions are standard for\nabstract syntax based on de Bruijn indices [1].\n4 The type “expr” of proper de Bruijn terms\nDefining a type designed specifically to represent syntax has been used in a variety of approaches to\nreasoning about the λ -calculus and other object logics (e.g. [17, 22]). Here, we use Isabelle/HOL’s\ntypedef mechanism to define expr as a bijective image of the set of proper terms of type dB.4 That\neliminates the proper conditions in object-language work using Hybrid, at the expense of having to\nconvert terms between expr and dB in defining LAM and abstr. This is a good trade-off, because those\ndefinitions are internal to Hybrid and need only be made once. It also turns out to be essential for\nstrengthening the quasi-injectivity property of LAM, as described in Sect. 6.\nDefinition 2\ntypedef (open) a expr = {x :: a dB. level 0 x} morphisms dB expr\nThis typedef statement first demands a proof that the specified set is nonempty (which is trivial here).\nThen it introduces the type expr, the functions dB :: (expr ⇒ dB) and expr :: (dB ⇒ expr), and axioms\nstating that they are inverse bijections between the type expr and the set {x :: dB. level 0 x}. (Although\naxioms are used, the overall mechanism is a form of definitional extension and preserves consistency of\nthe theory.)\n4The version of expr presented here is a modification of the one used in [14].\n82 An Improved Implementation and Abstract Interface for Hybrid\nWe may now define all of the first-order operators of Hybrid (i.e., all except LAM, with its functional-\ntype argument) in the obvious way.\nDefinition 3\nCON :: a ⇒ a expr CON a ≡ expr (CON′ a)\nVAR :: var ⇒ a expr VAR n ≡ expr (VAR′ n)\nAPP ::\n[\na expr, a expr\n] ⇒ a expr s $$ t ≡ expr (dB s $$′ dB t)\n(notation (s $$ t))\nERR :: a expr ERR ≡ expr ERR′\nERR is defined as if it were a separate operator, and it will sometimes be treated as such, but it will\nalso be generated by LAM applied to a non-syntactic function.\nThe functions dB and expr translate these operators to the corresponding constructors of dB (Defi-\nnition 1) and vice versa. This is formalized by a set of lemmas that follow straightforwardly from the\ndefinitions, of which we present just those for APP ($$) as an example.\nLemma 4\ndB (s $$ t) = dB s $$′ dB tq\nlevel 0 s; level 0 t\ny\n=⇒ expr (s $$′ t) = expr s $$ expr t\nDistinctness and injectivity for these operators follow from the corresponding properties of dB. In\nSect. 6, we will extend these results to LAM as well.\nThe (level 0) premises in the lemma above are needed because the typedef-generated function expr is\nundefined on terms with dangling indices. These premises could be eliminated by defining a more tightly-\nspecified version of expr, satisfying the same typedef-generated axioms while preserving the structure\nof its argument except for any dangling indices. This was done in the previous version of Hybrid [14]\n(with the help of an auxiliary function called trim). However, with a more systematic treatment of level\nand some additional lemmas for it, this was found to be unnecessary.\nAll versions of Hybrid follow a general pattern of making definitions and proving lemmas first for\narbitrary levels, and then deriving the desired results for proper terms as corollaries. In the present\nversion, arbitrary levels are handled by recursion and induction over de Bruijn syntax, using the type dB\nand the predicate level, while the results for proper terms are stated at type expr.\n5 Definition of “abstr” and “LAM”\nWe now turn to the task of defining abstr and LAM. The main ideas are from [1], but the details of the\ndefinitions and proofs are original. There are some improvements over the original version of Hybrid,\nwhich will be described in this section and Sect. 6.\nSince we will be defining abstr and LAM in terms of de Bruijn syntax, the definition of syntactic\nfunctions from Sect. 2 is not directly usable here: we need an analogous definition using de Bruijn\nsyntax in place of LAM.\nFor recursion, we must work with dB-valued functions (arbitrary levels) rather than expr-valued\nfunctions. However, the argument type need not also be dB, and in fact it will be more convenient to\nwork with functions of type (expr ⇒ dB). This simplifies the treatment of level by avoiding negative\noccurrences of the type dB.\nThus we define the syntactic dB-terms, as a subset of Isabelle/HOL terms of type dB, using variables\nof type expr converted via dB:\ns ::= dB x | CON′ a | VAR′ n | s1 $$′ s2 | ERR′ | BND′ i | ABS′ s\nAlan J. Martin & Amy P. Felty 83\nwhere s (with possible subscripts) stands for a syntactic dB-term, x for a variable of type expr, a for a\nconstant of type con, and n and i for natural-number constants. We define the syntactic dB-functions as\nthe functions of type (expr⇒ dB) of the form (λx.s), where s is a syntactic dB-term with (at most) one\nfree variable x. Such functions mix de Bruijn indices (BND′) with HOAS (using the Isabelle/HOL bound\nvariable x to represent an object-language variable).\nWe define a predicate Abstr to recognize the syntactic dB-functions, which formally defines the so-\nfar only informally identified set. We also define an auxiliary predicate ordinary needed in the definition\nof Abstr:\nDefinition 5\nordinary :: (b ⇒ a dB) ⇒ bool\nordinary X ≡ (∃ a. X = (λ x. CON′ a)) ∨ (∃ n. X = (λ x. VAR′ n)) ∨\n(∃ S T. X = (λ x. S x $$′ T x)) ∨ (X = (λ x. ERR′)) ∨\n(∃ j. X = (λ x. BND′ j)) ∨ (∃ S. X = (λ x. ABS′ (S x)))\nDefinition 6\nfunction Abstr :: (a expr ⇒ a dB) ⇒ bool\nAbstr (λ x. s) = True where s is (CON′ a), (VAR′ n), ERR′, or (BND′ i)\nAbstr (λ x. S x $$′ T x) = (Abstr S ∧ Abstr T)\nAbstr (λ x. ABS′ (S x)) = Abstr S\n¬ ordinary S =⇒ Abstr S = (S = dB)\nSyntactically, the defining equations forAbstr have the form of recursion on the body of a λ -abstraction.\nMathematically, they define (Abstr S) by recursion on the common structure of all the values of the func-\ntion S, i.e., on the common structure (if any) of (S x) for all x :: expr. The predicate ordinary recognizes\nthose functions that match one of the first three equations, so that the condition (¬ ordinary S) on the\nlast equation may be read as “otherwise”; that equation corresponds to the variable case for syntactic\ndB-terms as defined above.\nThis definition is formalized with the help of Isabelle/HOL’s function command. It demands proofs\nof pattern completeness, compatibility, and termination (not shown), and then in addition to defin-\ning Abstr and proving its defining equations, it automatically generates structural induction and case-\ndistinction rules for the type (expr ⇒ dB) corresponding to the pattern of recursion used in the definition;\nthese are called Abstr . induct and Abstr .cases respectively, and will be referred to later.\nWe may now define the predicate abstr in terms of Abstr by using post-\ncomposition with dB to convert its function argument from the type (expr ⇒ expr) to (expr ⇒ dB).\nDefinition 7\nabstr :: (a expr ⇒ a expr) ⇒ bool\nabstr S ≡ Abstr (dB ◦ S)\nNote that unlike the situation in [1], the definition of Abstr does not need to impose a constraint on\nthe argument of BND′, because in the case of (abstr S) dangling indices are excluded by the type of the\nfunction S :: (expr ⇒ expr).\nLemma 8\nAbstr const : Abstr (λ x. s)\nThe lemma Abstr const shows that any constant function of type (expr ⇒ dB) satisfies Abstr. It is\nused to prove a similar property for abstr, and will later be used directly as well. It is proved by induction\non s using Definition 6 (Abstr).\n84 An Improved Implementation and Abstract Interface for Hybrid\nLemma 9\nabstr id : abstr (λ x. x)\nabstr const : abstr (λ x. s)\nabstr APP: abstr (λ x. S x $$ T x) = (abstr S ∧ abstr T)\nThe lemma abstr const is a corollary of Abstr const, while the other two lemmas are proved directly,\nusing Definitions 7 (abstr) and 6 (Abstr).\nThese lemmas allow abstr conditions for syntactic functions to be proved compositionally without\nunfolding the definition, except when the body of the function contains a LAM subterm that involves\nthe function argument (so that it is not just a constant). In that case, previous versions of Hybrid re-\nquired unfolding the definitions of abstr and LAM to convert HOAS to de Bruijn syntax. The present\nwork improves on that situation by providing a compositional rule also for the LAM case (Lemma 20 in\nSect. 7).\nThe lemma abstr const will be important for Hybrid terms with nested LAM operators, to show that\nthe argument of an inner LAM satisfies abstr when its body contains a bound variable from an outer\nLAM; such a bound variable is a placeholder for an arbitrary term of type expr, which is exactly the role\nof s in abstr const.\nWe now define the function LAM, using the same form of recursion that was used in the definition of\nabstr.\nDefinition 10\nLAM :: (a expr ⇒ a expr) ⇒ a expr\nLAM S ≡ expr (Lambda (dB ◦ S))\nLambda :: (a expr ⇒ a dB) ⇒ a dB\nLambda S ≡ if (Abstr S) then (ABS′ (Lbind 0 S)) else ERR′\nThe function LAM, like abstr, first composes dB with the given function. It then applies the auxiliary\nfunction Lambda and converts the resulting term from type dB to type expr.\nThe function Lambda first checks if its argument satisfies Abstr, and produces ERR′ if not. (This is\nequivalent to checking if the argument of LAM satisfies abstr.) The original version of Hybrid [1] did\nnot do this check (and did not have the constant ERR′), making it impossible to determine from (LAM S)\nwhether S was a syntactic function or not. We include these features to support the stronger injectivity\nproperty for LAM proved in Sect. 6.\nIf its argument does satisfy Abstr, then Lambda applies another auxiliary function Lbind, defined by\nrecursion, to convert HOAS to de Bruijn syntax; i.e., to convert the variable represented by the function\nargument into a dangling de Bruijn index. It then applies a new ABS′ node to bind the variable and obtain\na proper de Bruijn term.\nDefinition 11\nfunction Lbind ::\n[\nbnd, (a expr ⇒ a dB) ] ⇒ a dB\nLbind i (λ x. s) = s where s is (CON′ a), (VAR′ n), ERR′, or (BND′ j)\nLbind i (λ x. S x $$′ T x) = Lbind i S $$′ Lbind i T\nLbind i (λ x. ABS′ (S x)) = ABS′ (Lbind (i + 1) S)\n¬ ordinary S =⇒ Lbind i S = BND′ i\nThe auxiliary function Lbind extracts the common structure of the values of its function argument,\nreplacing indecomposable uses of the bound variable (i.e., functions that do not match any of the first\nthree equations) with (BND′ i). This is a dangling de Bruijn index, and i is incremented each time the\nAlan J. Martin & Amy P. Felty 85\nrecursion passes an ABS′ node so that all such instances of BND′ will refer to the ABS′ node added by\nLambda. The Abstr condition checked in the definition of Lambda ensures that the last equation will be\napplied only when S = (λ x. dB x).\nLemma 12\nLbind const : Lbind i (λ x. s) = s\nThe lemma Lbind const shows that applying (Lbind i) to a constant function of type (expr ⇒ dB)\ngives the constant value of that function. It is proved by induction on s. This lemma will be important for\nHybrid terms with nested LAM operators, to allow the argument of an outer LAM to satisfy abstr when\nits bound variable occurs in the scope of an inner LAM.\nLemma 13\ndB LAM: dB (LAM S) = if (abstr S) then (ABS′ (Lbind 0 (dB ◦ S))) else ERR′\nabstr dB LAM: abstr S =⇒ dB (LAM S) = ABS′ (Lbind 0 (dB ◦ S))\nThe lemma dB LAM combines unfolding of Definition 10 (LAM and Lambda) with cancellation\nof the functions dB and expr, using the fact that both ERR′ and (ABS′ (Lbind 0 (dB ◦ S))) are proper.\n(Dangling indices are excluded from S :: (expr ⇒ expr) by its type, and the one introduced by Lbind is\nbound by the enclosing ABS′.) The lemma abstr dB LAM is a weaker version intended as a conditional\nrewrite rule for Isabelle’s simplifier, to do the unfolding only if the abstr condition simplifies to True.\nWith the definitions above, Hybrid terms using LAM (i.e., closed syntactic terms) are provably equal\nto the corresponding de Bruijn syntax representations, converted to the type expr using the function expr.\n(This is much the same situation as in [1], except for the type conversion which was not necessary there.)\nThus, starting from two distinct representations for free variables, we have established two ambiguous\nrepresentations for bound variables, in the sense that any given element of expr may be viewed as having\neither form. In the following sections, we will state results using the HOAS representation (LAM) but\nuse the de Bruijn syntax representation (ABS′/BND′) in proofs by induction, aiming to characterize the\nformer representation so that it stands on its own.\nAll versions of Hybrid have used essentially the same form of recursion to define abstr and LAM,\nand the corresponding form of induction to prove their properties. However, the means of formalizing it\nhave varied greatly. The original version [1] used inductively-defined predicates and induction on those\npredicates; the following version [14] used primitive recursion and induction on an auxiliary datatype\ndB fn; while the present version avoids many of the complications of the previous approaches with the\nhelp of the function command.\nA predicate called ordinary has also been present in all versions of Hybrid, though it originally\nincluded the variable case as well. Removing this case allowed ordinary to be generalized to dB-valued\nfunctions on any type; this will allow us to reuse it for binary functions in Sect. 7. (It is also reused for\nn-ary functions in [12, Sect. 3.3].)\n6 Injectivity of “LAM”\nAs stated in Sect. 2, Hybrid proves injectivity of LAM restricted to functions of type (expr ⇒ expr)\nsatisfying abstr. Improving on [1], this property is strengthened by requiring only one abstr premise,\nusing the fact that LAM maps functions not satisfying abstr to a recognizable placeholder term ERR.\nWe begin with an injectivity result for arbitrary de Bruijn levels. To state this result concisely, we\nfirst define an abbreviation Level for pointwise application of level to a function:\n86 An Improved Implementation and Abstract Interface for Hybrid\nDefinition 14\nabbreviation Level ::\n[\nbnd, (b ⇒ a dB) ] ⇒ bool\nLevel i S ≡ ∀ x. level i (S x)\nLemma 15\nAbstr Lbind inject :q\nAbstr S; Abstr T; Level i S; Level i T\ny\n=⇒ (Lbind i S = Lbind i T) = (S = T)\nThis lemma is proved by a straightforward induction on S :: (expr ⇒ dB) using Abstr . induct (from\nDefinition 6).\nTheorem 16 (Injectivity of LAM)q\nLAM S = LAM T; abstr S ∨ abstr T y =⇒ S = T\nProof. If one of S and T satisfies abstr and the other does not, then by Lemma 13 (dB LAM), one of\nthe terms (dB (LAM S)) and (dB (LAM T)) is of the form (ABS′ t) for some t :: dB, while the other\nis ERR′. But these terms cannot be equal, which contradicts the premise LAM S = LAM T. Thus the\noriginal assumption must be false, and we must have both (abstr S) and (abstr T).\nWe apply dB to both sides of the equality LAM S = LAM T and simplify using abstr dB LAM\n(Lemma 13) to obtain\nABS′ (Lbind 0 (dB ◦ S)) = ABS′ (Lbind 0 (dB ◦ T)).\nABS′ is a datatype constructor and thus injective, so we may cancel it:\nLbind 0 (dB ◦ S) = Lbind 0 (dB ◦ T).\nWe have (Abstr (dB ◦ S)) and (Abstr (dB ◦ T)) by unfolding Definition 7 (abstr), and we also have\n(Level 0 (dB ◦ S)) and (Level 0 (dB ◦ T)) since terms converted from type expr are proper by Defi-\nnition 2. Thus we may apply the preceding lemma (Abstr Lbind inject) to deduce dB ◦ S = dB ◦ T.\nSince dB is injective, it can be canceled to obtain S = T, as was to be proven. 2\nNote that (Lbind 0) is only injective on functions from expr to dB whose values are proper terms,\ni.e., those that factor through dB, because any pre-existing dangling indices at level 1 would be indistin-\nguishable from those resulting from conversion of the HOAS variable. For example,\nLbind 0 (λ x. dB x) = BND′ 0 = Lbind 0 (λ x. BND′ 0).\nThus, without the typedef limiting expr to proper terms, we would not be able to avoid conditions on\nboth S and T; at best, we could replace one abstr condition with something like (∀ x. proper x −→\nproper (T x)).\nThe advantage of an injectivity property that can work with a condition on only one of S and T is\nthat it simplifies the elimination rules for inductively-defined predicates on Hybrid terms, such as the\nformalization of evaluation for Mini-ML with references in [12, Sect. 5.3]. As a result, abstr conditions\nare more often available where they are needed, without having to add them as premises.\nDistinctness of LAM from the first-order operators of Definition 3 follows straightforwardly from\nDefinition 10, except that (LAM F) is distinct from ERR only under the premise (abstr F).\nAlan J. Martin & Amy P. Felty 87\n7 Characterizing “abstr”\nIn Sect. 5, an incomplete set of simplification rules for abstr was provided as Lemma 9. The missing\ncase is (abstr (λ x. LAM y. W x y)).\nBoth previous versions of Hybrid [1, 14] relied on conversion from HOAS to de Bruijn syntax to\nhandle this case. That is sufficient for proving that particular syntactic functions satisfy abstr, but it is\nless useful for partially-specified functions as found in inductive proofs.\nWe could obtain a compositional introduction rule for this case by defining a predicate biAbstr ::(\n[expr, expr] ⇒ expr) ⇒ bool generalizing abstr, and proving\nbiAbstr W =⇒ abstr (λ x. LAM y. W x y).\nThis was done by Momigliano et al. [13]; their formal theory BiAbstr is available online [6]. However,\nthe LAM case arises again for biAbstr, and for any higher-arity generalization. There are several ways to\naddress this:\n• Use Isabelle/HOL’s axiomatic type classes to define a polymorphic predicate generalizing abstr to\ncurried functions of arbitrary arity. This looks like a promising approach, but it remains as future\nwork.\n• Find a single type that can represent functions of arbitrary arity, and generalize Hybrid’s constructs\nto that type. (Some experimental work has been done in that direction [12, Sect. 3.3].) Such a type\nis also useful as a representation of open terms for induction.\n• Prove a result that reduces biAbstr to abstr. This seems to be the most direct solution, and it is the\napproach we take in the present work.\nIn this section, we will represent functions of two arguments using pairs, rather than in the usual\ncurried form, so that we may reuse Definition 5 (ordinary) and some technical lemmas (left unstated as\nthey are mathematically trivial), all of which refer to the polymorphic type (b ⇒ dB).\nDefinition 17\nabstr 2 :: (a expr × a expr ⇒ a expr) ⇒ bool\nabstr 2 S ≡ Abstr 2 (dB ◦ S)\nThe predicate abstr 2 generalizes abstr to functions on the Cartesian product type (expr × expr); it\ncorresponds to biAbstr [13]. It is defined in the same way as abstr, composing dB with its argument and\nthen applying a recursively-defined auxiliary predicate Abstr 2.\nDefinition 18\nfunction Abstr 2 :: (a expr × a expr ⇒ a dB) ⇒ bool\nAbstr 2 (λ p. s) = True where s is (CON′ a), (VAR′ n), ERR′, or (BND′ i)\nAbstr 2 (λ p. S p $$ T p) = (Abstr 2 S ∧ Abstr 2 T)\nAbstr 2 (λ p. ABS′ (S p)) = Abstr 2 S\n¬ ordinary S =⇒ Abstr 2 S = (S = dB ◦ fst ∨ S = dB ◦ snd)\nThe predicate Abstr 2 is similar to Abstr, except that it has two variable cases: (dB ◦ fst) and\n(dB ◦ snd), or equivalently, (λ (x, y). dB x) and (λ (x, y). dB y).\nLemma 19\nabstr 2 S =\n((∀ y. abstr (λ x. S (x, y))) ∧ (∀ x. abstr (λ y. S (x, y))))\n88 An Improved Implementation and Abstract Interface for Hybrid\nThis lemma shows that if a two-argument function satisfies abstr in each argument for any fixed\nvalue of the other argument, then it satisfies abstr 2. (And the converse, which is easier.) We omit the\nformal proof, but note that it is fairly long and requires several lemmas.\nHaving thus reduced abstr 2 to componentwise abstr, we may now derive the desired simplification\nrule for the case (abstr (λ x. LAM y. W x y)).\nLemma 20\nabstr LAM: ∀ x. abstr (λ y. W x y) =⇒\nabstr (λ x. LAM y. W x y) = (∀ y. abstr (λ x. W x y))\nThis lemma provides a compositional rule for proving abstr conditions on functions of the form\n(λ x. LAM y. W x y), via the reverse direction of the biconditional. Both directions are also used in the\nproof of adequacy. It was proved with the help of (a variant of) Lemma 19.\nWe consider a small example, the term (LAM x. LAM y. x $$ y), illustrating abstr LAM by proving\nthat the argument of the outer LAM satisfies abstr, without the use of de Bruijn syntax:\n∀ x. abstr (λ y. x) (by abstr const)\nabstr (λ y. y) (by abstr id)\n∀ x. abstr (λ y. (x $$ y)) (by abstr APP)\nabstr (λ x. x) (by abstr id)\n∀ y. abstr (λ x. y) (by abstr const)\n∀ y. abstr (λ x. (x $$ y)) (by abstr APP)\nabstr (λ x. LAM y. (x $$ y)) (by abstr LAM)\nNot only does the lemma abstr LAM allow abstr statements to be proved without the use of de Bruijn\nsyntax, but it also completes the task of characterizing expr on its own terms – that is, without reference to\nthe underlying de Bruijn syntax. This is demonstrated in [12] by the fact that representational adequacy\nfollows from Hybrid’s lemmas concerning the type expr, and it is a significant improvement over both\nprevious versions of Hybrid [1, 14].\nWe also obtain the characterization of abstr stated in Sect. 2 as a corollary of abstr LAM:\nLemma 21\nabstr Y =\n(\n(Y = (λ x. x)) ∨\n(∃ a. Y = (λ x. CON a)) ∨ (∃ n. Y = (λ x. VAR n)) ∨\n(∃ S T. abstr S ∧ abstr T ∧ Y = (λ x. S x $$ T x)) ∨(∃W. (∀ x. abstr (λ y. W x y)) ∧ (∀ y. abstr (λ x. W x y)) ∧\nY = (λ x. LAM y. W x y)\n) ∨ (Y = (λ x. ERR)))\n8 Conclusion\nHybrid is the first approach to formalizing variable-binding constructs that is both based on full HOAS\nand is built definitionally in a general-purpose proof assistant (Isabelle/HOL). More recently, Popescu et.\nal. have developed an approach motivated by a new proof of strong normalization for System F that takes\nadvantage of HOAS techniques [21]. It is also definitional, implements full HOAS, and is implemented\nin Isabelle/HOL, though the details of the formalizations as well as the case studies carried out in each\nsystem are quite different. A more in-depth comparison is the subject of future work.\nThere are many other related approaches, and we mention only a few here. See [8, 12] for a fuller\ndiscussion. Systems that implement logics designed specifically for reasoning using HOAS include\nAlan J. Martin & Amy P. Felty 89\nTwelf [18] (one of the most mature systems in this category), Abella [10], and Beluga [19]. These\nsystems have the advantage of being purpose-built for reasoning about formal systems, but this can also\nbe a disadvantage in that they cannot exploit the extensive libraries of formalized mathematics available\nfor proof assistants such as Isabelle/HOL. For a comparison of Hybrid to Twelf and Beluga, see [7]. The\nnominal datatype package [22] implements a different approach which seeks to formalize equivalence of\nclasses of terms up to renaming of bound variables, and also the Barendregt variable convention, using\nconcepts from nominal logic [9, 20].\nThere are several versions of Hybrid based on the Coq proof assistant. One such version [8] closely\nfollows the structure of the Isabelle/HOL version; another implements a constructive variant of Hybrid\nfor Coq [3] that aims to leverage the use of dependent types to simplify and provide new ways to specify\nOLs. There have also been a number of applications and case studies for Hybrid, the largest being the\ncomparison of five formalizations of subject reduction for Mini-ML with references [12], which uses the\nimproved Hybrid described in this paper. Future work includes porting other applications to use the new\nHybrid. This will be straightforward since they are simpler and will be further simplified by the new\ninterface. Future work also includes carrying out new case studies to further illustrate the benefits of the\nnew Hybrid.\nAlthough we have significantly improved Hybrid, there is always room for further improvement. For\nexample, the induction principle discussed at the end of Sect. 2 (the one whose LAM case is displayed)\nfalls back to named (or numbered) variables for inductive proofs, which means giving up some of the\nadvantages of HOAS. We are working on a more general approach to induction that preserves the HOAS\nfeature of substitution by function application. In fact, we have proved an induction principle for a type\nthat represents n-ary functions on the type expr [12], which we hope will serve as the basis for general\ninduction principles for HOAS in Hybrid. Its integration into Hybrid remains as future work. As another\nexample, we mentioned that Hybrid is untyped, requiring predicates to be introduced to distinguish\ndifferent kinds of OL terms encoded into expr. On one hand, these well-formedness predicates can\nprovide a convenient form of induction within the context of the two-level approach; on the other hand\nthis is a potential area for improvement. Some work in this direction has been done in the Coq version\nof Hybrid [4].\nReferences\n[1] Simon Ambler, Roy Crole & Alberto Momigliano (2002): Combining Higher Order Abstract Syntax with\nTactical Theorem Proving and (Co)Induction. In: 15th International Conference on Theorem Proving in\nHigher Order Logics, LNCS 2410, Springer, pp. 13–30, doi:10.1007/3-540-45685-6 3.\n[2] N. G. de Bruijn (1972): Lambda-calculus notation with nameless dummies: a tool for automatic formula\nmanipulation with application to the Church-Rosser theorem. Indagationes Mathematicae 34(5), pp. 381–\n392.\n[3] Venanzio Capretta & Amy P. Felty (2007): Combining de Bruijn Indices and Higher-Order Abstract Syntax\nin Coq. In: Types for Proofs and Programs, Intl. Workshop, TYPES 2006, LNCS 4502, Springer, pp. 63–77,\ndoi:10.1007/978-3-540-74464-1 5.\n[4] Venanzio Capretta & Amy P. Felty (2009): Higher-Order Abstract Syntax in Type Theory. In: Logic Collo-\nquium ’06, ASL Lecture Notes in Logic 32, Cambridge University Press, pp. 65–90.\n[5] Joe¨lle Despeyroux, Amy Felty & Andre´ Hirschowitz (1995): Higher-Order Abstract Syntax in Coq. In: 2nd\nInternational Conference on Typed Lambda Calculi and Applications, LNCS 902, Springer, pp. 124–138,\ndoi:10.1007/BFb0014049.\n90 An Improved Implementation and Abstract Interface for Hybrid\n[6] Amy Felty & Alberto Momigliano (2010): Web appendix of the paper “Hybrid: a Definitional Two Level\nApproach to Reasoning with Higher Order Abstract Syntax” [8]. http://hybrid.dsi.unimi.it/jar/\nindex.html.\n[7] Amy Felty & Brigitte Pientka (2010): Reasoning with Higher-Order Abstract Syntax and Contexts: A Com-\nparison. In: International Conference on Interactive Theorem Proving, LNCS 6172, Springer, pp. 227–242,\ndoi:10.1007/978-3-642-14052-5 17.\n[8] Amy P. Felty & Alberto Momigliano (2010): Hybrid: A Definitional Two-Level Approach to Rea-\nsoning with Higher-Order Abstract Syntax, doi:10.1007/s10817-010-9194-x. To appear in Journal of\nAutomated Reasoning. Available at Springer Online First (http://www.springerlink.com/content/\n92q14113413462t0/).\n[9] Murdoch Gabbay & Andrew M. Pitts (2002): A New Approach to Abstract Syntax with Variable Binding.\nFormal Aspects of Computing 13(3–5), pp. 341–363, doi:10.1007/s001650200016.\n[10] Andrew Gacek (2008): The Abella Interactive Theorem Prover (System Description). In: 4th Intl. Joint Conf.\non Automated Reasoning, LNCS 5195, Springer, pp. 154–161, doi:10.1007/978-3-540-71070-7 13.\n[11] Alan J. Martin (2010): http://hybrid.dsi.unimi.it/martinPhD/. Isabelle/HOL theory files.\n[12] Alan J. Martin (2010): Reasoning Using Higher-Order Abstract Syntax in a Higher-Order Logic Proof Envi-\nronment: Improvements to Hybrid and a Case Study. Ph.D. thesis, University of Ottawa.\n[13] Alberto Momigliano, Simon Ambler & Roy L. Crole (2002): A Hybrid Encoding of Howe’s Method for\nEstablishing Congruence of Bisimilarity. Electronic Notes in Theoretical Computer Science 70(2), pp. 60–\n75, doi:10.1016/S1571-0661(04)80506-1. Proceedings of LFM’02.\n[14] Alberto Momigliano, Alan J. Martin & Amy P. Felty (2008): Two-Level Hybrid: A System for Reasoning\nUsing Higher-Order Abstract Syntax. Electronic Notes in Theoretical Computer Science 196, pp. 85–93,\ndoi:10.1016/j.entcs.2007.09.019. Proceedings of LFMTP’07.\n[15] Tobias Nipkow, Lawrence C. Paulson & Markus Wenzel (2002): Isabelle/HOL: A Proof Assistant for Higher-\nOrder Logic. LNCS 2283, Springer.\n[16] Tobias Nipkow, Lawrence C. Paulson & Markus Wenzel (2011): Isabelle’s Logics: HOL. http://\nisabelle.in.tum.de/doc/logics-HOL.pdf. Accessed July 2011.\n[17] Michael Norrish (2006): Mechanising λ -Calculus using a Classical First Order Theory of Terms with Per-\nmutations. Journal of Higher Order Symbolic Computation 19(2–3), pp. 169–195, doi:10.1007/s10990-006-\n8745-7.\n[18] Frank Pfenning & Carsten Schu¨rmann (1999): System Description: Twelf — A Meta-Logical Framework\nfor Deductive Systems. In: 16th Intl. Conf. on Automated Deduction, LNCS 1632, Springer, pp. 202–206,\ndoi:10.1007/3-540-48660-7 14.\n[19] Brigitte Pientka & Joshua Dunfield (2010): Beluga:A Framework for Programming and Reasoning with\nDeductive Systems (System Description). In: 5th International Joint Conference on Automated Reasoning,\nLNCS 6173, Springer, pp. 15–21, doi:10.1007/978-3-642-14203-1 2.\n[20] Andrew M. Pitts (2003): Nominal Logic, a First Order Theory of Names and Binding. Information and\nComputation 186(2), pp. 165–193, doi:10.1016/S0890-5401(03)00138-X.\n[21] Andrei Popescu, Elsa L. Gunter & Christopher J. Osborn (2010): Strong Normalization for System F by\nHOAS on Top of FOAS. In: 25th Annual IEEE Symposium on Logic in Computer Science, pp. 31–40,\ndoi:10.1109/LICS.2010.48.\n[22] Christian Urban (2008): Nominal Techniques in Isabelle/HOL. Journal of Automated Reasoning 40(4), pp.\n327–356, doi:10.1007/s10817-008-9097-2.\n",
            "id": 787021,
            "identifiers": [
                {
                    "identifier": "oai:arxiv.org:1111.0090",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "10.4204/eptcs.71.6",
                    "type": "DOI"
                },
                {
                    "identifier": "25769629",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:citeseerx.psu:10.1.1.221.7391",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "190533515",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "1111.0090",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "2231649",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:doaj.org/article:f548310e6dd34404af6a236d55533734",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "21805200",
                    "type": "CORE_ID"
                }
            ],
            "title": "An Improved Implementation and Abstract Interface for Hybrid",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:citeseerx.psu:10.1.1.221.7391",
                "oai:arxiv.org:1111.0090",
                "oai:doaj.org/article:f548310e6dd34404af6a236d55533734"
            ],
            "publishedDate": "2011-10-01T00:00:00",
            "publisher": "'Open Publishing Association'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1111.0090",
                "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.221.7391",
                "http://doaj.org/search?source=%7B%22query%22%3A%7B%22bool%22%3A%7B%22must%22%3A%5B%7B%22term%22%3A%7B%22id%22%3A%22f548310e6dd34404af6a236d55533734%22%7D%7D%5D%7D%7D%7D",
                "http://www.site.uottawa.ca/%7Eafelty/dist/lfmtp11.pdf"
            ],
            "updatedDate": "2021-05-03T17:33:13",
            "yearPublished": 2011,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "2075-2180",
                        "issn:2075-2180"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1111.0090"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/787021"
                }
            ]
        },
        {
            "acceptedDate": "2007-06-22T00:00:00",
            "arxivId": "astro-ph/0703286",
            "authors": [
                {
                    "name": "A. Retter"
                },
                {
                    "name": "Burrows D. N."
                },
                {
                    "name": "D. Burrows"
                },
                {
                    "name": "G. J. Schwarz"
                },
                {
                    "name": "Iijima T."
                },
                {
                    "name": "J. H. M. M. Schmitt"
                },
                {
                    "name": "J. P. Osborne"
                },
                {
                    "name": "J.‐U. Ness"
                },
                {
                    "name": "Kiss L. L."
                },
                {
                    "name": "Lane B. F."
                },
                {
                    "name": "Lyke J. E."
                },
                {
                    "name": "Munari U."
                },
                {
                    "name": "N. Gehrels"
                },
                {
                    "name": "Ness J.-U."
                },
                {
                    "name": "Perry R. B."
                },
                {
                    "name": "Puetter R. C."
                },
                {
                    "name": "Rauch T."
                },
                {
                    "name": "S. Starrfield"
                },
                {
                    "name": "Siviero A."
                },
                {
                    "name": "van den Bergh S."
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/190427975"
            ],
            "createdDate": "2012-04-13T14:20:26",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2007-07-01T00:00:00",
            "abstract": "The new gamma-ray burst mission Swift has obtained pointed observations of\nseveral classical novae in outburst. We analyzed all the observations of\nclassical novae from the Swift archive up to 30 June, 2006. We analyzed usable\nobservations of 12 classical novae and found 4 non-detections, 3 weak sources\nand 5 strong sources. This includes detections of 2 novae exhibiting spectra\nresembling those of Super Soft X-ray binary Source spectra (SSS) implying\nongoing nuclear burning on the white dwarf surface. With these new Swift data,\nwe add to the growing statistics of the X-ray duration and characteristics of\nclassical novae.Comment: Accepted for ApJ; this version contains additional material: 18\n  pages, 16 figure",
            "documentType": "research",
            "doi": "10.1086/518084",
            "downloadUrl": "http://arxiv.org/abs/astro-ph/0703286",
            "fieldOfStudy": "computer science",
            "fullText": "ar\nX\niv\n:a\nstr\no-\nph\n/0\n70\n32\n86\nv4\n  2\n5 \nA\npr\n 2\n00\n7\nDraft version October 9, 2018\nPreprint typeset using LATEX style emulateapj v. 08/22/09\nSWIFT X-RAY OBSERVATIONS OF CLASSICAL NOVAE\nJ.-U. Ness1, G.J. Schwarz2, A. Retter3, S. Starrfield1, J.H.M.M. Schmitt4, N. Gehrels5, D. Burrows6, J.P.\nOsborne7\n(Dated: accepted by ApJ (March 8, 2007)∗)\nDraft version October 9, 2018\nABSTRACT\nThe new γ-ray burst (GRB) mission Swift has obtained pointed observations of several classical\nnovae in outburst. We analyzed all the observations of classical novae from the Swift archive up to\n30 June, 2006. We analyzed usable observations of 12 classical novae and found 4 non-detections, 3\nweak sources and 5 strong sources. This includes detections of 2 novae exhibiting spectra resembling\nthose of Super Soft X-ray binary Source spectra (SSS) implying ongoing nuclear burning on the white\ndwarf surface. With these new Swift data, we add to the growing statistics of the X-ray duration and\ncharacteristics of classical novae.\nSubject headings: stars: individual (V574 Pup, V382 Nor, V1663 Aql, V5116 Sgr, V1047 Cen, V476\nSct, V477 Sct, LMC 2005, V1494 Aql, V4743 Sgr, V1188 Sco) — stars: novae —\nX-rays: stars\n1. INTRODUCTION\nThe explosions of Classical Novae (CNe) occur in close\nbinary star systems where mass is accreted onto a white\ndwarf (WD) from a low-mass secondary star. The ac-\ncreted material gradually becomes degenerate, and when\ntemperatures become high enough and the pressure at\nthe bottom of the accreted envelope exceeds a certain\nvalue, a thermonuclear runaway results. Enough energy\nis deposited in the accreted material to eject some frac-\ntion from the WD. The outburst can last several months\nto years, and CNe become active in X-rays at some\ntime during their outburst (e.g., Pietsch et al. 2005;\nPietsch et al. 2006). The relatively slow evolution makes\nCNe ideal targets for Swift, as no accurate times of ob-\nservation need to be scheduled and they can be observed\nwith great flexibility.\nThe evolution of X-ray emission starts soon after out-\nburst when the expanding nova envelope is still small and\ndense. The energy peak during this earliest time, called\nthe “fireball” phase (Shore et al. 1994; Schwarz et al.\n2001), is expected to occur at X-ray wavelengths. This\nphase is extremely difficult to observe since it is pre-\ndicted to last only a few hours after the beginning of the\noutburst so that it is over before the nova can be dis-\ncovered in the optical. As the envelope increases in size\nand cools, the opacity increases, shifting the radiation\ntowards wavelengths longer than the hydrogen absorp-\n1 School of Earth and Space Exploration, Arizona\nState University, Tempe, AZ 85287-1404, USA: Jan-\nUwe.Ness,sumner.starrfield@asu.edu\n2 Department of Geology & Astronomy, West Chester Univer-\nsity, 750 S. Church Street, West Chester, PA 19383, USA\n3 Department of Astronomy and Astrophysics, Penn State\nUniversity, 525 Davey Lab, University Park, PA, 16802-6305, USA\n4 Hamburger Sternwarte, Gojenbergsweg 112, 21029 Hamburg,\nGermany\n5 NASA/Goddard Space Flight Center, Greenbelt, Maryland\n20771, USA\n6 Department of Astronomy and Astrophysics, Penn State\nUniversity, University Park, Pennsylvania 16802, USA\n7 Department of Physics & Astronomy, University of Leicester,\nLeicester, LE1 7RH, UK\n∗This version contains additional material\ntion edge at 13.6 eV (911 A˚). The expanding, cooling en-\nvelope gives rise to the large and rapid increase in the\nvisual luminosity of the nova. Around the time of visual\nmaximum, X-rays from the underlying WD are trapped\ninside the large column density of the ejecta. However,\nshocks may develop within the early expanding ejecta\nproducing a hard, sub-Eddington luminosity, X-ray spec-\ntral energy distribution (e.g., V382Vel, Orio et al. 2001a;\nMukai & Ishida 2001). As the shell continues to ex-\npand, the density and opacity drop, and X-rays emitted\nfrom the surface of the WD eventually become visible\n(Krautter et al. 1996). Soft X-ray emission originates\nfrom hot layers produced by the nuclear burning of the\nremaining accreted material on the WD. This material\nburns in equilibrium at a high but constant luminosity,\n∝ LEddington (Gallagher & Starrfield 1978). The spec-\ntral energy distribution resembles that of the super-soft\nX-ray sources (SSS), (e.g. Cal 83, Paerels et al. 2001;\nLanz et al. 2005).\nRecent observations of novae during this phase show\nthat the flux can also be highly variable on short\ntimescales (see, e.g. Ness et al. 2003; Osborne et al.\n2006). The duration of this SSS phase is predicted to\nbe inversely proportional to the WD mass (Starrfield\n1992), but a review of the ROSAT X-ray sky survey\n(Orio et al. 2001b) showed that the timescale is much\nshorter than predicted, implying either that the masses of\nWDs in CNe are much higher than commonly assumed,\nor that the X-ray turn-off is a function of more than\nthe WD mass. A different approach was proposed by\nGreiner et al. (2003) who found from the available data\nthat systems with shorter orbital periods display long\ndurations of supersoft X-ray phases, while long-period\nsystems show very short or no SSS phase at all. They\nspeculate that shorter periods may be related to a higher\nmass transfer rate (e.g., by increased irradiation) and\nthus to the amount of material accreted before the ex-\nplosion.\nAnother source of X-ray emission from novae are\nemission lines from material that has been radiatively\nionized. These lines are expected to be present during\n2the entire evolution, but are usually only observed once\nthe nuclear fuel is consumed and the bright continu-\nous SSS spectrum (that outshines the emission lines)\nfades. The ejecta quickly recombine until a collisional\nequilibrium is reached. This equilibrium reflects the\nkinetic temperature distribution of the plasma and the\nelemental abundances can be derived (Ness et al. 2005).\nThis phase is called the nebular phase, and is also seen\nat other wavelengths (Shore et al. 2003). In some cases,\nX-ray emission from lines has been observed earlier,\neither prior to the SSS phase, superimposed on the SSS\nspectrum (e.g., V1494Aql), or during times of extreme\nvariability when the SSS emission suddenly declines\nbefore recovery (e.g., V4743 Sgr, Ness et al. 2003).\nX-ray observations provide important clues to the\nproperties and dynamics of novae. The evolution of the\nSSS depends on the WD mass, the mass loss rate (via\nradiatively driven winds or common envelope mass loss),\nthe amount of ejected material, the amount of material\nremaining on the WD, the binary separation, and the ex-\npansion velocity of the ejecta. Analysis of the X-ray emis-\nsion can also provide insight into the composition of the\nejecta. Unfortunately, only a few novae have been stud-\nied extensively in X-rays with much of the recent infor-\nmation coming from Chandra and XMM-Newton. Now,\nwith Swift, we have the possibility of studying novae in\nlarge numbers in order to assess statistical properties.\nSwift also allows for coordination of higher-resolution ob-\nservations with Chandra and XMM-Newton since it has\nalways been difficult to predict the brightness in X-rays\nand to estimate the optimal exposure time for X-ray ob-\nservations with the gratings.\nIn this paper we discuss the Swift XRT instrument\nbriefly in Sect. 2.1. Since interstellar absorption plays\nan important role in Galactic novae, we discuss the ex-\npected effects on the detection of SSS in Sect. 2.2. In\nSect. 3 we sort the observations by non-detections, de-\ntections of weak sources, and strong sources based on our\nextraction statistics detailed out in the appendix section\nA.2. We discuss our results in Sect. 4 and expand on the\ntype of X-ray emission that was detected in the Swift ob-\nservations. Finally, Appendix A.1 provides background\ninformation for each nova.\n2. Swift OBSERVATIONS\n2.1. The instrument\nOur entire dataset has been obtained with the X-\nray Telescope (XRT) aboard Swift (Burrows et al. 2005).\nThe XRT instrument is a CCD detector behind a Wolter\nType I grazing incidence mirror consisting of 12 nested\nshells. The field of view covers 23.6′ × 23.6′, imaged on\na detector with 600 × 600 pixels, thus each pixel cor-\nresponds to 2.36′′. The point spread function (PSF)\ncan be parameterized, and source radii of 10 pixels and\n5 pixels include 80.5 percent and 60.5 percent of the to-\ntal energy, respectively (Moretti et al. 2004). The posi-\ntional accuracy is 2.5′′, or one pixel. The energy range\ncovers 0.2 − 10keV with the Full Width at Half Maxi-\nmum (FWHM) energy resolution varying from ∼ 50 eV\nat 0.1 keV to ∼ 190 eV at 10keV. In Fig. 1 we show\na comparison of effective areas of Swift, Chandra, and\nXMM-Newton. In general, the Swift XRT has a smaller\nFig. 1.— Comparison of XRT effective areas of Swift with other\nX-ray instruments that are sensitive in the same spectral region.\nNone of the Chandra or XMM-Newton gratings has a higher effec-\ntive area than the XRT. The CCD detectors: ACIS-S (Chandra)\nand EPIC (PN/MOS1; XMM-Newton) have higher effective areas.\nThe spectral resolution of the XRT is similar to either the ACIS or\nEPIC detectors, but the spectral resolution of the gratings is much\nsuperior.\neffective area than the Chandra/ACIS-S except at low\nenergies (long wavelengths) where the Chandra ACIS-\nS suffers from large calibration uncertainties. The EPIC\ndetectors PN and MOS1 (XMM-Newton) have 50 percent\nand 20 percent higher effective areas, respectively. The\nXRT, ACIS, and EPIC instruments have similar spec-\ntral resolution. The XRT effective area is larger than\nthat of the gratings aboard Chandra (LETG and MEG)\nand XMM-Newton (RGS1 and RGS2), but the XRT\nspectral resolution is much lower. Fig. 1 demonstrates\nthat the XRT is an ideal instrument for exploring the\nemission level of novae using reasonable exposure times\n(∼ 3 − 6 ksec). Once a nova is detected with Swift and\nfound sufficiently bright, additional observations can be\nrequested with the high-resolution grating instruments\naboard Chandra and XMM-Newton to obtain detailed\nspectral information. The detector was operated in Pho-\nton Counting (PC) mode which provides 2D imaging,\nspectral information, and 2.5-sec time resolution.\n2.2. Observability of the Super Soft Source phase in\nClassical Novae\nUnfortunately, many Galactic CNe have large extinc-\ntion rates which has a pronounced effect on the observ-\nability of the SSS emission. To illustrate this we at-\ntenuate a series of Cloudy models (Ferland et al. 1998)\nthrough different amounts of H I column densities and\ndetermine the resulting Swift counts. The models use\nnon-LTE atmospheres by Rauch (1997) for planetary\nnebula nuclei with effective temperatures in the range of\n(1− 6)× 105K (8–50eV) as the input source. The model\nluminosity was fixed at 1038 erg sec−1, which is approxi-\nmately the Eddington luminosity for a 1-M⊙ star. The\nspectral energy distributions were attenuated by H I col-\numn densities ranging from (1−20)×1021 cm−2 and then\nconverted to Swift net count rates using the effective area\nof the XRT and scaled to a 1 ksec exposure at 1 kpc. The\nlogarithms of the expected total XRT counts are shown\nas contours in Fig. 2. As expected, the ability to detect a\nSSS is highly dependent on NH and the source’s effective\ntemperature. Sources with low temperatures are more\ndifficult to detect if significant NH-absorption takes place.\n3Fig. 2.— Contour plot of the log10 Swift counts predicted with a\n1-ksec observation from a SSS a a function of temperature (Cloudy\nmodel) and interstellar extinction (NH) for a source at a distance\nof 1 kpc.\nWithout long (> 10 ksec) exposures, NH ∼ 10\n22 cm−2 is\nthe highest number that allows a detection of SSS emis-\nsion at 1 kpc distance with Swift. Since most novae are\nfurther away than 1 kpc, the situation is even worse.\n2.3. Targets\nAlthough primarily a mission to study the temporal\nevolution of γ-ray bursts (GRB), Swift also provides Tar-\ngets of Opportunity (ToO) observations for non-GRB\nevents. Through ToO observations we have obtained X-\nray observations of 9 recent (< 1−2 years since outburst)\nand 3 older (> 3 years) CNe. We consider all observa-\ntions, although those of V1188Sco on 21 May, 2006 and\nof V574Pup taken on 26 July, 2005 were extremely short\nwith hardly any signal. Table 1 provides a list of impor-\ntant physical parameters associated with these novae in-\ncluding the maximum visual magnitudes, the date of dis-\ncovery, the time to decline 2 magnitudes from maximum\n(t2), the early measured expansion velocities, E(B − V )\nreddening values, H i column densities, and distances. In\nthe appendix A.1 we summarize background information\nfor each nova in our sample.\n3. ANALYSIS OF THE Swift XRT DATA\nCurrently the Swift archive contains 27 observations\nof 12 novae obtained with the X-ray Telescope (XRT)\ninstrument (see Sect. 2.1). We analyzed the data using\nthe Swift reduction packages in HEAsoft version 6.068\nand the latest calibration data (version 20060427). We\nstarted with the photon event files (level 2) that are avail-\nable in the public Swift archive and carried out three sta-\ntistical tests to determine if the sources were detected.\nWe then calculated count rates or upper limits in cases\nwhere no detections can be claimed. The first criterion\nis the formal detection likelihood from comparison of the\nnumber of counts measured in the source extraction re-\ngion (circular with 10 pixels radius around expected sky\nposition) with the background extraction region (annu-\nlus around source extraction region with inner radius\n10 pixels and outer radius of 100pixels). We next de-\ntermined if there was a concentration of counts near the\n8 http://swift.gsfc.nasa.gov/docs/software/lheasoft/\ncenter of the source extraction region. Finally, we com-\npared the spectral distributions of photons in the source\nand background extraction regions, but this is only a soft\ncriterion, as similar spectral distributions do not rule out\nthe presence of a source. We present our statistical meth-\nods in more detail in the appendix Sect. A.2 to justify\nour tests.\nIn Table 2 we provide all the Swift observational data\nincluding observation date, time since visual maximum,\nexposure time, number of counts in the source and back-\nground extraction regions, and either the net source\ncount rate per detect cell with 1σ uncertainty, or the\nupper limit.\n3.1. Non-detections, failure of all criteria\nOnly upper limits could be established for the observa-\ntions of V476 Sct, V1188Sco, LMC2005, and V1494Aql.\nIn Fig. 3 we show the photon plot centered around the\nexpected source position of V476Sct and LMC2005. The\nextraction regions for source and background, as defined\nin Sects. 3 and A.2 are marked with dark and light gray\ncolors, respectively, and the counts within the source ex-\ntraction region are plotted with small black bullets. In\nthe upper right corner we list: the date of observation,\nsource extraction region, the number of counts in the re-\nspective extraction regions, and the exposure times. In\nthe upper left corner we display the source net count\nrate (or the 95.5-% upper limit if no significant excess\ncount rate is found) as well as the detection likelihood,\nthe number of net source- and background counts S and\nB, respectively, as defined in Eq. A1. For V1188Sco,\nall criteria fail for all observations, and the source is\nclearly not detected. A similar case is encountered for\nall three observations of LMC2005, where we show the\nlast observation in the right panel of Fig. 3. Further, we\nfound no significant detections in all three observations of\nV1494Aql. We found only a marginal detection in the\nChandra observation of V1494Aql five years prior to the\nSwift observations (see Sect. 3.4). We used the proposal\nplanning tool PIMMS in order to convert the Chandra\ncount rate into an expected Swift XRT count rate. With\nvarious model assumptions PIMMS predicts more than\n0.04 XRT counts per second, while we measured less than\n0.0014 counts per second in each observation (Table 2).\nThis implies that the source has faded by at least a factor\n30 within five years.\n3.2. Weak sources < 0.01 cps\nV1663Aql was the first nova observed by Swift. Unfor-\ntunately, the first planned observation was truncated by\na GRB after only 1.25 ksec. Nevertheless, we find a clear\ndetection and a strong concentration of counts towards\nthe center (Table 2). The source spectrum is similar to\nthe background spectrum, but that does not rule out a\ndetection (see Fig. 4). The spectrum is extremely weak\nand no conclusion can be drawn. A second observation\nseven months later was longer but yielded only an upper\nlimit which implies a fading of the source. Neither of our\nother two criteria supported a detection.\nFor V5116Sgr we only found a marginal (formal 93-\npercent) detection with a concentration of photons to-\nwards the center. The spectrum is too weak to apply\nour source-background comparison criterion (see Fig. 5).\n4TABLE 1\nBasic nova parameters\nObject Vmax Datea t2b vexpc E(B–V) NH\nd Dis. Refs.e\n(mag) (days) (km sec−1) (mag) (cm−2) (kpc)\nV574 Pup 8 11/26/2004 13 650H,860P 0.5 6.5e21 3.20 1,2,3\nV382Nor 9 03/19/2005 13 1100P 0.6-1.1 1.8e22 13.80 3,3,3\nV1663Aql 10.5 06/10/2005 16 700P ∼2 1.8e22 5.5±1 4,5,4\nV5116 Sgr <8 07/04/2005 20 1300P 0.24±0.08 1.6e21 11.30 3,3,3\nV1188 Sco 8.9 07/26/2005 12 1730H,4000Z ∼1? 5.0e21 7.5 3,3,3\nV1047Cen 8.83 09/04/2005 ? 850H 3.3? 1.6e22 ? -,3,-\nV476 Sct 10.9 09/30/2005 15 1200H 1.9±0.1 1.1e22 4±1 6,6,6\nV477 Sct 10.4 11/10/2005 3 2700H,6000Z >1.3 4.8e21 11 7,7,7\nLMC2005 12.6 11/27/2005 ? – 0.15 6.3e20 50 3,3,3\nV723Cas 7.1 12/17/1995 slow 700H ∼0.5 2.4e21 4 -,8,9\nV1494Aql 4 12/03/1999 6.6±0.5 1300H,1850P 0.6±0.1 4.2e21 1.6±0.2 10,-,11\nV4743 Sgr 5 09/20/2002 9 2400H low? 1.4e21 6.3 3,-,12\na Date of visual maximum (mm/dd/yyyy).\nb Time to decline 2 magnitudes from maximum.\nc Expansion velocity. Taken from early IAU circulars. Trailing letters indicate how the velocity was\nmeasured: H = Full-Width at Half-Maximum, P = P Cygni absorption, Z = Full-Width at Zero-Intensity.\nd The H i column densities were obtained from the Heasarc NH tool. They are the line-of-sight column\ndensities through the entire Galaxy toward the coordinates of each object.\ne References for t2, E(B–V), and Distance:\n1 = Siviero et al. (2005); 2 = R.J. Rudy 2006, private communication; 3 = t2: from AAVSO light curve\nand IAU circulars. E(B–V): using van den Bergh & Younger (1987) and the (B–V) color evolution or as-\nsuming E(B–V) ∼ NH/4.8×10\n21 cm−2. Distance: using the MV vs t2 relationship of Della Valle & Livio\n(1995); 4 = Lane et al. (2006); 5 = Puetter et al. (2005); 6 = Munari et al. (2006b),Perry et al. (2005);\n7 = Munari et al. (2006a); 8 = Munari et al. (1996); 9 = (Iijima et al. 1998); 10 = Kiss & Thomson\n(2000); 11 = Iijima & Esenoglu (2003); 12 = Lyke et al. (2002).\nFig. 3.— V476 Sct (left) and LMC2005 (right) were not detected. We show the recorded photon positions and mark the source extraction\nregion with dark shading and the background extraction region (annulus with inner radius 10 pixels and outer radius 100 pixels) with light\nshading. The pixel- and sky coordinates are given in the bottom. The upper limits are given in the upper left. Observation date, extraction\nradius, number of source- and background counts per detect cell, and exposure time are given in the upper right. The percentage of source\ncounts that reside within 25 percent area (radius 5 pixels) is given to the right of each source extraction region to give an impression of the\nconcentration towards the center within the source extraction region.\n5Fig. 4.— Two Swift observations of V1663Aql. The left panels (image) are the same as described in Fig. 3. The right panels provide the\nspectral information of the source (black) and background (downscaled to the area of the source extraction region; light shadings). The\nlikelihood for the background spectrum being softer than the source spectrum is estimated by drawing 1000 random sample spectra with\nthe number of source counts (in the upper panel 9) out of the background (0.31× 99 = 31 counts) and counted the number of cases where\nthe median energy of each sample spectrum was lower than that of the source spectrum. Much different values than 50 percent are a soft\ncriterion for a source detection. In the upper panel we find a clear detection and give the count rate with 1-σ uncertainty, ∆L according\nto Eq. A6, and the number of net counts for source and background S and B (per detect cell) according to Eq. A1.\n6TABLE 2\nMeasured count rates in our sample\nStart ∆ta Exp. Src. Bg. Rateb,d R1/2\nc\nDate (d) (ks) (cts)b (cts)b (cts/ks) (%)\nV574Pup\n05/20/2005 175 1.0 10 0.4 12.0 ± 4.0 60\n05/25/2005 180 1.9 22 0.4 14.0 ± 3.0 77\n05/26/2005 0.01 0 0 0 0\n07/29/2005 245 1.1 15 0.3 16.0 ± 4.0 67\n07/30/2005 246 7.1 51 2.1 8.5 ± 1.3 73\n08/06/2005 253 7.8 57 2.0 8.7 ± 1.2 67\n08/09/2005 256 2.2 22 0.6 12.0 ± 3.0 64\n08/10/2005 257 2.8 18 0.8 7.8 ± 2.0 67\n08/11/2005 258 1.8 11 0.5 7.3 ± 2.5 56\n08/17/2005 264 4.7 34 1.2 8.6 ± 1.6 79\nV382Nor\n01/26/2006 313 6.1 72 4.0 14.0 ± 2.0 75\nV1663Aql\n08/15/2005 66 1.3 9 0.3 8.6 ± 3.3 78\n03/04/2006 267 6.5 3 1.1 < 0.5d 0\nV5116Sgr\n08/29/2005 56 3.1 5 2.0 1.2 ± 1.0 80\nV1188 Sco\n05/21/2006 98 0.2 1 0.1 < 7d 0\n06/17/2006 124 5.0 3 2.3 < 9.1d 33\nV1047Cen\n09/11/2005 66 3.9 19 1.3 5.6 ± 1.5 53\n01/23/2006 141 5.3 53 3.1 11.0 ± 2.0 72\nV476 Sct\n02/12/2006 135 3.8 2 1.0 < 8.2d 50\nV477 Sct\n03/07/2006 117 6.2 15 1.8 2.7 ± 0.8 87\n03/15/2006 125 4.7 8 1.3 1.8 ± 0.8 50\nLMC2005\n12/02/2005 5 0.1 0 0.1 < 1.0d 0\n12/01/2005 4 4.1 1 1.3 < 8.6d 100\n12/02/2005 5 3.9 2 1.2 < 8.7d 100\n12/03/2005 6 5.7 1 1.7 < 7.0d 50\nV723Cas\n01/31/2006 3698 6.9 147 1.7 26.0 ± 2.0 69\nV1494Aql\n03/10/2006 2289 1.4 0 0.3 < 1.4d 0\n03/13/2006 2292 1.2 0 0.2 < 1.4d 0\n05/19/2006 2359 2.5 1 0.5 < 1.0d 100\nV4743Sgr\n03/08/2006 1265 5.8 267 2.3 27.0 ± 2.0 70\na Interval between visual maximum and the XRT observation in days\nb per detect cell (10 pixels radius); count rates with 1σ uncertainties\nc fraction of counts within 5 pixels (%)\nd 95.5-% upper limit\nTwo observations of V477 Sct were carried out 8 days\napart and resulted in two detections with a likelihood\n> 99 percent (see Fig. 6). The first observation suggests\na weak source with a clear concentration towards the\ncenter and a much softer spectrum than that of the in-\nstrumental background. The second observation reveals\na lower count rate, lower detection likelihood, and a less\nclear concentration of photons towards the center. While\nthis implies variability or that the source has faded over\nthe eight day interval, within the 1-σ errors, the count\nrate is consistent with that of the first observation. Nei-\nther spectrum allows quantitative analysis.\n3.3. Strong sources > 0.01 cps\nV574Pup, V382Nor, V1047Cen, V723Cas, and\nV4743Sgr were detected by Swift (see Figs. 7–11). The\nspectrum of V382Nor is hard whereas the background\nis softer. There appears to be some excess emission at\nwavelengths of prominent emission lines (e.g., 0.92 keV,\nNe IX; 1.02 keV, Ne X; and 0.83 keV, Fe XVII) but a firm\nidentification of these lines is not possible with the ex-\nisting data. There is also emission at energies with no\nwell-known emission lines, particularly at high energies.\nThe observed spectrum is too faint to derive any quan-\ntitative conclusions.\nWe obtained very clear detections of V1047Cen. In\nthe first observation of November 2005 the photons are\nclearly concentrated towards the center, and the energy\ndistribution is very different from the soft background.\nTwo months later the detection is even stronger and the\nspatial as well as spectral distributions clearly support a\ndetection. The count rate doubled from 5.6 cts ksec−1 on\n9 November, 2005 to 11 cts ksec−1 on 23 January, 2006.\nThe spectrum appears hard, but there are too few counts\nto derive any spectral properties.\nThe remaining novae were sufficiently bright to obtain\nspectral confirmation that they were detected during the\nSSS phase. More detailed analysis of these spectra, in-\ncluding additional Swift observations and supporting op-\ntical and near-IR spectra, is left for future papers. Here\nwe only provide a brief analysis of the current Swift data\nsets.\nV574Pup was first observed in late May 2005. It was\nthen observed multiple times from the end of July to mid\nAugust 2005 as it was perfectly positioned to cool the\nXRT detectors between GRB observations. In all nine\nobservations, it was clearly detected with a high degree\nof significance (see Table 2). V574Pup had an average\nof 7 − 14 cts ksec−1 in each observation. The individual\nobservations were too short for analysis, so we combined\nthem, yielding a higher signal-to-noise ratio. We grouped\nthe spectra by summing the number of counts and the\nnumber of background counts in each spectral bin. We\ncalculated exposure time-weighted effective areas from\nthe individual extracted effective areas.\nHowever, during the 50 days between the May 26 and\nJuly 29 observations, significant evolution probably oc-\ncurred which we would miss by combining all spectra to-\ngether. In order to search for any evolution, we grouped\nthe first two observations from May 2005 and the re-\nmaining observations from July and August 2005. These\nare shown in Fig. 12. The summed spectra of the two\nearly observations shows a uniform distribution with en-\nergy and may be an underexposed emission line spec-\n7Fig. 5.— Same as Fig. 4 but for V5116 Sgr.\ntrum. Unfortunately, we don’t have any more data from\nthis time period to increase the signal-to-noise ratio. The\nlater observations sum up to what appears to be a contin-\nuous spectrum ranging from 0.3–0.5 keV (25 A˚ to 45 A˚).\nThis spectrum resembles that of a SSS. We carried out\na blackbody fit using the most recent XRT response ma-\ntrices (version 8) and assuming an emitting radius of\n7000km. We found a satisfactory fit (Fig. 13) with the\nparameters Teff = (293 ± 10)× 10\n3K (25.2 ± 1 eV) and\nNH = (2.5 ± 0.6)× 10\n21 cm−2 (1-σ uncertainty ranges).\nWe fixed the radius because no unique solution could be\nfound with a variable radius. The uncertainties include\nvariations of the radius between 7000 and 9000km.\nThe XRT spectrum of V4743Sgr is faint, and no spec-\ntral features can be identified. We also found no features\nin the RGS observation taken in September 2004 (see\nbelow). While the XRT count rates of V4743Sgr and\nV723Cas are similar, the spectra are different (Fig. 11).\nV723Cas shows a clear peak occurring at 0.4 keV (31 A˚).\nNess et al. (2006b) reported that the spectrum resembled\na SSS, and they were able to fit a blackbody model. We\nrepeated these fits using the updated XRT response ma-\ntrices (version 8) and show the best-fit model in Fig. 14.\nWe fixed the radius at 7000km but included variations\nup to 9000km in the calculation of the 1-σ uncertainty\nranges. We find Teff = (371 ± 15)× 10\n3K (32 ± 2 eV)\nand NH = (3.3 ± 0.4) × 10\n21 cm−2. V723Cas is now,\nafter 11 years, the oldest Galactic nova that is still ac-\ntive. Such a long duration of the SSS phase is unusual\nfor a Classical Nova, and there is a possibility that the\nnova has transitioned into a SSS such as Cal 83. Our\nobservations of this interesting source are continuing.\n3.4. Supplemental observations\nIn addition to the Swift observations, we extracted\nunpublished X-ray observations from the Chandra and\nXMM-Newton archives. A series of Chandra observations\nof V1494Aql were carried out and we extracted the last\nobservation (LETGS, ObsID 2681) taken 727days after\noutburst (2001, Nov. 28). The source was only detected\nin the zeroth order on the HRC-S detector, but no dis-\npersed spectrum could be extracted. We found 56 counts\non a background of 10, corresponding to an HRC count\nrate of 0.02± 0.003 cts sec−1 and a 14-σ detection. Unfor-\ntunately, the HRC detector does not provide the energy\nresolution to construct a spectrum, and no spectrum can\nbe constructed from the dispersed photons as the back-\nground is too high.\nWe further analyzed XMM-Newton observations of\nLMC2005 taken 243 days after outburst (ObsID\n0311591201, 2006, July 18) and of V4743Sgr 1470 days\nafter outburst (ObsID 0204690101, 2004, Sept. 30).\nLMC2005 was again not detected.\nFrom MOS1 we extracted 0.11 ± 0.002 cts sec−1 for\nV4743Sgr. The source was also detected in the RGS\nwith a count rate of 0.01 ± 0.0005cts sec−1. Grat-\ning spectra allow the calculation of a flux without the\nneed of a model, and we measured a flux of 3 ×\n10−13 erg cm−2 sec−1 from the RGS1 and RGS29. Fig. 15\nshows a comparison of these last XMM-Newton spectra\nwith the recent Swift observation.\n4. DISCUSSION\nThe sample of X-ray observations of novae presented\nin this paper demonstrates that Swift significantly in-\ncreased the number of Galactic novae observed in X-\nrays. We provide a large sample taken with the same\n9 0.35–1.8 keV; corrected for the chip gaps\n8Fig. 6.— Same as Fig. 4 but for the two observations V477 Sct.\n9Fig. 7.— Same as Fig. 4 but for six usable observations of V574 Pup.\n10\nFig. 8.— Continuation of Fig. 7.\nFig. 9.— Same as Fig. 4 but for V382Nor. The spectrum appears to be an emission line spectrum, but we note that it is oversampled\nand the lines cannot be as clearly identified.\n11\nFig. 10.— Same as Fig. 4 but for the two observations of V1047Cen.\n12\nFig. 11.— Same as Fig. 4 but for V723Cass (top) and V4743 Sgr (bottom). While the count rates are similar, the spectra are very\ndifferent.\ninstrument, which reduces problems from cross calibra-\ntion. Also, Swift is capable of carrying out monitor-\ning observations with relatively little effort, the excep-\ntion being classical novae with large column densities.\nIn Sect. 2.2 we demonstrated that one can not obtain\nenough counts in a typical ToO exposure ( 3-5 ks) when\nthe column density exceeds NH > 10\n22 cm−2 to observe\nan SSS phase. In this survey the estimated column densi-\nties of four novae, V382Nor, V1047Cen, V1663Aql, and\nV476Sct, exceeded this value and only very hard spectra\nor non-detections were recorded. For Galactic novae with\nlow column densities Swift is an excellent instrument for\nfollowing all the stages of evolution in X-rays (e.g., the\nrecurrent nova RSOph Osborne et al. 2006).\nShortly after the outburst, X-ray emission from the\nWD is not expected to be observed. X-ray detections\nduring this phase thus originate from other processes,\ne.g., shocks within the expanding shell or a shock set off\nby a collision between the expanding shell and the atmo-\nsphere of the companion (as in RSOph, e.g., Bode et al.\n13\nFig. 12.— Grouped XRT observations of V574 Pup comparing\nthe May 2005 (top panel) and the July through August data sets\n(bottom panel). The comparison shows that V574Pup has evolved\nfrom a hard early spectrum (top panel) into a SSS spectrum (bot-\ntom). The count rate is higher in the top panel, which may be due\neither to the higher sensitivity of the detector at higher energies or\nthe higher amounts of absorption at soft energies.\nFig. 13.— Grouped and rebinned XRT spectrum of V574Pup\ncovering from July 2005 to August 2005 (shown in the bottom\npanel of Fig. 12). The histogram (plus error bars) gives the ob-\nserved spectrum and smooth line shows the best-fit blackbody\nmodel overplotted (parameters given in legend).\nFig. 14.— Blackbody fit to the XRT spectrum of V723Cas. The\nbackground lies on top of the horizontal axis.\n2006). Few observations during this phase have been car-\nried out (e.g. Mukai & Ishida 2001) and they only pro-\nvide information on dynamics within the ejecta. With\nthe flexibility of Swift a better assessment of pre-SSS X-\nray emission can be achieved.\nThe SSS phase is the brightest phase of X-ray emission\n(at constant bolometric luminosity), making it the best\nFig. 15.— Comparison of the XRT spectrum of V4743 Sgr (top)\nwith XMM-Newton RGS1 and RGS2 spectra from September 2004.\nAll are consistent with a fading nebular spectrum, but the signal\nto noise and the resolution are insufficient to characterize the spec-\ntrum as hard continuum or an emission line spectrum.\ntime to observe a nova. We found two clear detections of\nnovae in this phase. With these two new SSS spectra, the\nnumber of Galactic novae clearly observed in this state\nis seven (Orio et al. 2001b; Drake et al. 2003; Ness et al.\n2003, and reference therein). For V574Pup we were even\nable to identify the transition from an early emission line\nspectrum to a bright SSS spectrum within two months.\nV574Pup entered its SSS phase within 250 days after\noutburst. This is consistent with the evolution of V1974\nCyg, V1494Aql, and V4743Sgr (see Fig. 16). The detec-\ntion of a SSS in V723Cas suggests that nuclear burning is\nstill going on more than 11 years after outburst. It is not\nknown how much longer it will remain a SSS. More de-\ntailed discussions on V723Cas will be presented by Ness\net al. (in prep).\nWe detected V4743Sgr two years after XMM-Newton\nhad already measured a very low level of X-ray emission\n(see Sect. 3.4). This shows that the post-turn-off evolu-\ntion can last a few years. V1494Aql was not detected,\nbut five years prior to our observation it had been found\nto have marginal emission and no SSS spectrum.\nIn order to compare the evolution of the novae detected\nin our sample we plot their distance-scaled X-ray bright-\nnesses as a function of individual time after outburst in\nFig. 16. In the bottom panel we show the X-ray light\ncurves (in the same time units) of five more Galactic no-\nvae, but observed by different missions. The brightness is\ngiven as unabsorbed luminosity as extracted from direct\nmeasurements of fluxes (Greiner et al. 2003b; Orio et al.\n2001a; Balman et al. 1998; Ness et al. 2003; Petz et al.\n2005). As a general rule, any X-ray detection of no-\nvae earlier than 100 days after outburst arises from the\nhard, shock-generated phase that can either be emission\nlines or possibly bremsstrahlung emission (Orio et al.\n2001a; Mukai & Ishida 2001). Likewise, any X-ray de-\ntections more than 1000 days after outburst come from\nthe nebular phase, with the three exceptions of GQMus,\nLMC1995, and now V723Cas. These general rules also\nhold for novae in M31 (see figure 3 in Pietsch et al.\n2005).\nFinally, we can use the parameters derived from the\nmodel fits to the SSS spectra of V574Pup and V723Cas\n14\nFig. 16.— Top: XRT count rates of all sources in our sample, plotted against the day of observation after their outbursts. The count rates\nare corrected for distance squared which is uncertain. Multiple observations of the same targets are connected with dotted lines. Bottom:\nObservations from various other missions. The observed X-ray luminosities (not corrected for absorption, except for V1974Cyg, which is\ninstead rescaled) of recent well-observed novae with SSS phases. Similar plots have been created by Pietsch et al. (2005); Pietsch et al.\n(2006).\nto check the Swift counts that we predicted from our at-\ntenuated Cloudy model of an “average” classical nova\nejection in Fig. 2 (Sect. 2.2). From our blackbody fits we\nobtained effective temperatures of ∼ 3 × 105K (25 eV)\nfor both novae, while the derived column densities were\n2.7 and 1.7 × 1021 cm−2 for V574Pup and V723Cas,\nrespectively. With these parameters we find from the\ngrid of Cloudy models (Fig. 2) the predicted counts in\na 1-ksec observation at a distance of 1 kpc of ∼ 10 for\nV574Pup and ∼ 100 for V723Cas. Scaling these pre-\ndicted counts by the distances in Table 1 and the ex-\nposure times in Table 2 (texp(ksec)/D(kpc)\n2) gives 75\ncounts for V723Cas and 7 counts for the 30 July, 2005\nobservation of V574Pup. These values are consistent\nwith the detections given the uncertainties in the dis-\ntances and the use of a generalized nova model (e.g. a\nluminosity of 1038 erg s−1, an ejected mass ∼ 10−4 M⊙,\nSchwarz et al. 2007). For “normal” classical novae, cal-\nculations like those done for Fig. 2 can be used as a tool\nto plan X-ray observations by providing first-order esti-\nmates of expected X-ray brightness levels during the SSS\nphase.\nWe acknowledge the use of public data from the Swift\ndata archive. We acknowledge with thanks the vari-\nable star observations from the AAVSO International\nDatabase contributed by observers worldwide and used\nin this research. J.-U. N. gratefully acknowledges sup-\nport provided by NASA through Chandra Postdoctoral\nFellowship grant PF5-60039 awarded by the Chandra X-\nray Center, which is operated by the Smithsonian Astro-\nphysical Observatory for NASA under contract NAS8-\n03060. S. Starrfield received partial support from NSF\nand NASA grants to ASU. JPO acknowledges support\nfrom PPARC\nFacilities: Swift (XRT), XMM, CXO\nREFERENCES\nAvni, Y. 1976, ApJ, 210, 642\nBalman, S., Krautter, J., & O¨gelman, H. 1998, ApJ, 499, 395\nBode, M. F., O’Brien, T. J., Osborne, J. P., Page, K. L., Senziani,\nF., Skinner, G. K., Starrfield, S., Ness, J.-U., Drake, J. J., &\nSchwarz, G. 2006, ApJ, 652, 629\n15\nBurrows, D. N., Hill, J. E., Nousek, J. A., Kennea, J. A., Wells,\nA., Osborne, J. P., Abbey, A. F., Beardmore, A., Mukerjee, K.,\nShort, A. D. T., Chincarini, G., Campana, S., Citterio, O.,\nMoretti, A., Pagani, C., Tagliaferri, G., Giommi, P., Capalbi,\nM., Tamburelli, F., Angelini, L., Cusumano, G., Bra¨uninger,\nH. W., Burkert, W., & Hartner, G. D. 2005, Space Science\nReviews, 120, 165\nCooper, T., Africa, S., Aguiar, J. G. D. S., & Linnolt, M. 2005,\nIAU Circ., 8575, 2\nDella Valle, M., & Livio, M. 1995, ApJ, 452, 704\nDennefeld, M., Ricquebourg, F., & Damerdji, Y. 2005, IAUC,\n8544, 1\nDrake, J. J., Wagner, R. M., Starrfield, S., Butt, Y., Krautter, J.,\nBond, H. E., Della Valle, M., Gehrz, R. D., Woodward, C. E.,\nEvans, A., Orio, M., Hauschildt, P., Hernanz, M., Mukai, K., &\nTruran, J. W. 2003, ApJ, 584, 448\nEderoclite, A., Mason, E., dall, T. H., & Liller, W. 2005, IAUC,\n8497, 2\nFerland, G.J., Korista, K.T., Verner, D.A., Ferguson, J.W.,\nKingdon, J.B. & Verner, E.M. 1998, PASP, 110, 761\nFujii, M., & Yamaoka, H. 2005, IAU Circ., 8617, 3\nGallagher, J. S., & Starrfield, S. 1978, ARAA, 16, 171\nGilmore, A. C., & Kilmartin, P. M. 2005, IAUC, 8559, 2\nGonzalez-Riestra, R., Shore, S. N., Starrfield, S., & Krautter, J.\n1996, IAU Circ, 6295, 1\nGreiner, J., Orio, M., & Schartel, N. 2003, A&A, 405, 703\n—. 2003b, A&A, 405, 703\nHaseda, K., West, D., Yamaoka, H., & Masi, G. 2002, in IAU\nCircular, Vol. 7975, 1\nHeywood, I., O’Brien, T. J., Eyres, S. P. S., Bode, M. F., &\nDavis, R. J. 2005, MNRAS, 362, 469\nHirosawa, K., Yamamoto, M., Nakano, S., Kojima, T., Iida, M.,\nSugie, A., Takahashi, S., & Williams, G. V. 1995, IAU Circ.,\n6213, 1\nIijima, T., Rosino, L., & Della Valle, M. 1998, A&A, 338, 1006\nIijima, T., & Esenoglu, H. H. 2003, A&A, 404, 997\nKiss, L. L., & Thomson, J. R. 2000, A&A, 355, L9\nKrautter, J., O¨gelmann H., Starrfield S., Wichmann R.,\nPfeffermann E. 1996, ApJ 456, 788\nKrautter, J., & Williams, R. E. 1989, ApJ, 341, 968\nLane, B. F., Retter, A., Eisner, J. A., Thompson, R. R., &\nMuterspaugh, M. W., 2006, “Interferometric Observations of\nExplosive Variables: V838 Mon, Nova Aql 2005 & RS Oph,” in\n“Interferometry for Optical Astronomy,” eds., Proceedings of\nSPIE (the International Society of Photo-optical\nInstrumentation Engineers), 6268-161\nLanz, T., Telis, G. A., Audard, M., Paerels, F., Rasmussen, A. P.,\n& Hubeny, I. 2005, ApJ, 619, 517\nLiller, W. 2005a, IAUC, 8497, 1\nLiller, W. 2005b, IAUC, 8559, 1\nLiller, W. 2005c, IAUC, 8596, 1\nLiller, W. 2005d, IAUC, 8635, 1\nLyke, J. E., Kelley, M. S., Gehrz, R. D., & Woodward, C. E.\n2002, Bulletin of the American Astronomical Society, 34, 1161\nMazuk, S., Lynch, D. K., Rudy, R. J., Venturini, C. C., Puetter,\nR. C., Perry, R. B., & Walp, B. 2005, IAU Circ, 8644, 1\nMoretti, A., Campana, S., Tagliaferri, G., Abbey, A. F., Ambrosi,\nR. M., Angelini, L., Beardmore, A. P., Bra¨uninger, H. W.,\nBurkert, W., Burrows, D. N., Capalbi, M., Chincarini, G.,\nCitterio, O., Cusumano, G., Freyberg, M. J., Giommi, P.,\nHartner, G. D., Hill, J. E., Mori, K., Morris, D. C., Mukerjee,\nK., Nousek, J. A., Osborne, J. P., Short, A. D. T., Tamburelli,\nF., Watson, D. J., & Wells, A. A. 2004, in X-Ray and\nGamma-Ray Instrumentation for Astronomy XIII. Edited by\nFlanagan, Kathryn A.; Siegmund, Oswald H. W. Proceedings\nof the SPIE, Volume 5165, pp. 232-240 (2004)., ed. K. A.\nFlanagan & O. H. W. Siegmund, 232\nMukai, K., & Ishida, M. 2001, ApJ, 551, 1024\nMunari, U., Goranskij, V. P., Popova, A. A., Shugarov, S. Y.,\nTatarnikov, A. M., Yudin, B. F., Karitskaya, E. A., Kusakin,\nA. V., Zwitter, T., Lepardo, A., Passuello, R., Sostero, G.,\nMetlova, N. V., & Shenavrin, V. I. 1996, A&A, 315, 166\nMunari, U., Siviero, A., Navasardyan, H., & Dallaporta, S. 2006,\nA&A, 452, 567\nMunari, U., Henden, A., Pojmanski, G., Dallaporta, S., Siviero,\nA., & Navasardyan, H. 2006, MNRAS, 369, 1755\nNaito, H., Tokimasa, N., Yamaoka, H., & Fujii, M. 2005,\nIAU Circ., 8576, 2\nNakano, S., Tago, A., Sakurai, Y., Kushida, R., & Kadota, K.\n2004, IAUC, 8443, 1\nNess, J.-U., Starrfield, S., Burwitz, V., Wichmann, R.,\nHauschildt, P., Drake, J. J., Wagner, R. M., Bond, H. E.,\nKrautter, J., Orio, M., Hernanz, M., Gehrz, R. D., Woodward,\nC. E., Butt, Y., Mukai, K., Balman, S., & Truran, J. W. 2003,\nApJL, 594, L127\nNess, J.-U., Starrfield, S., Jordan, C., Krautter, J., & Schmitt,\nJ. H. M. M. 2005, MNRAS, 364, 1015\nNess, J.-U., Starrfield, S., Schwarz, G., Vanlandingham, K.,\nWagner, R. M., Lyke, J., Woodward, C. E., Lynch, D. K., &\nKrautter, J. 2006b, IAU Circ, 8676, 2\nOrio, M., Parmar, A., Benjamin, R., et al. 2001a, MNRAS, 326,\nL13\nOrio, M., Covington, J., Ogelman, H. 2001b, A&A, 373, 542\nOsborne, J. P., Page, K. L., & Bode, M. 2006, in prep for ApJ\nPaerels, F., Rasmussen, A. P., Hartmann, H. W., Heise, J.,\nBrinkman, A. C., de Vries, C. P., & den Herder, J. W. 2001,\nA&A, 365, L308\nPereira, A. 1999, IAU Circ, 7323, 1\nPerry, R. B., Venturini, C. C., Rudy, R. J., Mazuk, S., Lynch,\nD. K., Puetter, R. C., & Walp, B. 2005, IAUC, 8638, 1\nPietsch, W., Fliri, J., Freyberg, M. J., Greiner, J., Haberl, F.,\nRiffeser, A., & Sala, G. 2005, A&A, 442, 879\nPetz, A., Hauschildt, P. H., Ness, J.-U., & Starrfield, S. 2005,\nA&A, 431, 321\nPietsch, W., Haberl, F., Sala, G., Stiele, H., Hornoch, K.,\nRiffeser, A., Fliri, J., Bender, R., Buehler, S., Burwitz, V.,\nGreiner, J., & Seitz, S. 2007, A&A, 465, 375\nPojmanski, G., & Oksanen, A. 2005, IAUC, 8540, 1\nPojmanski, G., Nakano, S., Nishimura, H., Hashimoto, N., &\nUrata, T. 2005a, IAU Circ., 8574, 1\nPojmanski, G., Yamaoka, H., Haseda, K., Puckett, T., Hornoch,\nK., Schmeer, P., & Samus, N. N. 2005b, IAU Circ, 8617, 1\nPuetter, R. C., Rudy, R. J., Lynch, D. K., Mazuk, S., Venturini,\nC. C., Perry, R. B., & Walp, B. 2005, IAUC, 8640, 2\nRauch 1997, A&A, 320, 237\nRudy, R. J., Lynch, D. K., Mazuk, S., Venturini, C. C., Puetter,\nR. C., Perry, R. B., & Walp, B. 2005, IAU Circ, 8643, 2\nSchwarz, G.J., Shore, S.N., Starrfiedl, S., Hauschildt, P.H., Della\nValle, M., & Baron, E. 2001, MNRAS, 320, 103\nSchwarz, G.J., et al. 2007, ApJ, 657, 453\nShore, S.N., Sonneborn, G., Starrfield, S., Gonzalez-Riestra, R., &\nPolidan, R.S. ‘994, ApJ, 421,344\nShore, S. N., Schwarz, G., Bond, H. E., Downes, R. A., Starrfield,\nS., Evans, A., Gehrz, R. D., Hauschildt, P. H., Krautter, J., &\nWoodward, C. E. 2003, AJ, 125, 1507\nSitko, M. L., Kimes, R., Lynch, D. K., Russell, R. W., Kim,\nD. L., & Griep, D. 2005, IAU Circ., 8575, 1\nSiviero, A., Munari, U., & Jones, A. F. 2005, Informational\nBulletin on Variable Stars, 5638, 1\nStarrfield, S. 1992 in ”Reviews in Modern Astronomy Volume 5:\nVariability in Stars and Galaxies”, ed. G. Klare,\nSpringer-Verlag, NY, p. 73\nStarrfield, S., Shore, S. N., Butt, Y., Drake, J., Bond, H. E.,\nDownes, R., Krautter, J., Wagner, R. M., Gehrz, R. D.,\nWoodward, C. E., Della Valle, M., Hauschildt, P. H., & Truran,\nJ. W. 2000, Bulletin of the American Astronomical Society, 32,\n1253\nSoma, M., Takao, A., Yamaoka, H., Haseda, K., Gilmore, A. C.,\nKilmartin, P. M., Nakano, S., & Kadota, K. 2005, IAUC, 8607,\n1\nvan den Bergh, S., & Younger, P. F. 1987, A&AS, 70, 125\nWalter, F. M., Bond, H. E., & Pasten, A. 2005, IAU Circ., 8576, 3\n16\nAPPENDIX\nA. APPENDIX\nA.1. Targets\nIn this section we summarize some background information of our targets in addition to that given in Table 1,\ncollected mostly from unreferenced literature and private communications.\nA.1.1. V574Pup\nV574Pup was independently discovered by Tago and Sakurai (Nakano et al. 2004). The evolution of its early light\ncurve and spectral energy distribution is provided in Siviero et al. (2005). The FWHM of Hα in the early spectrum\nwas 650 km s−1 but with P-Cygni absorption extending to 860km s−1. Based on the intrinsic colors of the nova both\nat maximum and at t2, the same authors derived an extremely low reddening value of E(B–V)∼ 0.05. However, this\nvalue seems too low due to the low galactic latitude (b ∼ 2◦) and the distance (∼ 3.5 kpc) of this nova. Optical and\nnear-IR spectra obtained one year after outburst showed that V574Pup had entered a coronal phase with lines of\n[S VIII], [S IX], [Si VI], [Si VII], and [Ca VIII] (Rudy et al. 2005). Analysis of the ratio of the O I lines in this data set\nand those obtained later implies a higher reddening of E(B–V)= 1.27 (Rudy, private communication).\nA.1.2. V382Nor\nV382Nor was discovered prior to visual maximum on 13.3 Mar, 2005 (Liller 2005a). Examining the AAVSO light\ncurve, visual maximum probably occurred on 19 March, 2005 at V∼ 9. The B–V color obtained on 20 March was\n0.8 ± 0.11 (Liller 2005a), which gives an E(B–V) of 0.57 ± 0.17 based on the intrinsic color at maximum (B–V\n= +0.23 ± 0.06; van den Bergh & Younger 1987). Another determination of the reddening uses the intrinsic (B–V)\ncolor at t2, −0.02 ± 0.04 (van den Bergh & Younger 1987). The (B–V) color at t2 was ∼ 1.1, which implies E(B–\nV)∼ 1.1. This high a reddening estimate is supported by the saturated Na I interstellar lines in the spectra taken\nat the same time (Ederoclite et al. 2005). The P-Cygni profiles seen in the early spectra imply an average expansion\nvelocity of 1100km s−1.\nA.1.3. V1663Aql\nV1663Aql was discovered on 9.2 June, 2005 during routine All Sky Automated Survey patrols (Pojmanski & Oksanen\n2005). Spectroscopy obtained one day after maximum showed an extremely red continuum implying significant red-\ndening toward the source (Dennefeld et al. 2005). The emission lines had narrow P-Cygni profiles with expansion\nvelocities of order 700 kms−1. By 14 November V1663Aql had entered its nebular phase with [O III] in the optical\nand the coronal lines of [S VIII], [S IX], [Si VI] and [Si VII] present in the near-IR (Puetter et al. 2005). The expansion\nvelocity, as measured from the FWHM of the emission lines, was 2000km s−1 (Puetter et al. 2005). The ratio of the O I\nlines also confirmed the large extinction, with a value as large as E(B–V)= 2 (Puetter et al. 2005). Lane et al. (2006)\nreported an angular expansion of 0.2mas/day at 2.2µm with the Palomar Testbed Interferometer. This corresponds\nto a distance of 5.5 ± 1 kpc assuming expansion velocities between 700− 1000km s−1. If the expansion velocities are\nlarger, as measured by Puetter et al. (2005), then the distance must also be larger than that derived by Lane et al.\n(2006).\nA.1.4. V5116 Sgr\nOn 4 July, 2005 Liller discovered V5116 Sgr already on the decline. His first observations implied that visual\nmaximum was brighter than 8 mag (Liller 2005b). The P-Cygni line profile of Hα suggested an expansion velocity of\n1300km s−1 from spectra taken a day after discovery (Gilmore & Kilmartin 2005). They also reported a B–V color of\n+0.47 ± 0.02 on 5 July. Using the intrinsic (B–V) color at maximum for V5116 Sgr implies an E(B–V)= 0.24 ± 0.08.\nA.1.5. V1188 Sco\nV1188 Sco was discovered in late July 2005 (Pojmanski et al. 2005a). The visual maximum was measured by\nCooper et al. (2005). The only spectral information in the literature was obtained within days after visual maximum.\nOptical spectra were consistent with a typical CO type nova with broad Hα, FWHM and Full Width at Zero Intensity\n(FWZI) of 1730 and 4000km s−1, and Fe II P-Cyg emission lines (Naito et al. 2005; Walter et al. 2005). A 3 − 14\nmicron spectrum taken one day after visual maximum displayed a featureless continuum consistent with the Rayleigh-\nJeans tail of the Planck function (Sitko et al. 2005). The Na I D line was observed with three absorption components\nand an equivalent width of 0.5 nm (Walter et al. 2005) implying a large extinction.\nA.1.6. V1047Cen\nLittle is known about V1047Cen due to its extreme southern declination (> −62◦). It was discovered by Liller\n(2005c) on 1.031 September, 2005. Visual maximum probably occurred three days later at V= 8.83 but there is very\nlittle reported data on the early light curve, so its subsequent behavior, including its t2 decay time, is unknown. The\nspectrum at maximum resembled that of V5114Sgr and V5116Sgr with an Hα FWHM of 840 ± 50 km s−1 (Liller\n2005c).\n17\nA.1.7. V476 Sct\nV476Sct was discovered after visual maximum on 30.5 Sept, 2005 by A. Takao (Soma et al. 2005). Haseda (2005)\nreported an independent discovery on the same day with a visual magnitude of 10.9. Early optical spectra showed\nmany emission lines, notably of Fe II (Munari et al. 2006b). In the spectra the prominent lines showed double-peak\nprofiles with velocity separations of ∼ 700km s−1 and FWHM of order 1000km s−1. Optical and near-IR spectra\nobtained six weeks after maximum showed similar emission features but with a strong red continuum implying the\npresence of a dust shell in the ejecta. The reddening inferred from the ratio of the O I lines at that time was E(B–\nV)∼ 2 (Perry et al. 2005). This value is similar to that derived by Munari et al. (2006b) using the color evolution and\nstrength of the diffuse 6614 A˚ interstellar band.\nA.1.8. V477 Sct\nThe second nova discovered in the constellation Scutum in 2005, V477Sct, was detected independently by G. Po-\njmanski and K. Haseda on images obtained on the 11th and 13th of October, 2005, respectively, and the maximum\nvisual magnitude was recorded on 13.07 October at V = 10.44mag (Pojmanski et al. 2005b). Early JHK spec-\ntroscopy of V477 Sct showed broad H I emission lines with FWZI of 6000km s−1 and no evidence of dust emission\n(Pojmanski et al. 2005b). Optical spectra confirmed broad emission lines with a FWHM value of Hα = 2900km s−1\n(Fujii & Yamaoka 2005). The optical spectra of Munari et al. (2006a) showed that unlike V476 Sct which was rich in\nFe II lines, V477 Sct had He and N lines. Although sparse, the light curve revealed a rapid decline with an estimated\nt2 of only 3 days (Munari et al. 2006a). This rapid decline was consistent with the large expansion velocities. Optical\nand near-IR spectra taken on 15.094 November, 2005 showed a continuum increasing toward the red, possibly due\nto thermal dust emission (Mazuk et al. 2005). Numerous emission lines were observed which had developed double\npeak profiles with a FWHM of 2700kms−1. The features present included the coronal lines [S VII] and [S IX] but not\n[Ca VII] or [Si VI]. The ratio of the O I lines implied a substantial reddening of E(B–V)= 1.2 (Mazuk et al. 2005). A\nsimilar value of ≥ 1.3 was determined by Munari et al. (2006a) based on the early color evolution, extinction maps\nalong the line of sight, and the large equivalent width of the diffuse interstellar band at 6614 A˚.\nA.1.9. LMC2005\nLiller discovered LMC2005 on 26.16 November 2005, but it was also present on images that he had taken four\ndays earlier. Visual maximum occurred 27 November at V = 12.6mag (Liller 2005d). The light curve evolution was\nextremely slow and in early 2006 exhibited photometric evidence of dust formation (F. Walter, private communication).\nA.1.10. V723Cas\nV723Cas is a slow nova. It was discovered on 24 August, 1995 by Yamamoto (Hirosawa et al. 1995) at V = 9.2mag.\nPrediscovery observations showed that it had slowly brightened over the previous 20 days. The subsequent light curve\nwas unusual. V723 Cas reached a maximum magnitude of V ∼ 8.6mag one hundred days after discovery. At that point\nV723Cas exhibited a flare to 7.1mag on 17 December, 1995. During the flare the U − B colors became significantly\nbluer while the B–V color remained constant (Munari et al. 1996). The light curve also had additional secondary flares\nduring the following 400 days. V723Cas has been extensively observed from the ultraviolet (Gonzalez-Riestra et al.\n1996) to radio (Heywood et al. 2005) wavelengths. Recent optical and NIR spectra showed that V723Cas reached\na ”coronal” phase with emission from high ionization stages such as [Fe X] (6373 A˚) similar to spectra obtained of\nGQ Mus in the late 1980s (Krautter & Williams 1989). The spectral similarity suggests ongoing nuclear burning on\nthe WD (Krautter & Williams 1989). As a result, we requested Swift observations in order to search for the X-ray\nemission that would be related to ongoing nuclear burning.\nA.1.11. V1494Aql\nV1494Aql was discovered on 1.8 December, 1999 at V = 6.0mag by Pereira (1999). Within a few days of discovery\nthis nova reached a peak magnitude exceeding 4. It subsequently declined by two magnitudes in 6.6 ± 0.5 days thus\nclassifying V1494Aql as a fast nova (Kiss & Thomson 2000). Three ChandraACIS-I observations were carried spectrum\nwith no obvious soft component. However, a bright SSS spectrum was present in the August spectrum (Starrfield et al.\n2000). One month later, two grating observations were obtained on 28 September and 1 October, 2000, which showed\nremarkable variability (Drake et al. 2003). The spectrum resembles that of the SSS Cal 83 (Paerels et al. 2001), except\nthat it appeared somewhat hotter and the ”emission features” were at different wavelengths. A final observation\nwith Chandra LETGS one year later (ObsID 2681) showed no emission in the dispersed spectrum but a reasonable\ndetection in the zeroth order on the HRC-S detector. We extracted 56 counts on a background of 10, corresponding\nto a count rate of 2 cts/ksec and a 14-σ detection likelihood. Unfortunately, the HRC detector does not provide the\nenergy resolution to construct a spectrum.\nA.1.12. V4743 Sgr\nV4743Sgr was discovered by Haseda et al. (2002). A distance of 6.3 kpc is assumed based on infrared observations\n(Lyke et al. 2002). Chandra grating observations were carried out on 19 March, 2003 (Ness et al. 2003). The light\ncurve showed large amplitude oscillations (∼ 20% of the count rate) with a period of 1324 sec (almost 40% power)\nwith the 2nd and 3rd harmonic overtones present. V4743Sgr was observed by Chandra on five more dates and details\nof the spectral evolution will be presented by Ness et al. (in prep).\n18\nA.2. Extraction of source counts\nIn this section we give a detailed account of our methods to determine the count rates and detection probabilities.\nWe determined the positions of the sources on the chip from the individual sky coordinates and used circular extraction\nregions (radius 10pixels= 23.6′′, which encircles ∼ 80.5 percent of the total PSF). For the extraction of the background\nwe defined annular extraction regions around the source position with an inner radius of 10 pixels and an outer radius\nof 100pixels. We carefully checked to make sure that no sources were present in the background extraction regions. We\nscaled the number of extracted background counts by the ratio of extraction regions (99.0) to determine the number\nof expected background counts per detect cell. We assume Poissonian noise and thus apply a maximum likelihood\nestimation in order to determine the net source count rate. We define two model parameters S and B that represent\nthe number of expected counts of source and background for the source extraction region and calculate the number of\nexpected counts in the source- and background extraction regions Ns and Nb:\nNs = αS +B Nb = (1 − α)S + βB (A1)\nwith α = 0.80 the fraction of the PSF included in the detect cell and β = 99 the ratio of background- to source extraction\nareas. Under the assumption of Poissonian statistics we can calculate the probability of finding the measured numbers\nof counts ns and nb in the respective extraction regions when S and B are given:\nP =\nNnss\nns!\ne−Ns\nNnbb\nnb!\ne−Nb (A2)\nand thus the likelihood can be calculated\nL = −2 lnP = −2ns ln(αS +B)− (αS +B) (A3)\n+nb ln((1 − α)S + βB)− ((1− α)S + βB) + const .\nWe seek solutions for S and B where L reaches a minimum, which holds for Ns = ns and Nb = nb, and thus\nS =\nnb − βns\n1− α− αβ\n, B =\nns − α(ns + nb)\n1− α− αβ\n(A4)\nleading to a minimum value Lmin of\nLmin = −2[ns(lnns − 1) + nb(lnnb − 1)] . (A5)\nWith the definition of L the range between Lmin and Lmin + 1 is equivalent to the 68.3-percent uncertainty range of\nthe critical parameter S. We thus obtained the formal 1-σ errors by varying S until the value of L had increased by\n1.0 from the minimum while leaving B fixed at the value obtained from Eq. A4.\nThe value of Lmin can also be used to calculate the detection probability (in percent) based on the likelihood ratio\ntest, which is a statistical test of the goodness-of-fit between two models. A relatively more complex model is compared\nto a simpler model (null hypothesis) to see if it fits a particular dataset significantly better. We define the likelihood\nfor the null hypothesis L0 as L(S = 0), and\n∆L = L(S)− L0 (A6)\nquantifies the improvement in L after including S > 0. The parameter B is an ”uninteresting parameter” as defined by\nAvni (1976), and we thus convert ∆L to the probability for one degree of freedom using the IDL10 function chisqr pdf.\nIn cases where no counts were found in the detect cell (ns = 0), or when the number of counts in the detect cell is\nsmaller than that expected from the background (ns < nb), we calculated the 95-percent upper limits from the number\nof expected background counts and those actually measured in the detect cell. We again used the likelihood ratio test\nand solved Eq. A6 for L(S) according to Eq. A3 yielding the value of S that returns ∆L = 4. We also calculated\nupper limits in cases where the derived 1-σ uncertainties were larger than the measured count rate.\nIn addition to the formal detection likelihood we consider whether the photons inside the source extraction region\nshow some concentration towards the center, and whether their energy distribution is different from that of the\ninstrumental background. In order to assess the concentration towards the center, we give the percentage of counts\nthat we found in the central quarter of the detect cell in the last column of Table 2. From the PSF we expect\n60.5 percent of source photons to be within a circle of radius 5 pixels, and any number much below this fraction is\nlikely not a source. We also studied the spectral energy distribution of the background compared to that of the source\nas an auxiliary criterion. We computed the median of recorded energies of the source counts and compared with\nthe same value from the background. These values are marked by vertical lines (gray dotted for background and\nblack dashed for source in Figs. 3 to 11). In order to assess the probability that the background spectrum is softer\nthan the source spectrum, we generated 1000 spectra with ns counts drawn as random sub-samples out of the pool\nof extracted background counts nb and calculated the median value for each generated spectrum. While a situation\nin which 50 percent of all random cases returned lower median energies does not imply a non-detection, any strong\ndeviation from 50percent is supportive of an independent source spectrum.\n10 Interactive Data Language, ITT Corporation\n",
            "id": 856944,
            "identifiers": [
                {
                    "identifier": "2314901",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1086/518084",
                    "type": "DOI"
                },
                {
                    "identifier": "1972707693",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "190427975",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "astro-ph/0703286",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "oai:arxiv.org:astro-ph/0703286",
                    "type": "OAI_ID"
                }
            ],
            "title": "Swift X-ray Observations of Classical Novae",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:arxiv.org:astro-ph/0703286"
            ],
            "publishedDate": "2007-04-25T00:00:00",
            "publisher": "'University of Chicago Press'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/astro-ph/0703286"
            ],
            "updatedDate": "2021-04-22T07:59:26",
            "yearPublished": 2007,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "0004-637X"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/astro-ph/0703286"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/856944"
                }
            ]
        },
        {
            "acceptedDate": "2011-12-08T00:00:00",
            "arxivId": "1109.4373",
            "authors": [
                {
                    "name": "A. Sinclair"
                },
                {
                    "name": "A.G. Dimakis"
                },
                {
                    "name": "C. Intanagonwiwat"
                },
                {
                    "name": "D.R. Kowalski"
                },
                {
                    "name": "G. Kollios"
                },
                {
                    "name": "I.F. Akyildiz"
                },
                {
                    "name": "J.-Y. Chen"
                },
                {
                    "name": "J.-Y. Chen"
                },
                {
                    "name": "J.-Y. Chen"
                },
                {
                    "name": "L. Gasieniec"
                },
                {
                    "name": "L. Xiao"
                },
                {
                    "name": "M. Jelasity"
                },
                {
                    "name": "P. Erdos"
                },
                {
                    "name": "P. Jesus"
                },
                {
                    "name": "R. Olfati-Saber"
                },
                {
                    "name": "S. Boyd"
                }
            ],
            "contributors": [
                "Paulo Sérgio",
                "The Pennsylvania State University CiteSeerX Archives",
                "Antonio"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/191337004",
                "https://api.core.ac.uk/v3/outputs/55616496"
            ],
            "createdDate": "2012-04-13T14:19:12",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 145,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/145",
                    "logo": "https://api.core.ac.uk/data-providers/145/logo"
                },
                {
                    "id": 2274,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/2274",
                    "logo": "https://api.core.ac.uk/data-providers/2274/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2011-01-01T00:00:00",
            "abstract": "Flow-Updating (FU) is a fault-tolerant technique that has proved to be\nefficient in practice for the distributed computation of aggregate functions in\ncommunication networks where individual processors do not have access to global\ninformation. Previous distributed aggregation protocols, based on repeated\nsharing of input values (or mass) among processors, sometimes called\nMass-Distribution (MD) protocols, are not resilient to communication failures\n(or message loss) because such failures yield a loss of mass. In this paper, we\npresent a protocol which we call Mass-Distribution with Flow-Updating (MDFU).\nWe obtain MDFU by applying FU techniques to classic MD. We analyze the\nconvergence time of MDFU showing that stochastic message loss produces low\noverhead. This is the first convergence proof of an FU-based algorithm. We\nevaluate MDFU experimentally, comparing it with previous MD and FU protocols,\nand verifying the behavior predicted by the analysis. Finally, given that MDFU\nincurs a fixed deviation proportional to the message-loss rate, we adjust the\naccuracy of MDFU heuristically in a new protocol called MDFU with Linear\nPrediction (MDFU-LP). The evaluation shows that both MDFU and MDFU-LP behave\nvery well in practice, even under high rates of message loss and even changing\nthe input values dynamically.Comment: 18 pages, 5 figures, To appear in OPODIS 201",
            "documentType": "research",
            "doi": "10.1007/978-3-642-25873-2_35",
            "downloadUrl": "https://core.ac.uk/download/55616496.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "Fault-Tolerant Aggregation:\nFlow-Updating Meets Mass-Distribution?\nPaulo Se´rgio Almeida1, Carlos Baquero1, Mart´ın Farach-Colton2,\nPaulo Jesus1, and Miguel A. Mosteiro3\n1 Depto. de Informa´tica (CCTC-DI), Universidade do Minho, Braga, Portugal\n{psa,cbm,pcoj}@di.uminho.pt\n2 Dept. of Computer Science, Rutgers University, Piscataway, NJ, USA &\nTokutek, Inc.\nfarach@cs.rutgers.edu\n3 Dept. of Computer Science, Rutgers University, Piscataway, NJ, USA &\nLADyR, GSyC, Universidad Rey Juan Carlos, Madrid, Spain\nmosteiro@cs.rutgers.edu\nAbstract. Flow-Updating (FU) is a fault-tolerant technique that has\nproved to be efficient in practice for the distributed computation of aggre-\ngate functions in communication networks where individual processors\ndo not have access to global information. Previous distributed aggre-\ngation protocols, based on repeated sharing of input values (or mass)\namong processors, sometimes called Mass-Distribution (MD) protocols,\nare not resilient to communication failures (or message loss) because\nsuch failures yield a loss of mass.\nIn this paper, we present a protocol which we call Mass-Distribution\nwith Flow-Updating (MDFU). We obtain MDFU by applying FU\ntechniques to classic MD. We analyze the convergence time of MDFU\nshowing that stochastic message loss produces low overhead. This is the\nfirst convergence proof of an FU-based algorithm. We evaluate MDFU ex-\nperimentally, comparing it with previous MD and FU protocols, and ver-\nifying the behavior predicted by the analysis. Finally, given that MDFU\nincurs a fixed deviation proportional to the message-loss rate, we adjust\nthe accuracy of MDFU heuristically in a new protocol called MDFU\nwith Linear Prediction (MDFU-LP). The evaluation shows that both\nMDFU and MDFU-LP behave very well in practice, even under high\nrates of message loss and even changing the input values dynamically.\nKeywords. Aggregate computation, Distributed computing, Radio net-\nworks, Communication networks.\n? This work is supported in part by the Comunidad de Madrid grant S2009TIC-1692,\nSpanish MICINN grant TIN2008–06735-C02-01, Portuguese FCT grant PTDC/EIA-\nEIA/104022/2008, and National Science Foundation grant CCF-0937829.\nThe authors appear in alphabetical order.\nar\nX\niv\n:1\n10\n9.\n43\n73\nv1\n  [\ncs\n.D\nC]\n  2\n0 S\nep\n 20\n11\n1 Introduction\nThe distributed computation of algebraic aggregate functions is particularly chal-\nlenging in settings where the processing nodes do not have access to global in-\nformation such as the input size. A good example of such scenario is Sensor\nNetworks [1, 28] where unreliable sensor nodes are deployed at random and the\noverall number of nodes that actually start up and sense input values may be\nunknown. Under such conditions, well-known techniques for distributing infor-\nmation throughout the network such as Broadcast [21] or Gossiping [11] cannot\nbe directly applied, and data collection is only practicable if aggregation is per-\nformed. Even more challenging is that loss of messages between nodes or even\nnode crashes are likely in such harsh settings. It has been proved [2] that the\nproblem of aggregating values distributedly in networks where processing nodes\nmay join and leave arbitrarily is intractable. Hence, arbitrary adversarial mes-\nsage loss also yields the problem intractable, but a weaker adversary, for instance\na stochastic one as in Dynamic Networks [7], is of interest. In this paper, under a\nstochastic model of message loss, we study communication networks where each\nnode holds an input value and the average of those values 4 must be obtained\nby all nodes, none of whom have access to global information of the network,\nnot even the total number of nodes n.\nA classic distributed technique for aggregation, sometimes called Mass-\nDistribution (MD) [10], works in rounds. In each round, each node shares\na fraction of its current average estimation with other nodes, starting from the\ninput values [3, 5, 6, 19, 27, 29, 32, 33]. Details differ from paper to paper but a\ncommon problem is that, in the face of message loss, those protocols either\ndo not converge to a correct output or they require some instantaneous failure\ndetector mechanism that updates the topology information at each node in each\nround. Recently [17, 18], a heuristic termed Flow-Updating (FU) addressed\nthe problem assuming stochastic message loss [18], and even assuming that input\nvalues change and nodes may fail [17]. The idea underlying FU is to keep track\nof an aggregate function of all communication for each pair of communicating\nnodes, since the beginning of the protocol, so that a current value at a node\ncan be re-computed from scratch in each round. Empirical evaluation has shown\nthat FU behaves very well in practice [17, 18], but such protocols have eluded\nanalysis until now.\nIn this paper, we introduce the concept of FU to MD. First, we present a\nprotocol that we call Mass-Distribution with Flow-Updating (MDFU).\nThe main difference with MD is that, instead of computing incrementally, the\naverage is computed from scratch in each round using the initial input value and\nthe accumulated value shared with other nodes so far (which we refer to as either\nmass shared , or flow passed). The main difference with FU is that if messages\nare not lost the algorithm is exactly MD, which facilitates the theoretical analysis\n4 Other algebraic aggregate functions can be computed in the same bounds using an\naverage protocol [6, 19].\nof the convergence time under failures parameterized by the failure probability\n(or message-loss rate).\nOur results.We first leverage previous work on bounding the mixing time of\nMarkov chains [30] to show that, for any 0 < ξ < 1, the convergence time\nof MDFU under reliable communication is 2 ln(n/ξ)/Φ(G)2, where Φ(G) is the\nconductance of the underlying graph characterizing the execution of MDFU on\nthe network. Then, we show that, with probability at least 1−1/n, for a message-\nloss rate f < 1/ ln(2∆e)3, the multiplicative overhead on the convergence time\nproduced by message loss is less than 1/(1 −√f ln(2∆e)3), and it is constant\nfor f ≤ 1/(e(2∆e)e), where ∆ is the maximum number of neighbors of any\nnode. Also, we show that, with probability at least 1 − 1/n, for any 0 < ξ < 1,\nafter convergence the expected average estimation at any node is in the interval\n[(1 − ξ)(1 − f)v, (1 + ξ)v]. This is the first convergence proof for an FU-based\nalgorithm.\nIn MDFU, if some flow is not received, a node computes the current esti-\nmation using the last flow received. Thus, in presence of message loss, nodes do\nnot converge to the average and only some parametric bound can be guaran-\nteed as shown. Aiming to improve the accuracy of MDFU, we present a new\nheuristic protocol that we call MDFU with Linear Prediction (MDFU-LP).\nThe difference with MDFU is that if some flow is not received a node computes\nthe current estimation using an estimation of the flow that should have been\nreceived.\nWe evaluate MDFU and MDFU-LP experimentally and find that the per-\nformance of MDFU is comparable to FU and other competing algorithms under\nreliable communication. In the presence of message loss, the empirical evaluation\nshows that MDFU behaves as predicted in the analysis converging to the average\nwith a bias proportional to the message-loss rate. This bias is not present in the\noriginal FU, which converges to the correct value even under message loss. In\na third set of evaluations, we observe that MDFU-LP converges to the correct\nvalue even under high message loss rates, with the same speed as under reliable\ncommunication. We also test MDFU under changing input values to verify that\nit tolerates dynamic changes in practice, in contrast to classic MD algorithms,\nwhich need to restart the computation each time values are changed.\nRoadmap. In Section 2 we formally define the model and the problem, and we\ngive an overview of related work. Section 3 includes the details of MDFU and its\nanalysis, whereas its empirical evaluation is covered in Section 4. In Section 5\nwe present the details of MDFU-LP and its experimental evaluation. Section 6\nevaluates MDFU in a dynamic setting, where input values change over time.\n2 Preliminaries\nModel. We consider a static connected communication network formed by a\nset V of n processing nodes. We assume that each node has an identifier (ID).\nAny pair of nodes i, j ∈ V such that i may send messages to j without relying\non other nodes (one hop) are called neighbors. We assume that the IDs are\nassigned so that each node is able to distinguish all its neighbors. The set of\nordered pairs of neighbors (or, edges) is called E. The network is symmetric,\nmeaning that, for any i, j ∈ V , (i, j) ∈ E if and only if (j, i) ∈ E. The set of\nneighbors of a given node i is denoted as Ni and |Ni| is called the degree of i.\nFor each pair of nodes i, j ∈ V , the maximum degree between i, j is denoted as\nDij = max{|Ni|, |Nj |}. The maximum degree throughout the network is denoted\nas ∆ = maxi∈V |Ni|. Each node i knows Ni and Dij for each j ∈ Ni, but does\nnot know the size of the whole network n. The time is slotted in rounds and\neach round is divided in two phases. In each round, a node is able to send\n(resp. receive) one message to (resp. from) all its neighbors (communication\nphase) and to perform local computations (computation phase). However, for\neach (i, j) ∈ E and for each communication phase, a message from i to j is lost\nindependently with probability f . This is a crucial difference with previous work\nwhere, although edge-failures are considered, messages are not lost thanks to the\navailability of some failure detection mechanism. More details are given in the\nprevious work section. Nodes are assumed to be reliable, i.e. they do not fail.\nProblem. Each node i holds an input value vi, for 1 ≤ i ≤ n. The aim is for each\nnode to compute the average v =\n∑n\ni=1 vi/n without any global knowledge of the\nnetwork. We focus on the algorithmic cost of such computation, counting only\nthe number of rounds that the computation takes after simultaneous startup of\nall nodes, leaving aside medium access issues to other layers. This assumption\ncould be removed as in [10].\nPrevious Work. Previous work on aggregate computations has been partic-\nularly prolific for the area of Radio Networks, including both theoretical and\nexperimental work [8, 12–16, 19, 20, 22–24, 26, 35]. Many of those and other ag-\ngregation techniques exploit global information of the network [10,12,22,23], or\nare not resilient to message loss [3, 5, 19].\nFU is a recent fault-tolerant approach [17,18] inspired on the concept of flows\n(from graph theory). Like common MD techniques, it is based on the execution\nof an iterative averaging process at all nodes, and all estimates eventually con-\nverge to the system-wide average. MD protocols exchange “mass”, which lead\nthem to converge to a wrong result in the case of message loss. In contrast,\nFU does not exchange “mass”. Instead it performs idempotent flow exchanges\nwhich provide resilience against message loss. In particular, FU keeps the initial\ninput value at each node unchanged (in a sense, always conserving the global\nmass), exchanging and updating flows between neighbors for them to produce a\nnew estimate. The estimate is computed at each node from the input values and\nthe contribution of the flows. No theoretical bounds on the performance of the\nalgorithm were provided. Empirical evaluation shows that FU performs better\nthan classic MD algorithms, especially in low-degree networks, and it supports\nhigh levels of message loss [18]. Moreover, it self-adapts to dynamic changes (i.e.\nnodes leaving/arriving and input value change) without any restart mechanism\n(like other approaches), and tolerates node crashes [17].\nMD protocols for average computations in arbitrary networks based on gos-\nsiping (exchange values in pairs) were studied in [3, 19]. Results in [3] are pre-\nsented for all gossip-based algorithms by characterizing them by a matrix that\nmodels how the algorithm evolves while sharing values in pairs iteratively. As\nin our results, the time bounds shown are given as a function of the spectral\ndecomposition of the graph underlying the computation. The work is focused\non optimizing distributedly the spectral gap, in order to minimize convergence\ntime. The dynamics of the model are motivated by changes in topology induced\nby nodes leaving and joining the network. Those changes may be introduced in\nthe probability of establishing communication between any two nodes. However,\nthe delivery of messages has to be reliable to ensure mass conservation. An al-\ngorithm called Push-Sum that takes advantage of the broadcast nature of Radio\nNetworks (i.e., it is not restricted to gossip) is included in [19], yielding similar\nbounds. Chen, Pandurangan, and Hu [5] present an MD algorithm that first\nbuilds a forest over the network, where each root collects the information, and\nthen a gossiping algorithm among the roots is used. The authors show a reduc-\ntion on the energy consumption with respect to the uniform gossip algorithm. On\nthe other hand, the MD algorithm presented in [6] relies on a different randomly\nchosen local leader in each round to distribute values. The bounds given are also\nparameterized by the eigen-structure of the underlying graph. This result was\nextended more recently [4] to networks with a time-varying connection graph,\nbut the protocol requires to update the matrix underlying such graph in each\nround.\nMD protocols have been used also for Distributed Average Consensus [27,29,\n31–34] within Control Theory, but they do not apply to our model. For example,\nin [33,34] the model includes unreliable communication links, but the algorithm\nrequires instantaneous update of the topology information held at each node at\nthe beginning of each round. Others, either rely on similar features [27, 29, 31]\nor do not consider changes in topology at all [32].\nThe common problem in all the MD protocols is that they are not resilient\nto message loss, because it implies a loss of mass. Hence, if messages are lost,\nthey need to restart the computation from scratch. In MDFU, message loss has\nan impact on convergence time, which we show to be small, but the compu-\ntation recovers from those losses, yielding the correct value. In fact, it is this\ncharacteristic of MDFU and FU in general what makes the technique suitable\nfor dynamic settings in which the input values change with time.\n3 MDFU\nAs in previous work [3, 6, 10, 19], MDFU is based on repeatedly sharing among\nneighbors a fraction of the average estimated so far. Unlike in those papers, in\nMDFU the estimation is computed from scratch in each round, as in FU [17,\n18]. For that purpose, each node keeps track of the cumulative value passed to\neach neighbor (or, cumulative flow) since the protocol started. Together with\nthe original input value, those flows allow each node to recompute the average\nAlgorithm 1: MDFU. Pseudocode for node i. ei is the estimate of node\ni. Fin(j) is the cumulative inflow from node j. Fout(j) is the cumulative\noutflow to node j.\n// initialization\nei ← vi1\nforeach j ∈ Ni do2\nFin(j)← 03\nFout(j)← ei/ (2Dij)4\nforeach round do5\n// communication phase\nforeach j ∈ Ni do6\nSend j message 〈i, Fout(j)〉7\nforeach 〈j, F 〉 received do8\nFin(j)← F9\n// computation phase\nei ← vi +∑j∈Ni(Fin(j)− Fout(j))10\nforeach j ∈ Ni do11\nFout(j)← Fout(j) + ei/ (2Dij)12\nestimation in each round. Should some flow from node i to node j be lost, j\ntemporarily computes the estimation using the last flow received from i. Further\ndetails can be found in Algorithm 1.\nRecall that the aim is to compute the average v =\n∑n\ni=1 vi/n of all input\nvalues. Let ei(r) be the average estimate of node i in round r, and ε(r) =\nmaxi{|ei(r)− v|/v} be the maximum relative error of the average estimates in\nround r. We want to bound the number of rounds after which the maximum\nrelative error is below some parametric value ξ.\nIn each round, a node shares a fraction of its current estimate with each\nneighbor. Therefore, the execution of each round can be characterized by a tran-\nsition matrix , denoted as P = (pij), ∀i, j ∈ V , such that for any round r where\nmessages are not lost\npij =\n\n1/(2Dij) if i 6= j and (i, j) ∈ E,\n1−∑k∈Ni 1/(2Dik) if i = j,\n0 (i, j) /∈ E\nand e(r + 1) = e(r)P, where e(·) is the row vector (e1(·)e2(·) . . . en(·)).\n3.1 Convergence Time for f = 0\nConsider first the case when the communication is reliable, that is f = 0. Then,\nthe above characterization is round independent and, given that P is stochastic,\nit can be seen as the transition matrix of a time-homogeneous Markov chain\n(Xr)\n∞\nr=1 with finite state space V . Furthermore, (Xr)\n∞\nr=1 is irreducible, and ape-\nriodic, then it is ergodic and it has a unique stationary distribution. Given that\nP is doubly stochastic such stationary distribution is pii = 1/n for all i ∈ V .\nThus, bounding the convergence time of (Xr)\n∞\nr=1 we have a bound for the con-\nvergence time of MDFU without message loss. The following notation will be\nuseful. Let G be a weighted undirected graph with set of nodes V and where,\nfor each pair i, j ∈ V , the edge (i, j) has weight piipij . G is called the underlying\ngraph of the Markov chain (Xr)\n∞\nr=1. The following quantity characterizes the\nlikelihood that the chain does not stay in a subset of the state space with small\nstationary probability. Let the conductance of graph G be\nΦ(G) = min\n∅⊂S⊂V∑\ni∈S pii≤1/2\n∑\ni,j∈S pijpii∑\ni∈S pii\n.\nThe following theorem shows the convergence time of MDFU with reliable com-\nmunication parameterized in the conductance of G.\nTheorem 1. For any communication network of n nodes running MDFU, for\nany 0 < ξ < 1, and for rc = 2 ln(n/ξ)/Φ(G)\n2, if f = 0, it holds that ε(r) ≤ ξ\nfor any round r ≥ rc, where Φ(G) is the conductance of the underlying graph\ncharacterizing the execution of MDFU on the network.\nProof. We want to find a value of rc such that for all r ≥ rc it holds that\nmaxi{|ei(r) − v|/v} ≤ ξ. Then, we want maxi{|ei(r)/\n∑\nj∈V vj − 1/n|} ≤ ξ/n.\nGiven that ei(r) =\n∑\nj∈V vj(P\nr)ji, it is enough to have maxj,i∈V {|(Pr)ji −\n1/n|} ≤ ξ/n. On the other hand, given that pijpii = pjipij for all i, j ∈ V , the\nMarkov chain is time-reversible. Then, as proved in [30], it is maxi,j∈V |(Pr)ij −\npij |/pij ≤ λr1/minj∈V pij , where λ1 is the second largest eigenvalue of P (all\nthe eigenvalues of P are positive because pii ≥ 1/2 for all i ∈ V ). Given that\npii = 1/n for all i ∈ V , we have maxi,j∈V |(Pr)ij − 1/n| ≤ λr1. Thus, from the\ninequality above, it is enough to have λr1 ≤ ξ/n. As proved also in [30], given\nthat (Xr)\n∞\nr=1 is ergodic and time-reversible, it is λ1 ≤ 1 − Φ(G)2/2. Then, it is\nenough (1− Φ(G)2/2)r ≤ ξ/n. Given that Φ(G) ≤ 1, using that 1− x ≤ e−x for\nx < 1, the claim follows.\n3.2 Convergence Time for f > 0\nMixing time of a multiple random walk. Recall that we carry out an average\ncomputation of n input values where each node i shares a 1/(2Dij) fraction of\nits estimate in each round of the computation with each neighboring node j. We\nhave characterized each round of the computation with a transition matrix P so\nthat in each round r the vector of estimates e(r) is multiplied by P.\nThe Markov chain defined in Section 3.1 that models the average computation\nis also a characterization of a random walk, that is, a stochastic process on the\nset of nodes V where a particle moves around the network randomly. In our\ncase, for each round, instead of choosing the next node where the particle will\nbe located uniformly among neighbors, the matrix of transition probabilities is\nP. A state of this process (which of course is also Markovian) is a distribution of\nthe location of the particle over the nodes. The measure of this random walk that\nbecomes relevant in our application is the mixing time, that is, the number of\nrounds before such distribution will be close to uniform. The mixing time of this\nrandom walk is the same as the convergence time of the Markov chain (Xr)\n∞\nr=1,\nsetting appropriately for each case the desired maximum deviation with respect\nto the stationary distribution as follows.\nA useful representation of this process in our application is to assume a set S\nof particles, all of the same value ν, so that at the beginning each node i holds a\nsubset Si of particles such that |Si|ν = vi. In order to analyze the computation\nalong many rounds, we assume that ν is small enough so that particles are not\ndivided. We define the mixing time of this multiple random walk as the number\nof rounds before the distribution of all particles is within ξ/n of the uniform,\nfor 0 < ξ < 1. Without message loss, it can be seen that the mixing time of the\nabove defined multiple random walk is the same as the convergence time of the\nMarkov chain (Xr)\n∞\nr=1 defined in Section 3.1. We consider now the case where\nmessages may be lost.\nThe following lemma shows that, for f < 1/ ln(2∆e)3, the multiplica-\ntive overhead on the mixing time produced by message loss is less than\n1/(1 −√f ln(2∆e)3), and it is constant for f ≤ 1/(e(2∆e)e). The proof uses\nconcentration bounds on the delay that any particle may suffer due to message\nloss.\nLemma 1. Consider any communication network of n nodes running MDFU,\nany 0 < f ≤ 1/ ln(2∆e)3, any 0 < ξ < 1, let rc = 2 ln(n/ξ)/Φ(G)2, and let\nq =\n{\n1/e if f ≤ 1/(e(2∆e)e)\nf\n(√\n4 ln(2∆e)3/f − 3− 1\n)\n/2 otherwise.\nConsider a multiple random walk modeling MDFU as described. With probability\nat least 1 − 1/n, after r = rc/(1 − q) rounds it holds that maxx∈S,i∈V |px(i) −\n1/n| ≤ ξ/n, where px(i) is the probability that particle x is located at node i.\nProof. For clarity, we model the network with a directed graph {V,E}, with V\nand E as defined in the model. A message loss in the edge (i, j) ∈ E is modeled\nwith a buffer on the edge (i, j) where a particle is “delayed”. For a computation\nof r rounds, it is enough to consider at most n(2∆)r particles, because initially\nthere are n input values and each value is divided r times by at most 2∆. Consider\nthe random walk of a given particle x ∈ S. For each round, x is delayed with\nprobability f . We bound the mixing time by bounding the number of rounds\nthat any particle is delayed as follows.\nAssume first that 1/(e(2∆e)e) < f ≤ 1/ ln(2∆e)3. For r rounds, the ex-\npected number of rounds when a given particle is delayed is fr. Using Chernoff-\nHoeffding bounds [25], the probability that a given particle x is delayed more\nthan qr rounds, f ≤ q ≤ 1, is at most exp(−fr(q/f − 1)2/3). Then, the proba-\nbility that some particle is delayed more than qr rounds is\nPr(∃x : x delayed > qr) ≤ n\n(\n2∆\nexp((q − f)2/(3f))\n)r\n.\nAssuming that 2∆ exp(1− q) ≤ exp((q − f)2/(3f)), we get that\nPr(∃x : x delayed > qr) ≤ n\n(\n1\nexp(1− q)\n) 2 ln(n/ξ)\n(1−q)Φ(G)2\n= n exp\n(\n−2 ln(n/ξ)\nΦ(G)2\n)\n, given that ξ ≤ 1 and Φ(G) ≤ 1,\n≤ n exp(−2 lnn)\n= 1/n.\nThen, it remains to prove\n2∆ exp(1− q) ≤ exp((q − f)2/(3f))\nq2 + fq + f2 − f ln(2∆e)3 ≥ 0.\nWhich is true for q = f\n(√\n4 ln(2∆e)3/f − 3− 1\n)\n/2, which is feasible because,\nfor f ≤ 1/ ln(2∆e)3, such value of q implies f ≤ q ≤ 1 .\nConsider now the case 0 < f ≤ 1/(e(2∆e)e). Again, using Chernoff-Hoeffding\nbounds, the probability that a given particle x is delayed more than qr rounds,\nf ≤ q ≤ 1, is at most ((fe/q)q/ef)r Then, the probability that some particle is\ndelayed more than qr rounds is\nPr(∃x : x delayed > qr) ≤ n\n(\n2∆\nef\n(\nfe\nq\n)q)r\n.\nAssuming that 2∆(fe/q)q/ef ≤ 1/ exp(1− q) we get as before,\nPr(∃x : x delayed > qr) ≤ 1/n.\nThen, it remains to prove\n2∆\nef\n(\nfe\nq\n)q\n≤ 1\ne1−q\n2∆e1−f ≤ (q/f)q\n2∆e ≤ (q/f)q .\nWhich is true for f ≤ 1/(e(2∆e)e) and q = 1/e.\nThe expected number of particles at each node as a function of f .\nAnalyzing a multiple random walk of a set of particles, in Lemma 1 we obtained\na bound on the time that any particle takes to converge to a stationary uniform\ndistribution. However, for any probability of message loss f > 0 and for any\nround, there is a positive probability that some particles are located in the edge\nbuffers defined in the proof of such lemma. Hence, the fact that each particle\nis uniformly distributed over nodes does not imply that the expected average\nheld at the nodes has converged, because only particles located at nodes are\nuniformly distributed. We bound the expected error in this section. The proof of\nthe following lemma is based on computing the overall expected ratio of particles\nin nodes with respect to delayed particles.\nLemma 2. Consider a multiple random walk modeling MDFU under the con-\nditions of Lemma 1. Then, with probability at least 1 − 1/n, for any round\nr ≥ rc/(1 − q), the expected number of particles E(|S(r)i |) in each node i is\n(1− ξ)(1− f)|S|/n ≤ E(|S(r)i |) ≤ (1 + ξ)|S|/n.\nProof. We consider a multiple random walk of a set of particles S over a directed\ngraph V,E, with V and E as defined in the model. A message loss in the edge\n(i, j) ∈ E is modeled with a buffer on the edge (i, j) where a particle is “delayed”.\nThe following notation will be useful. For any round r, S\n(r)\nX is the set of particles\nheld at the set X (node set or edge-buffer set), and S\n(r)\ni is the set of particles\nheld at the node i. Let pi =\n∑\nj∈Ni 1/(2Dij) for any node i. By linearity of\nexpectation, at the end of round r, the expected number of particles in all buffer-\nedges and the expected number of particles in all nodes are\nE(|S(r)E |) =\n∑\ni∈V\nE(|S(r−1)i |)fpi + fE(|S(r−1)E |) (1)\nE(|S(r)V |) =\n∑\ni∈V\nE(|S(r−1)i |)(1− fpi) + (1− f)E(|S(r−1)E |). (2)\nUsing that pi ≤ 1/2 in 1 and 2, we have\nE(|S(r)E |) ≤ (f/2)E(|S(r−1)V |) + fE(|S(r−1)E |)\nE(|S(r)V |) ≥ (1− f/2)E(|S(r−1)V |) + (1− f)E(|S(r−1)E |).\nThen,\nE(|S(r)E |)\nE(|S(r)V |)\n≤ (f/2)E(|S\n(r−1)\nV |) + fE(|S(r−1)E |)\n(1− f/2)E(|S(r−1)V |) + (1− f)E(|S(r−1)E |)\n≤ f\n1− f , because\n1− f/2\n1− f ≥\n1\n2\n.\nThen, given that E(|S(r)V |)+E(|S(r)E |) = |S|, we have E(|S(r)V |) ≥ (1−f)|S|. As\nproved in Lemma 1, with probability at least 1−1/n, for any round r ≥ rc/(1−q),\nmaxx∈S,i∈V |px(i)− 1/n| ≤ ξ/n, where px(i) is the probability that particle x is\nlocated at node i and q as defined in such lemma. Then, for any node i ∈ V , it\nis (1− ξ)(1− f)|S|/n ≤ E(|S(r)i |) ≤ (1 + ξ)|S|/n and the claim follows.\nBased on the previous lemmata, the following theorem shows the convergence\ntime of MDFU.\nTheorem 2. Consider any communication network of n nodes running MDFU.\nFor any 0 < f ≤ 1/ ln(2∆e)3, let q = 1/e if f ≤ 1/(e(2∆e)e), or q =\nf\n(√\n4 ln(2∆e)3/f − 3− 1\n)\n/2 otherwise, and let rc = 2 ln(n/ξ)/Φ(G)\n2. Then,\nwith probability at least 1−1/n, for any 0 < ξ < 1 and any round r ≥ rc/(1−q),\nthe expected average estimation at any node i ∈ V is (1− ξ)(1−f)v ≤ E(e(r)i ) ≤\n(1 + ξ)v, where Φ(G) is the conductance of the underlying graph characterizing\nthe execution of MDFU on the network.\nProof. From Lemmas 1 and 2, we know that, under the conditions of this theo-\nrem, for any round r ≥ rc/(1− q) and any node i ∈ V , with probability at least\n1−1/n the expected number of particles (of the multiple random walk modeling\nMDFU) is (1− ξ)(1− f)|S|/n ≤ E(|S(r)i |) ≤ (1 + ξ)|S|/n. Then, multiplying by\nthe value of each particle the claim follows.\n4 Empirical Evaluation of MDFU\nWe evalutated MDFU in a synchronous network simulator, using an Erdo˝s–Re´nyi\n[9] network with 1000 nodes and 5000 links (giving an average degree of 10). The\ninput values were chosen as when performing node counting [16]; i.e., all values\nbeing 0 except a random node with value 1; this scenario is more demanding,\nleading to slower convergence, than uniformly random input values. The evalua-\ntion aimed at: 1) comparing its convergence speed under no loss with competing\nalgorithms; 2) evaluating its behavior under message loss; 3) checking its ability\nto perform continuous estimation over time-varying input values.\n4.1 Convergence Speed Against Related Algorithms Under no\nFaults\nTo evaluate wether MDFU is a practical algorithm in terms of convergence speed,\nwe compared it against three other algorithms: the original Flow-Updating [17,\n18](FU), Distributed Random Grouping [6] (DRG), and Push-Synopses [19].\nFigure 1 shows the coefficient of variation of the root mean square error as\na function of the number of rounds (averaging 30 runs), with CV(RMSE) =√∑\ni∈V (ei − v)2/n/v.\nIt can be seen that MDFU is competitive, providing approximate estimates\nslightly faster than FU and DRG and giving reasonably accurate results roughly\nin line with them. It loses to them for very high precision estimation and to\nPush-Synopses for all precisions (but both DRG and Push-Synopses are not\nfault-tolerant).\n4.2 Fault Tolerance: Resilience to Message Loss\nTo evaluate the resilience of MDFU to message loss, we performed simulations\nusing different rates of message loss (0, 1%, 5%, 10%), where each individual\nmessage may fail to reach the destination with these given probabilities. We\nmeasured the effect of message loss on both the CV(RMSE) and also on the\nmaximum relative error. As can be seen in Figure 2, as long as there is some\nmessage loss, they do not tend to zero anymore, but converge to a value that is\na function of the message loss rate.\n 0.0001\n 0.001\n 0.01\n 0.1\n 1\n 10\n 100\n 0  20  40  60  80  100  120  140\nCV\n(R\nM\nSE\n)\nRounds\nMDFU\nFU\nDRG\nPush-Synopses\nFig. 1. CV(RMSE) over rounds in a 1000 node 5000 link Erdos-Renyi network.\n 0.0001\n 0.001\n 0.01\n 0.1\n 1\n 10\n 100\n 0  20  40  60  80  100  120  140\nCV\n(R\nM\nSE\n)\nRounds\nNo Loss\n1% msg loss\n5% msg loss\n10% msg loss\n 0.001\n 0.01\n 0.1\n 1\n 10\n 100\n 1000\n 0  20  40  60  80  100  120  140\nM\nax\n E\nrro\nr\nRounds\nNo Loss\n1% msg loss\n5% msg loss\n10% msg loss\nFig. 2. Coefficient of variation of the RMSE and maximum relative error for MDFU\nin a 1000 node 5000 link Erdos-Renyi network.\nWe also measured the behavior of the average of the estimates over the whole\nnetwork, and observed that there is a deviation from the correct value (v, the\naverage of the input values) towards lower values. Figure 3 shows the relative\ndeviation from the correct value over time, for different message loss rates. It\ncan be seen that this bias is roughly proportional to the message loss rate (for\nthese small message loss rates).\nRelating these results with the theoretical analysis of MDFU, we can see\nthat this bias should not come as a surprise. From Theorem 2, the expected\nvalue of the estimation converges to a band between (1−f)v and v. The relative\ndeviation of the lower boundary is thus proportinal to the message loss rate.\nFigure 3 also shows this boundary for the different message loss rates.\nThis kind of bias was not present in the original FU, in which the average\nof the estimates tends to the correct value. In MDFU the message loss rate\nlimits the precision that can be achieved, but it does not impact convergence,\ncontrary to classic mass distribution algorithms where, given message loss, the\nmore rounds pass, the more mass is lost and the more the estimates deviate from\nthe correct value, failing to converge.\n 0\n 0.02\n 0.04\n 0.06\n 0.08\n 0.1\n 0.12\n 0.14\n 0  20  40  60  80  100  120  140\nBi\nas\n o\nn \nth\ne \nAv\ner\nag\ne\nRounds\nf=0.01\nf=0.05\nf=0.10\nMDFU (1% loss)\nMDFU (5% loss)\nMDFU (10% loss)\nFig. 3. Bias on the average estimation over rounds in a 1000 node 5000 link Erdos-\nRenyi network.\n5 MDFU with Linear Prediction\nThe explanation for the behavior of MDFU under message loss lies in that only\nthe estimate converges, but flows keep steadily increasing over time. This can be\nseen in the formula: Fout(j)← Fout(j) + ei/ (2Dij) where the flow sent to some\nneighbor increases at each round by a value depending on the estimate and their\nmutual degrees. What happens is that during convergence, the extra flow that\neach of two nodes send over a link tend to the same value, and the extra outgoing\nflow cancels out the extra incoming flow. We can say that it is the velocity (rate\nof increase) of flows over a link that converge (to some different value for each\nlink).\nThis means that, even if the estimate had already converged to the correct\nvalue, given a message loss, the extra flow that should have been received is\nnot added to the estimate, implying a discrete deviation from the correct value.\nThis discrete deviation does not converge to zero; thus, we have a bias towards\nlower values and the relative estimation error is prevented from converging to\nzero given some message loss rate.\nHere we improve MDFU by exploring velocity convergence. We keep, for\neach link, the velocity (rate of increase) of the flow received. If a message is lost,\nwe predict what would have been the flow received, given the stored flow, the\nvelocity and the rounds passed since the last message received over that link,\ni.e., we perform a linear prediction of incoming flow. When a message is received\nwe update the flow and recalculate the velocity. This algorithm is presented in\nAlgorithm 2.\nUnder no message loss MDFU-LP is the same as MDFU and the theoreti-\ncal results on convergence speed also apply to MDFU-LP. Under message loss\nthe velocities converge over time and the prediction will be increasingly more\naccurate. Therefore, message loss should not cause discrete deviations in the\nestimate, allowing the estimation error to converge to zero.\nWe have evaluated MDFU-LP for the same network as before, but now with\na wide range of message loss rates. We have observed that the behavior under\nAlgorithm 2: MDFU-LP. Pseudocode for node i. ei is the estimate of node\ni. Fin(j) is the cumulative inflow from node j. Fout(j) is the cumulative\noutflow to node j. V (j) is the velocity of incoming flow from node j. R(j)\nis the number of rounds since the last message received from node j.\n// initialization\nei ← vi1\nforeach j ∈ Ni do2\nFin(j)← 03\nFout(j)← ei/ (2Dij)4\nV (j)← 05\nR(j)← 16\nforeach round do7\n// communication phase\nforeach j ∈ Ni do8\nSend j message 〈i, Fout(j)〉9\n// computation phase\nforeach 〈j, F 〉 received do10\nV (j)← (F − Fin(j))/R(j)11\nR(j)← 012\nFin(j)← F13\nei ← vi +∑j∈Ni(Fin(j) + V (j)×R(j)− Fout(j))14\nforeach j ∈ Ni do15\nFout(j)← Fout(j) + ei/ (2Dij)16\nR(j)← R(j) + 117\nmessage loss rates below 50% is almost indistinguishable from the behavior under\nno message loss. Figure 4 shows the CVRMSE and maximum relative error for\n0%, 60%, 70%, and 80% message loss rates. It can be seen that even for 60%\nloss rate, after 60 rounds we have basically the same estimation errors as under\nno message loss.\n6 Continuous Estimation Over Time-Varying Input\nValues\nUp to thus point we have considered that the input values vi are fixed through-\nout the computation. In most practical situations this will not be the case and\ninput values will change along time. The common approach in MD algorithms\nis to periodically reset the algorithm and start a new run that freezes the new\ninput values and aggregates the new average. Naturally, resets are inefficient\nand mechanisms that can adapt the ongoing computation have the potential to\nadjust the estimates in a much shorter number of rounds.\nWithout any further modifications, MDFU (and MDFU-LP) share with FU\nthe capability of adapting to input value changes, since vi is considered in the\ncomputation of the local estimate ei, and this regulates how much the outgoing\n 0.0001\n 0.001\n 0.01\n 0.1\n 1\n 10\n 100\n 0  20  40  60  80  100  120  140\nCV\n(R\nM\nSE\n)\nRounds\nNo Loss\n60% msg loss\n70% msg loss\n80% msg loss\n 0.001\n 0.01\n 0.1\n 1\n 10\n 100\n 1000\n 0  20  40  60  80  100  120  140\nM\nax\n E\nrro\nr\nRounds\nNo Loss\n60% msg loss\n70% msg loss\n80% msg loss\nFig. 4. Coefficient of variation of the RMSE and maximum relative error for MDFU-LP\nin a 1000 node 5000 link Erdos-Renyi network.\n 0\n 50\n 100\n 150\n 200\n 250\n 0  20  40  60  80  100  120  140  160  180  200\nE\nst\nim\nat\ned\n V\nal\nue\ns\nRounds\nReal Value\nFig. 5. Estimated value over rounds in a 1000 node 5000 link Erdos-Renyi network,\nwith changes of the initial input value at 50% of the nodes.\nflows are to be incremented. If vi decreases, ei decreases in the same proportion\nand node i will share less through its flows to the neighbours. The converse\noccurring when vi increases. The overall effect is convergence to the new average,\neven if multiple nodes are having changes in their input values.\nIn Figure 5 we show an example of how MDFU handles input value changes.\nIn this setting, starting at round 50 and during 50 rounds, we increase by 5% in\neach round the input value in 500 nodes (a random half of the 1000 nodes). In\nthe following 50 rounds, the same 500 nodes will have its value decreased by 5%\nper round. Initial input values are chosen uniformly at random (from 25 to 35)\nand the run is made with message loss at 10%. In Figure 5 one can observe that\nindividual estimates5 closely follow the global average, with only a slight lag of\nsome rounds.\nNotice that the lag could never be zero, since we are updating the new global\naverage (black line) instantaneously and even the fastest theoretical algorithm\nwould need information that takes diameter rounds to acquire.\n5 To avoid clutering the graph only shows individual estimate evolution for a random\nsample of 100 of the 1000 nodes.\nReferences\n1. I. F. Akyildiz, W. Su, Y. Sankarasubramaniam, and E. Cyirci. Wireless sensor\nnetworks: A survey. Computer Networks, 38(4):393–422, 2002.\n2. M. Bawa, H. Garcia-Molina, A. Gionis, and R. Motwani. Estimating aggregates\non a peer-to-peer network. Technical report, Stanford University, Database group,\n2003.\n3. Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized\ngossip algorithms. IEEE/ACM Transactions on Networking, 14(SI):2508–2530,\n2006.\n4. Jen-Yeu Chen and Jianghai Hu. Analysis of distributed random grouping for ag-\ngregate computation on wireless sensor networks with randomly changing graphs.\nIEEE Trans. Parallel Distr. Syst., 19(8):1136–1149, 2008.\n5. Jen-Yeu Chen, Gopal Pandurangan, and Jianghai Hu. Brief announcement:\nlocality-based aggregate computation in wireless sensor networks. In PODC ’09:\nProceedings of the 28th ACM symposium on Principles of distributed computing,\npages 298–299, New York, NY, USA, 2009. ACM.\n6. Jen-Yeu Chen, Gopal Pandurangan, and Dongyan Xu. Robust computation of ag-\ngregates in wireless sensor networks: distributed randomized algorithms and anal-\nysis. IEEE Trans. Parallel Distr. Syst., 17(9):987–1000, 2006.\n7. A.E.F. Clementi, F. Pasquale, A. Monti, and R. Silvestri. Communication in dy-\nnamic radio networks. In Proc. 26th Ann. ACM Symp. on Principles of Distributed\nComputing, pages 205–214, 2007.\n8. A. G. Dimakis, A.D. Sarwate, and M.J. Wainwright. Geographic gossip : Ef-\nficient averaging for sensor networks. IEEE Transactions on Signal Processing,\n56(3):1205–1216, 2008.\n9. P. Erdos and A. Renyi. On random graphs–i. Publicationes Matematicae, 6:290–\n297, 1959.\n10. A. Ferna´ndez Anta, M. A. Mosteiro, and C. Thraves. An early-stopping protocol\nfor computing aggregate functions in sensor networks. In Proc. of the IEEE 15th\nPacific Rim International Symposium on Dependable Computing, pages 357–364,\n2009.\n11. Leszek Gasieniec. Randomized gossiping in radio networks. In Ming-Yang Kao,\neditor, Encyclopedia of Algorithms. Springer, 2008.\n12. Indranil Gupta, Robbert van Renesse, and Kenneth P. Birman. Scalable fault-\ntolerant aggregation in large process groups. In DSN, pages 433–442. IEEE Com-\nputer Society, 2001.\n13. John S. Heidemann, Fabio Silva, Chalermek Intanagonwiwat, Ramesh Govindan,\nDeborah Estrin, and Deepak Ganesan. Building efficient wireless sensor networks\nwith low-level naming. In SOSP, pages 146–159, 2001.\n14. C. Intanagonwiwat, R. Govindan, D. Estrin, J. Heidemann, and F. Silva. Directed\ndiffusion for wireless sensor networking. IEEE/ACM Transactions on Networking,\n11(1):2–16, 2003.\n15. Chalermek Intanagonwiwat, Deborah Estrin, Ramesh Govindan, and John S. Hei-\ndemann. Impact of network density on data aggregation in wireless sensor net-\nworks. In ICDCS, pages 457–458, 2002.\n16. Ma´rk Jelasity, Alberto Montresor, and Ozalp Babaoglu. Gossip-based aggregation\nin large dynamic networks. ACM Transactions on Computer Systems, 23(3):219–\n252, 2005.\n17. P. Jesus, C. Baquero, and P.S. Almeida. Fault-tolerant aggregation for dynamic\nnetworks. In Proc. of the 29th IEEE Symposium on Reliable Distributed Systems,\npages 37–43, 2010.\n18. Paulo Jesus, Carlos Baquero, and Paulo Almeida. Fault-tolerant aggregation by\nflow updating. In Proc. of the 9th IFIP WG 6.1 International Conference Dis-\ntributed Applications and Interoperable Systems, volume 5523 of Lecture Notes in\nComputer Science, pages 73–86. Springer, 2009.\n19. D. Kempe, A. Dobra, and J. Gehrke. Gossip-based computation of aggregate\ninformation. In Proc. of the 44th IEEE Ann. Symp. on Foundations of Computer\nScience, pages 482–491, 2003.\n20. G. Kollios, J. W. Byers, J. Considine, M. Hadjieleftheriou, and F. Li. Robust\naggregation in sensor networks. IEEE Data Engineering Bulletin, 28(1):26–32,\n2005.\n21. D. R. Kowalski and A. Pelc. Time complexity of radio broadcasting: adaptive-\nness vs. obliviousness and randomization vs. determinism. Theoretical Computer\nScience, 333:355–371, 2005.\n22. Bhaskar Krishnamachari, Deborah Estrin, and Stephen B. Wicker. The impact of\ndata aggregation in wireless sensor networks. In ICDCS Workshops, pages 575–578.\nIEEE Computer Society, 2002.\n23. Samuel Madden, Michael J. Franklin, Joseph M. Hellerstein, and Wei Hong. Tag:\na tiny aggregation service for ad-hoc sensor networks. In Proc. of the 5th Symp.\non Operating Systems Design and Implementation, pages 131–146, 2002.\n24. Samuel Madden, Robert Szewczyk, Michael J. Franklin, and David Culler. Sup-\nporting aggregate queries over ad-hoc wireless sensor networks. In Proceedings\nof the Fourth IEEE Workshop on Mobile Computing Systems and Applications,\npage 49, 2002.\n25. M. Mitzenmacher and E. Upfal. Probability and Computing: Randomized Algo-\nrithms and Probabilistic Analysis. Cambridge University Press, 2005.\n26. Suman Nath, Phillip B. Gibbons, Srinivasan Seshan, and Zachary R. Anderson.\nSynopsis diffusion for robust aggregation in sensor networks. In Proceedings of\nthe 2nd international conference on Embedded networked sensor systems, pages\n250–262, 2004.\n27. Reza Olfati-Saber and Richard M. Murray. Consensus problems in networks of\nagents with switching topology and time-delays. Transactions on Automatic Con-\ntrol, 49(9):1520–1533, 2004.\n28. P. Rentala, R. Musumuri, U. Saxena, and S. Gandham. Survey on sensor networks.\nhttp://citeseer.nj.nec.com/479874.html.\n29. Dzulkifli S. Scherber and Haralabos C. Papadopoulos. Locally constructed algo-\nrithms for distributed computations in ad-hoc networks. In Proceedings of the 3rd\nInternational Symposium on Information Processing in Sensor Networks, pages\n11–19, 2004.\n30. Alistair Sinclair and Mark Jerrum. Approximate counting, uniform generation and\nrapidly mixing markov chains. Information and Computation, 82(1):93–133, 1989.\n31. D Spanos, R Olfati-Saber, and R Murray. Dynamic consensus on mobile networks.\nIn 16th IFAC World Congress, 2005.\n32. Lin Xiao and Stephen Boyd. Fast linear iterations for distributed average. Systems\nand Control Letters, 53:65–78, 2004.\n33. Lin Xiao, Stephen Boyd, and Sanjay Lall. A scheme for robust distributed sen-\nsor fusion based on average consensus. In Proceedings of the 4th International\nSymposium on Information Processing in Sensor Networks, pages 63–70, 2005.\n34. Lin Xiao, Stephen Boyd, and Sanjay Lall. A Space-Time Diffusion Scheme for\nPeer-to-Peer Least-Squares Estimation. In Proceedings of the 5th International\nConference on Information Processing in Sensor Networks, pages 168–176, 2006.\n35. Jerry Zhao, Ramesh Govindan, and Deborah Estrin. Computing aggregates for\nmonitoring wireless sensor networks. In Proc. of the 1st IEEE Intl. Workshop on\nSensor Network Protocols and Applications, pages 139–148, 2003.\n",
            "id": 779261,
            "identifiers": [
                {
                    "identifier": "21854839",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "1109.4373",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "10.1007/978-3-642-25873-2_35",
                    "type": "DOI"
                },
                {
                    "identifier": "191337004",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:citeseerx.psu:10.1.1.227.6791",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "55616496",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:arxiv.org:1109.4373",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "2222064",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:repositorium.sdum.uminho.pt:1822/15608",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "1878190159",
                    "type": "MAG_ID"
                }
            ],
            "title": "Fault-Tolerant Aggregation: Flow-Updating Meets Mass-Distribution",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "1878190159",
            "oaiIds": [
                "oai:repositorium.sdum.uminho.pt:1822/15608",
                "oai:citeseerx.psu:10.1.1.227.6791",
                "oai:arxiv.org:1109.4373"
            ],
            "publishedDate": "2011-01-01T00:00:00",
            "publisher": "'Springer Science and Business Media LLC'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1109.4373",
                "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.227.6791"
            ],
            "updatedDate": "2021-12-30T07:05:50",
            "yearPublished": 2011,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "0302-9743"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/55616496.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/55616496"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/55616496/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/55616496/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/779261"
                }
            ]
        },
        {
            "acceptedDate": "2009-10-27T00:00:00",
            "arxivId": "0712.1169",
            "authors": [
                {
                    "name": "Cui, Shengshan"
                },
                {
                    "name": "Haimovich, Alexander M."
                },
                {
                    "name": "Poor, H. Vincent"
                },
                {
                    "name": "Somekh, Oren"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/189673013"
            ],
            "createdDate": "2012-04-13T14:16:09",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2009-11-01T00:00:00",
            "abstract": "Relay networks having $n$ source-to-destination pairs and $m$ half-duplex\nrelays, all operating in the same frequency band in the presence of block\nfading, are analyzed. This setup has attracted significant attention and\nseveral relaying protocols have been reported in the literature. However, most\nof the proposed solutions require either centrally coordinated scheduling or\ndetailed channel state information (CSI) at the transmitter side. Here, an\nopportunistic relaying scheme is proposed, which alleviates these limitations.\nThe scheme entails a two-hop communication protocol, in which sources\ncommunicate with destinations only through half-duplex relays. The key idea is\nto schedule at each hop only a subset of nodes that can benefit from\n\\emph{multiuser diversity}. To select the source and destination nodes for each\nhop, it requires only CSI at receivers (relays for the first hop, and\ndestination nodes for the second hop) and an integer-value CSI feedback to the\ntransmitters. For the case when $n$ is large and $m$ is fixed, it is shown that\nthe proposed scheme achieves a system throughput of $m/2$ bits/s/Hz. In\ncontrast, the information-theoretic upper bound of $(m/2)\\log \\log n$ bits/s/Hz\nis achievable only with more demanding CSI assumptions and cooperation between\nthe relays. Furthermore, it is shown that, under the condition that the product\nof block duration and system bandwidth scales faster than $\\log n$, the\nachievable throughput of the proposed scheme scales as $\\Theta ({\\log n})$.\nNotably, this is proven to be the optimal throughput scaling even if\ncentralized scheduling is allowed, thus proving the optimality of the proposed\nscheme in the scaling law sense.Comment: 17 pages, 8 figures, To appear in IEEE Transactions on Information\n  Theor",
            "documentType": "research",
            "doi": "10.1109/tit.2009.2030435",
            "downloadUrl": "http://arxiv.org/abs/0712.1169",
            "fieldOfStudy": "computer science",
            "fullText": "ar\nX\niv\n:0\n71\n2.\n11\n69\nv3\n  [\ncs\n.IT\n]  \n24\n Ju\nl 2\n00\n9\n1\nOpportunistic Relaying in Wireless Networks\nShengshan Cui, Member, IEEE, Alexander M. Haimovich, Senior Member, IEEE,\nOren Somekh, Member, IEEE, and H. Vincent Poor, Fellow, IEEE\nAbstract—Relay networks having n source-to-destination pairs\nand m half-duplex relays, all operating in the same frequency\nband and in the presence of block fading, are analyzed. This\nsetup has attracted significant attention, and several relaying\nprotocols have been reported in the literature. However, most\nof the proposed solutions require either centrally coordinated\nscheduling or detailed channel state information (CSI) at the\ntransmitter side. Here, an opportunistic relaying scheme is\nproposed that alleviates these limitations, without sacrificing the\nsystem throughput scaling in the regime of large n. The scheme\nentails a two-hop communication protocol, in which sources\ncommunicate with destinations only through half-duplex relays.\nAll nodes operate in a completely distributed fashion, with no\ncooperation. The key idea is to schedule at each hop only a\nsubset of nodes that can benefit from multiuser diversity. To\nselect the source and destination nodes for each hop, CSI is\nrequired at receivers (relays for the first hop, and destination\nnodes for the second hop), and an index-valued CSI feedback at\nthe transmitters. For the case when n is large and m is fixed, it\nis shown that the proposed scheme achieves a system throughput\nof m/2 bits/s/Hz. In contrast, the information-theoretic upper\nbound of (m/2) log logn bits/s/Hz is achievable only with\nmore demanding CSI assumptions and cooperation between the\nrelays. Furthermore, it is shown that, under the condition that\nthe product of block duration and system bandwidth scales\nfaster than logn log logn, the achievable throughput of the\nproposed scheme scales as Θ(logn). Notably, this is proven to\nbe the optimal throughput scaling even if centralized scheduling\nis allowed, thus proving the optimality of the proposed scheme\nin the scaling law sense. Simulation results indicate a rather\nfast convergence to the asymptotic limits with the system’s size,\ndemonstrating the practical importance of the scaling results.\nIndex Terms—Ad hoc networks, channel state information\n(CSI), multiuser diversity, opportunistic communication, scaling\nlaw, throughput.\nManuscript submitted December 6, 2007; revised January 6, 2009 and May\n31, 2009.\nThe work of S. Cui and A. M. Haimovich was supported in part by\nthe National Science Foundation under Grant CNS-0626611. The work of\nO. Somekh was supported in part by a Marie Curie Outgoing International\nFellowship within the 6th European Community Framework Programme. The\nwork of H. V. Poor was supported in part by National Science Foundation\nunder Grants ANI-0338807 and CNS-0625637. The material in this paper was\npresented in part at the 45th Annual Allerton Conference on Communications,\nControl and Computing, Monticello, IL, USA, September 2007, and the IEEE\nInternational Symposium on Information Theory, Toronto, ON, Canada, July\n2008.\nS. Cui was with the Department of Electrical and Computer Engineering,\nNew Jersey Institute of Technology, Newark, NJ 07102 USA. He is now with\nQualcomm Inc., San Diego, CA 92121 USA (e-mail: scui@qualcomm.com).\nA. M. Haimovich is with the Department of Electrical and Computer\nEngineering, New Jersey Institute of Technology, Newark, NJ 07102 USA\n(e-mail: alexander.m.haimovich@njit.edu).\nO. Somekh was with the Department of Electrical Engineering, Princeton\nUniversity, Princeton, NJ 08544 USA. He is now with with Department\nof Electrical Engineering, Technion–Israel Institute of Technology, Technion\nCity, Haifa 32000, Israel (e-mail: orens@princeton.edu).\nH. V. Poor is with the Department of Electrical Engineering, Princeton\nUniversity, Princeton, NJ 08544 USA (e-mail: poor@princeton.edu).\nI. INTRODUCTION\nTHE DEMAND for ever larger and more efficient wirelesscommunication networks necessitates new network archi-\ntectures, such as ad hoc networks and relay networks. As such,\nthere has been significant activity in the past decade toward\nunderstanding the fundamental system throughput limits of\nsuch architectures and developing communication schemes\nthat seek to approach these limits.\nAmong other notable recent results on the throughput scal-\ning of wireless networks, Gowaikar et al. [1] proposed a new\nwireless ad hoc network model, whereby the strengths of\nthe connections between nodes are drawn independently from\na common distribution, and analyzed the system throughput\nunder various fading distributions. Such a model is appropriate\nfor environments with rich scattering but small physical size,\nso that the connections are governed by random fading instead\nof deterministic path loss attenuations. When the random\nchannel strengths follow a Rayleigh fading model, the system\nthroughput scales as logn. This result is achievable through\na multihop scheme that requires central coordination of the\nrouting of nodes. Moreover, full knowledge of the channel\nstate information (CSI) of the entire network is needed to\nenable the central coordination.\nAlong with the work on multihop schemes, such as [1] and\n[2], there is another line of work characterizing the system\nthroughput for wireless networks operating with two-hop\nrelaying. The listen-and-transmit protocol, studied by Dana\nand Hassibi [3] from the power-efficiency perspective, turns\nout to have interesting properties from the system throughput\nstandpoint as well. This is in fact a two-hop amplify-and-\nforward scheme, where relays are allowed to adjust the phase\nand amplitude of the received signals. A throughput of Θ(n)\nbits/s/Hz is achieved by allowing n source-to-destination (S–\nD) pairs to communicate, while m = Θ(n2) nodes in the\nnetwork act as relays. It is assumed that each relay node\nhas full knowledge of its local channels (backward channels\nfrom all source nodes, and forward channels to all destination\nnodes), so that the relays can perform distributed beamform-\ning. Morgenshtern and Bo¨lcskei worked in [4] with a similar\ndistributed beamforming setup, and their results reveal trade-\noffs between the level of available channel state information\nand the system throughput. In particular, utilizing a scheme\nwith relays partitioned into groups, and where relays in each\ngroup have CSI knowledge of only one backward and one\nforward channel, the number of relays required to support\na Θ(n) throughput is m = Θ(n3). In other words, with\nlower level CSI, the number of required relays increases from\nΘ(n2) to Θ(n3). An equivalent point of view is to state the\nthroughput in terms of the total number of transmitting nodes\n2in the system, p = n + m. Then the system throughput is\nΘ\n(\np1/3\n)\n, when the relays in each group know the channel\nfor only one source-destination pair. When relays know the\nchannels for all source and destination nodes, the throughput\nscales as Θ\n(\np1/2\n)\n.\nAlthough these works have made great strides toward under-\nstanding wireless ad hoc network capacity, implementations of\nthe schemes require either central coordination among nodes\n[1], [2] or some level of CSI (channel amplitude and/or phase)\nat the transmitter side [3], [4]. The centralized coordination\nbetween wireless relays does not come for free, since the\noverhead to set up the cooperation may drastically reduce the\nuseful throughput [5].1 Likewise, in a large system, obtaining\nthis level of CSI, especially at the transmitter side, may not\nbe feasible. This paper addresses the need to alleviate these\nlimitations by proposing an opportunistic relaying scheme that\nworks in a completely decentralized fashion and imposes less\nstringent CSI requirements.\nA. Main Contributions and Related Work\nThe main contributions of this work can be summarized as\nfollows.\n• A two-hop opportunistic relaying scheme for operating\nover fading channels is proposed and analyzed. The\nscheme’s salient features are:\n— It operates in a decentralized fashion. No cooperation\namong nodes is assumed or required.\n— Only modest CSI requirements are imposed. At each\nhop, each receiver is assumed to have knowledge\nof its local incoming channel realizations, while\ntransmitters have access to only index-valued CSI\nvia low-rate feedback from the receivers.\n• The throughput of the proposed scheme is characterized\nby:\n— It is shown that, in the regime of a large number\nof nodes n and fixed number of relays m, the\nproposed scheme achieves a system throughput of\nm/2 bits/s/Hz. This can be contrasted with the\ninformation-theoretic upper bound (m/2) log logn\non the scaling of the throughput, achievable only\nwith full cooperation among the relays and full CSI\n(backward and forward) at the relays. These results\nreveal an interesting feature of multiuser diversity:\nwhereas full cooperation between relays can readily\nform parallel channels, and multiuser diversity can\nboost the throughput of each channel by a factor of\nlog logn, when cooperation is not possible, multiuser\ndiversity succeeds in restoring the parallel chan-\nnels, but must forsake the multiuser diversity factor\nlog logn.\n— We show that m can grow (as a function of n) as\nfast as Θ(logn), while still guaranteeing the linear\nthroughput scaling in m. The linearity breaks down\n1However, in throughput scaling law studies, see, e.g., [1], [2] and [5],\namong many others, the overhead needed to set up cooperation is usually not\nexplicitly accounted for.\nif m grows faster than Θ(logn). Furthermore, when\nthe product of the block duration and the system\nbandwidth scales faster than logn log logn, the over-\nhead due to feedback is negligible, and therefore,\nthe achievable throughput scaling of the proposed\nopportunistic relaying scheme is given by Θ(logn).\n• It is proven that, under the assumption of independent en-\ncoding (i.e., no cooperative encoding) at the transmitters\nand independent decoding (i.e., no cooperative decoding)\nat the receivers, the system throughput is upper-bounded\nby the order of logn, even if centralized scheduling is\nallowed. This result is of interest in its own right, since\nit quantifies the system throughput of wireless ad hoc net-\nworks under the scenario where neither transmitters nor\nthe receivers can cooperate in avoiding and/or canceling\ninterference. Thus, the network is interference-limited,\nunlike other works in which global CSI is assumed (and\nthus either cooperative encoding or decoding is possible),\nleading to a linear throughput scaling. The throughput\nscaling results under our pessimistic, yet more realistic,\nscenario, improve the understanding of throughput scal-\ning of wireless ad hoc networks.\n• The proposed scheme is order-optimal in achieving the\nΘ(logn) throughput scaling. This suggests that, as far as\nthroughput scaling is concerned, operating the network\nin a decentralized fashion, with local CSI at the receivers\nand low-rate feedback, incurs no loss.\n• Simulation results show that the asymptotic conclusions\ndeveloped in this work settle rapidly. Hence, the above\nscaling laws provide rule-of-thumb guidance for the de-\nsign of practical wireless systems.\nThe key idea behind the proposed scheme is to schedule\nat each hop only the subset of nodes that can benefit from\nmultiuser diversity. The concept of multiuser diversity was\noriginally studied in the context of cellular systems [6]–[8]. It\nis known that the capacity of single-cell system is maximized\nby allowing only the user with the best channel to transmit\nat any given time. The concept is by now well understood\nin the context of infrastructure wireless networks, and has\nbeen adopted in 3G cellular systems and other emerging\nwireless standards [9]. However, to the best of our knowledge,\nit has received less attention for wireless ad hoc networks,\nwith some exceptions such as [10], in which the potential\nof opportunistic relaying is reported in a setup with one\nS–D pair and multiple relay nodes, and the focus is on\ndiversity-multiplexing trade-off analysis [11]. In this work, we\nhighlight another aspect of multiuser diversity: its application\nto simplify network operations and its effect on throughput\nscaling. The opportunistic scheme proposed here is in the spirit\nof [12], where distance-dependent, random channel gains were\nexploited in scheduling.\nIn this work, we restrict ourselves to those assumptions\nthat are implementable with the state-of-the-art technologies.\nSpecifically, we focus on the assumptions of perfect CSI at\nthe receivers and partial CSI at the transmitters via low-rate\nfeedback. With these less idealistic CSI assumptions, it is\nenvisioned that independent encoding at the transmitters and\n3independent decoding at the receivers are employed. To the\nbest of the authors’ knowledge, there are no analogous results\nin the literature that consider the same scenario. It was recently\nshown, however, that under more optimistic assumptions on\nCSI in the network, linear throughput can be achieved by\neither joint encoding at the transmitters [13] or hierarchical\ncooperation with joint decoding at the receivers [5].\nB. Organization of the Paper\nThe rest of the paper is organized as follows. The sys-\ntem model and the proposed two-phase relay protocol are\nintroduced in Section II. Section III characterizes the sys-\ntem throughput in the regime of large n and fixed m. The\nthroughput scaling of the proposed scheme is evaluated in\nSection IV by explicitly taking the feedback overhead into\naccount. Also in Section IV, the throughput upper bound is\ndeveloped, valid even when centralized scheduling is allowed.\nSection V presents numerical performance results. Section VI\nbriefly discusses the impact of relay cooperation on system\nthroughput and the delay consideration. Finally, Section VII\nconcludes the paper. Technical details and proofs are placed\nin the appendices.\nNotation: The symbol |X | denotes the cardinality of the\nset X , and unless specified otherwise, log(·) indicates the\nnatural logarithm. We write X ∼ Exp(1) to indicate that\nthe random variable X follows the standard exponential\ndistribution with probability density function (pdf) given by\nfX(x) = exp(−x), x > 0. The indicator function is denoted\nby 1(·), and we use “χ2(2p)” to denote a chi-square random\nvariable with 2p degrees of freedom. For two functions f(n)\nand g(n), f(n) = O(g(n)) means that limn→∞|f(n)/g(n)| <\n∞, and f(n) = Ω(g(n)) means that g(n) = O(f(n)). We\nwrite f(n) = o(g(n)) to denote limn→∞|f(n)/g(n)| = 0,\nand f(n) = Θ(g(n)) to denote f(n) = O(g(n)) and\ng(n) = O(f(n)).\nII. SYSTEM MODEL\nConsider a wireless network with n S–D pairs and m relay\nnodes, all operating in the same frequency band of width W\nHz, in the presence of fading. Ad hoc nodes that generate data\ntraffic are referred to as source nodes; nodes that receive data\ntraffic are referred to as destination nodes. Relay nodes have\nno intrinsic traffic demands. We consider a two-hop, decode-\nand-forward communication protocol, in which sources can\ncommunicate with their destinations only through half-duplex\nrelays. In the first hop of the protocol, a subset of sources\nis scheduled for transmission to relays. The relays decode\nand buffer the received packets. During the second hop of the\nprotocol, the relays forward packets to a subset of destinations\n(not necessarily the set of destinations associated with the\nsource set in the first hop). These two phases are interleaved:\nthe first hop is run in the even-indexed time-slots, and the\nsecond hop is run in the odd-indexed time-slots. An example\nof the two-hop relay protocol is depicted in Fig. 1.\nWe first describe the channel model. It is assumed that the\nwireless network has independent and identically distributed\n(i.i.d.) Rayleigh connections hi,r from source nodes i, 1 ≤ i ≤\nn, to relay nodes r, 1 ≤ r ≤ m. Thus, the channel gains follow\nan exponential distribution, i.e., γi,r = |hi,r|2 ∼ Exp(1).\nLikewise, we assume that the channel gains ξr,j from relays\nr, 1 ≤ r ≤ m, to destination nodes j, 1 ≤ j ≤ n,\nbe i.i.d. Exp(1), and that channel gains γi,r and ξr,j are\nindependent for all i, r, and j. This model is appropriate\nfor dense networks in a rich scattering environment, where\nthe distance between transmitters and receivers has only a\nmarginal effect on attenuation, and the channel attenuation is\ndominated by the small-scale fading due to multipath. Quasi-\nstatic fading is assumed, with channel gains fixed during\nthe transmission of each hop, which is assumed to have a\nduration of T seconds, and taking on independent values at\ndifferent transmission times. In practice, T can be as large\nas the coherence time of the channel allows. Regarding CSI,\nwe assume that at each hop, each receiver has perfect CSI\nknowledge, while the transmitters have access only to an index\nvalue via receiver feedback used to indicate a source chosen\nfor transmission. This CSI assumption is reasonable in practice\nas most wireless access network standards incorporate some\nform of pilot signals, and the type of feedback specified has\nlow overhead.\nWe now describe the scheduling at each hop. We start\nwith the first hop (Phase 1). All relays operate independently.\nThus, without loss of generality, let us focus on any specific\nrelay, say r. By assumption, relay r has the knowledge of\nγi,r, i = 1, . . . , n, and it will schedule the transmission of the\nstrongest source node, say ir = argmaxi γi,r, by feeding back\nthe index ir at the beginning of the block. The overhead of\nthis phase of the protocol is a single integer per relay node.\nSuppose the scheduled nodes constitute a set K ⊂ {1, . . . , n};\nthen since there are m relays, up to m source nodes can be\nscheduled in this fashion, i.e., |K| ≤ m. It is noted that it is\npossible for multiple relays to schedule the same source. In\nsuch cases, |K| < m. The scheduled source nodes transmit\nsimultaneously with constant power P and fixed transmission\nrate of 1 bit/s/Hz.2 Each relay sees a superposition of all the\ntransmitting signals, i.e.,\nyr =\n√\nP hir ,r xir +\n∑\nt∈K\nt6=ir\n√\nP ht,r xt + nr, r = 1, . . . ,m\nwhere xi denotes the transmitted signal of source i, nr denotes\nthe additive noise at relay r. In this paper, we assume xi’s\nare letters from codewords of a Gaussian capacity-achieving\ncodebook satisfying E[|xi|2] = 1. We further assume nr’s\nare i.i.d. complex Gaussian with zero mean and unit variance\nCN (0, 1) and are independent of the fading channels. Since\nthe transmission rate is 1 bit/s/Hz, communication can be\nsupported in an information-theoretic sense if the correspond-\ning signal-to-interference-plus-noise ratio (SINR) is greater or\nequal to one, i.e.,\nSINR\nP1\nir ,r =\nγir ,r\n1/ρ+\n∑\nt∈K\nt6=ir\nγt,r\n≥ 1, r = 1, . . . ,m (1)\n2Generalizing to higher transmission rates is straightforward, but it encum-\nbers notation without adding insight. See discussions in Remark 6 for the\nmotivation of choosing the rate of 1 bit/s/Hz.\n4S1\nS2\nS3\nS4\nS5\nD1\nD2\nD3\nD4\nD5\nR1\nR2\nR3\n(a)\nS1\nS2\nS3\nS4\nS5\nD1\nD2\nD3\nD4\nD5\nR1\nR2\nR3\n(b)\nFig. 1. A two-hop network with n = 5 S–D pairs and m = 3 relay nodes (denoted by the blue disks). (a) In the first hop, source nodes {3, 4, 5} transmit\nto the relays. (b) In the second hop, the relays transmit to the destination nodes {2, 3, 4}. Solid lines indicate scheduled links, while dashed lines indicate\ninterfering links.\nwhere ρ = P is the average signal-to-noise ratio (SNR) of the\nsource–relay link.\nThe scheduling at the second hop (Phase 2) works as\nfollows. All relay nodes transmit simultaneously with fixed\npower PR. Assume that the additive noise at the destination\nnodes are i.i.d. CN (0, 1) and are independent of the fading\nprocesses {ξr,j}, and assume that independent messages are\nsent across relay nodes (which is the case in the proposed\nscheduling), destination node j can compute m SINRs by\nassuming that relay r is the desired sender and the other relays\nare interference as follows:\nSINR\nP2\nr,j =\nξr,j\n1/ρR +\n∑\n1≤ℓ≤m\nℓ 6=r\nξℓ,j\n, r = 1, . . . ,m (2)\nwhere ρR = PR denotes the average SNR of a relay–\ndestination link. If the destination node j captures one good\nSINR, say, SINRP2k,j ≥ 1 for some k, it instructs relay k to\nsend data by feeding back the relay index k at the beginning\nof the block. Otherwise, the node j does not provide feedback.\nIt follows that the overhead of the second hop is also at most\nan index value per destination node. When scheduled by a\nfeedback message, relay k relays the data to the destination\nnode at rate 1 bit/s/Hz. In case a relay receives multiple\nfeedback messages, it randomly chooses one destination for\ntransmission. It is noted that in steady-state operation of the\nsystem, the relays have the ability to buffer the data received\nfrom source nodes, such that it is available when the oppor-\ntunity arises to transmit it to the intended destination nodes\nover the second hop of the protocol. This ensures that relays\nalways have packets destined to the nodes that are scheduled.\nIn addition, due to the opportunistic nature of scheduling, the\nreceived packets at the destinations are possibly out of order,\nand therefore each destination is assumed to be able to buffer\nthem before decoding.\nRemark 1: It is noteworthy to draw a comparison between\nPhase 1 and Phase 2. From the relays’ perspective, both hops\nof the communication protocol rely on scheduling a subset of\n“good” source/destination nodes for transmission. However,\nthese two phases of the protocol differ in one key aspect:\ntransmission over the second hop can be guaranteed to be\nsuccessful since receivers (the destinations) have access to\nthe SINRs, but this is not the case for the first hop. This is\nbecause in the first hop, each receiver (relay) selects a source\nnode without knowledge of what the other relays select. As\na consequence, each relay has no access to the interference\nstemming from all other concurrent transmitting sources, and\ntherefore has no a priori knowledge of its own SINR. For\nexample, in (1) relay r knows the desired link strength γir ,r,\nbut it does not know K and the corresponding interference\nterm\n∑\nt∈K,t6=ir\nγt,r. For the second hop, the senders (now\nthe relays) are known a priori, and therefore the destination\nnodes have direct access to SINRs. This implies that once the\ndestination node captures an SINRP2 ≥ 1, and accordingly\nrequests a transmission, this transmission will be successful at\na data rate of 1 bit/s/Hz. This key difference between the two\nphases is mirrored in the analysis in Section III.\nRemark 2: In both hops, independent encoding at the trans-\nmitters and independent decoding at the receivers are em-\nployed. By independent encoding, it is meant that the transmit-\nters encode their message independently. This is a consequence\nof the fact that the transmitters have access to only partial CSI.\nSimilarly, by independent decoding is meant that receivers\ndecode their message independently by treating interference as\nnoise. It is worth pointing out that, with the assumption of CSI\nat the receivers, techniques like interference cancelation are\npossible at the receivers. However, as elaborated in Remark 6,\nthey are not interesting in our setup and thus are not considered\nhere.\nIII. THROUGHPUT: LARGE n AND FIXED m\nMotivated by the observation that as communication devices\n(source and destination nodes in our system) become more and\nmore pervasive, the number of infrastructure nodes (relays) is\nnot likely to keep pace, the throughput analysis in this paper\npays special attention to a regime in which the number of\nsource and destination nodes, n, is large, while the number\nof relay nodes, m, is relatively small. We show that both\nPhase 1 and Phase 2 achieve average throughput (by averaging\nover random channel gains) of m bits/s/Hz, yielding a m/2\nbits/s/Hz throughput for the complete two-hop scheme. We\nalso show that for any two-hop protocol, the throughput is\n5upper-bounded by m2 log logn bits/s/Hz. This information-\ntheoretic upper bound holds even if we allow full cooperation\nbetween relays and assume full CSI is available at the relays.\nThus, the proposed scheme, with much simplified assumptions\nof decentralized relay operations and CSI at the receiver,\nsucceeds in maintaining the linearity of the throughput in the\nnumber of relay nodes.\nA. Phase 1: Source Nodes to Relays\nIn Phase 1, m relays operate independently and each\nschedules one source node for transmission. Hence, the total\nnumber of scheduled source nodes can be any integer between\n1 and m, i.e., |K| ≤ m. In cases when |K| < m, multiple\nrelays schedule the same source node, and the analysis of\nthe probability of successful transmission should consider\nexplicitly those links with multiple receivers. Due to the mul-\ntiplicity of possible combinations, the exact characterization\nof the average throughput of Phase 1, R1, is mathematically\ninvolved. Fortunately, in order to show the achievability of m\nsuccessful concurrent transmissions, it suffices to lower-bound\nR1 by considering only cases in which the m scheduled source\nnodes are distinct (thereby discarding the contributions to the\nthroughput of the other combinations).\nBy symmetry, each source node has a probability of 1/n to\nbe the best node with respect to a relay. Thus, the probability\nthat the scheduled users are distinct, i.e., no source node is\nscheduled by more than one relay, is given by Pr[Nm] =\nn(n − 1) · · · (n −m + 1)/nm, where Nm denotes the event\n{m distinct source nodes are scheduled}. Now, a lower bound\non R1 is\nR1 ≥ Pr\n[\nNm\n] m∑\nr=1\nPr\n[\nSINRir , r ≥ 1\n] · 1\n= m Pr\n[\nNm\n]\nPr\n[\nSINR\nP1 ≥ 1], (3)\nwhere, for notational brevity and by the i.i.d. channel model,\nwe drop the source node and relay indices in the last equation\nand use SINRP1 to denote the SINRs at all relay nodes.\nNow, we focus on the Pr\n[\nSINR\nP1 ≥ 1] term. Again, for\nnotational convenience, for a realization of n i.i.d. random\nvariables X1, . . . , Xn, we introduce X := max{X1, . . . , Xn}\nand Y :=\n∑\ni∈K′ Xi where K′ is any randomly selected (m−\n1)-element subset of {1, . . . , n} \\ {j : Xj = X}. With these\ndefinitions, we have Pr\n[\nSINR\nP1 ≥ 1] = Pr[ X1/ρ+Y ≥ 1]. For\nthe Rayleigh fading case, in which the link strengths are i.i.d.\nExp(1) random variables, the cumulative distribution function\n(cdf) of X (largest of n i.i.d. Exp(1) random variables) can\nbe written explicitly as FX(x) = (1− e−x)n. The asymptotic\nproperties of X are well studied in literature (see [8] and [14]).\nFor s0 = logn−log logn, it can be shown that Pr[X ≤ s0]→\n0 [14, eq. (A4)]. With the help of this property, we proceed to\nlower-bound Pr\n[\nSINR\nP1 ≥ 1] by introducing a real variable\ns (0 < s ≤ s0). By the law of total probability, we have\nPr\n[\nX\n1/ρ+ Y\n≥ 1\n]\n= Pr[X > s] · Pr\n[\nX\n1/ρ+ Y\n≥ 1\n∣∣∣X > s]\n+ Pr[X ≤ s] · Pr\n[\nX\n1/ρ+ Y\n≥ 1\n∣∣∣X ≤ s]\n≥ Pr[X > s] · Pr\n[\nX\n1/ρ+ Y\n≥ 1\n∣∣∣X > s]\n≥ Pr[X > s] · Pr\n[\ns\n1/ρ+ Y\n≥ 1\n∣∣∣X > s]\n= Pr[X > s] · Pr\n[\ns\n1/ρ+ Y\n≥ 1\n]\n(4)\n=\n(\n1− FX(s)\n)\nFY (s− 1/ρ), (5)\nwhere (4) follows from the fact that, with 0 < s ≤ s0 and\nPr[X ≤ s0] → 0, we have Pr[X > s] → 1, and thus\nPr\n[\ns\n1/ρ+Y ≥ 1\n∣∣X > s] → Pr[ s1/ρ+Y ≥ 1] (which can\nbe trivially shown by the law of total probability). Note that\nthe lower bound (5) suggests a suboptimal scheduling scheme\naccording to which, each relay schedules the transmission of\nthe “strongest” source only if the source’s power gain exceeds\na prescribed threshold s. The probability of such event is given\nby 1 − FX(s), and FY (s − 1/ρ) is a lower bound on the\nprobability of a successful communication with the relay at a\nrate of 1 bit/s/Hz.\nThe characterization of distribution of the interference term\nY in (5) needs more care. This is due to the fact that, condi-\ntioned on not being the maximum among n channel strengths,\neach interference term in Y is no longer exponentially dis-\ntributed, and the interference terms are not independent in\ngeneral. However, as shown in Appendix A, these properties\nhold asymptotically with n. Numerical results in Appendix A\nshow that these asymptotic trends are achieved for relatively\nsmall values of n. Thus, we can approximate Y as chi-square\nrandom variable with 2(m−1) degrees of freedom, whose cdf\nis thus given by [15, eq. (2.1–114)]\nFY (y) = 1− e−y\nm−2∑\nk=0\n1\nk!\nyk. (6)\nSubstituting (5) and (6) into (3) yields the following lower\nbound on the throughput of Phase 1.\nLemma 1: For any ρ, m and 0 < s ≤ logn − log logn,\nthe achievable throughput of the opportunistic relay scheme\nin Phase 1 is lower-bounded by\nR1 ≥ mn(n−1)···(n−m+1)nm\n(\n1− (1 − e−s)n)\n×\n(\n1− e−(s−1/ρ)\nm−2∑\nk=0\n1\nk!\n(s− 1ρ)k\n)\n,\n(7)\nas n→∞.\nA tighter lower bound can be found by maximizing (5)\nover s, but we find that little insight can be gained from this\nexercise. The tightness of the lower bound (7) is substantiated\nby numerical results shown in Fig. 2 of Section V.\nRemark 3: Inspecting (7), we note that the lower bound on\nR1 exhibits a tradeoff between quantity and quality of sched-\nuled links. By increasing the number of relays m, one can\nschedule more simultaneous transmissions, which is beneficial\nfrom the throughput perspective. However, more transmissions\ngenerate more interference, degrading the SINR and lowering\nthe probability of successful transmissions. In fact, as shown\nin Section V, not only the lower bound discussed here, but\nalso the actual throughput R1 demonstrates this tradeoff. The\n6characterization of the best m (in terms of scaling) that\nmaximizes throughput is pursued in Section IV.\nFor the regime of interest, where n is large and m fixed, it\ncan be trivially shown, e.g., by setting s = logn− log log n,\nthat the above lower bound approaches m. Note that this is\nalso the best we can hope for in Phase 1, since the SINR ≥ 1\nconstraint to decode a transmitter implies that no more than\nm sources can be successful.\nThe following corollary to Lemma 1 follows immediately.\nCorollary 1: For fixed m, R1 → m as n→∞.\nB. Phase 2: Relays to Destination Nodes\nWe now develop an exact expression for the sum-rate of\nthe relay-destination links. This is done by first showing that\nonly a single relay per destination can produce a required\nSINR larger than one, and then computing the probability of\nthe event that a relay is scheduled and consequently delivers\nthroughput.\nLemma 2: For any ρR, m and n, the achievable throughput\nof the opportunistic relay scheme at Phase 2 is given by\nR2 = m\n(\n1−\n(\n1− e\n−1/ρR\n2m−1\n)n)\n. (8)\nBefore embarking on the proof, it is worth examining the\nstatistics of the SINR in (2). Given the i.i.d. channel model\nintroduced in the previous section, the SINRs measured at\neach destination (cf. (2)) are of the generic form SINRP2r,j =\nχ2(2)\n1/ρR+χ2(2m−2)\n. With the help of (6), the pdf of the SINR can\nbe shown as [14]:\nf(x) =\n∫ ∞\n0\nf(x|y)fY (y)dy\n=\ne−x/ρR\n(1 + x)m\n(\n1\nρR\n(1 + x) +m− 1\n)\n. (9)\nThe corresponding cdf is\nF (x) = 1− e\n−x/ρR\n(1 + x)m−1\n, x ≥ 0. (10)\nNote that the SINRP2r,js are i.i.d. over j = 1, . . . , n (but are not\nindependent over r = 1, . . . ,m).\nProof: First, we observe that each destination node j has\nat most one SINRP2k,j ≥ 1 for all relays 1 ≤ k ≤ m. To see this,\nassume SINRP2k,j ≥ 1 for some relay k, and consider another\nindex k′ 6= k. From (2), we have\nξk,j ≥ 1/ρR +\n∑\n1≤ℓ≤m\nℓ 6=k\nξℓ,j ,\nfrom which it follows that\nξk,j > ξk′,j , ∀ k′ 6= k.\nTherefore\nSINR\nP2\nk′,j =\nξk′,j\n1/ρR +\n∑\n1≤ℓ≤m\nℓ 6=k′\nξℓ,j\n<\nξk′,j\nξk,j\n< 1.\nThus, each destination node can have at most one good relay\nas its sender.\nNow the sum-rate for Phase 2 depends on how many relays\nare scheduled by destinations. The probability that a relay finds\nno destination satisfying SINR ≥ 1 is3\nPr[relay r does not receive feedback]\n= Pr\n[\nSINR\nP2\nr,j ≤ 1, ∀ j\n]\n=\n(\nF (1)\n)n\n=\n(\n1− e\n−1/ρR\n2m−1\n)n\n.\nThe throughput of the relay-destination links is given by\nsumming the probabilities of the relays engaged in transmis-\nsion. Accounting for the 1 bit/s/Hz rate per relay, we have that\nthe average throughput of the second hop is given by\nR2 =\nm∑\nr=1\nPr[relay r transmits data to a destination] · 1\n= m\n(\n1− (F (1))n) (11)\n= m\n(\n1−\n(\n1− e\n−1/ρR\n2m−1\n)n)\n. (12)\nThis completes the proof of the lemma.\nThe following corollary ensues by direct computation:\nCorollary 2: For fixed m, R2 → m as n→∞.\nRemark 4: It is interesting at this point to draw a connection\nbetween the scheduling of Phase 2 of the opportunistic scheme\nproposed here with the random beamforming scheme due to\nSharif and Hassibi in the context of multiple-input multiple-\noutput broadcast channels (MIMO-BCs) [14]. Seemingly un-\nrelated, the SINRs of both setups turn out to have the same\ndistribution (cf. (2)). To explain this subtlety, note that in the\nrandom beamforming scheme of [14], a random unitary matrix\nΦ is applied to the data streams before sending them over the\nchannel H (hence the terminology “random beamforming”).\nWith the assumption of i.i.d. Rayleigh fading, entries of H\nfollow i.i.d. circularly symmetric complex Gaussian random\nvariables CN (0, 1). By the isotropic property of the i.i.d.\ncomplex Gaussian random matrix H , ΦH has the same\ndistribution as H [16]. It follows that the channel statistics\nof the random beamforming scheme in the beam domain are\nthe same as in the original antenna domain. In other words,\nthe SINR in the beam domain is still of the generic form\nSINR =\nχ2(2)\n1/ρR + χ2(2m− 2) , (13)\nwhich is the same as in our Phase 2 (cf. (2)).\nDespite the mathematical equivalence, our proposed scheme\nfor Phase 2 simplifies the random beamforming scheme in\nseveral respects:\n3It may seem logical to turn off a relay for which the highest SINR is\nstill less than one, but we still allow such relays to transmit (say, control\ninformation). This is because, as shown by numerical results, the performance\nis limited by the source-relay link.\n7• Random beamforming requires cooperation among the\ntransmitters to form a beam. Opportunistic relaying op-\nerates in a completely decentralized fashion.\n• Random beamforming requires the feedback of an integer\n(the beam index) as well as a real number (the in-\nstantaneous SINR). The proposed opportunistic relaying\nscheme requires the feedback of only an index number.\nThis simplification is justified by [14, Th. 2], which\nimplies that when the system operates in the limit as\nn → ∞ with m = Θ(logn), the aggregate interference\nfrom concurrent transmissions eventually hardens the\ninstantaneous SINR near the value 1. Thus, there is no\nlonger a need to feed back the SINR value. Furthermore,\nin terms of the throughput scaling law (as discussed later\nin Section IV), this simplification incurs no loss.\nC. Feedback Overhead Analysis\nA detailed study of feedback overhead in the regime of\nlarge n and fixed m is omitted here for the sake of brevity.\nThe calculation can follow the same steps as in Section IV-C,\nwhere we present a detailed analysis of the feedback overhead\nin the limiting regime of large n and m.\nD. Two-Hop Communication\nWith the help of Corollaries 1 and 2, and by taking into\naccount the 1/2 penalty due to the two hops, the overall system\nthroughput, defined as 12 min{R1, R2}, can be readily shown\nto be given as follows.\nTheorem 1: For fixed m, the two-hop opportunistic relaying\nscheme achieves a system throughput of m/2 bits/s/Hz as n→\n∞.\nSince the proposed scheme works in a decentralized fashion\nand with low rate CSI feedback, it is natural to expect some\nthroughput degradation compared to more intensive schemes.\nWe will show that the opportunistic relaying scheme exhibits\nthe pre-loglog factor of the scaling law of the throughput of\nmore intensive schemes. To see this, we find an information-\ntheoretic upper bound on the achievable scaling law for the\naggregate throughput of any two-hop relaying scheme.\nLemma 3: For any two-hop relaying architecture, with\nfixed m and SNR, the sum rate capacity scales at most as\n1\n2m log logn as n→∞.\nProof: In two-hop relay schemes, all data traffic passes\nthrough relays. Therefore, the best scheme would be one in\nwhich all m relay nodes can cooperate and the relays have full\nCSI (i.e., backward as well as forward channel realizations).\nIn such case, the two-hop communication can be interpreted\nas MIMO multiple access channels (MACs) followed by\na MIMO-BC. The capacity region of the MIMO-BC, and\nthe optimality of dirty-paper-coding (DPC) in achieving the\ncapacity region have been shown in [17]. Furthermore, the\ncapacity scaling of the DPC scheme is shown in [14] to be\nm log logn, which is also the capacity scaling for MIMO-\nMAC due to the MAC–BC duality [18]. Now, Lemma 3\nfollows by taking the two-hop penalty 1/2 into account.\nRemark 5: Contrasting Theorem 1 to Lemma 3 reveals\ntwo different facets of multiuser diversity. Fundamentally,\nmultiuser diversity gain is a power gain, e.g., in the Rayleigh\nfading case, multiuser diversity schedules the best user for\ntransmission, and boosts the average power by a factor of logn\n[8]. With the assumption of relay cooperation, as in Lemma 3,\na spatial multiplexing gain equal to the number of relays m can\nbe readily achieved (e.g., even by a suboptimal zero-forcing\nreceiver [16]). Then, multiuser diversity can further boost\nthe rate of each parallel channel by log logn, as shown by\nLemma 3. In contrast, with the proposed opportunistic scheme,\nwhere relays operate independently, there is no guarantee\nof achieving the multiple parallel channels. Here, multiuser\ndiversity is used as a mechanism that compensates for the\ninterference plus noise so that the scheduled link can support 1\nbit/s/Hz. Ultimately, one achieves the linear scaling in m. Note\nthat only with multiuser diversity gain does the SINR of each\nnoncooperative link have the chance to meet the threshold.4\nRemark 6: At this point, it is worthwhile to revisit the\nassumption of 1 bit/s/Hz fixed transmission rate. According\nto the scheduling scheme, receivers select their transmitting\nnodes by feeding back their indices. Accordingly, the nodes\ntransmit independently at 1 bit/s/Hz. Receivers decode their\nscheduled transmitters independently, by treating concurrent\ninterference as noise. In other words, it is assumed that 1)\nthe transmitters do not adapt their transmission rate to the\ninstantaneous channel realizations; and 2) the receivers do not\nattempt to perform any interference cancelation. It is reason-\nable to expect that a higher throughput can be achieved if we\nallow rate adaptation and interference cancelation at the cost of\nmore feedback overhead and higher computational complexity.\nHowever, what Lemma 3 tells us is that the return is at\nmost the multiplicative factor log logn. Simulation results in\nSection V indicate a rather fast convergence to the asymptotic\nlimits with increasing number of nodes. In a practical system\nwith finite (but maybe large) n, the term log logn is a small\nnumber. On the other hand, given the decentralized scheduling\npolicy adopted here, it is not straightforward to determine\nthe adaptive transmission rate for each transmitting node (cf.\nRemark 1).\nIV. HOW FAST CAN m GROW?\nAs discussed in Remark 3, in Phase 1 there is a tradeoff\nbetween the number of relays m that serve as conduits between\nthe source and destination nodes and the mutual interference\ncaused by the transmissions. The same is true for Phase 2.\nThis brings up the question: What is the optimal m that\nmaximizes the throughput? This is equivalent to asking the\nmaximum throughput of the network. In this section, we show\nthat both hops of the proposed decentralized scheme succeed\nin achieving Θ(logn) throughput scaling (without taking the\nfeedback overhead into account). We then quantify the feed-\nback overhead and conclude that, under the condition that the\nproduct of the block duration and the system bandwidth scales\nfaster than log n log logn, the feedback overhead is negligible\nand therefore the useful throughput of the proposed scheme\nis given by Θ(logn). As a by-product in characterizing the\n4If one schedules the transmission randomly, the average receiver SINR\ncan be shown to be 1\nm−1\n.\n8throughput upper bound of the first hop, we also conclude\nthat Θ(logn) is indeed the best throughput scaling even if\ncentralized scheduling is allowed. Thus, as far as throughput\nscaling is concerned, operating the network in a decentralized\nfashion, with local CSI at the receivers and low-rate feedback\nto the transmitters, incurs no loss.\nA. Phase 1\nEarlier, in Section III-A, the lower bound (7) of the system\nthroughput of Phase 1 was found. This lower bound was\nadequate for the discussion in that section which assumed\na large n and fixed m. However, in seeking to determine\nhow the throughput scales with m, the lower bound (7)\nmight considerably underestimate the true throughput. In light\nof this, in order to address the question of optimal m, we\nreason as follows: First, we consider a genie-aided scheme\nby relaxing the assumptions of decentralized relay scheduling.\nThus, the throughput scaling for a genie-aided network with m\nrelays serves as an upper bound on the proposed decentralized\nscheme. We show that the throughput scaling law of this\ngenie-aided scheme is Θ(logn). Next, we show that the lower\nbound (7) of the proposed decentralized scheme also achieves\nthe Θ(logn) scaling. Thus, we are able to conclude that the\nthroughput scaling of the original scheme of Phase 1 is given\nby Θ(logn), and is optimal in a scaling law sense.\n1) Phase 1: Upper bound due to genie-aided scheme:\nIn this subsubsection, we establish the upper bound on the\nthroughput scaling of Phase 1 based on the following genie-\naided network. The genie-aided network has access to the\nfull CSI of the network, and can coordinate the operation\nof the entire network, i.e., centralized scheduling is allowed.\nTherefore, the genie network can always achieve the maximum\nthroughput in that, by assumption, it can enumerate all possible\ncombinations of source-to-relay transmissions. Nevertheless,\nwe still assume that independent encoding at the source nodes\nand independent decoding at the relay nodes. These constraints\nare needed to keep the genie-aided upper-bound result not\ntoo loose with respect to the proposed decentralized network\n(cf. Section II). Note that given a set of channel realizations,\nthe successful source–relay pairs, in the proposed decen-\ntralized scheme, must also be successful in the genie-aided\nscheduling scheme. Thus, the throughput of the genie-aided\nscheduling scheme upper-bounds the proposed decentralized\nscheme.\nTheorem 2: Under the assumption of independent encoding\nat the source nodes and independent decoding at the relay\nnodes, one cannot achieve lognlog 2+2 throughput with probability\napproaching one. Conversely, with probability approaching\none, (1−ǫ) log n2 log 2+2 throughput is achievable for all ǫ ∈ (0, 1).\nOutline of proof: The upper bound result is established\nby showing that, given m = lognlog 2 +2 relays, with probability\napproaching 1, one cannot find lognlog 2 + 2 sources whose con-\ncurrent transmissions to the relays are all successful, even by\nenumerating all possibilities of choosing sources and mapping\nsources to relays. The achievability result follows from the\nfact that, by exhaustive search, with probability of one, one can\nfind successful concurrent transmissions from (1−ǫ) logn2 log 2+2\nsources to m = (1− ǫ) logn2 log 2 + 2 relays.\nSee Appendix B for detailed proof.\nRemark 7: These results may be of interest in their own\nright, since, given the assumptions of independent encoding\nat the transmitters and independent decoding at the receivers\n(reasonable assumptions in ad hoc networks in which global\nCSI is not available to enable cooperative encoding and/or\ndecoding), Theorem 2 establishes the upper bound and the\nachievable throughput scaling that are valid even if centralized\nscheduling is allowed. The Θ(logn) throughput exemplifies\nthe interference-limited nature of the network, and this sub-\nlinear throughput scaling (compared to other works, e.g., [3]–\n[5], [13]) precisely demonstrates the price one has to pay for\nnot having global CSI knowledge to mitigate the interference.\nIndeed, recent works show that if one allows either cooperative\ndecoding between receivers (see, e.g., [5]) or cooperative\nencoding between transmitters (see, e.g., [13]), one can indeed\navoid/cancel interference, enabling linear throughput scaling.\n2) Achievable throughput scaling of Phase 1: Theorem 2\nstates that, with high probability, there exists a valid group\nwith m = (1− ǫ) logn2 log 2 +2 sources such that all transmissions\nare successful. However, the proof is nonconstructive: it does\nnot afford insight into how to find such a set in practice.\nThe proof assumes that there is a genie with global channel\ninformation that can enumerate all possibilities and select\na good one for scheduling. In contrast to the genie-aided\nscheme, the opportunistic relaying scheme seeks to operate\nin a decentralized manner, and it is not clear whether this\noperational simplification incurs a loss in the scaling order of\nthe throughput. Serendipitously, it can be shown that the logn\nscaling is also met by the lower bound in (7). To see this, we\nexamine the asymptotic behavior of (7).\nConsider the exemplary case of m = logn and s = logn−\nlog logn. With n→∞, the term n(n−1)···(n−m+1)nm → 1. The\nterm\n(\n1− (1− e−s)n) is independent of m, and approaches 1\nfor s = logn− log logn as n→∞. Therefore, a throughput\nof Θ(logn) can be achieved as long as FY (s−1/ρ) = Θ (1).\nIndeed, for m = logn, the interference term Y in (6), by\nthe central limit theorem, can be approximated as Gaussian\nrandom variable with mean and variance both equal to logn.\nNow, we have\nFY (logn− log logn− 1/ρ) ≈ FY (logn) = 1\n2\n, (14)\ndue to the symmetry of the Gaussian distribution. Conse-\nquently R1 ≈ 12 logn. This result implies that for m = logn\nrelays, each running the two-hop opportunistic relaying pro-\ntocol, it is possible to schedule up to logn source nodes to\ntransmit simultaneously, but half of them will fail to satisfy\nthe SINR requirement due to the multiple access interference.\nIn terms of throughput, this example yields 12 logn, which\nconfirms that the scheme is in fact order-optimal in achieving\na throughput of Θ(logn) at Phase 1.\nB. Phase 2\nIn this subsection, we will show that the optimal value of\nm in Phase 2 exhibits a sharp phase transition phenomenon.\n9That is, m = logn−log logn−1/ρRlog 2 +1 succeeds in retaining the\nlinearity of R2 in m, but m = log n+log logn−1/ρRlog 2 + 1 does\nnot. As far as the scaling law is concerned, this implies that\nthe throughput of Phase 2 scales as Θ(logn).\nTheorem 3: For Phase 2 of the two-hop opportunistic relay-\ning scheme, if the number of relays m = log n−log logn−1/ρRlog 2 +\n1, then R2 = Θ(m) = Θ (logn). Conversely, if m =\nlogn+log log n−1/ρR\nlog 2 + 1, then R2 = o(m).\nProof: For convenience, we repeat R2 of (11) and (12):\nR2 = m\n(\n1− (F (1))n) (15)\n= m\n(\n1−\n(\n1− e\n−1/ρR\n2m−1\n)n)\n. (16)\nWith m = log n−log logn−1/ρRlog 2 + 1, we have\n1− F (1) = e\n−1/ρR\n2m−1\n= e−(m−1) log 2−1/ρR =\nlogn\nn\n.\nThen, (\nF (1)\n)n\n=\n(\n1− logn\nn\n)n\n= en log(1−\nlogn\nn )\n= e− logn+O(\nlog2 n\nn ) = e− logn+o(logn)\n= O\n(\n1\nn\n)\n, (17)\nwhere we have used the fact that, for small x, log(1 − x) =\n−x+O(x2) and ex = 1+O(x). Thus, most of the transmis-\nsions meet the SINR threshold (with probability 1−O(1/n)),\nand consequently the throughput R2 is given by m(1−O( 1n )).\nR2 = Θ(m) = Θ(logn) follows readily.\nSimilarly, when m = log n+log logn−1/ρRlog 2 + 1, we have 1−\nF (1) = 1n logn and(\nF (1)\n)n\n= e\n− 1logn+O(\n1\nn log2 n\n)\n= e−\n1\nlogn+o(1/ logn)\n= 1−O(1/ logn). (18)\nNow, in contrast to the case of m = logn−log logn−1/ρRlog 2 + 1,\nwhen we increase m to logn+log logn−1/ρRlog 2 +1, Phase 2 of the\ntwo-hop scheme cannot support a throughput that scales with\nm. With probability one, the SINRs cannot meet the threshold.\nIn this case, the throughput does not scale linearly with m\nanymore, i.e., R2 = o(m).\nThis Θ(logn) scaling result is consistent with the random\nbeamforming scheme of [14], an outcome that is not surprising\nin light of the connection discussed in Remark 4.\nC. Feedback Overhead Analysis\nOne of the contributions of the paper is the proposal of a\ntwo-hop scheme that alleviates the assumption of full CSI at\nthe transmitters and the assumption of centralized scheduling.\nIn the proposed scheme, only CSI at the receivers is employed,\nbut low-rate feedback from the receivers to the transmitters is\nassumed to enable scheduling. In this subsection, we quantify\nthe overhead due to feedback, and formalize a sufficient\ncondition for which the overhead is negligible.\n1) Overhead per fading block:\nFeedback overhead in the first hop: In the first hop, each relay\nschedules one source. Since a total of n sources need to be\nidentified, the feedback overhead per relay is log2 n bits. The\noverall feedback overhead of all relays is thus given by m ·\nlog2 n, which scales as Θ\n(\n(logn)2\n)\nsince m = Θ(logn).\nFeedback overhead in the second hop: In the second hop, any\ndestination feeds back the index of a relay only if there is one\nrelay meeting SINR ≥ 1; otherwise, no feedback is sent. One\nneeds Θ(log2m) = Θ(log logn) bits to identify a relay. The\nnumber of users that capture a good SINR, and consequently\nfeed back follows the binomial distribution Bi(n, q) with\nq being the probability that the destination will provide a\nfeedback. Then, the average overall feedback overhead is given\nby the average number of destinations that feed back, nq, times\nthe number of bits of each feedback.\nTo calculate q, we have,\nq = Pr[the destination feeds back index]\n= Pr[∪mr=1{the rth relay has SINR ≥ 1}]\n≤\nm∑\nr=1\nPr[the rth relay has SINR ≥ 1]\n= m(1− F (1))\n=\nm\n2m−1\ne−1/ρ, (19)\nwhere the last equality is due to (10). The quantity in (19) is of\nthe order of Θ\n(\n(logn)2/n\n)\nwhen m = logn−log logn−1/ρRlog 2 +1\n(cf. Theorem 3).\nFinally, the average feedback overhead of the second hop\nis O(nq log logn) = O\n(\n(log n)2 log logn\n)\n.\n2) Condition for Θ(logn) useful throughput: In this work,\na quasi-static fading model is assumed. Specifically, it is\nassumed that channel gains are fixed during the transmission\nof each hop, and take on independent values at different hops.\nThe feedback overhead per block is analyzed above. The\nthroughput of the system scales with the duration of the block\nas well as system bandwidth. That is, the total throughput\nscales as Θ\n(\nTW · logn).\nNow, we can find conditions of TW for which the feedback\noverhead does not imperil the throughput in the sense of\nthroughput scaling. This is true when the feedback overhead\nis less than (in terms of scaling) the total throughput of each\nhop. Let us look at the second hop, which has larger feedback\noverhead. To have Θ(logn) useful throughput, we need\n(log n)2 log logn = O(TW · logn),\nwhich holds whenever TW = Ω(logn log logn).\nIn practical system design, T can be as large as the\ncoherence time of the channel Tc, and W can be as large\nas the coherence bandwidth of the channel Wc. For a typical\nwireless channel, this condition is easy to meet. This is due\nto the fact that typical wireless channels are underspread, that\nis, they satisfy TcWc ≫ 1. In typical urban environments,\nthe coherence bandwidth is of the order of several MHz, and\nthe coherence time is of the order of milliseconds [16]. Thus,\nthe product of coherence bandwidth and coherence time is\n10\n \n \nPSfrag replacements\nNumber of Relays, m\nAv\ner\nag\ne\nTh\nro\nu\ngh\npu\nt\n[b\nits\n/s/\nH\nz]\nsimulated R1\nsimulated R1 with distinct nodes\nlower bound (7), optimal s\nlower bound (7), s = log n− log log n\n1 2 3 4 5 6 7 8 9\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nFig. 2. First hop average throughput R1 as a function of the number of relays for n = 1200 S–D pairs. From the top: simulation results utilizing all source node\nassignments, simulated results with distinct scheduled nodes, lower bound with optimized threshold s, and lower bound with threshold s = logn− log logn.\nof the order of 103. As a concrete example, suppose the\ncarrier frequency is fc = 900 MHz, and the delay spread is\nTd = 1 µs. Based on the definitions of coherence bandwidth\nand coherence time in [16], the coherence bandwidth is given\nby Wc = 1/(2Td) = 0.5 MHz. The coherence time depends\non velocity v, where let us assume v = 3 km/h. This leads\nto a maximum Doppler spread of Ds = fcv/c = 2.5 Hz,\nand accordingly, to a coherence time of Tc = 1/4Ds = 100\nms. In this example, TcWc = 5× 104, which makes TcWc ≥\nlogn log logn hold even for extremely large n. For example,\nfor n = 1.0× 108, logn log logn = 53.7.\nD. Two-Hop Communications\nWith the results in previous subsections, we can conclude\nthe achievable throughput scaling of the scheme in the follow-\ning theorem.\nTheorem 4: Under the setup of Section II, and given\nTW = Ω(logn log logn), the proposed two-hop opportunistic\nrelaying scheme yields a maximum achievable throughput of\nΘ(logn).\nRemark 8: The throughput scaling results in this paper\nafford a multiuser diversity interpretation. To see this, it is\nuseful to take a closer look at the first hop. The power of the\nsignal of each scheduled link is given by logn [8] (due to\nmultiuser diversity). In the regime where m is fixed (cf. Sec-\ntion III), the signal power can mitigate the interference power\n(which is of the order of one), and therefore each scheduled\ntransmission is successful. This translates to m bits/s/Hz total\nthroughput of the first hop, as shown in Corollary 1. In the\nlimiting operating regime with m = Θ(logn) relays, the\naggregate interference for each scheduled link is of the order\nof Θ(logn). Now, the network saturates as the interference\npower is of the same order as the signal power. The system\nthroughput is calculated as Θ(logn) · Θ(1) = Θ(logn),\nsince one has Θ(logn) concurrent transmissions, and each of\nthem has successful probability of Θ(1). Further increasing m\nresults in a decreased probability of successful transmission.\nReferring back to Remark 3, we see that the optimal m (in the\nsense of maximizing system throughput) is given by the order\nof multiuser diversity. The interpretation of the throughput\nscaling in terms of multiuser diversity is discussed in more\ndetail in [19].\nV. NUMERICAL RESULTS\nIn this section, we provide some numerical examples pro-\nduced by simulations of the proposed opportunistic relaying\nscheme under Rayleigh fading. Throughout these examples,\nthe SNR for both hops is set at 10 dB (ρ = ρR = 10 dB).\nWe examine in Fig. 2 the average throughput R1 of the first\nhop of the protocol and its various lower bounds. The figure\ncontains four curves. The two simulation curves were obtained\nby averaging throughputs over 2, 000 channel realizations. The\n“simulated R1” curve was obtained using all assignments of\nsource nodes, while the curve marked “simulated R1 with\ndistinct nodes” represents only assignments of distinct source\nnodes. The other two lower bounds shown are computed with\n(7): one is obtained by optimizing (7) over s (numerically);\nthe other lower bound is for s = logn − log logn. Three\nobservations are noteworthy relative to Fig. 2. First, both\nthe simulated throughput and the analytical lower bound (7)\nexhibit linearity with respect to m, consistent with the analysis\nof Section III-A. Second, it is observed that when m exceeds a\ncertain value (in this case, 6), the throughput R1 starts to fall\noff. Noting that log 1200 = 7, this effect is consistent with\nthe analysis in Section IV-A that established that the linear\nincrease in throughput with the number of relays holds only\n11\n \n \nPSfrag replacements\nNumber of Relays, m\nAv\ner\nag\ne\nTh\nro\nu\ngh\npu\nt\n[b\nits\n/s/\nH\nz]\naverage R1\naverage R2\naverage R\n1 2 3 4 5 6 7 8 9 10\n0.0\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\nFig. 3. First hop average throughput R1, second hop average throughput R2, and average system throughput R as a function of the number of relays m\nfor n = 1200 S–D pairs.\nas long as m is of the order logn. Third, the lower bound\nof R1 of (7) becomes loose when m grows. The development\nleading to (5) suggests two possible reasons for this behavior.\nThe first is that the computation of Pr [Nm] is based on only\ndistinct source nodes. However, the close match between the\ntwo simulation curves in Fig. 2 eliminates this possibility.\nIt follows then that the bound is loosened due to the series\nof lower-boundings of Pr\n[\nX\n1/ρ+Y\n]\nleading to (5) being too\nconservative.\nIn Fig. 3, we illustrate throughputs R1 and R2, as well as\nthe corresponding system throughput of the full scheme given\nby R = 12 min{R1, R2}. As discussed in Section II, the trans-\nmissions over the second hop are destined to be successful,\nsince they are scheduled based on SINR measurements at the\ndestination nodes, whereas the transmissions over the first hop\nare not guaranteed to be successful since they are based only\non SNR measurements. As a consequence, we observe from\nFig. 3 that R1 is lower than R2, and is the bottleneck to the\nsystem throughput, i.e., R = 12R1. In addition, we observe\nthat the optimal number of relays for Phase 2 is consistent\nwith the analysis of Theorem 3 in Section III-B. Nevertheless,\nboth R1 and R2 display the linearity in m as predicted by\nCorollaries 1 and 2 in Section III.\nThe total throughput of the two-hop opportunistic relaying\nscheme is shown in Fig. 4 as a function of the number of nodes\nn. We observe that the throughput exhibits the logn trend,\nas predicted by Theorem 4. In fact, the system throughput\ncurve can be perfectly approximated by 0.36 logn. Since\nthe system throughput is always limited by Phase 1, i.e.,\nR = 12 min{R1, R2} = 12R1, we also plot two bounds of 12R1\nfor reference. More specifically, the genie bound logn4 log 2 + 1\n(cf. Theorem 2) serves as an upper bound, and the 14 logn\ncurve from (14) serves as a lower bound for the system\nthroughput. In Fig. 5, the optimal value of the number of\nrelays, m, is shown versus the number of nodes, n. Comparing\nthe values of m from the curve, with the value logn2 log 2+2, which\nis the bound on the number of relays for the genie scheme in\nTheorem 2, we observe that the optimal m is very close to\nthat of the genie-bound. This explains why the scheme can\nharness large portions of throughput as promised by Theorem\n4.\nVI. OTHER CONSIDERATIONS: RELAY COOPERATION AND\nDELAY\nOne of the key contributions of this work is to propose\nan opportunistic relaying scheme that features decentralized\nrelay operations and practical CSI assumptions. In this section,\nwe discuss the case in which relays are allowed to cooperate\nin encoding/decoding. In order to isolate the impact of the\nrelay cooperation on the two-hop scheme, we leave the CSI\nassumptions unchanged. Specifically, it is assumed that the\nrelays have full CSI knowledge of the source–relay link, but\nhave only partial index-valued CSI knowledge of the relay–\ndestination link via feedback. This discussion will help identify\nthe fundamental limits of the opportunistic relaying scheme.\nFinally, we briefly address the issue of network delay.\nA. Cooperative Relays\nIn the proposed opportunistic relaying scheme, we assume\nthe relays perform independent decoding (in the first hop) and\nindependent encoding (at the second hop). In particular, the\nrelays treat the received interference as noise, and no attempt\nis made to cancel the mutual interference caused by concurrent\ntransmissions. As a consequence, the system is interference\nlimited. In this subsection, we address the question: How\ndoes cooperation between relays in decoding/encoding change\n12\n \n \nPSfrag replacements\nNumber of S-D Pairs, n\nAv\ner\nag\ne\nTh\nro\nu\ngh\npu\nt\n[b\nits\n/s/\nH\nz]\nlog n\n4 log 2\n+ 1\naverage R\n1\n4\nlog n\n0 2000 4000 6000 8000 10000\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nFig. 4. Simulated average system throughput of the proposed scheme as a function of the number of S–D pairs n and for optimized number of relays m.\nAlso shown are a genie upper bound and the lower bound 1\n4\nlogn.\n \n \nPSfrag replacements\nNumber of Nodes, n\nO\npt\nim\nal\nm\noptimal m\nlog n\n2 log 2\n+ 2\n0 2000 4000 6000 8000 10000\n5\n6\n7\n8\n9\nFig. 5. Optimal value of m that maximizes the average throughput, and log n\n2 log 2\n+ 2 curve (cf. Theorem 2)).\nthe scheduling operation and the throughput scaling? For\nexample, it is conceivable that the relays could be implemented\nas infrastructure nodes that are connected to a wired backbone.\nThis setup has been referred to as a hybrid network (see for\nexample [20] and references therein).\nWhen the relays are allowed to fully cooperate in decod-\ning/encoding, they can be considered to be a multi-antenna\narray. Accordingly, the first and second hops are equivalent\nto a MIMO MAC with receiver CSI, and a MIMO BC with\npartial transmitter CSI, respectively. Now, the scheduling in\nPhase 1 can be simplified. It is well-known that for the MIMO\nMAC, the sum-capacity can be achieved by allowing all\nusers to transmit. The receiver can retrieve the data via some\nsophisticated signal processing algorithm, e.g., MMSE-SIC\n(minimum-mean square estimator with successive interference\ncancelation) [21]. The optimal scaling in the large n and fixed\nm regime is given by m log logn. However, if we seek to\nachieve only linear scaling in m, it suffices to schedule any\nm source nodes for transmission. With high probability, the\nresulting m × m channel is well-conditioned, and a spatial\nmultiplexing gain of m is achieved [16]. In contrast, Phase 2\ndoes not benefit from the cooperation of relays. This is be-\n13\nTABLE I\nDEPENDENCE OF THROUGHPUT SCALING ON RELAY COOPERATION IN ENCODING/DECODING AND CSI KNOWLEDGE OF THE RELAY–DESTINATION\nLINK\nScenarioa Throughput Scaling of R1 Throughput Scaling of R2 Throughput Scaling of R\nCase 1 Θ(log n) Θ(logn) Θ(logn)\nCase 2 Θ(n) Θ(logn) Θ(logn)\nCase 3 Θ(n) Θ(n) Θ(n)\naCase 1: independent decoding/encoding at the relays; perfect CSI in the first hop and partial CSI in the second hop\nCase 2: cooperative decoding/encoding at the relays; perfect CSI in the first hop and partial CSI in the second hop\nCase 3: cooperative decoding/encoding at the relays; perfect CSI in the first hop and perfect CSI in the second hop\ncause, with only partial transmitter CSI, and since destination\nnodes are not allowed to collaborate, arbitrarily selecting m\ndestination nodes cannot yield a throughput linear in m [14].\nThe impact of relay cooperation on throughput scaling\nexhibits similar behavior to that demonstrated for scheduling.\nPhase 1 benefits from relay cooperation, and in principle\nR1 = Θ(n) is possible (note that the capacity of a n×n MIMO\nchannel scales linearly with n [22]); Phase 2 is still bounded by\nΘ(logn), and becomes the bottleneck of the two-hop scheme.\nThe reason for this is that, due to the lack of full CSI at the\nrelays, there is no way to generate more than Θ(logn) parallel\nchannels. The reader is referred to [14] for a discussion of the\nimpact of CSI knowledge on MIMO downlink channels.\nWe summarize the discussion of relay cooperation in Ta-\nble I. In the first two scenarios, we examine the impact of\ncooperation in decoding/encoding at the relays on system\nthroughput by fixing the CSI assumptions to perfect receiver\nCSI in the first hop and partial CSI in the second hop.\nSpecifically, case 1 corresponds to the setup considered in\nSection II, where the relays perform independent decoding\n(in the first hop) and independent encoding (in the second\nhop), and case 2 allows for cooperation among relays in\ndecoding/encoding. In the comparison, we also include the\noptimistic scenario, case 3, where the relays are assumed to\nhave full CSI knowledge of both the source–relay link, and the\nrelay–destination link and the relays are allowed to cooperate.\nIn this case, Θ(n) throughput is obtained in both hops, a result\nnot surprising from the MIMO theory [22], [23]. From the\ntable, one can readily identify that CSI plays a critical role\nin determining if linear throughput scaling is achievable. This\nobservation justifies our study on throughput scaling based on\nthe seemingly pessimistic, yet practical, assumptions on CSI\nknowledge in this paper.\nIt is important to point out that in the above discussion, the\nfocus is on CSI. In cases where perfect CSI is available at\nthe relays, but cooperative decoding/encoding is not available\n(e.g., due to nodes located randomly), different conclusions\ncan be drawn. For example, one can operate the two-hop\namplify-and-forward scheme [3] to achieve Θ(n1/2) through-\nput scaling. A detailed discussion of the case of perfect CSI\nbut no cooperation between relays is outside the scope of this\npaper. It is also important to point out that the discussion\napplies only to the underlying Rayleigh fading model. For\nother fading models, the opportunistic relaying scheme may\nexhibit a different scaling law. See [24] for discussions of\nthroughput scaling under more general fading models.\nB. Delay Considerations\nThere is always a tension between opportunistic scheduling\nand delay considerations [8]. The delay issue is more salient\nin the two-hop scheme than in the cellular setup [8], because\npackets transmitted by one particular source in Phase 1 must\nbe buffered at a relay, until that relay schedules the original\ndestination during Phase 2. While one can partially relieve\nthe problem by, say, prioritizing the destination in cases when\na relay receives multiple requests from multiple destinations\n(including the destination of interest, of course), the delay\nmay still be large. The detailed study of end-to-end delay is\ncurrently underway.\nVII. CONCLUSION\nIn this work, we have proposed an opportunistic relaying\nscheme that alleviates the demanding assumptions of central\nscheduling and CSI at transmitters. The scheme entails a two-\nhop communication protocol, in which sources can commu-\nnicate with destinations only through half-duplex relays. The\nkey idea is to schedule at each hop only a subset of nodes that\ncan benefit from multiuser diversity. To select the source and\ndestination nodes for each hop, relays operate independently\nwith receiver CSI only, and with an index-valued feedback to\nthe transmitter. The system throughput has been characterized\nfor the operating regime in which n is large and m is relatively\nsmall. In this case, the proposed scheme achieves a system\nthroughput of m/2 bits/s/Hz, while the upper bound with\nfull cooperation among relays and full CSI is (m/2) log logn.\nMoreover, we have further shown that, given that the product\nof the block duration and the system bandwidth scales as\nΩ(logn log logn), the achievable throughput scaling of the\nproposed decentralized scheme is given by Θ(logn), which\nis the optimal scaling even if centralized scheduling is al-\nlowed. Thus, operating the network in a decentralized fashion,\nwith only CSI at the receivers and low-rate feedback to the\ntransmitters, incurs no penalty. Finally, compared to the linear\nthroughput scaling results reported in the literature (see, e.g.,\n[5] and [13]) with more optimistic CSI assumptions, this work\nquantifies the price that one has to pay for not being able\nto mitigate interference. The delay behavior of the proposed\nopportunistic relaying scheme is left for future work.\nAPPENDIX A\nCHARACTERIZATION OF INTERFERENCE Y OF (4)\nIn this appendix, we characterize the statistical properties of\nthe interference term Y in (4). More specifically, it is shown\n14\n \n \nPSfrag replacements\nx\npd\nff\n(x\n)\nempirical pdf\nf(x) = e−x\nn = 10\nn = 20\nx\npd\nff\n(x\n)\nn = 40\nx\npd\nff\n(x\n)\nn = 100\nx\npd\nff\n(x\n)\n0.0 2.0 4.0 6.00.0 2.0 4.0 6.0\n0.0 2.0 4.0 6.00.0 2.0 4.0 6.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFig. 6. Empirical pdf f(xi|Xi is not the maximum) with n = 10, 20, 40 and 100 respectively and the pdf of a standard exponential random variable,\nf(x) = e−x, x ≥ 0.\nthat, asymptotically in n, each individual term that comprises\nY has an exponential distribution, and all interferers are\nasymptotically independent. It is also illustrated by numerical\nresults that these asymptotic trends are achieved quickly,\nenabling the approximation of Y as a chi-square random\nvariable with 2(m− 1) degrees of freedom.\nFor notional convenience, we denote the channel connec-\ntions from n sources to the relay as X1, . . . , Xn. According\nto the scheduling of Phase 1, for each time-slot, i.e., each\nrealization of X1, . . . , Xn, the desired signal strength is the\nmaximum among all connections. The interference term Y is\nthe summation of (m−1) out of the remaining (n−1) channel\nconnections.\nWe first show that each interferer is asymptotically expo-\nnentially distributed. By the law of total probability, for events\nB and A, we have\nPr[B] = Pr[B|A] Pr[A] + Pr[B|A] Pr[A], (20)\nwhere A denotes the complement of the event A. Now define\nthe event B as {Xi ≤ xi}, and A as {Xi is not the maximum}.\nThen we have for the cdfs\nFXi(xi) = FXi (xi|A) Pr[A] + FXi (xi|A) Pr[A]. (21)\nIn our i.i.d. model, by symmetry, each node has probability\nof 1/n to be the maximum, i.e., Pr[A] = 1 − 1/n. Thus, the\nabove equation can be written as\nFXi (xi) = FXi(xi|A) (1−\n1\nn\n)︸ ︷︷ ︸\n→1\n+FXi(xi|A)\n1\nn︸ ︷︷ ︸\n→0\n. (22)\nTherefore, we have\nFXi(xi|A)→ FXi(xi) as n→∞, (23)\nand thus, asymptotically, each interferer is still exponentially\ndistributed.\nWhile the above results are of an asymptotic nature,\nnumerical result shows that they hold for practical values\nof n as well. For example, in Fig. 6, the empirical pdf\nf(xi|Xi is not the maximum) is plotted together with the pdf\nof Exp(1), i.e., f(x) = e−x, x ≥ 0, for various values of n.\nIt is seen that the empirical pdf is well approximated by the\nstandard exponential distribution.\nNext, we show that the interferers are asymptotically\nindependent. Define A as {none of X1, . . . , Xm−1 is\nthe maximum}. The event A is then {at least one of\nX1, . . . , Xm−1 is the maximum}. Again, by the law of total\nprobability,\nF (x1, . . . , xm−1) = F (x1, . . . , xm−1|A) Pr[A]\n+ F (x1, . . . , xm−1|A) Pr[A].\n(24)\nDue to the underlying i.i.d. assumption, Pr[A] = (1−1/n)(1−\n1/(n−1)) · · · (1−1/(n−m+2))→ 1 and Pr[A]→ 0 when\nn is large and m small relative to n. Then it follows readily\nfrom (24) that in the regime of interest\nf(x1, . . . , xm−1|A)→ f(x1, . . . , xm−1) =\nm−1∏\ni=1\nf(xi). (25)\nTherefore, asymptotically, all interferers are independent.\nCombining the facts that, in the regime of interest, 1) each\ninterferer is exponentially distributed and 2) all interferers\nare independent, the aggregate interference Y can thus be\nmodelled as chi-square with 2(m − 1) degrees of freedom.\n15\nNumerical result shows that this approximation is accurate for\nvalues as low as n = 40.\nAPPENDIX B\nPROOF OF THEOREM 2\nHere, we prove Theorem 2 of Section IV-A. For conve-\nnience, the theorem is repeated below.\nTheorem 2: Under the assumption of independent encoding\nat the source nodes and independent decoding at the relay\nnodes, one cannot achieve lognlog 2+2 throughput with probability\napproaching one. Conversely, with probability approaching\none, (1−ǫ) log n2 log 2+2 throughput is achievable for all ǫ ∈ (0, 1).\nProof: The proof relies on the probabilistic method [25].\nThe basic idea of the probabilistic method is that in order\nto prove the existence of a structure with certain properties,\none defines an appropriate probability space of structures and\nthen shows that the desired properties hold in this space with\npositive probability. This method of proof has been seen in\nvarious subjects of information theory, for instance, see [26,\nCh. 8], which studies the bandwidth scaling problem in the\ncontext of spectrum sharing. The line of our proof follows\n[26].\nThe upper bound is established by the genie-aided schedul-\ning, which performs an exhaustive search for the maximum\nconcurrent successful transmissions. Specifically, in testing\nwhether m bits/s/Hz is achievable, the genie-aided scheme\nenumerates all m-element subset of source nodes and tests\nwhether the resulting m transmissions to the m relays are all\nsuccessful. According to the genie-aided scheme, we define\nthe probability space Ω =\n{\n(A, π) : A ⊂ {1, . . . , n}, |A| =\nm, π is any permutation on {1, . . . ,m}}, where A denotes\na random m-set of all n source nodes and π denotes any\npossible m-to-m mappings from m source nodes in A to m\nrelays. Let BπA be the event {all nodes in A can transmit\nsimultaneously and successfully under mapping rule π} and\nIπA the corresponding indicator random variable, i.e., IπA =\n1\n(\nγi,Rπ(i)\n1/ρ+\nP\nt∈A\nt6=i\nγt,Rπ(i)\n≥ 1, ∀i ∈ A\n)\n, where the subscript\nRπ(i) denotes the corresponding relay for source i under\nmapping rule π. For any π, we have {Rπ(i), ∀i ∈ A} :=\nR = {1, . . . ,m}. Finally, define the number of valid sets that\nsatisfy the SINR threshold as X(m) =\n∑\nA\n∑\nπ I\nπ\nA.\nThen\nE[IπA] = Pr[B\nπ\nA]\n= Pr\n[\nSINR\nP1\ni,Rπ(i) ≥ 1, ∀i ∈ A\n]\n=\n(\nPr\n[\nSINR\nP1\ni,Rπ(i) ≥ 1\n])m\n(26)\n= (pm)\nm, (27)\nwhere (26) follows from the fact that for i, j ∈ A, i 6= j,\nSINR\nP1\ni,Rπ(i) and SINR\nP1\nj,Rπ(j) are i.i.d. The term pm = 1−F (1)\nin (27) is the probability that a transmission is successful when\nthere are m concurrent transmissions, and F (·) is the cdf of\nthe SINR computed in (10).\nThe linearity of the expectation yields\nE [X(m)] =\n(\nn\nm\n)\nm! (pm)\nm. (28)\nThen, the upper bound is established by showing\nPr[X(m) ≥ 1] → 0 when m = log nlog 2 + 2. This can be seen\nfrom Markov’s inequality:\nPr[X(m) ≥ 1] ≤ E[X(m)]\n=\nn!\n(n−m)! (pm)\nm\n≤ (npm)m ≤ (ne−1/ρ2m−1 )m\n= em(logn−(m−1) log 2−1/ρ)\n≤ em(logn−(m−1) log 2). (29)\nNow substituting m = log nlog 2 + 2 into (29), we have\nPr[X(m) ≥ 1] ≤ e− logn+o(logn)\n= O\n(\n1\nn\n)\n. (30)\nWhat (30) tells us is that when m = lognlog 2 + 2, the proba-\nbility of finding a set of m nodes for concurrent successful\ntransmissions decreases to zero as n increases. Since the\ntransmission rate is fixed at 1 bit/s/Hz, it is equivalent to\nconcluding that, with probability approaching one, lognlog 2 + 2\nbits/s/Hz throughput is not achievable.\nNext we look at achievability. In proving the achievability\nresult, we consider a variant of the genie-aided scheme used\nabove. Here, the scheme divides the total n sources into m\ngroups Gi, i = 1, . . . ,m with each group having n/m sources.\nEach group is associated with one relay node. For example,\nas illustrated in Fig. 7, without loss of generality, we can\nlabel the total source nodes from 1 to n and assign sources\n{1, . . . , n/m} to G1, {n/m + 1, . . . , 2n/m} to G2, and so\non. In testing whether m concurrent successful transmissions\nare possible, each relay chooses one source from its own\ngroup.5 Following the scheme, we define the sample space\nΩ′ =\n{A : |A| = m,R(i) 6= R(j) ∀i, j ∈ A}, where R(i)\ndenotes the index of the relay associated with the group to\nwhich source i belongs. Also define IA as the indicator random\nvariable of the event {transmission from source i to relay R(i)\nis successful, ∀ i ∈ A}. Finally, let X ′(m) =∑A∈Ω′ IA.\nTo prove the achievability, we seek to find a lower bound\non Pr[X ′(m) ≥ 1], or equivalently, an upper bound on\nPr[X ′(m) = 0]. We need the following probabilistic tool from\n[26].\nLemma 4: Let µ = E[X ′(m)] and ∆ =∑\nA∈Ω′\n∑\nA′∈Ω′\nA∩A′ 6=0\nE[IAIA′ ]. Then,\nPr[X(m) = 0] ≤ e−µ\n2\n∆ . (31)\nProof: Following the explanation in the proof of [26,\nTh. 10], the proof of the lemma follows in a fairly straight-\nforward way from [26, Lemma 7]. We skip the details for the\nsake of saving space.\n5Similar scheme has been considered by Etkin [26, Ch. 8] in the context of\ncharacterizing the bandwidth scaling of spectrum sharing systems. Our setup\nis different from Etkin’s scheme in that the number of nodes in each group\nis a function of m, which is not the case in [26].\n16\n1\n2\n.\n.\n.\nn\nm\nG1\n.\n.\n.\n2n\nm\nG2\n.\n.\n.\n.\n.\n.\nn\nGm\nR\n1\n2\n.\n.\n.\nm\nFig. 7. Illustration of the genie-aided scheme used in the achievability proof.\nSource nodes are divided into m groups Gi, i = 1, . . . , m, each with n/m\nnodes. Each group associates with one relay node and the nodes in the group\nhave common receiver (i.e., the associated relay). A is formed by selecting\none node from each group.\nThe proof of achievability is more involved than that of\nthe upper bound. This is because IA’s are generally not\nindependent. In the upper bound proof, however, dependence\namong IA’s is irrelevant due to the linearity of the expectation.\nThe quantity ∆ in Lemma 4 is a measure of the pairwise\ndependence between the IA’s. Note that, in the case when\nIA’s are all independent, the lemma reduces to Pr[X ′(m) =\n0] = e−µ, a result which can be reached by direct probability\ncalculations.\nWe begin with µ:\nµ = E[X ′(m)] = E\n[∑\nA∈Ω′IA\n]\n=\n∑\nA∈Ω′E[IA] (32)\n=\n( n\nm\n)m\n(pm)\nm, (33)\nwhere (32) follows from linearity of expectation, and (33)\nis due to the fact that, for any A ∈ Ω′, all relays see\ni.i.d. channel realizations. The term pm = e\n−1/ρ\n2m−1 (cf. (10))\ndenotes the probability of successful decoding when there are\nm concurrent transmissions in total.\nTo compute ∆, let us start with computing the expectation\nof IAIA′ conditioned on {A ∈ Ω′,A′ ∈ Ω′, |A ∩ A′| = q}.\nIn the expressions (shown at the bottom of the page), (35)\nupper-bounds (34) by neglecting the interference coming from\nthe sources belonging to A′ ∩ A in the second term of\nthe product. In so doing, the two products\n∏\nk∈A 1(·) and∏\nℓ∈A′\\A 1(·) in (35) involve independent random variables\nnow and therefore are independent (note that this is not true\nin (34)). A minimal example is illustrated in Fig. 8. The upper-\nbounding in (35) can be thought of as reducing the number\nof concurrent transmissions from m to m− q by keeping the\nelements {t : t ∈ A′∩A} silent. The probability of successful\ntransmission when there are m− q concurrent transmissions,\ndenoted as pm−q, can be shown to be pm−q = e\n−1/ρ\n2m−q−1 .\n1\nA\ni\nR\n1\n2\n1\nA′\nj\nR\n1\n2\nFig. 8. Example of A = {1, i} and A′ = {1, j}, i 6= j. We can upper-bound\nthe SINR of source j in A′ as γj,2\n1/ρ+γ1,2\n≤\nγj,2\n1/ρ\n, which now is independent\nof the SINRs of source nodes in A.\nNow, we proceed with ∆. In particular, we have\n∆ =\n∑\nA∈Ω′\n∑\nA′∈Ω′\nA∩A′ 6=0\nE[IAIA′ ]\n=\nm∑\nq=1\n∑\nA∈Ω′\n∑\nA′∈Ω′\n|A∩A′|=q\nE\n[\nIAIA′\n∣∣∣ A∈Ω′A′∈Ω′\n|A∩A′|=q\n]\n=\nm∑\nq=1\n( n\nm\n)m(m\nq\n)( n\nm\n− 1\n)m−q\n(pm)\nm(pm−q)\nm−q.\nIn order to apply Lemma 4, we check ∆µ2 :\n∆\nµ2\n=\nm∑\nq=1\n(\nm\nq\n)(\nn\nm − 1\n)m−q(\nn\nm\n)m (pm−q)m−q(pm)m =\nm∑\nq=1\naq,\nwhere aq =\n(mq )(\nn\nm−1)\nm−q\n( nm )\nm\n(pm−q)\nm−q\n(pm)m\n. Now on defining bq =\nE\n[\nIAIA′\n∣∣∣ A∈Ω′A′∈Ω′\n|A∩A′|=q\n]\n= E\n[∏\nk∈A\n1\n(\nγk,R(k)\nσ2/P +\n∑\nt∈A\nt6=k\nγt,R(k)\n≥ 1\n) ∏\nℓ∈A′\n1\n(\nγℓ,R(ℓ)\nσ2/P +\n∑\nt∈A′\nt6=ℓ\nγt,R(ℓ)\n≥ 1\n)]\n≤ E\n[∏\nk∈A\n1\n(\nγk,R(k)\nσ2/P +\n∑\nt∈A\nt6=k\nγt,R(k)\n≥ 1\n) ∏\nℓ∈A′\\A\n1\n(\nγℓ,R(ℓ)\nσ2/P +\n∑\nt∈A′\nt6=ℓ\nγt,R(ℓ)\n≥ 1\n)]\n(34)\n≤ E\n[∏\nk∈A\n1\n(\nγk,R(k)\nσ2/P +\n∑\nt∈A\nt6=k\nγt,R(k)\n≥ 1\n) ∏\nℓ∈A′\\A\n1\n(\nγℓ,R(ℓ)\nσ2/P +\n∑\nt∈A′\\A\nt6=ℓ\nγt,R(ℓ)\n≥ 1\n)]\n(35)\n= E\n[∏\nk∈A\n1\n(\nγk,R(k)\nσ2/P +\n∑\nt∈A\nt6=k\nγt,R(k)\n≥ 1\n)]\nE\n[ ∏\nℓ∈A′\\A\n1\n(\nγℓ,R(ℓ)\nσ2/P +\n∑\nt∈A′\\A\nt6=ℓ\nγt,R(ℓ)\n≥ 1\n)]\n= (pm)\nm(pm−q)\nm−q (36)\n17\naq+1/aq, we have that bq = (m−q)e\n1/ρ\n( nm−1)(q+1)\n22(m−q−1) decreases\nwith q, bq ≤ b1. Therefore, aq ≤ bq−11 a1.\nOn setting m = (1 − ǫ) logn2 log 2 + 2 for any ǫ ∈ (0, 1), we\nhave\nb1 =\n(m− 1)e1/ρ\n2( nm − 1)\n22(m−2)\n= elog(m−1)−log(\nn\nm−1)+2(m−2) log 2+1/ρ−log 2\n= e−ǫ log n+o(logn)\n= O\n( 1\nnǫ\n)\n.\nFurthermore, with m = (1− ǫ) logn2 log 2 + 2, we have\n∆\nµ2\n=\nm∑\nq=1\naq ≤ a1\nm∑\nq=1\nbq−11 ≤\na1\n1− b1\n=\n(\nm\n1\n)\n( nm − 1)m−1\n( nm )\nm\n(pm−1)\nm−1\n(pm)m\n1\n1− b1\n<\nm2\nn\n(pm−1)\nm−1\n(pm)m\n1\n1− b1\n=\nm2\nn\ne1/ρ 22(m−1)\n1− b1\n= e2 logm−logn+2(m−1) log 2−log(1−b1)+1/ρ\n= e−ǫ logn+o(logn)\n= O\n( 1\nnǫ\n)\n.\nFinally, Lemma 4 yields\nPr[X(m) = 0] < e−n\nǫ\n. (37)\nIn words, (37) tells us is that when m = (1−ǫ) logn2 log 2+2 for\nany ǫ ∈ (0, 1), the probability of not finding a set of m nodes\nfor concurrent successful transmissions decreases to zero as\nn increases. In other words, the probability of finding m\nconcurrent successful transmissions with m = (1−ǫ) log n2 log 2+2\napproaches 1. Again, since the transmission rate is fixed at 1\nbit/s/Hz, it is equivalent to concluding that, with probability\napproaching one, m = (1 − ǫ) logn2 log 2 + 2 bits/s/Hz throughput\nis achievable by the genie-aided scheme.\nThis achievability result, together with the upper bound (30),\ncompletes the proof of the theorem.\nACKNOWLEDGMENT\nThe authors wish to thank Dr. Raul Etkin for helpful discus-\nsions on the proof of Theorem 2, and for pointing out (35) in\nparticular. The authors would also like to thank the Associate\nEditor Sennur Ulukus and the anonymous reviewers for their\nthoughtful comments and valuable suggestions which helped\nus improve the initial submitted version of this manuscript.\nREFERENCES\n[1] R. Gowaikar, B. M. Hochwald, and B. Hassibi, “Communication over\na wireless network with random connections,” IEEE Trans. Inf. Theory,\nvol. 52, no. 7, pp. 2857–2871, Jul. 2006.\n[2] P. Gupta and P. R. Kumar, “The capacity of wireless networks,” IEEE\nTrans. Inf. Theory, vol. 46, no. 2, pp. 388–404, Mar. 2000.\n[3] A. F. Dana and B. Hassibi, “On the power efficiency of sensory and\nad hoc wireless networks,” IEEE Trans. Inf. Theory, vol. 52, no. 7, pp.\n2890–2914, Jul. 2006.\n[4] V. I. Morgenshtern and H. Bo¨lcskei, “Crystallization in large wireless\nnetworks,” IEEE Trans. Inf. Theory, vol. 53, no. 10, pp. 3319–3349,\nOct. 2007.\n[5] A. ¨Ozgu¨r, O. Le´veˆque, and D. N. C. Tse, “Hierarchical cooperation\nachieves optimal capacity scaling in ad hoc networks,” IEEE Trans. Inf.\nTheory, vol. 53, no. 10, pp. 3549–3572, Oct. 2007.\n[6] R. Knopp and P. Humblet, “Information capacity and power control\nin single cell multiuser communications,” in Proc. IEEE Int. Conf.\nCommunications, vol. 1, Seattle, WA, Jun. 1995, pp. 331–335.\n[7] D. N. C. Tse, “Optimal power allocation over parallel Gaussian broad-\ncast channels,” in Proc. IEEE Int. Symp. Information Theory, Ulm,\nGermany, Jun. 1997, p. 27.\n[8] P. Viswanath, D. N. C. Tse, and R. Laroia, “Opportunistic beamforming\nusing dumb antennas,” IEEE Trans. Inf. Theory, vol. 48, no. 6, pp. 1277–\n1294, Jun. 2002.\n[9] P. Bender, P. Black, M. Grob, R. Padovani, N.Sindhushayana, and\nA. Viterbi, “CDMA/ HDR: A bandwidth–efficient high–speed wireless\ndata service for nomadic users,” IEEE Commun. Mag., vol. 38, pp. 70–\n78, Jul. 2000.\n[10] A. Bletsas, A. Khisti, D. P. Reed, and A. Lippman, “A simple cooperative\ndiversity method based on network path selection,” IEEE J. Sel. Areas\nCommun., vol. 24, no. 3, pp. 659–672, Mar. 2006.\n[11] L. Zheng and D. N. C. Tse, “Diversity and multiplexing: a fundamental\ntradeoff in multiple-antenna channels,” IEEE Trans. Inf. Theory, vol. 49,\nno. 5, pp. 1073–1096, May 2003.\n[12] M. Grossglauser and D. N. C. Tse, “Mobility increases the capacity of\nad hoc wireless networks,” IEEE/ACM Trans. Netw., vol. 10, no. 4, pp.\n477–486, Aug. 2002.\n[13] V. R. Cadambe and S. A. Jafar, “Interference alignment and degrees of\nfreedom of the K–user interference channel,” IEEE Trans. Inf. Theory,\nvol. 54, no. 8, pp. 3425–3441, Aug. 2008.\n[14] M. Sharif and B. Hassibi, “On the capacity of MIMO broadcast channels\nwith partial side information,” IEEE Trans. Inf. Theory, vol. 51, no. 2,\npp. 506–522, Feb. 2005.\n[15] J. G. Proakis, Digital Communications, 4th ed. New York: McGraw-\nHill, 2000.\n[16] D. N. C. Tse and P. Viswanath, Fundamentals of Wireless Communica-\ntion. Cambridge, U.K.: Cambridge Univ. Press, 2005.\n[17] H. Weingarten, Y. Steinberg, and S. Shamai (Shitz), “The capacity region\nof the Gaussian multiple-input multiple-output broadcast channel,” IEEE\nTrans. Inf. Theory, vol. 52, no. 9, pp. 3936–3964, Sep. 2006.\n[18] N. Jindal, S. Vishwanath, and A. Goldsmith, “On the duality of Gaussian\nmultiple-access and broadcast channels,” IEEE Trans. Inf. Theory,\nvol. 50, no. 5, pp. 768–783, May 2004.\n[19] S. Cui and A. M. Haimovich, “Multiuser diversity in wireless ad hoc\nnetworks,” in Proc. IEEE Global Communications Conf., New Orleans,\nLA, Nov.-Dec. 2008.\n[20] A. Zemlianov and G. de Veciana, “Capacity of ad hoc wireless networks\nwith infrastructure support,” IEEE J. Sel. Areas Commun., vol. 23, no. 3,\npp. 657–667, Mar. 2005.\n[21] M. K. Varanasi and T. Guess, “Optimum decision feedback multiuser\nequalization and successive decoding achieves the total capacity of\nthe Gaussian multiple-access channel,” in Proc. of Asilomar Conf. on\nSignals, Systems, and Computers, Pacific Grove, CA, Nov. 1997, pp.\n1405–1409.\n[22] ˙I. E. Telatar, “Capacity of multi-antenna Gaussian channels,” Europ.\nTrans. Telecommun., vol. 10, pp. 585–595, Nov. 1999.\n[23] G. J. Foschini, “Layered space-time architecture for wireless commu-\nnication in a fading environment when using multi-element antennas,”\nBell Labs. Tech. J, vol. 1, no. 2, pp. 41–59, 1996.\n[24] S. Cui, A. M. Haimovich, O. Somekh, H. V. Poor, and S. Shamai (Shitz),\n“Throughput scaling of wireless networks with random connections,”\nIEEE Trans. Inf. Theory, Sep. 2008, submitted for publication. [Online].\nAvailable: http://arxiv.org/pdf/0809.4019\n[25] N. Alon and J. H. Spencer, The Probabilistic Method, 2nd ed. New\nYork: Wiley, 2000.\n[26] R. Etkin, “Spectrum sharing: Fundamental limits, scaling laws, and self-\nenforcing protocols,” Ph.D. dissertation, Univ. California, Berkeley, Dec.\n2006.\n",
            "id": 576571,
            "identifiers": [
                {
                    "identifier": "189673013",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "0712.1169",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "2114825384",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "1972402",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:arxiv.org:0712.1169",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "10.1109/tit.2009.2030435",
                    "type": "DOI"
                }
            ],
            "title": "Opportunistic Relaying in Wireless Networks",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2114825384",
            "oaiIds": [
                "oai:arxiv.org:0712.1169"
            ],
            "publishedDate": "2009-07-24T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/0712.1169"
            ],
            "updatedDate": "2021-04-23T04:08:52",
            "yearPublished": 2009,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "0018-9448"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/0712.1169"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/576571"
                }
            ]
        },
        {
            "acceptedDate": "2009-10-08T00:00:00",
            "arxivId": "0910.1217",
            "authors": [
                {
                    "name": "Bogdan Aman"
                },
                {
                    "name": "Erik de Vink"
                },
                {
                    "name": "Gabriel Ciobanu"
                },
                {
                    "name": "Ion Petre"
                },
                {
                    "name": "Ralph-Johan Back"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/194545747",
                "https://api.core.ac.uk/v3/outputs/26625854"
            ],
            "createdDate": "2012-04-13T14:17:29",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 645,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/645",
                    "logo": "https://api.core.ac.uk/data-providers/645/logo"
                }
            ],
            "depositedDate": "2009-10-08T00:00:00",
            "abstract": "A feature of current membrane systems is the fact that objects and membranes\nare persistent. However, this is not true in the real world. In fact, cells and\nintracellular proteins have a well-defined lifetime. Inspired from these\nbiological facts, we define a model of systems of mobile membranes in which\neach membrane and each object has a timer representing their lifetime. We show\nthat systems of mutual mobile membranes with and without timers have the same\ncomputational power. An encoding of timed safe mobile ambients into systems of\nmutual mobile membranes with timers offers a relationship between two\nformalisms used in describing biological systems",
            "documentType": "research",
            "doi": "10.4204/eptcs.6.1",
            "downloadUrl": "http://arxiv.org/abs/0910.1217",
            "fieldOfStudy": "computer science",
            "fullText": "R.J.Back, I.Petre, E. de Vink (Eds.): Computational\nModels for Cell Processes (CompMod 2009)\nEPTCS 6, 2009, pp. 1–15, doi:10.4204/EPTCS.6.1\nMutual Mobile Membranes with Timers∗\nBogdan Aman\nRomanian Academy, Institute of Computer Science\nA.I.Cuza University of Ias¸i, Romania\nbaman@iit.tuiasi.ro\nGabriel Ciobanu\nRomanian Academy, Institute of Computer Science\nA.I.Cuza University of Ias¸i, Romania\ngabriel@info.uaic.ro\nA feature of current membrane systems is the fact that objects and membranes are persistent. How-\never, this is not true in the real world. In fact, cells and intracellular proteins have a well-defined\nlifetime. Inspired from these biological facts, we define a model of systems of mobile membranes in\nwhich each membrane and each object has a timer representing their lifetime. We show that systems\nof mutual mobile membranes with and without timers have the same computational power. An en-\ncoding of timed safe mobile ambients into systems of mutual mobile membranes with timers offers\na relationship between two formalisms used in describing biological systems.\n1 Introduction\nMembrane systems are essentially parallel and nondeterministic computing models inspired by the com-\npartments of eukaryotic cells and their biochemical reactions. The structure of the cell is represented\nby a set of hierarchically embedded regions, each one delimited by a surrounding boundary (called\nmembrane), and all of them contained inside an external special membrane called skin. The molecular\nspecies (ions, proteins, etc.) floating inside cellular compartments are represented by multisets of objects\ndescribed by means of symbols or strings over a given alphabet. The objects can be modified or commu-\nnicated between adjacent compartments. Chemical reactions are represented by evolution rules which\noperate on the objects, as well as on the compartmentalized structure (by dissolving, dividing, creating,\nor moving membranes).\nA membrane system can perform computations in the following way: starting from an initial con-\nfiguration which is defined by the multiset of objects initially placed inside the membranes, the system\nevolves by applying the evolution rules of each membrane in a nondeterministic and maximally parallel\nmanner. A rule is applicable when all the objects which appear in its left hand side are available in the\nregion where the rule is placed. The maximally parallel way of using the rules means that in each step, in\neach region of the system, we apply a maximal multiset of rules, namely a multiset of rules such that no\nfurther rule can be added to the set. A halting configuration is reached when no rule is applicable. The\nresult is represented by the number of objects from a specified membrane.\nSeveral variants of membrane systems are inspired by different aspects of living cells (symport and\nantiport-based communication through membranes, catalytic objects, membrane charge, etc.). Their\ncomputing power and efficiency have been investigated using the approaches of formal languages and\ngrammars, register machines and complexity theory. Membrane systems (also called P systems) are\npresented together with many variants and examples in [33]. Several applications of these systems are\npresented in [20]. An updated bibliography can be found at the P systems web page [35].\nA first attempt to define mobile P systems is presented in [34] where the rules are similar to those of\nmobile ambients [10]. Inspired by the operations of endocytosis and exocytosis, namely moving a mem-\nbrane inside a neighbouring membrane (endocytosis) and moving a membrane outside the membrane\n∗The research for this paper was partially supported by CNCSIS IDEI 402/2007 and CNCSIS TD 345/2008.\n2 Mutual Mobile Membranes with Timers\nwhere it is placed (exocytosis), the P systems with mobile membranes are introduced in [26] as a variant\nof P systems with active membranes [33].\nThe systems of mutual mobile membranes represent a variant of P systems with mobile membranes in\nwhich the endocytosis and exocytosis work whenever the involved membranes “agree” on the movement;\nthis agreement is described by using dual objects a and a in the corresponding rules. The operations gov-\nerning the mobility of the systems of mutual mobile membranes are called mutual endocytosis (mutual\nendo), and mutual exocytosis (mutual exo).\nThe structure of the paper is as follows. In Section 2 we give a formal definition of the new class\nof mutual mobile membranes together with their biological motivation. Section 3 contains the formal\ndefinition of systems of mutual mobile membranes with timers, a variant of systems of mutual mobile\nmembranes in which timers are attached to each object and each membrane. Section 4 contains some\nresults which show that we do not obtain more computational power by adding timers to objects and\nmembranes into a system of mutual mobile membranes. Section 5 presents a translation of timed safe\nmobile ambients into systems of mutual mobile membranes with timers. Related work, conclusion and\nreferences finalize the paper.\n2 Systems of Mutual Mobile Membranes\nEndocytosis and exocytosis are general terms which refer to the process by which anything is taken into\nor expelled from the cell through the action of vacuoles. Exocytosis involves the movement of materials\nout of the cytoplasm of the cell using ATP energy. In exocytosis, a vesicle (vacuole) migrates to the\nmembrane inner surface and fuses with the cell membrane. This process of exocytosis is how the cells\nof glands producing proteins (enzyme and steroids) export molecules for use in other areas of the body\n(for example, enzymes made in the pancreas act in the small intestine). Endocytosis of large particles is\ncalled phagocytosis; in our bodies, various types of white blood cells ingest foreign particles and bacteria\nby phagocytosis. Endocytosis of small particles is called pinocytosis; an example of pinocytosis is the\nabsorption of small nutrient particles into the small intestine.\nExocytosis and endocytosis operations were considered in terms of process algebra by Cardelli [11],\nwith careful biological motivation and formulation, while in terms of membrane computing, by Cardelli\nand Pa˘un [12].\nFigure 1: Receptor-Mediated Endocytosis [36]\nWe analyze the processes of endocytosis and exocytosis in order to define appropriate operations for\nmobile membranes. In receptor-mediated endocytosis, specific reactions at the cell surface trigger the\nuptake of specific molecules [36]. We present this process by an example. In such an endocytosis, a cell\ntakes in a particle of low-density lipoprotein (LDL) from the outside. To do this, the cell uses receptors\nB. Aman and G. Ciobanu 3\nwhich specifically recognize and bind to the LDL particle. An LDL particle contains one thousand or\nmore cholesterol molecules at its core. A monolayer of phospholipids surrounds the cholesterol and it is\nembedded with proteins called apo-B. These apo-B proteins are specifically recognized by receptors in\nthe cell membrane. The receptors in the coated pit bind to the apo-B proteins on the LDL particle. The\npit is re-enforced by a lattice like network of proteins called clathrin. Additional clathrin molecules are\nthen added to the lattice; eventually engulfing the LDL particle entirely.\nFigure 2: SNARE-Mediated Exocytosis\nSNARE-mediated exocytosis is the movement of materials out of a cell via vesicles [1]. SNARES\n(Soluble NSF Attachment Protein Receptor)) located on the vesicles (v-SNARES) and on the target\nmembranes (t-SNARES) interact to form a stable complex which holds the vesicle very close to the\ntarget membrane.\nEndocytosis and exocytosis are modelled by mobile membranes [26]. Based on the previous exam-\nples (Figure 1 and Figure 2) where the endocytosis is triggered by the “agreement” between specific\nreceptors and LDL particles and exocytosis by the agreement of SNARES, we introduced in [5] the mu-\ntual mobile membranes. In systems of mutual mobile membranes, any movement takes place only if\nthe involved membranes agree on the movement, and this agreement is described by means of objects a\nand co-objects a present in the membranes involved in such a movement. An object a marks the active\npart of the movement, and an object a marks the passive part. The duality relation is distributive over\na multiset, namely u = a1 . . .an for u = a1 . . .an. The motivation for introducing the systems of mutual\nmobile membranes comes both from biology (e.g., receptor-mediated endocytosis), and from theoretical\ncomputer science, namely for defining models closer to the biological reality.\nFor an alphabet V = {a1, . . . ,an}, we denote by V ∗ the set of all strings over V ; λ denotes the empty\nstring and V+ = V ∗\\{λ}. A multiset over V is represented by a string over V (together with all its\npermutations), and each string precisely identifies a multiset.\nDefinition 1. A system of n ≥ 1 mutual mobile membranes is a construct\n∏ = (V,H,µ ,w1, . . . ,wn,R, iO)\nwhere:\n1. V is an alphabet (its elements are called objects);\n2. H is a finite set of labels for membranes;\n3. µ ⊂H×H describes the membrane structure, such that (i, j)∈ µ denotes that a membrane labelled\nby j is contained into a membrane labelled by i; we distinguish the external membrane (usually\n4 Mutual Mobile Membranes with Timers\ncalled the “skin” membrane) and several internal membranes; a membrane without any other\nmembrane inside it is said to be elementary;\n4. w1, . . . ,wn ∈V ∗ are multisets of objects placed in the n regions of µ;\n5. iO is the output membrane;\n6. R is a finite set of developmental rules of the following forms:\nmutual endocytosis\n(a) [uv]h[uv′]m → [ [w]hw′]m for h,m ∈ H,u,u ∈V+,v,v′,w,w′∈V ∗;\nAn elementary membrane labelled h enters the adjacent membrane labelled m under the\ncontrol of the multisets of objects u and u. The labels h and m remain unchanged during this\nprocess; however the multisets of objects uv and uv′ are replaced with the multisets of objects\nw and w′, respectively.\nmutual exocytosis\n(b) [uv′[uv]h]m → [w]h[w′]m for h,m ∈ H,u,u ∈V+,v,v′,w,w′∈V ∗;\nAn elementary membrane labelled h exits a membrane labelled m, under the control of the\nmultisets of objects u and u. The labels of the two membranes remain unchanged, but the\nmultisets of objects uv and uv′ are replaced with the multisets of objects w and w′, respec-\ntively.\nThe rules are applied according to the following principles:\n1. All rules are applied in parallel; the rules, the membranes, and the objects are chosen nondetermin-\nistically, but in such a way that the parallelism is maximal; this means that in each step we apply a\nset of rules such that no further rule can be added to the set.\n2. The membrane m from the rules of type (a) and (b) is said to be passive (identified by the use of u),\nwhile the membrane h is said to be active (identified by the use of u). In any step of a computation,\nany object and any active membrane can be involved in one rule at most, while passive membranes\nare not considered to be involved in the use of the rules (hence they can be used by several rules at\nthe same time as passive membranes).\n3. When a membrane is moved across another membrane, by endocytosis or exocytosis, its whole\ncontents (its objects) are moved.\n4. If a membrane exits the system (by exocytosis), then its evolution stops.\n5. All objects and membranes which do not evolve at a given step (for a given choice of rules which\nis maximal) are passed unchanged to the next configuration of the system.\nBy using the rules in this way, we can describe transitions among the configurations of the system. Some\nexamples on how rules are applied can be found in [5].\n3 Mutual Mobile Membranes with Timers\nThe evolution of complicated real systems frequently involves various interdependence among compo-\nnents. Some mathematical models of such systems combine both discrete and continuous evolutions on\nmultiple time scales with many orders of magnitude. For example, in nature the molecular operations\nof a living cell can be thought of such a dynamical system. The molecular operations happen on time\nB. Aman and G. Ciobanu 5\nscales ranging from 10−15 to 104 seconds, and proceed in ways which are dependent on populations of\nmolecules ranging in size from as few as approximately 101 to approximately as many as 1020. Molecular\nbiologists have used formalisms developed in computer science (e.g. hybrid Petri nets) to get simplified\nmodels of portions of these transcription and gene regulation processes. According to [28]:\n(i) “the life span of intracellular proteins varies from as short as a few minutes for mitotic cyclins,\nwhich help regulate passage through mitosis, to as long as the age of an organism for proteins in\nthe lens of the eye.”\n(ii) “Most cells in multicellular organisms . . . carry out a specific set of functions over periods of days\nto months or even the lifetime of the organism (nerve cells, for example).”\nIt is obvious that timers play an important role in the biological evolution. We use an example from\nthe immune system.\nExample 1 ([25]). T-cell precursors arriving in the thymus from the bone marrow spend up to a week\ndifferentiating there before they enter a phase of intense proliferation. In a young adult mouse the thymus\ncontains around 108 to 2× 108 thymocytes. About 5× 107 new cells are generated each day; however,\nonly about 106 to 2× 106 (roughly 2− 4%) of these will leave the thymus each day as mature T cells.\nDespite the disparity between the numbers of T cells generated daily in the thymus and the number\nleaving, the thymus does not continue to grow in size or cell number. This is because approximately 98%\nof the thymocytes which develop in the thymus also die within the thymus.\nInspired from these biological facts, we add timers to objects and membranes. We use a global clock\nto simulate the passage of time in a membrane system.\nDefinition 2. A system of n ≥ 1 mutual mobile membranes with timers is a construct\nΠ = (V,H,µ ,w1, . . . ,wn,R,T, iO)\nwhere:\n1. V , H, µ , w1, . . . ,wn, iO are as in Definition 1.\n2. T ⊆ {∆t | t ∈ N} is a set of timers assigned to membranes and objects of the initial configuration;\na timer ∆t indicates that the resource is available only for a determined period of time t;\n3. R is a finite set of developmental rules of the following forms:\nobject time-passing\n(a) a∆t → a∆(t−1), for all a ∈V and t > 0\nIf an object a has a timer t > 0, then its timer is decreased.\nobject dissolution\n(b) a∆0 → λ , for all a ∈V\nIf an object a has its timer equal to 0, then the object is replaced with the empty multiset λ ,\nand so simulating the degradation of proteins.\nmutual endocytosis\n(c) [u∆t˜u v∆t˜v ]∆thh [u ∆t˜u v\n′∆t˜v′ ]∆tmm → [ [w\n∆t˜w ]\n∆(th−1)\nh w\n′∆t˜w′ ]∆tmm for h,m ∈H,u,u ∈V+,v,v′,w,w′∈V ∗ and\nall timers are greater than 0;\nFor a multiset of objects u, t˜u is a multiset of positive integers representing the timers of ob-\njects from u. An elementary membrane labelled h enters the adjacent membrane labelled m\nunder the control of the multisets of objects u and u. The labels h and m remain unchanged\n6 Mutual Mobile Membranes with Timers\nduring this process; however the multisets of objects uv and uv′ are replaced with the multi-\nsets of objects w and w′, respectively. If an object a from the multiset uv has the timer ta, and\nis preserved in the multiset w, then its timer is now ta−1. If there is an object which appears\nin w but it does not appear in uv, then its timer is given according to the right hand side of\nthe rule. Similar reasonings for the multisets uv′ and w′. The timer th of the active membrane\nh is decreased, while the timer tm of the passive membrane m remains the same in order to\nallow being involved in other rules.\nmutual exocytosis\n(d) [u ∆t˜u v′∆t˜v′ [u∆t˜u v∆t˜v ]∆thh ]∆tmm → [w∆t˜w ]∆(th−1)h [w\n′∆t˜w′ ]∆tmm for h,m ∈H,u,u ∈V+,v,v′,w,w′∈V ∗ and\nall timers are greater than 0;\nAn elementary membrane labelled h exits a membrane labelled m, under the control of the\nmultisets of objects u and u. The labels of the two membranes remain unchanged, but the mul-\ntisets of objects uv and uv′ are replaced with the multisets of objects w and w′, respectively.\nThe notations and the method of decreasing the timers are similar as for the previous rule.\nmembrane time-passing\n(e) [ ]∆th → [ ]∆(t−1)h , for all h ∈ H\nFor each membrane which did not participate as an active membrane in a rule of type (c) or\n(d), if its timer is t > 0, this timer is decreased.\nmembrane dissolution\n(f) [ ]∆0h → [δ ]∆0h , for all h ∈ H;\nA membrane labelled h is dissolved when its timer reaches 0.\nThese rules are applied according to the following principles:\n1. All the rules are applied in parallel: in a step, the rules are applied to all objects and to all mem-\nbranes; an object can only be used by one rule and is nondeterministically chosen (there is no\npriority among rules), but any object which can evolve by a rule of any form, should evolve.\n2. The membrane m from the rules of type (c)− (d) is said to be passive (marked by the use of u),\nwhile the membrane h is said to be active (marked by the use of u). In any step of a computation,\nany object and any active membrane can be involved in at most one rule, while passive membranes\nare not considered involved in the use of rules (c) and (d), hence they can be used by several rules\n(c) and (d) at the same time. Finally rule (e) is applied to passive membranes and other unused\nmembranes; this indicates the end of a time-step.\n3. When a membrane is moved across another membrane, by endocytosis or exocytosis, its whole\ncontents (its objects) are moved.\n4. If a membrane exits the system (by exocytosis), then its evolution stops.\n5. An evolution rule can produce the special object δ to specify that, after the application of the rule,\nthe membrane where the rule has been applied has to be dissolved. After dissolving a membrane,\nall objects and membranes previously contained in it become contained in the immediately upper\nmembrane.\n6. The skin membrane has the timer equal to ∞, so it can never be dissolved.\n7. If a membrane or object has the timer equal to ∞, when applying the rules simulating the passage\nof time we use the equality ∞−1 = ∞.\nB. Aman and G. Ciobanu 7\n4 Mutual Mobile Membranes with and without Timers\nThe following results describing some relationships between systems of mutual mobile membranes with\ntimers and systems of mutual mobile membranes without timers.\nProposition 1. For every systems of n mutual mobile membranes without timers there exists a systems of\nn mutual mobile membrane with timers having the same evolution and output.\nProof (Sketch). It is easy to prove that the systems of mutual mobile membranes with timers includes\nthe systems of mutual mobile membranes without timers, since we can assign ∞ to all timers appearing\nin the membrane structure and evolution rules.\nA somehow surprising result is that mutual mobile membranes with timers can be simulated by\nmutual mobile membrane without timers.\nProposition 2. For every systems of n mutual mobile membranes with timers there exists a systems of n\nmutual mobile membrane without timers having the same evolution and output.\nProof. We use the notation rhs(r) to denote the multisets which appear in the right hand side of a rule r.\nThis notation is extended naturally to multisets of rules: given a multiset of rules R, the right hand side\nof the multiset rhs(R) is obtained by adding the right hand sides of the rules in the multiset, considered\nwith their multiplicities.\nEach object a ∈V from a system of mutual mobile membranes with timers has a maximum lifetime\n(we denote it by li f etime(a)) which can be calculated as follows:\nli f etime(a) = max({t | a∆t ∈ wt˜ii ,1 ≤ i≤ n}∪{t | a∆t ∈ rhs(R)})\nIn what follows we present the steps which are required to build a systems of mutual mobile mem-\nbranes without timers starting from a system of mutual mobile membranes with timers, such that both\nprovide the same result and have the same number of membranes.\n1. A membrane structure from a system of mutual mobile membrane with timers\nmem∆tmem\nw∆t˜\nis translated into a membrane structure of a system of mutual mobile membranes without timers\nin the following way\nmem\nw b˜w 0\nbmem 0\nThe timers of elements from a system of mutual mobile membranes with timers are simulated\nusing some additional objects in the corresponding system of mutual mobile membranes without\ntimers, as we show at the next steps of the translation. The object bmem 0 placed inside the mem-\nbrane labelled mem is used to simulate the passage of time for the membrane. The initial multiset\nof objects w∆t˜ from membrane mem in the system of mutual mobile membranes with timers is\n8 Mutual Mobile Membranes with Timers\ntranslated into the multiset w inside membrane mem in the corresponding system of mutual mobile\nmembranes without timers together with a multiset of objects b˜w 0. The multiset b˜w 0 is constructed\nas follows: for each object a ∈ w, an object ba 0 is added in membrane mem in order to simulate\nthe passage of time.\n2. The rules a∆t → a∆(t−1), a ∈V , 0 < t ≤ li f etime(a) from the system of mutual mobile membranes\nwith timers can be simulated in the system of mutual mobile membranes without timers using the\nfollowing rules:\na ba t → a ba (t+1), for all a ∈V and 0≤ t ≤ li f etime(a)−1\nThe object ba t is used to keep track of the units of time t which have passed since the object a\nwas created. This rule simulates the passage of a unit of time from the lifetime of object a in\nthe system of mutual mobile membranes with timers, by increasing the second subscript of\nthe object ba t in the system of mutual mobile membranes without timers.\n3. The rules a∆0 → λ , a ∈V from the system of mutual mobile membranes with timers can be simu-\nlated in the system of mutual mobile membranes without timers using the following rules:\naba ta → λ for all a ∈V such that ta = li f etime(a)\nIf an object ba ta has the second subscript equal with li f etime(a) in the system of mutual\nmobile membranes without timers, it means that the timer of object a is 0 in the corresponding\nsystem of mutual mobile membranes with timers. In this case, the objects ba ta and a are\nreplaced by λ , thus simulating that the object a disappears together with its timer in the\nsystem of mutual mobile membranes with timers.\n4. The rules [u∆t˜u v∆t˜v ]∆thh [u ∆t˜u v\n′∆t˜v′ ]∆tmm → [ [w\n∆t˜w ]\n∆(th−1)\nh w\n′∆t˜w′ ]∆tmm , h,m ∈ H,u,u ∈ V+,v,v′,w,w′∈ V ∗\nwith all the timers greater than 0, from the system of mutual mobile membranes with timers can\nbe simulated in the system of mutual mobile membranes without timers using the following rules:\n[u b˜u t1v b˜v t2 bh t3]h[u b˜u t4 v′b˜v′ t5 bh t6]m → [ [w b˜w t7bh (t3+1)]hw b˜w′ t8 bh (t6+1)]m, where\nh,m ∈ H,u,u ∈V+,v,v′,w,w′∈V ∗ and for each ba j we have that 0 ≤ j ≤ li f etime(a)−1.\nA multiset b˜u t1 consists of all objects ba j, where a is an object from the multiset u. If an\nobject a from the multiset uv has its timer ta and it appears in the multiset w, then its timer\nbecomes ta − 1. If there is an object which appears in w but it is not in uv, then its timer is\ngiven according to the right hand side of the rule. Similar reasonings are also true for the\nmultisets uv′ and w′. The timer of the active membrane h is increased (object bh t3 is replaced\nby bh (t3+1)), while the timer of the passive membrane m remains the same in order to allow\nbeing used in other rules.\n5. The rules [u ∆t˜u v′∆t˜v′ [u∆t˜u v∆t˜v ]∆thh ]∆tmm → [w∆t˜w ]\n∆(th−1)\nh [w\n′∆t˜w′ ]∆tmm , h,m ∈ H,u,u ∈ V+,v,v′,w,w′∈ V ∗\nwith all the timers greater than 0, from the system of mutual mobile membranes with timers can\nbe simulated in the system of mutual mobile membranes without timers using the following rules:\n[u b˜u t4 v′b˜v′ t5 bh t6[u b˜u t1v b˜v t2 bh t3]h]m → [w b˜w t7bh (t3+1)]h[w b˜w′ t8 bh (t6+1)]m, where h,m ∈\nH,u,u ∈V+,v,v′,w,w′∈V ∗ and for each ba j we have that 0≤ j ≤ li f etime(a)−1.\nThe way these rules work is similar to the previous case.\n6. The rules [ ]∆th → [ ]\n∆(t−1)\nh from the system of mutual mobile membranes with timers can be simu-\nlated in the system of mutual mobile membranes without timers using the following rules:\nB. Aman and G. Ciobanu 9\nbh t → bh (t+1) for all h ∈H and 0 ≤ t ≤ th−1.\nFor a membrane h from the system of mutual mobile membranes with timers, th represents\nits lifetime. The object bh t is used to keep track of the units of time t which have passed\nfrom the lifetime of the membrane h. This rule simulates the passage of a unit of time\nfrom the lifetime of membrane h in the system of mutual mobile membranes with timers, by\nincreasing the second subscript of the object bh t in the system of mutual mobile membranes\nwithout timers.\n7. The rules [ ]∆0h → [δ ]∆0h from the system of mutual mobile membranes with timers can be simulated\nin the system of mutual mobile membranes without timers using the following rules:\n[bh t ]h → [δ ]h for all h ∈ H such that t = th\nIf an object bh t has the second subscript equal with th in the system of mutual mobile mem-\nbranes without timers, it means that the timer of membrane h is 0 in the corresponding sys-\ntem of mutual mobile membranes with timers. In this case, the object bh t is replaced by δ ,\nthus marking the membrane for dissolution and simulating that the membrane is dissolved\ntogether with its timer in the system of mutual mobile membranes with timers.\nWe are now able to prove the computational power of systems of mutual mobile membranes with\ntimers. We denote byNtMMm(mutual endo,mutual exo) the family of sets of natural numbers generated\nby systems of m ≥ 1 mutual mobile membranes with timers by using mutual endocytosis and mutual\nexocytosis rules. We also denote byNRE the family of all sets of natural numbers generated by arbitrary\ngrammars.\nProposition 3. NtMM3(mutual endo,mutual exo) =NRE.\nProof (Sketch). Since the output of each system of mutual mobile membranes with timers can be ob-\ntained by a system of mutual mobile membranes without timers, we cannot get more than the com-\nputability power of mutual mobile membranes without timers. Therefore, according to Theorem 3 from\n[5], we have that the family NtMM3 of sets of natural numbers generated by systems of mutual mobile\nmembranes with timers is the same as the family NRE of sets of natural number generated by arbitrary\ngrammars.\n5 From Timed Mobile Ambients to Mobile Membranes with Timers\nA translation of safe mobile ambients into mobile membranes is presented in [19], providing also an\noperational correspondence between these two formalisms such that every step in safe mobile ambients\nis translated into a series of well-defined steps of mobile membranes. Since an extension with time for\nmobile ambients already exists [2, 3, 4], and one for mobile membranes is presented in this paper, it is\nnatural to study what is the relationship between these two extensions: timed safe mobile ambients and\nsystems of mutual mobile membranes with timers.\n5.1 Timed Safe Mobile Ambients\nAmbient calculus is a formalism introduced in [10] for describing distributed and mobile computation.\nIn contrast with other formalisms for mobile processes such as the pi-calculus [29] whose computational\n10 Mutual Mobile Membranes with Timers\nmodel is based on the notion of communication, the ambient calculus is based on the notion of movement.\nAn ambient represents a unit of movement. Ambient mobility is controlled by the capabilities in, out, and\nopen. Capabilities are similar to prefixes in CCS and pi-calculus [29]. Several variants of the ambient\ncalculus have been proposed by adding and/or removing features of the original calculus [8, 23, 27].\nTime has been considered in the framework of ambient calculus in [2, 3, 4].\nWe use P to denote the set of timed safe mobile ambients; m,n for ambient names; a, p for ambi-\nent tags (a stands for active ambients, while p stands for passive ambients), and ρ as a generic notation\nfor both tags. We write n∆t [P]ρ to denote an ambient having the timer ∆t and the tag ρ ; the tag ρ indi-\ncates that an ambient is active or passive. An ambient n∆t [P]ρ represents a bounded place labelled by n\nin which a process P is executed.\nThe syntax of the timed safe mobile ambients is defined in Table 1. Process 0 is an inactive process\n(it does nothing). A movement C∆t .P is provided by the capability C∆t , followed by the execution of P.\nP |Q is a parallel composition of processes P and Q.\nTable 1: Syntax of Timed Safe Mobile Ambients\nn,m, . . . names P,Q ::= processes\nC ::= capabilities 0 inactivity\nin n can enter an ambient n C∆t .P movement\nout n can exit an ambient n n∆t [P]ρ ambient\nin n allows an ambient n to enter P |Q composition\nout n allows an ambient n to exit\nIn timed safe mobile ambients the capabilities and ambients are used as temporal resources; if nothing\nhappens in a predefined interval of time, the waiting process goes to another state. The timer ∆t of each\ntemporal resource indicates that the resource is available only for a determined period of time t. If t > 0,\nthe ambient behaves exactly as in untimed safe mobile ambients. When the timer ∆t expires (t = 0), the\nambient n is dissolved and the process P is released in the surrounding parent ambient. When we initially\ndescribe the ambients, we consider that all ambients are active, and associate the tag a to them.\nThe passage of time is described by the discrete time progress functions φ∆ defined over the set P of\ntimed processes. This function modifies a process accordingly with the passage of time; all the possible\nactions are performed at every tick of a universal clock. The function φ∆ is inspired from [7] and [21],\nand it affects the ambients and the capabilities which are not consumed. The consumed capabilities and\nambients disappear together with their timers. If a capability or ambient has the timer equal to ∞ (i.e.,\nsimulating the behaviour of an untimed capability or ambient), we use the equality ∞− 1 = ∞ when\napplying the function φ∆. Another property of the time progress function φ∆ is that the passive ambients\ncan become active at the next unit of time in order to participate to other reductions.\nFor the process C∆t .P the timers of P are activated only after the consumption of capability C∆t (in at\nmost t units of time). Reduction rules (Table 2) show how the time progress function φ∆ is used.\nDefinition 3. (Global time progress function) We define φ∆ : P →P , by:\nφ∆(P) =\n\n\nC∆(t−1).R if P =C∆t .R, t > 0\nR if P =C∆t .R, t = 0\nφ∆(R) | φ∆(Q) if P = R |Q\nn∆(t−1)[φ∆(R)]a if P = n∆t [R]ρ , t > 0\nR if P = n∆t [R]ρ , t = 0\nP if P = 0\nB. Aman and G. Ciobanu 11\nProcesses can be grouped into equivalence classes by an equivalence relation Ξ called structural\ncongruence which provides a way of rearranging expressions so that interacting parts can be brought\ntogether. We denote by 699K the fact that none of the rules from Table 2, except the rule (R-TimePass)\ncan be applied. The evolution of timed safe mobile ambients is given by the following reduction rules:\nTable 2: Reduction rules\n(R-In) −\nn∆t1[in∆t2m.P |Q]a |m∆t3[in m∆t4.R]ρ 99K m∆t3[n∆t1[P |Q]p |R]ρ\n(R-Out) −\nm∆t3[n∆t1[out∆t2m.P |Q]a |out m∆t4.R]ρ 99K n∆t1[P |Q]p |m∆t3[R]ρ\n(R-Amb) P 99K Q\nn∆t [P]ρ 99K n∆t [Q]ρ (R-Par1)\nP 99K Q\nP |R 99K Q |R\n(R-Par2) P 99K Q, P\n′\n99K Q′\nP |P′ 99K Q |Q′ (R-Struct)\nP′ΞP, P 99K Q, QΞQ′\nP′ 99K Q′\n(R-TimePass) M 699KM 99K φ∆(M)\nIn the rules (R-In), (R-Out) ambient m can be passive or active, while the ambient n is active.\nThe difference between passive and active ambients is that the passive ambients can be used in several\nreductions in a unit of time, while the active ambients can be used in at most one reduction in a unit of\ntime, by consuming their capabilities. In the rules (R-In) and (R-Out) the active ambient n becomes\npassive, forcing it to consume only one capability in one unit of time. The ambients which are tagged as\npassive become active again by applying the global time function (R-TimePass).\nIn timed safe mobile ambients, if a process evolves by one of the rules (R-In), (R-Out), while\nanother one does not perform any reduction, then rule (R-Par1) should be applied. If more than one\nprocess evolves in parallel by applying one of the rules (R-In), (R-Out), then the rule (R-Par2) should\nbe applied. We use the rule (R-Par2) to compose processes which are active, and the rule (R-Par1) to\ncompose processes which are active and passive.\n5.2 Translation\nWe denote by M (Π) the set of configurations obtained along all the possible evolution of a system Π of\nmutual mobile membranes with timers.\nDefinition 4. For a system Π of mutual mobile membranes with timers, if M and N are two configurations\nfrom M (Π), we say that M reduces to N (denoted by M → N) if there exists a rule in the set R of Π,\napplicable to configuration M such that we can obtain configuration N.\nIn order to give a formal encoding of timed safe mobile ambients into the systems of mutual mobile\nmembranes with timers, we define the following function:\nDefinition 5. A translation T : P →M (Π) is given by:\nT (A) =\n\n\nC∆tT (A1) if A =C∆t .A1\n[ T1(A1) ]∆tn if A = n∆t [ A1 ]ρ\nT1(A1)T1(A2) if A = A1 |A2\nλ if A = 0\n12 Mutual Mobile Membranes with Timers\nwhere the system Π of mutual mobile membranes with timers is constructed as follows:\nΠ = (V,H,µ ,w1, . . . ,wn,R,T, iO)\nas follows:\n• n≥ 1 is the number of ambients from A;\n• V is an alphabet containing the C objects from T (P);\n• H is a finite set of labels containing the labels of ambients from A;\n• µ ⊂ H×H describes the membrane structure, which is similar with the ambient structure of A;\n• wi ∈V ∗, 1 ≤ i ≤ n are multisets of objects which contain the C objects from T (A) placed inside\nmembrane i;\n• T ⊆ {∆t | t ∈ N} is a multiset of timers assigned to each membrane and object; the timer of each\nambient or capability from A is the same in the corresponding translated membrane or object;\n• iO is the output membrane - can be any membrane;\n• R is a finite set of developmental rules, of the following forms:\n1. [in∆t2m]∆t1n | [in m\n∆t4\n]∆t3m → [[ ]\n∆t1\nn ]\n∆t3\nm , for all n,m ∈ H and all in m, in m ∈V\n2. [[out∆t2m]∆t1n |out m\n∆t4]∆t3m → [ ]\n∆t1\nn | [ ]\n∆t3\nm , for all n,m ∈ H and all out m,out m ∈V\nWhen applying the translation function we do not take into account the tag ρ , since in mobile mem-\nbranes a membrane is active or passive depending on the rules which are applied in an evolution step and\nwe do not need something similar to ambients tags.\nProposition 4. If P is a timed safe mobile ambient such that P → Q, then there exists a system Π of\nmutual mobile membranes with timers and two configurations M,N ∈ M (Π), such that M = T (P),\nM → N and N = T (Q).\nProof (Sketch). The construction of Π is done following similar steps as in Definition 5.\nIf P 99K Q, then there exists a rule in the set of rules R of Π such that M → N and N = T (Q).\nProposition 5. If P is a timed safe mobile ambient, Π is a system of mutual mobile membranes with\ntimers and M,N ∈M (Π) are two configurations, with M = T (P) and M → N, then there exists a timed\nsafe mobile ambient Q such that N = T (Q).\nProof (Sketch). The system Π of mutual mobile membranes with timers is constructed in the same way\nas in Definition 5. If M → N in the Π system of mutual mobile membranes with timers, then there exist\na timed safe mobile ambient Q such that N = T (Q).\nRemark 1. In Proposition 5 it is possible to have P 699KQ. Let us suppose that P= n∆t4[in∆t1m.in∆t2k.out∆t3s]ρ |\nm∆t6[in m∆t5]ρ . By translation we obtain M = [in∆t1m in∆t2k out∆t3s]∆t4n [in m\n∆t5\n]∆t6m . By constructing a\nsystem Π of mutual mobile membrane with timers as shown in Definition 5, we have that M,N ∈M (Π)\nwith M → N and N = [[in∆t2k out∆t3s]∆t4n ]∆t6m . For this N there exists a Q = m∆t6[n∆t4[out∆t3s.in∆t2k]ρ ]ρ\nsuch that N = T (Q) but P 699K Q.\nB. Aman and G. Ciobanu 13\n6 Related Work\nThere are some papers using time in the context of membrane computing. However time is defined and\nused in a different manner than in this paper. In [15] a timed P system is introduced by associating to each\nrule a natural number representing the time of its execution. Then a P system which always produces the\nsame result, independently from the execution times of the rules, is called a time-independent P systems.\nThe notion of time-independent P systems tries to capture the class of systems which are robust against\nthe environment influences over the execution time of the rules of the system. Other types of time-free\nsystems are considered in [13, 16].\nTime of the rules execution is stochastically determined in [14]. Experiments on the reliability of the\ncomputations have been considered, and links with the idea of time-free systems are also discussed.\nTime can also be used to “control” the computation, for instance by appropriate changes in the execu-\ntion times of the rules during a computation, and this possibility has been considered in [18]. Moreover,\ntimed P automata have been proposed and investigated in [6], where ideas from timed automata have\nbeen incorporated into timed P systems.\nFrequency P systems has been introduced and investigated in [30]. In frequency P systems each\nmembrane is clocked independently from the others, and each membrane operates at a certain frequency\nwhich could change during the execution. Dynamics of such systems have been investigated.\nIf one supposes the existence of two scales of time (an external time of the user, and an internal\ntime of the device), then it is possible to implement accelerated computing devices which can have more\ncomputational power than Turing machines. This approach has been used in [9] to construct accelerated\nP systems where acceleration is obtained by either decreasing the size of the reactors or by speeding-up\nthe communication channels.\nIn [17, 24] the time of occurrence of certain events is used to compute numbers. If specific events\n(such as the use of certain rules, the entering/exit of certain objects into/from the system) can be freely\nchosen, then it is easy to obtain computational completeness results. However, if the length (number of\nsteps) are considered as result of the computation, non-universal systems can be obtained.\nIn [24, 31, 32] time is considered as the result of the computation by using special “observable”\nconfigurations taken in regular sets (with the time elapsed between such configurations considered as\noutput). In particular, in [24, 31] P systems with symport and antiport rules are considered for prov-\ning universality results, and in [32] this idea is applied to P systems with proteins embedded on the\nmembranes.\nThe authors of the current paper have also considered time to “control” the computation in two other\nformalisms: mobile ambients [2, 3, 4] and distributed pi-calculus [21, 22]. Timers define timeouts for\nvarious resources, making them available only for a determined period of time. The passage of time is\ngiven by a discrete global time progress function.\n7 Conclusion\nWe introduce a new class of mobile membranes, namely the mobile membranes with timers. Timers are\nassigned to each membrane and to each object. This new feature is inspired from biology where cells and\nintracellular proteins have a well defined lifetime. In order to simulate the passage of time, we use rules\nof the form a∆t → a∆(t−1) for objects, and [ ]∆ti → [ ]∆(t−1)i for membranes. If the timer of an object reaches\n0 then the object is consumed by applying a rule of the form a∆0 → λ , while if the timer of a membrane\ni reaches 0 then the membrane is marked for dissolution by applying a rule of the form [ ]∆0i → [δ ]∆0i .\n14 Mutual Mobile Membranes with Timers\nAfter dissolving a membrane, all objects and membranes previously contained in it become elements of\nthe immediately upper membrane.\nWe do not obtain a more powerful formalism by adding timers to objects and to membranes into\na system of mutual mobile membranes. According to Proposition 1, Proposition 2 and Proposition 3,\nsystems of mutual mobile membranes with timers and systems of mutual mobile membranes without\ntimers have the same computational power.\nIn order to relate the new class to some known formalism involving mobility and time, we give a\ntranslation of timed safe mobile ambients into systems of mutual mobile membranes with timers. This\nencoding shows that the class of mutual mobile membranes with timers is a powerful formalism. Such\na result is related to a previous one presented in [19], where it is proved an operational correspondence\nbetween the safe mobile ambients and the systems of mutual mobile membranes.\nReferences\n[1] B. Alberts, A. Johnson, J. Lewis, M. Raff, K. Roberts, P. Walter. Molecular Biology of the Cell - Fifth Edition.\nGarland Science, Taylor & Francis Group, 2008.\n[2] B. Aman, G. Ciobanu. Timers and Proximities for Mobile Ambients. Lecture Notes in Computer Science,\nvol.4649, 33–43, 2007.\n[3] B. Aman, G. Ciobanu. Mobile Ambients with Timers and Types. Lecture Notes in Computer Science, vol.4711,\n50–63, 2007.\n[4] B. Aman, G. Ciobanu. Timed Mobile Ambients for Network Protocols. Lecture Notes in Computer Science,\nvol.5048, 234–250, 2008.\n[5] B. Aman, G. Ciobanu. Turing Completeness Using Three Mobile Membranes. Lecture Notes in Computer\nScience, vol.5715, 42–55, 2009.\n[6] R. Barbuti, A. Maggiolo-Schettini, P. Milazzo, L. Tesei. Timed P Automata. Electronic Notes in Theoretical\nComputer Science, vol.227, 21–36, 2009.\n[7] M. Berger. Towards Abstractions for Distributed Systems PhD thesis, Imperial College, Department of Com-\nputing, 2002.\n[8] M. Bugliesi, G. Castagna, S. Crafa. Boxed Ambients. Lecture Notes in Computer Science, vol.2215, 38-63,\n2001.\n[9] C.S. Calude, Gh. Pa˘un. Bio-Steps Beyond Turing. Biosystems, vol.77(1-3), 175–194, 2004.\n[10] L. Cardelli, A. Gordon. Mobile Ambients. Theoretical Computer Science, vol.240(1), 170-213, 2000.\n[11] L. Cardelli. Brane Calculi - Interactions of Biological Membranes. Lecture Notes in Computer Science,\nvol.3082, 257–280, 2005.\n[12] L. Cardelli, Gh. Pa˘un. An Universality Result for a (Mem)Brane Calculus Based on Mate/Drip Operations.\nInternational Journal of Foundations of Computer Science, vol.17(1), 49–68, 2006.\n[13] M. Cavaliere, V. Deufemia. Further Results on Time-Free P Systems. International Journal on Foundational\nComputer Science, vol.17(1), 69–89, 2006.\n[14] M. Cavaliere, I. Mura. Experiments on the Reliability of Stochastic Spiking Neural P Systems. Natural\nComputing, vol.7(4), 453–470, 2008.\n[15] M. Cavaliere, D. Sburlan. Time-Independent P Systems. Lecture Notes in Computer Science, vol.3365,\n239–258, 2005.\n[16] M. Cavaliere, D. Sburlan. Time and Synchronization in Membrane Systems. Fundamenta Informaticae,\nvol.64(1-4), 65–77, 2005.\n[17] M. Cavaliere, R. Freund, A.Leitsch, Gh. Pa˘un. Event-Related Outputs of Computations in P Systems. Journal\nof Automata, Languages and Combinatorics, vol.11(3), 263–278, 2006.\nB. Aman and G. Ciobanu 15\n[18] M. Cavaliere, C. Zandron. Time-Driven Computations in P Systems. Proceedings of Fourth Brainstorming\nWeek on Membrane Computing, 133–143, 2006.\n[19] G. Ciobanu, B. Aman. On the Relationship Between Membranes and Ambients. Biosystems, vol.91(3),\n515–530, 2008.\n[20] G. Ciobanu, Gh. Pa˘un, M.J. Pe´rez-Jime´nez (editors). Applications of Membrane Computing, Springer, Nat-\nural Computing Series, 2006.\n[21] G. Ciobanu, C. Prisacariu. Timers for Distributed Systems. Electronic Notes in Theoretical Computer Sci-\nence, vol.164(3), 81–99, 2006.\n[22] G. Ciobanu, C. Prisacariu. Coordination by Timers for Channel-Based Anonymous Communications. Elec-\ntronic Notes in Theoretical Computer Science, vol.175(2), 3–17, 2007.\n[23] D. Hirschkoff, D. Teller, P. Zimmer. Using Ambients to Control Resources. Lecture Notes in Computer\nScience, vol.2421, 288-303, 2002.\n[24] O.H. Ibarra, A. Pa˘un. Computing Time in Computing with Cells. Lecture Notes In Computer Science,\nvol.3892, 112–128, 2006.\n[25] C.A. Janeway, P. Travers, M. Walport, M.J. Shlomchik. Immunobiology - The Immune System in Health and\nDisease. Fifth Edition. Garland Publishing, 2001.\n[26] S.N. Krishna, Gh. Pa˘un. P Systems with Mobile Membranes. Natural Computing, vol.4(3), 255–274, 2005.\n[27] F. Levi, D. Sangiorgi. Controlling Interference in Ambients. Principles of Programming Languages, 352-\n364, 2000.\n[28] H. Lodish, A. Berk, P. Matsudaira, C. Kaiser, M. Krieger, M. Scott, L. Zipursky, J. Darnell. Molecular Cell\nBiology. Fifth Edition, 2003.\n[29] R. Milner. Communicating and Mobile Systems: the pi-calculus. Cambridge University Press, 1999.\n[30] D. Molteni, C. Ferretti, G. Mauri. Frequency Membrane Systems. Computing and Informatics, vol.27(3),\n467–479, 2008.\n[31] H. Nagda, A. Pa˘un, A. Rodrı´guez-Pato´n. P Systems with Symport/Antiport and Time. Lecture Notes In\nComputer Science, vol.4361, 463–476, 2006.\n[32] A. Pa˘un, A. Rodrı´guez-Pato´n. On Flip-Flop Membrane Systems with Proteins. Lecture Notes In Computer\nScience, vol.4860, 414–427, 2007.\n[33] Gh. Pa˘un. Membrane Computing. An Introduction. Springer, 2002.\n[34] I. Petre, L. Petre. Mobile Ambients and P Systems. Journal of Universal Computer Science, vol.5(9), 588–\n598, 1999.\n[35] Web page of the P systems: http://ppage.psystems.eu.\n[36] Web page http://bcs.whfreeman.com/thelifewire.\n",
            "id": 665750,
            "identifiers": [
                {
                    "identifier": "oai:arxiv.org:0910.1217",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "10.4204/eptcs.6.1",
                    "type": "DOI"
                },
                {
                    "identifier": "2082594",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:doaj.org/article:6a9108f5b81c48479c1f7b4697e42acb",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "26625854",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2134341831",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "0910.1217",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "194545747",
                    "type": "CORE_ID"
                }
            ],
            "title": "Mutual Mobile Membranes with Timers",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2134341831",
            "oaiIds": [
                "oai:arxiv.org:0910.1217",
                "oai:doaj.org/article:6a9108f5b81c48479c1f7b4697e42acb"
            ],
            "publishedDate": "2009-10-01T00:00:00",
            "publisher": "'Open Publishing Association'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://doaj.org/search?source=%7B%22query%22%3A%7B%22bool%22%3A%7B%22must%22%3A%5B%7B%22term%22%3A%7B%22id%22%3A%226a9108f5b81c48479c1f7b4697e42acb%22%7D%7D%5D%7D%7D%7D",
                "http://arxiv.org/abs/0910.1217"
            ],
            "updatedDate": "2021-05-03T17:32:27",
            "yearPublished": 2009,
            "journals": [
                {
                    "title": "Electronic Proceedings in Theoretical Computer Science",
                    "identifiers": [
                        "2075-2180"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/0910.1217"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/665750"
                }
            ]
        },
        {
            "acceptedDate": "2006-11-23T00:00:00",
            "arxivId": "cs/0601032",
            "authors": [
                {
                    "name": "Henry Kautz"
                },
                {
                    "name": "James Schmolze"
                },
                {
                    "name": "Tamara Babaian"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/188983842"
            ],
            "createdDate": "2012-04-13T14:21:50",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2006-09-26T00:00:00",
            "abstract": "We consider the problem of reasoning and planning with incomplete knowledge\nand deterministic actions. We introduce a knowledge representation scheme\ncalled PSIPLAN that can effectively represent incompleteness of an agent's\nknowledge while allowing for sound, complete and tractable entailment in\ndomains where the set of all objects is either unknown or infinite. We present\na procedure for state update resulting from taking an action in PSIPLAN that is\ncorrect, complete and has only polynomial complexity. State update is performed\nwithout considering the set of all possible worlds corresponding to the\nknowledge state. As a result, planning with PSIPLAN is done without direct\nmanipulation of possible worlds. PSIPLAN representation underlies the PSIPOP\nplanning algorithm that handles quantified goals with or without exceptions\nthat no other domain independent planner has been shown to achieve. PSIPLAN has\nbeen implemented in Common Lisp and used in an application on planning in a\ncollaborative interface.Comment: 39 pages, 13 figures. to appear in Logical Methods in Computer\n  Scienc",
            "documentType": "research",
            "doi": "10.2168/lmcs-2(3:5)2006",
            "downloadUrl": "http://arxiv.org/abs/cs/0601032",
            "fieldOfStudy": "computer science",
            "fullText": "Logical Methods in Computer Science\nVol. 2 (3:5) 2006, pp. 1–39\nwww.lmcs-online.org\nSubmitted Jan. 21, 2005\nPublished Sep. 26, 2006\nEFFICIENT OPEN WORLD REASONING FOR PLANNING\nTAMARA BABAIAN a AND JAMES G. SCHMOLZE b\na Department of Computer Information Systems, Bentley College, Waltham, MA 02452 USA\ne-mail address: tbabaian@bentley.edu\nb Department of Computer Science, Tufts University, Medford, MA 02155 USA\nAbstract. We consider the problem of reasoning and planning with incomplete knowl-\nedge and deterministic actions. We introduce a knowledge representation scheme called\nPSIPLAN that can effectively represent incompleteness of an agent’s knowledge while al-\nlowing for sound, complete and tractable entailment in domains where the set of all objects\nis either unknown or infinite. We present a procedure for state update resulting from tak-\ning an action in PSIPLAN that is correct, complete and has only polynomial complexity.\nState update is performed without considering the set of all possible worlds correspond-\ning to the knowledge state. As a result, planning with PSIPLAN is done without direct\nmanipulation of possible worlds. PSIPLAN representation underlies the PSIPOP plan-\nning algorithm that handles quantified goals with or without exceptions that no other\ndomain independent planner has been shown to achieve. PSIPLAN has been implemented\nin Common Lisp and used in an application on planning in a collaborative interface.\n1. Introduction\nMuch progress has been made in the area of planning with a correct but incomplete\ndescription of the world, i.e. an open world. However, so far there have been relatively few\nformalisms proposed for efficient open world representation and reasoning in the domains\nwith a very large or unknown set of individual objects. This paper describes a represen-\ntation, called PSIPLAN, for planning with incomplete information about the initial state,\nthat handles open world planning problems that have not been shown to be solved by any\nother implemented domain independent planning system.\nFor an example of an open world problem, consider a robot operating in a warehouse\nthat is given a goal of delivering box A to the front door. The robot can pick up only\nthose boxes that do not contain fragile items. To pick up box A the robot does not need\nto know the location of all fragile items, nor the precise contents of box A. It is sufficient\nto know that A has no fragile items. Thus, in such situations, if the robot knows that (a)\nno containers have fragile goods except for B, it can safely pick up box A. When the list\nof all containers that the robot might eventually have to reason about is unknown, it is\nimpossible to represent (a) by enumerating all containers that do not have fragile goods.\n2000 ACM Subject Classification: I.2.4; I.2.8; F 4.1; F.2.2.\nKey words and phrases: Knowledge representation and reasoning, planning with incomplete information.\nLOGICAL METHODS\nl IN COMPUTER SCIENCE DOI:10.2168/LMCS-2 (3:5) 2006\nc© T. Babaian and J. G. Schmolze\nCC© Creative Commons\n2 T. BABAIAN AND J. G. SCHMOLZE\nThus to include (a) in the robot’s description of the state, or to represent a goal such as\nthere is nothing at the front door except for box A, a quantified statement is necessary.\nThe robot must be able to update quickly and correctly its knowledge of the state that\nresults from performing an action. For instance, after a new container C, whose contents\nare unknown, is added to the warehouse, the robot’s updated state must reflect that (b)\nthere are no containers with fragile goods except for B and possibly C.\nEffective and efficient operation in a partially known environment may require the\nability to satisfy knowledge goals, such as for example, (c) identify all containers with fragile\ngoods that will be shipped to Boston, and sensing actions that return information about the\nworld, such as examining the contents of a box, or determining a box’s destination from its\nlabel. A robot that is able to reason in a sound and complete manner about its knowledge\nand lack of knowledge is better equipped to handle such knowledge goals. For instance,\nknowing only (b) plus that C’s destination is Chicago, when given a knowledge goal (c),\nthe robot must be able to conclude that the only necessary information that is missing is\nthe destination of B. Such precision of reasoning ensures that sensing is non-redundant and\nrelies on sound and complete reasoning in the underlying representation.\nAs this example demonstrates, a representation used in an open-world application must\ndistinguish between propositions whose truth value the agent knows and those whose truth\nvalue it does not know. Merely listing all facts marked with their truth value is inefficient\nwhen the set of all domain objects is very large because, typically, the number of atoms\nwhose value is known to be false is large. For example, the set of all objects that are\nnot in box A could be very large. Furthermore, such enumeration of known atoms is\nimpossible when the set of all domain objects is infinite or unknown. A key requirement\nof an open world planning representation with partially unknown or infinite domains is to\nallow quantification for (at least) negative information while retaining efficient reasoning.\nIn this paper we present a representation for open world reasoning that uses a class\nof quantified sentences with exceptions that we call ψ-forms. ψ-forms represent quantified\nnegative information such as the following statement: there are no containers with fragile\ngoods except for possibly B and C.We present an efficient calculus for ψ-forms, that includes\na sound and complete entailment procedure that takes polynomial time under certain rea-\nsonable assumptions on the structure of ψ-forms.\nFurther, we present a formalism called PSIPLAN, based on ψ-forms, for reasoning and\nplanning in partially known worlds. PSIPLAN offers a unique combination of tractability,\nexpressivity and completeness of reasoning. This makes it uniquely suited for applications\nwhere the set of all domain objects can never be acquired, and the ability to generate\nand explore alternative plans while avoiding redundant execution is critical. One of the\nadvantages of PSIPLAN compared to other representation used in open world planners,\nis the fact that all reasoning about actions and states in PSIPLAN is carried out without\nan explicit manipulation of the set of possible worlds. This property reduces a planner’s\nsensitivity to the amount of irrelevant information, which causes some planners that use\nexplicit enumeration of the possible worlds to blow up.\nA partial order planning algorithm ([31]) called PSIPOP based on PSIPLAN is pre-\nsented in [5]. PSIPOP is sound and complete for the planning domains in which the set\nof all domain objects is infinite or can never be fully acquired. As a partial order planner,\nPSIPOP produces a solution in a form of a partially ordered set of steps, which can be\nlinearlized or used as a basis for a parallelized plan. The goal language of PSIPOP includes\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 3\nquantified goals with exceptions such as (b) that no other implemented domain independent\nplanner has been shown to handle.\nAn extension of PSIPLAN that includes sensing actions and knowledge goals is pre-\nsented in [2]. A planner that uses the extended representation for planning with sensing and\ninterleaved execution (PSIPOP-SE) was used in a collaborative interface called Writer’s Aid\n[3]. Writer’s Aid helps an author writing an academic manuscript by simultaneously and\nautonomously identifying, locating and downloading relevant bibliographic records and pa-\npers from the author’s preferred local and Internet sources. The use of PSIPLAN at the core\nof Writer’s Aid ensures the expressiveness of its goal language, and its ability to precisely\nidentify the missing information and never engage in redundant search, while exploring all\npossible courses of action.\nIn this paper we make the following contributions:\n• We present a complete description of the language of ψ-forms and ψ-form calculus,\nand report on the complexity of the calculus operations. The ψ-form calculus is an\nintegral and critical part of PSIPLAN reasoning, and, thus, its algorithms, complex-\nity and completeness properties bear direct effect on the properties of PSIPLAN-\nbased planners ([5, 2, 3], as well as a Graphplan-style ([9]) conformant planner\ncurrently under development.)\n• We introduce PSIPLAN representation of an agent’s incomplete state of knowledge\nand illustrate it with examples. We further present PSIPLAN’s action language and\na procedure for state update after an action, and prove important properties of the\nupdate procedure, including its completeness.\n1.1. Prior Approaches. The lack of universally quantified reasoning in open-world plan-\nners (e.g.[11], [7], [14],[36], [40], [13], [1], [25], [32],) precludes their use in domains in which\nthe set of all objects is not known, very large, or infinite ([39], [16]).\nOn the other hand, situation calculus-based approaches (e.g. [18], [34], [26]) have the\nexpressivity of full first order logic (FOL), and thus admit planning problems with arbi-\ntrary quantified formulas. However, a complete planner based on the unrestricted situation\ncalculus , i.e. that relies upon the full FOL, is impossible due to the undecidability of en-\ntailment in FOL. Recently, Liu and Levesque have presented a subset of situation calculus,\nwith a tractable, sound and complete action projection under certain restrictions ([30]).\nOther related approaches to reasoning about actions incorporating first-order features are\npresented in [37] and [35]. These works are reviewed in Section 5.\nThe LCW (for Locally Closed Worlds) language of Etzioni et. al. [16] is designed to\nachieve expressivity and tractability for open world reasoning and is most closely related to\nPSIPLAN. LCW sentences specify the parts of the world for which the agent has complete\ninformation. It does this by collecting formulas Φ where the agent knows the truth value\nof every ground instance of Φ. For example, if the agent knows all fragile items that are\nin box B, it states LCW (In(x,B)∧Fragile(x)). Combined with a propositional database\nthat states that In(Vase, B), F ragile(Vase), the agent can conclude that nothing is fragile\nin B except for the Vase.\nLCW reasoning, although tractable, is incomplete [16, 5]. There are two sources of\nincompleteness in LCW: (1) incompleteness of inference and (2) inability of LCW state-\nments to represent exceptions, i.e., the inability to state that the agent knows the value of\nall instantiations of formula Φ except some. These are the key difference between the LCW\n4 T. BABAIAN AND J. G. SCHMOLZE\nand PSIPLAN representations; PSIPLAN reasoning is complete, and PSIPLAN can also\nexpress what exactly is not known via the ψ-form exceptions mechanism. Adding a similar\nmechanism to the LCW framework would require the development of new entailment pro-\ncedures, methods for state update and operations underlying the planning techniques akin\nto those developed in this paper.\nAs a result of the lack of exceptions in LCW sentences, when one or more instances\nof Φ is unknown, LCW (Φ) cannot be stated. This limitation on the expressive power will\nsometimes cause known information to be discarded from the LCW knowledge base upon\nupdating it after an action, even when the effects of the action are completely specified, do\nnot cause information loss, and create no new objects. For example, consider the result of\nmoving an object Cup from some other box to box B in the situation where LCW (In(x,B)∧\nFragile(x)) is asserted. If it is not known whether the Cup object is fragile, the LCW\nstatement above no longer holds and thus must be discarded, effectively discarding from\nthe knowledge base all instances of In(x,B) ∧ Fragile(x) that are known to be false.\nPSIPLAN subsumes a large part of the LCW language. Every knowledge state repre-\nsented by an LCW-based representation can also be represented in PSIPLAN, except for\nthose situations which require an LCW statement LCW (Φ), where Φ contains atoms that\nunify when all variables are renamed to be distinct1. On the other hand,there are states of\nknowledge that PSIPLAN can represent accurately, but LCW cannot.\nBoth LCW and PSIPLAN representations can be used in planning with sensing [2, 3].\nSince LCW’s reasoning is incomplete, however, planners based on LCW are inherently\nincomplete. To help remedy this, LCW based planners use sensing actions to find out facts\nthey cannot infer, but this only works when an appropriate sensing action is both available\nand not too costly. In general, there is no effective replacement for sound and complete\nreasoning.\nThe LCW [16] representation is extended in [28, 19] to handling exceptions. However,\nboth of these works only consider the setting in which there are no actions that can change\nthe world, do not address a changing world or planning, and do not present any methods\nthat would make these extensions amenable to their use in reasoning about actions.\n1.2. A brief look at PSIPLAN.\nExample 1.1. A robot operating in a warehouse is told that there are no fragile goods in\nany of the boxes except possibly for the box marked FragileStuff, i.e.\n∀g, c .¬Box(c) ∨ ¬In(g, c) ∨ ¬Fragile(g) ∨ c = FragileStuff (1.1)\nHere Box(c) states that c is a box, In(g, c) states that item g is in container c, and\nFragile(g) states that g is fragile. Note that (1.1) does not state whether or not FragileStuff\nactually contains any fragile items.\nIn PSIPLAN, statement (1.1) is represented by the following ψ-form that represents\na conjunction of all ground clauses 2 that can be obtained by instantiating the formula\n¬Box(c) ∨ ¬In(g, c) ∨ ¬Fragile(g) in all possible ways except for instantiations in which\nc = FragileStuff. This is written as the ψ-form\nψ = [¬Box(c) ∨ ¬In(g, c) ∨ ¬Fragile(g) except {{c = FragileStuff}}]. (1.2)\n1These limitations are related to the definition of fixed length ψ-forms presented in Section 2 and are\ncritical to the tractability of ψ-form reasoning.\n2A clause is a disjunction of literals. A clause is ground if it contains no variables.\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 5\nwhose interpretation is exactly the same as (1.1). Here and below, lowercase letters in\nψ-forms denote implicitly universally quantified variables, while symbols that start with a\ncapital letter denote constants.\nSuppose, it is also known that a bottle of wine is the only fragile item. Consequently,\nit is in the FragileStuff box, i.e.\nBox(FragileStuff), In(Wine,FragileStuff), F ragile(Wine) (1.3)\nWith PSIPLAN, the original situation comprised by (1.1) and (1.3) is represented as\nthe following state of knowledge\ns =\n{\nψ = [¬Box(c) ∨ ¬In(g, c) ∨ ¬Fragile(g) except {{c = FragileStuff}}],\nBox(FragileStuff), In(Wine,FragileStuff), F ragile(Wine)\n}\n(1.4)\nNow suppose that a new container Box10 is brought into the room whose contents are\ncompletely unknown. In the resulting situation, the location of all fragile goods is known\nexcept for those that might be in Box10. The PSIPLAN state update would yield the\nfollowing new state of knowledge s′ by adding the atom Box(Box10) to s and adding an\nexception to ψ yielding\nψ′ = [¬Box(c) ∨ ¬In(g, c) ∨ ¬Fragile(g) except {{c = FragileStuff}, {c = Box10}}].\nThe updated state of knowledge s′ represents the new situation precisely:\ns′ ={\nψ′ = [¬Box(c) ∨ ¬In(g, c) ∨ ¬Fragile(g) except {{c = FragileStuff}, {c = Box10}}],\nBox(FragileStuff), Box(Box10), In(Wine,FragileStuff), F ragile(Wine)\n}\n(1.5)\nPSIPLAN’s action language includes actions with ψ-form preconditions. For example,\nthe action lift of lifting object B, requires that there be no fragile items in it, i.e.\n[¬In(g,B) ∨ ¬Fragile(g)] (1.6)\nWhen an agent whose state of knowledge is described with s′ ∪ Box(Box5) is given\na goal of lifting box Box5 from its location, it will establish that the precondition\n[¬In(g,Box5) ∨ ¬Fragile(g)] of the lift action is entailed by ψ′ and proposition Box(Box5).\nIndeed, if no boxes contain fragile goods, except for the box FragileStuff and possibly Box10,\nthen there are no fragile goods in Box5. The PSIPLAN reasoning algorithms that are in-\nvolved in this inference do not expand the universal quantification in the universal base and\ndo not require the knowledge of all domain objects by the agent.\nAnother illustration of the advantage of PSIPLAN’s ability to reason with quantified\nsentences is an example from the blocks world domain. PSIPLAN eliminates the need for\npredicate Clear(B) as a way of stating that nothing is on block B. Instead, PSIPLAN’s\nrepresentation uses [¬On(b,B)]. The advantage of using the latter representation is that\nthe fact that block A is on block B by itself implies that block B is not clear, eliminating\nthe need to state ¬Clear(B) as another effect of moving block A onto B.\nThis use of quantified preconditions distinguishes PSIPLAN from other representations\n([11], [7], [14],[36], [40], [13], [1], [25], [32]), that only admit actions with preconditions\nlimited to atoms or literals. On the other hand, many of these conformant planners handle\nactions with conditional effects, which are absent from PSIPLAN. While PSIPLAN can\nbe easily extended to represent actions with conditional effects, complete planning with\nconditional effects is in the complexity class Σ2P (e.g. [6, 38]), while complete planning\n6 T. BABAIAN AND J. G. SCHMOLZE\nwith PSIPLAN appears to be an NP-complete problem. This issue is further discussed in\nSection 5.2.\nPSIPLAN admits procedures for entailment and state update after an action (including\nactions that introduce a new object) that are sound, complete and take polynomial time.\nIn particular, the complexity of entailment grows linearly with respect to the number of\nψ-forms in the knowledge base when the number of literals, variables and exceptions in\neach ψ-form are bounded. The complexity bound on ψ-form reasoning is polynomial in the\nnumber of exceptions.\nThe rest of the paper is organized as follows: Section 2 formally defines ψ-forms and\npresents a few simple properties. In Section 3, we present the ψ-form calculus and com-\nplexity results. Section 4 introduces the PSIPLAN representation of a state of knowledge,\nactions and state update after an action. Section 5 contains an overview of the related\nwork. Finally, Section 6 summarizes and draws conclusions.\n2. The language of ψ-forms\n2.1. Definitions and notation. We assume no function symbols except for constants in\nthe language. The number of constants is infinite.\nThe general form of a ψ-form is:\nψ = [¬Q1( ~x1) ∨ . . . ∨ ¬Qk( ~xk) except {σ1, . . . , σn}] (2.1)\nk ≥ 1 and n ≥ 0, and each Qi(~xi) is any atom whose only variables are ~xi. The clause\n¬Q1( ~x1)∨ . . . ∨ ¬Qk( ~xk) is called the main clause of ψ and is denoted by M(ψ). The set\nof all variables of the main clause, i.e. the set ~x =\n⋃k\ni=1 ~xi, is denoted by V(ψ).\nEach σi is a substitution on a non-empty subset of variables in V(ψ), that binds a\nvariable to another variable from V(ψ), or to a constant. The set of all substitutions\nappearing in a ψ-form is denoted Σ(ψ). Each σi represents exceptions of ψ. Thus, a ψ-form\ncan be abbreviated as [M(ψ) except Σ(ψ)].\nWhen Σ(ψ) is empty, we call such a ψ-form simple and write [M(ψ)] instead of\n[M(ψ) except {}]. A simple ψ-form with no variables is called a singleton and represents\na single ground clause.\nGiven a ψ-form ψ = [M(ψ) except Σ(ψ)] we will need to refer to the following:\n• a simple ψ-form [M(ψ)] that is obtained from the main clause of ψ, called main\npart.\n• simple ψ-forms that are obtained by instantiating the main clause with substitutions\nfrom Σ(ψ), called exception forms. For each σi from Σ(ψ), we call M(ψ)σi the\ni-th exception clause denoted Ei(ψ). E(ψ) is the set of all of ψ’s exception forms:\n{[Ei(ψ)] | 1 ≤ i ≤ ‖Σ(ψ)‖}.\nWe use C and V to denote respectively the maximum number of literals and number of\nvariables in the main clause for a given set of ψ-forms. We use E to denote the maximum\nsize of the set Σ(ψ). The cardinality of each predicate is assumed to be constant bounded,\nthus the time for unification of two literals is also constant bounded.\nUnless noted otherwise, everywhere in this paper symbols x, y, z, x1, y1, z1, . . . denote\nvariables, capital letters A,B, . . . denote constants , and ψ,ψ1, . . . – denote ψ-forms. Also,\nwe assume that the variables in two distinct ψ-forms are always renamed to be distinct.\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 7\nName Notation Example ψ = [¬P (x, y, z) ∨ ¬Q(y,A)\nexcept {x = A}, {x = C, y = D}]\nmain clause M(ψ) ¬P (x, y, z) ∨ ¬Q(y,A)\nmain part [M(ψ)] [¬P (x, y, z) ∨ ¬Q(y,A)]\nvariables V(ψ) {x, y, z}\nexceptions Σ(ψ) {{x = A}, {x = C, y = D}}\nexception clauses E1(ψ) ¬P (A, y, z) ∨ ¬Q(y,A)\nE2(ψ) ¬P (C,D, z) ∨ ¬Q(D,A)\nexception forms E(ψ) {[¬P (A, y, z) ∨ ¬Q(y,A)], [¬P (C,D, z) ∨ ¬Q(D,A)]}\nFigure 1: Summary of ψ-form notation.\nφ([M(ψ)])\nφ([E1(ψ)])\nφ([E2(ψ)])\nφ([E3(ψ)])\nFigure 2: The set of clauses defined by a ψ. φ(ψ) is depicted as the gray area and consists\nof all clauses of the main part [M(ψ)] that are not exceptions, i.e. are not in any\nof [E1(ψ)], . . . , [E3(ψ)] . The main part [M(ψ)] contains the superset of all clauses\nof ψ.\n2.2. ψ-forms as Sets. A ψ-form is a representation of a possibly infinite set of ground\nclauses. Here and throughout the paper, clauses that consist of the same set of literals are\nconsidered equal. The logical equivalence of such clauses allows us to disregard the order\nof their literals.\nWe define the set of ground clauses represented by a ψ-form ψ, called a ψ-set and\ndenoted φ(ψ), as follows:\n(1) When ψ is simple, the set defined by ψ consists of all ground instantiations of the\nmain clause\nφ(ψ) = {M(ψ)σ |M(ψ)σ is ground} (2.2)\nThis definition implies φ([c]) = {c}, when c is a ground clause.\n(2) When ψ is not simple, the set defined by ψ consists of all ground instantiations of\nthe main clause minus the set of all ground instantiations of exception clauses.\nφ(ψ) = φ([M(ψ)])− φ([E1(ψ)])− . . . − φ([En(ψ)]), (2.3)\nwhere n = ‖Σ(ψ)‖. Any clause in the set φ([E1(ψ)]) ∪ . . . ∪ φ([En(ψ)]) is called an\nexception of ψ.\nFigure 2 illustrates the definition of a ψ-set.\nTo combine and compare ψ-sets represented by different ψ-forms, we introduce set\noperations and define the resulting ψ-set as follows.\n8 T. BABAIAN AND J. G. SCHMOLZE\n(1) For a set of ψ-forms {ψ1, . . . , ψk} their ψ-set is the union of the ψ-sets of its elements.\nφ({ψ1, . . . , ψk}) = ∪\nk\ni=1φ(ψi), (2.4)\n(2) An expression ✷1 ∗✷2, where ✷1 and ✷2 denote either a single ψ-form or a set of ψ-\nforms, and ∗ denotes any of the set operations ∩,∪,−, ⊲ or e (last two operations are\ndefined in the next section) represents a set of ground clauses obtained by applying\nthe * operation to the corresponding ψ-sets.\nφ(✷1 ∗ ✷2) = φ(✷1) ∗ φ(✷2). (2.5)\n(3) Let A and B be ψ-forms or ψ-form expressions. We write A = B and call A and B\nequivalent if and only if φ(A) = φ(B), in other words, the sets of ground clauses\nrepresented by each ψ-form or expression are the same.\nFor example, the statement ψ1 = ψ2∩ψ3 represents the equivalence of two ψ-sets: φ(ψ1)\nand φ(ψ2 ∩ ψ3). The latter, in turn, according to definition (2.5) denotes the intersection\nφ(ψ2) ∩ φ(ψ3)\n2.2.1. ψ-set Membership. We say that a ground clause c is in ✷, written c ∈ ✷ instead of\nc ∈ φ(✷).\nThus, according to (2.3), given a ψ-form ψ, a ground clause c is in ψ if and only if it\ncan be obtained by instantiating the main clause, M(ψ), with some ground substitution σ,\nand cannot be obtained by instantiating any of ψ’s exception clauses E1(ψ), . . . , En(ψ).\nc ∈ ψ iff ∃σ . c =M(ψ)σ and ∀θ, i . 1 ≤ i ≤ n =⇒ c 6= Ei(ψ)θ (2.6)\nWe also define membership of a clause in a set of ψ-forms Ψ in the obvious way:\nc ∈ Ψ iff ∃ψ ∈ Ψ . c ∈ ψ (2.7)\nDeciding the membership of a ground clause in a simple ψ-form amounts to finding a\nsubstitution σ that matches the literals in the main clause of the ψ-form with the literals\nof the clause. In a ψ-form with exceptions, after establishing membership in the main part,\n[M(ψ)], it is necessary to verify non-membership in the ψ-form’s exception forms, all of\nwhich in turn are simple ψ-forms.\nWe define an operation set-match that computes such matching substitutions used to\ngenerate a clause from the main clause of a ψ-form. Given two clauses a and b, where\nvariables in a and b are distinct and denoted Va and Vb respectively, we say that a set-\nmatches with b if and only if there exists a substitution σ on variables in Va such that\naσ = b. The set of all most general such matching σ’s is denoted MGU≡(a, b, Va).\n3\nExample 2.1 below demonstrates that there can be more than one way a ψ-form clause\ncan set-match with a ground clause.\nExample 2.1. Let ψ = [¬P (x) ∨ ¬P (y) except {{x = A}}] and let c = ¬P (A) ∨ ¬P (B).\nThere are two substitutions σ1 = {x = A, y = B} and σ2 = {x = B, y = A} such that\nM(ψ)σ1 = M(ψ)σ2 = c. However, c can also be generated by instantiating the exception\nclause E1(ψ) = ¬P (A) ∨ ¬P (y) with substitution {y = B}, therefore c 6∈ ψ.\n3Two substitutions σ1 and σ2 are equal if and only if the two can be made identical by consistently\nrenaming variables in both. Thus MGU≡(a, b, Va) does not include two equivalent substitutions.\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 9\nThe possibility of multiple different instantiations producing the same ground clause\ncomplicates the reasoning with ψ-forms of this type, as even deciding membership of a\nground clause in a ψ-form becomes somewhat more problematic compared to the case of\nψ-forms for which each ground clause is generated with a unique substitution. Essentially,\nwhen each ground clause is obtained with a unique instantiation of the main form, checking\nc ∈ ψ amounts to computing a set-match MGU≡(M(ψ), c,V(ψ)) and, in case the set-\nmatch results in a substitution σ, checking that σ is not a superset of any substitution\nin ψ’s exceptions Σ(ψ). When, as in the Example 2.1, MGU≡(M(ψ), c,V(ψ)) consists of\nmore than a single substitution, we must consider all such substitutions in relation to the\nexceptions. To avoid the increase in the complexity of reasoning we introduce a notion of\na fixed length ψ-form.\nA ψ-form is called fixed length if and only if no two literals of the main clause unify\nwhen the variables in both literals are renamed to be distinct. Thus, [¬P (x,A) ∨ ¬P (B,x)]\nis not a fixed length ψ-form, because ¬P (x1, A) and ¬P (B,x2) are unifiable. On the other\nhand, [¬P (x,A) ∨ ¬P (x,B)] and [¬P (x,A) ∨ ¬Q(y)] are examples of fixed length ψ-forms.\nObservation 2.2. When ψ is a fixed length ψ-form, there is a unique σ for each clause\nc ∈ ψ such that M(ψ)σ = c.\nProof. The proof is by contradiction. Assuming there is a ground clause that is generated\nby more than one substitution on M(ψ), it is possible to construct a unifier for two literals\nof the main clause.\nOther important properties of fixed length ψ-forms ensuring reduced complexity of\nreasoning are discussed in the end of Section 3.6.\nThus everywhere except for general ψ-form entailment theorems in Section 3.3 we limit\nour attention to fixed length ψ-forms.\nA ψ-form is called well-formed if and only if it has no redundant exception forms, i.e.\nthere is no subsumption between any two exception clauses. Any ψ-form can be reduced to\na well-formed equivalent; henceforth, we only consider well-formed ψ-forms. The reduction\nprocedure is simple and consists of examining pairs of different substitutions σi, σj of Σ(ψ).\nIf σi ⊆ σj , then (and only then) [Ej(ψ)] ⊆ [Ei(ψ)], and so we remove σj from Σ(ψ) and vice\nversa. In the worst case we will need to examine E(E − 1)/2 pairs.\n2.3. ψ-form Logic. An interpretation or world is a triple (D, M, A), where D is a\ndomain, M is a mapping between the constants of the language and the domain objects,\nand A is a truth assignment on all ground atoms of the language. We limit worlds to those\nwith infinite domains. We further assume each constant denotes a distinct domain object.\nA model of a proposition is a world that assigns true to that formula.\nWhen s is a proposition or a set of propositions and w is a world, we write w(s) if and\nonly if w(s) is true in w. For a set of propositions to be true in w, each element must be\ntrue in w. We write I(s) to denote the set of all models of s, i.e. I(s) = {w|w(s)}.\nWe use the standard rules regarding the interpretation of atoms, negation and logical\nconnectives. A ψ-form or a set of ψ-forms, denoted below by ✷, is interpreted as (a possibly\ninfinite) conjunction of all ground clauses it represents, and therefore\nI(✷) =\n⋂\nc∈φ(✷)\nI(c). (2.8)\n10 T. BABAIAN AND J. G. SCHMOLZE\nNow that we have defined an interpretation for ψ-forms we can define entailment in\na logical language containing ψ-forms in the usual way. A formula a entails a formula b,\ndenoted a |= b, if and only if every model of a is also a model of b, i.e. I(a) ⊆ I(b).\nWe first examine the entailment between two ground clauses of negated literals. We\nwrite c1 ⊆ c2 when a set of literals of the clause c1 is a subset of the set of literals of the\nclause c2. It is easy to see, that given two non-empty grownd clauses of negated literals\nc1 and c2, c1 |= c2 if and only if c1 ⊆ c2. Further, observe, that when ✷1 and ✷2 are two\nψ-forms or sets of ψ-forms ✷1 = ✷2 if and only if ✷1 |= ✷2 and ✷2 |= ✷1.\nNote as well, that ψ = [Q(~x) except {σ1, . . . , σn}] can be equivalently written as a first\norder formula that universally quantifies the variables of ψ:\n∀~x .Q(~x) ∨ c1 ∨ . . . ∨ cn,\nwhere each ci is an equality constraint obtained from σi, for instance if σi = {x = A, y = B}\nthen ci = (x = A ∧ y = B). We therefore call non-singleton ψ-forms quantified.\n3. Calculus of ψ-forms\nIn this section we present the calculus of ψ-forms. We first demonstrate how subset,\nintersection and set-difference between ψ-forms are computed in simple cases. These opera-\ntions lay the foundation for algorithms that compute entailment, as well as the computation\nof image and e-difference operations, which we define here. These operations are essential\nparts of reasoning and planning with ψ-forms. For example, they are used in PSIPOP ([5])\nand PSIGraph ([12]) planners to determine if an effect of an action can bring about or\nundo a goal. E-difference is also used in the PSIPLAN’s state update computation (4.6),\npresented later in this paper. Sound and complete methods of computing entailment, image\nand e-difference of fixed length ψ-forms are presented in the form of theorems that are easily\nconvertible to algorithms. We summarize the complexity of computing entailment, image\nand e-difference between ψ-forms in PSIPLAN.\n3.1. Operations ⊆,∩ and −. The operations subset, intersection and set-difference be-\ntween ψ-forms are defined in the obvious way: for any ground clause c\nc ∈ ✷1 ∗ ✷2 if and only if c ∈ φ(✷1) ∗ φ(✷2)\nwhere ✷ represents either a single ψ-form or a set of ψ-forms, and ∗ represents any of the\noperations ⊆,∩ or −. Calculation of ⊆,∩ and − is straightforward for simple fixed length\nψ-forms.\n[M(ψ1)] ⊆ [M(ψ2)] if and only if all of [M(ψ1)]’s clauses are also clauses of [M(ψ2)].\nThis requires that the main clause M(ψ2) set-matches onto M(ψ1) with some substitu-\ntion σ. When this is true, for every ground substitution σ1 on the variables of ψ1, there\nis a ground substitution σ2 = σσ1 on M(ψ2) such that M(ψ1)σ1 = M(ψ2)σ2. Thus,\nfor any ψ1 and ψ2, checking whether or not [M(ψ1)] ⊆ [M(ψ2)] amounts to computing\nMGU≡(M(ψ2),M(ψ1),V(ψ2)).\n[M(ψ1)] ∩ [M(ψ2)] is defined by all ground substitutions σg such that M(ψ1)σg =\nM(ψ2)σg. The set of most general σ’s for which M(ψ1)σ =M(ψ2)σ defines a set of main\nclauses that generate the ψ-forms denoting the intersection [M(ψ1)] ∩ [M(ψ2)].\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 11\nTrans(σ, ψ)\n– Returns part of σ that binds variables of ψ in exception-conformant format\n1. In σ, replace all groups of bindings of the form v1 = v, . . . , vn = v,\nwhere n > 1, v1, . . . vn ∈ V(ψ), and v 6∈ V(ψ) with a set of bindings:\nv1 = vn, . . . , vn−1 = vn.\n2. Further, remove from σ all bindings involving variables that are not in V(ψ).\n3. Return σ\nFigure 3: Procedure Trans(σ, ψ) transforms a substitution σ into format suitable for the\nexceptions of ψ.\nWe say clause a set-unifies with clause b if and only if there exists a substitution σ\nsuch that aσ = bσ, denoting the set of all most general such σ’s by MGU≡(a, b). Thus,\n[M(ψ1)] ∩ [M(ψ2)] = {[M(ψ1)σ] |σ ∈MGU≡(M(ψ1),M(ψ2))}.\nThe intersection of the main parts of two ψ-forms ψ1 and ψ2 consists of the clauses\ndenoted equivalently by each of the two sets of ψ-forms\n{[M(ψ1)σ] |σ ∈MGU≡(M(ψ1),M(ψ2))}, and\n{[M(ψ2)σ] |σ ∈MGU≡(M(ψ1),M(ψ2))}.\nFor example, when ψ1 = [¬P (x,A)] and ψ2 = [¬P (B, y)], MGU≡(M(ψ1),M(ψ2)) =\n{{x = B, y = A}} and ψ1 ∩ ψ2 equals {¬P (B,A)}.\nWhen ψ1 is simple and is a subset of [M(ψ2)] and both ψ-forms are fixed length, sub-\ntracting ψ1 from ψ2 is a matter of adding a substitution σ =MGU≡(M(ψ2),M(ψ1),V(ψ2)),\nwhich generates ψ1 from M(ψ2), to Σ(ψ2). Indeed, [M(ψ2)σ] equals ψ1, thus, by adding\nσ to Σ(ψ2) we are subtracting from ψ2 the clauses of ψ1. As a matching substitution, σ\nmay contain bindings on variables of ψ2 either to variables of ψ1 or to constants. Procedure\nTrans(σ, ψ2), presented in Figure 3, generates an equivalent substitution, which uses only\nvariables from V(ψ2), as only those variables can appear in the set of ψ2’s exceptions.\nFigure 4 contains examples of ψ-form calculations described to this point.\nWhen ψ1 is simple, and both ψ1 and ψ2 are fixed length, but ψ1 is not necessarily a\nsubset of [M(ψ2)], the computation of ψ2−ψ1 is reduced to the previous case by observing\nthat ψ2 −ψ1 = ψ2− ([M(ψ2)]∩ [M(ψ1)]), since [M(ψ2)]∩ [M(ψ1)] is a subset of [M(ψ2)].\nWhen [M(ψ1)] ∩ [M(ψ2)] is empty, then ψ2 − ψ1 is just ψ2.\nOperations of ∩,− and ⊆ for two simple fixed length ψ-forms and membership of a\nclause in any ψ-form each take constant bounded time when the number of clauses, variables\nand exceptions in a ψ-form are constant bounded.\n3.2. E-Difference and Image. To capture the relations between parts of ψ-forms neces-\nsary to formulate the ψ-form entailment theorem, we introduce two new operations: the\nimage, denoted ψ1 ⊲ ψ2, is the subset of ψ2 that is entailed by ψ1, and the e-difference, de-\nnoted ψ2\ne ψ1, is the subset of ψ2 that is not entailed by ψ1. Thus, (ψ2\ne ψ1) and (ψ1 ⊲ψ2)\nalways partition ψ2.\nFormally, for any two sets of ground propositions A and B, e-difference and image\nare defined respectively as follows.\nB e A = {b | b ∈ B ∧ A 6|= b},\nA ⊲ B = {b | b ∈ B ∧ A |= b}.\n12 T. BABAIAN AND J. G. SCHMOLZE\nCalculation a,b/Operator Result\na = ¬P (x, y), b = ¬P (v,A)\nMGU≡(a, b, Va) = {{x = v, y = A}}\nyes\na = ¬P (B, y), b = ¬P (A,x)\nMGU≡(a, b, Va) = ∅\n[b] ⊆ [a] ? no\na = ¬R(x, y, z,A) ∨ ¬Q(t), b = ¬R(w,C, v,A) ∨ ¬Q(w)\nMGU≡(a, b) = {{x = w, y = C, z = v, t = w}}\nTrans(MGU≡(a, b), [a]) = {{x = t, y = C}}\n[a] ∩ [b] {[¬R(t, C, z,A) ∨ ¬Q(t)]}\n[a]− [b] [¬R(x, y, z,A) ∨ ¬Q(t) except {{x = t, y = C}}]\nFigure 4: Examples of the calculus computations involving set-match (p. 8) and set-\nunification (p. 11) operators on simple ψ-forms [a] and [b].\nExample 3.1. Let A = {p,¬q} and B = {p, r,¬q ∨ ¬v}. Then, A ⊲ B = {p,¬q ∨ ¬v} and\nB e A = {r}.\nThe following equivalences trivially follow from the definitions.\nB e A = B − (A ⊲ B) (3.1)\nA ⊲ B = B − (B e A) (3.2)\nAs we show later in this section, the operations ⊲ and e applied to fixed length ψ-forms\nproduce sets of clauses that can always be represented by a finite set of fixed length ψ-forms.\n3.3. Entailment. The next Theorem establishes the fact that a set of ψ-forms Ψ entails\nanother ψ-form ψ if and only if for each clause of ψ there exists a clause in Ψ entailing\nit. The intuition behind this observation is: any two negated ground clauses c1 and c2\nconjoined entail the same set of negated ground clauses as the union of clauses entailed by\neach of c1 and c2 separately.\nTheorem 3.2. Given a set of ψ-forms Ψ = {ψ1, . . . , ψn} and a ψ-form ψ, Ψ |= ψ if and\nonly if for every ground clause c ∈ ψ there exists a ground clause c′ ∈ Ψ such that c′ |= c.\nProof. (⇒) A proof by contradiction is straightforward and thus omitted.\n(⇐) Trivially follows from the definition of entailment.\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 13\nψ1\nψ2\nψ3\nψ\nFigure 5: For a set of ψ-forms {ψ1, ψ2, ψ3} to entail another ψ-form ψ there must be a\nψ-form in the set (ψ2 in the figure) whose main part entails the main part of ψ.\nThe property critical for the efficiency of ψ-form reasoning is formulated in Theorem\n3.4 and depicted in Figure 5: given a set of ψ-forms Ψ = {ψ1, . . . , ψn} Ψ |= ψ only if there\nis a ψ-form ψi ∈ Ψ that nearly entails ψ, i.e. [M(ψi)] |= [M(ψ)].\nNearly entailment is a necessary condition for ψ-form entailment. As follows from\nTheorem 3.3, entailment between two simple ψ-forms is a matter of finding what we call a\nsubset-match, or subsumption of their main clauses.\nGiven two clauses a and b, where variables in a and b are distinct and denoted Va and\nVb respectively, we say that a subsumes b (or a subset-matches b ) if and only if there\nexists a substitution σ on variables in Va such that aσ ⊆ b. We denote by MGU⊆(a, b, Va)\nthe set of all such σ’s.\nNote that there can be more than one way a clause can subsume another clause. For\nexample, matching a = P (x, y) onto b = P (z,D) ∨ Q(D,E) ∨ P (A,B), produces two dif-\nferent substitutions: {x = z, y = D} and {x = A, y = B}, and hence MGU⊆(a, b, {x, y}) =\n{{x = z, y = D}, {x = A, y = B}}.\nTheorem 3.3. Given two simple ψ-forms, ψ1 and ψ2, ψ1 |= ψ2 if and only if the main\nclause of ψ1 subsumes the main clause of ψ2, i.e. there exists a substitution σ such that\nM(ψ1)σ ⊆M(ψ2) and consequently MGU⊆(M(ψ1),M(ψ2),V(ψ1)) 6= ∅.\nProof. (⇒) Given ψ1 |= ψ2, suppose that\nM(ψ1) = {¬Q1(~x1), . . . ,¬Qn(~xn)} and M(ψ2) = {¬P1(~y1), . . . ,¬Pk(~yk)} .\nAssume MGU⊆(M(ψ1),M(ψ2),V(ψ1)) = ∅.\nThis means that for any substitution σ on variables of ψ1, there exists a literal in\nM(ψ1)σ, which does not match any of {¬P1(~y1), . . . ,¬Pk(~yk)}. We can assume without\nany loss of generality that the mismatched literal is ¬Q1(~x1). Note that the mismatch can\noccur due to one of the following reasons (we also call them mismatch types):\n(1) the predicate denoted by Q1 is not the same as denoted by Pi,\n(2) the argument list of Q1(~x1) has a constant, call it a matching value, at the position\nwhere Pi(~yi)’s argument list has a variable, call it, a mismatched variable.\n(3) the argument list of Q1(~x1) has a constant, call it Bi, at the position where Pi(~yi)\nhas a different constant, Ci.\nWe now construct a clause from ψ2 and show that it cannot contain as a subclause any\nclause of ψ1. According to Theorem 3.2 we would then contradict the fact that ψ1 |= ψ2.\nLet P ′ be an instance of M(ψ2), which is obtained by assigning to each variable in\nV(ψ2) a constant value which does not occur anywhere in M(ψ1). Since the number of\nconstants in the language is infinite, this can always be done. SinceM(ψ1) does not subset\n14 T. BABAIAN AND J. G. SCHMOLZE\nmatch onto M(ψ2), it surely does not subset match onto P\n′, because for any substitution\nσ on V(ψ1) the number of mismatches between any Q and any literal of P\n′ is at least the\nsame as the number of mismatches between a Q and a P in M(ψ2), or higher, because of\nthe new mismatches of type 3. Therefore, there is no instance ofM(ψ1) that is a subclause\nof P ′.\n(⇐) The existence of σ such that M(ψ1)σ ⊆M(ψ2) means that for every ground clause c2\nof ψ2, assuming c2 =M(ψ2)σ\n′, there is a clause c1 in ψ1, c1 =M(ψ1)σσ\n′, which is a subset\nof c2, and therefore ψ1 |= ψ2.\nWe proceed to the necessary condition for ψ-form entailment. Theorem 3.4 states that\nin order for a set of ψ-forms Ψ to entail another ψ-form ψ, there must exist a ψ-form in Ψ\nthat nearly entails ψ, i.e. whose main part entails the main part of ψ.\nTheorem 3.4. Given a set of ψ-forms Ψ = {ψ1, . . . , ψn} and a ψ-form ψ, Ψ |= ψ only if\nthere is a ψ-form ψi in Ψ such that ([M(ψi)] |= [M(ψ)]).\nProof. We construct a clause of [M(ψ)] and show that if none of [M(ψ1)], . . . , [M(ψn)]\nentail it, then {ψ1, . . . , ψn} does not entail ψ.\nSuppose none of [M(ψ1)], . . . , [M(ψn)] entail [M(ψ)]. Therefore according to Theo-\nrem 3.3 none of the main parts of these ψ-forms subset match onto M(ψ). Let σ be a\nsubstitution on V(ψ) that assigns to each variable a constant value that does not occur\nin any of ψ1, . . . , ψn, nor in the exceptions of ψ. This is always possible due to infinite\nnumber of constants in the language. None of the clauses in M(ψ1), . . . ,M(ψn) subset\nmatch onto c = M(ψ)σ, because none of the main clauses of these ψ-forms subsume\nM(ψ) and the constants of σ do not appear in any of [M(ψ1)], . . . , [M(ψn)]. Thus, the\nclause c = M(ψ)σ of ψ is not entailed by any clause in {[M(ψ1)], . . . , [M(ψn)]}. Since\nΦ ⊆ {[M(ψ1)], . . . , [M(ψn)]}, according to Theorem 3.2 we conclude that Ψ 6|= ψ. We\narrive at a contradiction.\nThe next example demonstrates that Theorem 3.4 contains a necessary but not sufficient\ncondition for the ψ-form entailment, and further motivates the operations of image and e-\ndifference.\nExample 3.5. Consider two ψ-forms ψ1 and ψ2 below.\nψ1 = [¬In(x,Box1) ∨ ¬Fragile(x) except {{x =Wine}}]\nψ2 = [¬In(y,Box1) ∨ ¬Fragile(y) ∨ ¬Owner(y, Joe)]\nHere, In(x, y) states that x is in y, Fragile(x) denotes that x is a fragile item, and\nOwner(x, y) denotes that x’s owner is y. Thus, ψ1 states that there are no fragile items in\nBox1 except for possibly a bottle of wine. ψ2 states that there are no fragile items in Box1\nthat are owned by Joe. Notice that ψ2 is simple and thus ψ2 = [M(ψ2)].\nThe main clause of ψ1 subsumes the main clause of ψ2, so ψ1 nearly entails ψ2. There-\nfore, the main part of ψ1, [M(ψ1)] entails ψ2, but because the exception of ψ1 weak-\nens it, ψ1 does not entail ψ2. In fact, ψ1 entails all clauses of ψ2 except for the clause\n¬In(Wine,Box1) ∨ ¬Fragile(Wine) ∨ ¬Owner(Wine, Joe).\nThe only clause of ψ2 that is not entailed by ψ1 is ¬In(Wine,Box1)∨¬Fragile(Wine)∨\n¬Owner(Wine, Joe), which is exactly the clause entailed by ψ1’s single exception, i.e.\nψ2\ne ψ1 = [E1(ψ1)] ⊲ ψ2 = [¬In(Wine,Box1) ∨ ¬Fragile(Wine) ∨ ¬Owner(Wine, Joe)]\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 15\nψ1\n[E1(ψ1)] = [¬In(wine,Box1) ∨ ¬Fragile(wine)]\nψ2\ne ψ1 = [¬In(wine,Box1) ∨ ¬Fragile(wine) ∨ ¬Owner(wine, Joe)]\nψ2\nFigure 6: Illustrates Example 3.5. The small ellipse inside ψ2 represents the only clause of\nψ2 not entailed by ψ1, i.e. ψ2\ne ψ1. The area between the outer and the inner\nellipsis is the image ψ1 ⊲ ψ2.\nThe image ψ1 ⊲ ψ2 is simply ψ2 with a single exception added:\nψ1 ⊲ ψ2 = [¬In(y,Box1) ∨ ¬Fragile(y) ∨ ¬Owner(y, Joe) except {{y =Wine}}].\nSo, while ψ1 nearly entails ψ2, the e-difference ψ2\ne ψ1 is not empty, i.e. ψ1 does not\nentail ψ2. This is illustrated in Figure 6.\nTheorem 3.6. ψ-form Entailment. Let ψ1, . . . , ψn and ψ be arbitrary ψ-forms.\n{ψ1, . . . , ψn} |= ψ if and only if there exists a k,1 ≤ k ≤ n, such that:\n• [M(ψk)] |= [M(ψ)] (i.e., the main part of ψk entails the main part of ψ), and\n• {ψ1, . . . , ψk−1, ψk+1, . . . , ψn} |= ψ\ne ψk.\nProof. The first requirement of this Theorem follows from Theorem 3.4. While the main\npart of ψk entails ψ, the exceptions of ψk weaken ψk. Thus, each clause in ψ\ne ψk must be\nentailed by some other ψ-form in {ψ1, . . . , ψk−1, ψk+1, . . . , ψn}.\nThus, in order for a set of ψ-forms Ψ to entail another ψ-form ψ, there must exist a\nψ-form ψk in Ψ that entails most of ψ, and the rest of ψ, i.e. ψ\ne ψk must be entailed by Ψ\nwithout ψk.\nWe have formulated the necessary and sufficient conditions for ψ-form entailment using\ne-difference. We next present the methods of computing image and e-difference via simple\noperations of subset matching and unification, first for simple fixed length ψ-forms and\nthen for fixed length ψ-forms with exceptions. Complexity bounds for the computation of\nentailment, image and e-difference appear in Section 3.6.\n3.4. Simple Fixed Length ψ-forms. In this section, we present methods of computing\nthe operations of ψ-form image and e-difference for two simple fixed length ψ-forms.\nThe image ψ1 ⊲ ψ2 denotes a set of all clauses of ψ2 that are entailed by ψ1, i.e. all\nclauses of ψ2 that have a subclause in ψ1. Thus, when ψ1 and ψ2 are simple fixed length ψ-\nforms, computing ψ1⊲ψ2 reduces to instantiatingM(ψ2) with subset-unifying substitutions,\ni.e. substitutions σ for which M(ψ1)σ ⊆M(ψ2)σ. Formally, we say that a subset-unifies\nwith b if and only if there exists a substitution σ such that aσ ⊆ bσ, denoting the set of all\nmost general such σ’s by MGU⊆(a, b).\nFor example, consider a = P (x, y), b = P (z,D) ∨ Q(D,E) ∨ P (A,B), and c =\nP (A,x). We have MGU⊆(a, b) = {{x = z, y = D}, {x = A, y = B}} and MGU⊆(c, b) =\n{{z = A,x = D}, {x = B}}.\n16 T. BABAIAN AND J. G. SCHMOLZE\nCalculation a,b/Operator Result\na = ¬P (x,A), b = ¬P (B, y) ∨ ¬P (C, z) ∨ ¬Q(y)\nMGU⊆(a, b, Va) = ∅\n[a] |= [b] ? no\nMGU⊆(a, b) = {{x = B, y = A}, {x = C, z = A}}\n[a] ⊲ [b] {[¬P (B,A) ∨ ¬P (C, z) ∨ ¬Q(A)],\n[¬P (B, y) ∨ ¬P (C,A) ∨ ¬Q(y)]}\n[b] e [a] [¬P (B, y) ∨ ¬P (C, z) ∨ ¬Q(y) except {{y = A}, {z = A}}]\nFigure 7: Examples of the entailment, image and e-difference computations on simple ψ-\nformsThese computations utilize subset-match and subset-unification operators\n(defined on pages 13 and 15 respectively).\nTheorem 3.7. Let ψ1 and ψ2 be simple fixed length ψ-forms.\nψ1 ⊲ ψ2 = {[M(ψ2)σ] |σ ∈MGU⊆(M(ψ1),M(ψ2))}\nProof. It is easy to verify equality of the two sets by showing inclusion both ways.\nComputing the e-difference, ψ2\ne ψ1, similar to the regular difference ψ2 − ψ1 (page\n11), amounts to adding exceptions to ψ2. These exceptions represent the set of all clauses\nof [M(ψ2)] entailed by [M(ψ1)], i.e. the image [M(ψ1)] ⊲ [M(ψ2)], and are obtained by\ncomputing the MGU⊆(M(ψ1),M(ψ2)).\nTheorem 3.8. Let ψ2 be an arbitrary fixed length ψ-form and ψ1 be a simple fixed length\nψ-form.\nψ2\ne ψ1 =\n{\n∅, if ψ1 |= ψ2,\n{[M(ψ2) except Σ(ψ2) ∪Σ\n′]}, otherwise,\nwhere Σ′ = {σ′ |σ′ = Trans(σ, ψ2), where σ ∈MGU⊆(M(ψ1),M(ψ2))}. (Recall that\nTrans(σ, ψ2) defined in Figure 3 transforms substitution σ to an equivalent one that con-\nforms to the format of exceptions of ψ2.)\nProof. To prove this theorem we use Theorem 3.7 and the equality ψ2\ne ψ1 = ψ2−(ψ1 ⊲ ψ2).\nAccording to definition (2.3)\n[M(ψ2) except Σ(ψ2) ∪ Σ\n′] = [M(ψ2)]− E(ψ2)− [M(ψ2)σ\n′\n1]− . . . − [M(ψ2)σ\n′\nn],\nwhere Σ′ = {σ′1, . . . , σ\n′\nn}. Note that [M(ψ2)] − E(ψ2) = ψ2, and that [M(ψ2)σ\n′\n1] ∪ . . . ∪\n[M(ψ2)σ\n′\nn] = ψ1 ⊲ [M(ψ2)], and thus\n[M(ψ2) except Σ(ψ2) ∪Σ\n′] = ψ2 − (ψ1 ⊲ [M(ψ2)]).\nIt remains to show that\nψ2 − (ψ1 ⊲ [M(ψ2)]) = ψ2 − (ψ1 ⊲ ψ2). (3.3)\nIndeed (ψ1 ⊲ ψ2) = (ψ1 ⊲ [M(ψ2)])− (ψ1 ⊲ E(ψ2)). Substituting the right hand side instead\nof (ψ1 ⊲ ψ2) in (3.3), we get\nψ2 − (ψ1 ⊲ [M(ψ2)]) = ψ2 − [(ψ1 ⊲ [M(ψ2)])− (ψ1 ⊲ E(ψ2))]\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 17\nSince (ψ1 ⊲ E(ψ2)) is in E(ψ2) and therefore definitely not in ψ2,\nψ2 − (ψ1 ⊲ [M(ψ2)]) = ψ2 − [(ψ1 ⊲ [M(ψ2)])− (ψ1 ⊲ E(ψ2))] = ψ2 − (ψ1 ⊲ [M(ψ2)])\nWe have arrived at a tautology, which proves (3.3).\nNote that the result of the e-difference may not be a well-formed ψ-form.\nComputeSimpleImg(ψ1, ψ2)\n– Computes [M(ψ1)] ⊲ [M(ψ2)]\nCompute Θ =MGU⊆(M(ψ1),M(ψ2))\nSet Ψ = ∅\nFor each θi ∈ Θ do\nSet Ψ = Ψ ∪ {[M(ψ2)θi]}\nReturn Ψ\nComputeSimpleEDiff(ψ2, ψ1)\n– Computes ψ2\ne [M(ψ1)]\nIf MGU⊆(M(ψ1),M(ψ2),V(ψ1)) 6= ∅\nReturn ∅.\nCompute Θ =MGU⊆(M(ψ1),M(ψ2))\nΣ′ = ∅\nFor each θi ∈ Θ do\nSet Σ′ = Σ′ ∪ Trans(θi, ψ2)\nReturn {[M(ψ2) except Σ(ψ2) ∪Σ′]}.\nFigure 8: Image and e-difference operations for simple ψ-forms.\nComputeSimpleImg(ψ1, ψ2) and ComputeSimpleEDiff(ψ1, ψ2) return\n[M(ψ1)] ⊲ [M(ψ2)] and ψ2\ne [M(ψ1)] respectively.\nFigure 7 presents examples of computing image and e-difference between simple ψ-\nforms, and Figure 8 presents algorithms for these computations, based on Theorems 3.7\nand 3.8.\n3.5. Arbitrary Fixed Length ψ-forms. In this section, we present methods of computing\nthe operations of ψ-form image and e-difference for two arbitrary fixed length ψ-forms.\nLet ψ1 and ψ2 be arbitrary fixed length ψ-forms. To find either the image or the e-\ndifference we first find the image of the main part of ψ1 onto the main part of ψ2. Since the\nexceptions of ψ1 weaken it, we must then calculate the part of [M(ψ2)] that is not entailed\nby ψ1 due to the exceptions. We’ll call this a set of “holes” ( denoted by H(ψ1, ψ2)).\nFormally, we define set of holes H(ψ1, ψ2) as follows\nH(ψ1, ψ2) = ([M(ψ1)] ⊲ [M(ψ2)])− (ψ1 ⊲ [M(ψ2)]), (3.4)\ni.e. holes are parts of [M(ψ2)] that are entailed by [M(ψ1)], but not by ψ1.\nImage and e-difference operations are easily formulated using H(ψ1, ψ2). The image\nof ψ1 onto ψ2 consists of clauses of the main part of ψ2 entailed by the main part of ψ1,\ni.e. [M(ψ1)] ⊲ [M(ψ2)], minus the set of holes H(ψ1, ψ2) and minus ψ2’s own exceptions.\nSimilarly, the e-difference ψ2\ne ψ1 consists of the part of the main part of ψ2, [M(ψ2)], not\nentailed by [M(ψ1)], i.e. [M(ψ2)]\ne [M(ψ1)] plus the set of holes H(ψ1, ψ2), minus the set\nof ψ2’s exceptions. These two facts are presented in the following two Lemmas.\nLemma 3.9. ψ1 ⊲ ψ2 = (([M(ψ1)] ⊲ [M(ψ2)])−H(ψ1, ψ2))− E(ψ2) .\nProof. Let A = [M(ψ1)] ⊲ [M(ψ2)]. We substitute the definition of H(ψ1, ψ2) from (3.4) on\nthe right hand side.\nψ1 ⊲ ψ2 = A− (A− (ψ1 ⊲ [M(ψ2)]))− E(ψ2)\n18 T. BABAIAN AND J. G. SCHMOLZE\nWhen X,Y and Z denote arbitrary sets, we have\nX − (Y − Z) = (X − Y ) ∪ (X ∩ Y ∩ Z), (3.5)\nso\nψ1 ⊲ ψ2 = (A−A) ∪ (A ∩A ∩ (ψ1 ⊲ [M(ψ2)]))− E(ψ2)\n= (A ∩ (ψ1 ⊲ [M(ψ2)]))− E(ψ2)\n= (ψ1 ⊲ [M(ψ2)])− E(ψ2)\n= ψ1 ⊲ ([M(ψ2)]− E(ψ2))\n= ψ1 ⊲ ψ2\nLemma 3.10. ψ2\ne ψ1 = (([M(ψ2)]\ne [M(ψ1)]) ∪H(ψ1, ψ2))− E(ψ2) .\nProof. From Lemma 3.9 and equivalence (3.1) we have ψ2\ne ψ1 = ψ2 − (ψ1 ⊲ ψ2), or\nψ2\ne ψ1 = ψ2︸︷︷︸\nA\n−[(([M(ψ1)] ⊲ [M(ψ2)])︸ ︷︷ ︸\nB\n−H(ψ1, ψ2)︸ ︷︷ ︸\nC\n)− E(ψ2)︸ ︷︷ ︸\nD\n]\nUsing (3.5), we rewrite the right hand side equivalently\nψ2\ne ψ1 = A− ((B − C)−D) = (A− (B − C)) ∪ (A ∩ (B − C) ∩D)\nSince in our case A ∩D = ∅, therefore (A ∩ (B − C) ∩D) = ∅ and we get\nψ2\ne ψ1 = (A− (B − C)) = (A−B) ∪ (A ∩B ∩ C)\nWe now evaluate A∩B∩C. We note that A∩B = ([M(ψ2)]−D)∩B and since (X−Y )∩Z =\nX ∩ Z − Y ∩ Z, we have\nA ∩B = [M(ψ2)] ∩B −D ∩B = B −B ∩D = B −D.\nNext, (B−D)∩C = B ∩C −D∩C and since C ⊆ B, (B−D)∩C = C −D∩C = C −D,\nso we get\nψ2\ne ψ1 = (A−B) ∪ (C −D)\n= (ψ2 − ([M(ψ1)] ⊲ [M(ψ2)])) ∪ (H(ψ1, ψ2)− E(ψ2))\n= ([M(ψ2)]− E(ψ2)− ([M(ψ1)] ⊲ [M(ψ2)])) ∪ (H(ψ1, ψ2)− E(ψ2))\n= ([M(ψ2)]\ne [M(ψ1)]− E(ψ2)) ∪ (H(ψ1, ψ2)− E(ψ2))\n= (([M(ψ2)]\ne [M(ψ1)]) ∪H(ψ1, ψ2))− E(ψ2).\nWe calculate ψ1 ⊲ ψ2 and ψ1\ne ψ1 separately in each of the following three cases\nCase 1: MGU⊆(M(ψ1),M(ψ2)) = ∅, i.e. the image [M(ψ1)] ⊲ [M(ψ2)] is empty.\nCase 2: MGU⊆(M(ψ1),M(ψ2),V(ψ1)) 6= ∅, i.e. by Theorem 3.3, [M(ψ1)] |= [M(ψ2)] and\nhence the image [M(ψ1)] ⊲ [M(ψ2)] equals the entire [M(ψ2)].\nCase 3: MGU⊆(M(ψ1),M(ψ2)) 6= ∅, i.e. the image [M(ψ1)] ⊲ [M(ψ2)] is non-empty.\nCases 1 and 3 are complementary. However we have separated case 2, which is a\nspecific subcase of 3, because it comes up while deciding ψ-form entailment (see Theorem\n3.6). Moreover, case 3 is reduced to case 2, as we will demonstrate.\nCase 1 is the simplest and is covered by Theorem 3.11.\nTheorem 3.11. If MGU⊆(M(ψ1),M(ψ2)) = ∅, ψ1 ⊲ ψ2 = ∅ and ψ2\ne ψ1 = {ψ2}.\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 19\nProof. As follows from Theorem 3.7, MGU⊆(M(ψ1),M(ψ2)) = ∅ implies that the image\n[M(ψ1)]⊲[M(ψ2)] is empty. Since (ψ1⊲ψ2) ⊆ ([M(ψ1)] ⊲ [M(ψ2)]), we have that ψ1⊲ψ2 = ∅\nand therefore by equivalence (3.1) ψ2\ne ψ1 = ψ2.\nCase 2 amounts to ψ1 nearly entailing ψ2.\nTheorem 3.12. When [M(ψ1)] |= [M(ψ2)]\nψ1 ⊲ ψ2 = ψ2 −H(ψ1, ψ2) (3.6)\nψ2\ne ψ1 = H(ψ1, ψ2)− E(ψ2) (3.7)\nProof. (3.6) and (3.7) trivially follow from Lemma 3.9 and definition (3.4) by substituting\n[M(ψ2)] in place of [M(ψ1)]⊲ [M(ψ2)] and substituting ∅ in place of [M(ψ1)]\ne [M(ψ2)].\nThe expression for the set of holes H(ψ1, ψ2) in this case is derived in Lemma 3.14. We\nfirst demonstrate the computation of the set of holes, image and e-difference in the following\nexample.\nThe algorithm is straightforward when there is only one subset-unifier of M(ψ1) with\nM(ψ2), i.e. each clause of [M(ψ1)] ⊲ [M(ψ2)] is entailed by exactly one clause of [M(ψ1)].\nThe set of holes in this case is simply the union of images from each exception of ψ1 onto\n[M(ψ2)].\nExample 3.13. Consider\nψ1 = [¬P (x, y, z) except {{x = B}, {x = C, y = D}, {x = A}}]\nψ2 = [¬P (w,E,A) except {{w = G}}]\nSince there is only one subset-unifier of M(ψ1) with M(ψ2), the image of ψ1 onto ψ2 is\nsimply the image [M(ψ1)] onto [M(ψ2)] minus exceptions of ψ2 and the image of exceptions\nof ψ1 on [M(ψ2)], i.e.\nψ1 ⊲ ψ2 = [M(ψ1)] ⊲ [M(ψ2)]− E(ψ2)−\n3⋃\ni=1\n([Ei(ψ1)] ⊲ [M(ψ2)])\nIn this case [M(ψ1)] ⊲ [M(ψ2)] = [M(ψ2)] and, since (by definition (2.3)) ψ2 =\n[M(ψ2)]− E(ψ2),\nψ1 ⊲ ψ2 = [¬P (w,E,A) except {{w = G}}]− [¬P (B, y, z)] ⊲ [¬P (w,E,A)]\n− [¬P (C,D, z)] ⊲ [¬P (w,E,A)][¬P (A, y, z)] ⊲ [¬P (w,E,A)]\n= [¬P (w,E,A) except {{w = G}}]− [¬P (B,E,A)]− [¬P (A,E,A)]\n= [¬P (w,E,A) except {{w = G}, {w = B}, {w = A}}]\nComputing the holes is more complex when there is more than one subset-unifier of\nM(ψ1) withM(ψ2), because in this case some clauses of [M(ψ1)]⊲ [M(ψ2)] are entailed by\nmore than one clause of [M(ψ1)]. Then, even though an exception removes from [M(ψ1)]\nan entailing clause for some clause c of ψ2, c may be entailed by another clause in ψ1, and\nconsequently the set of holes is not simply a set of images from ψ1’s exceptions, but rather\nan intersection of such images. Example 3.15 illustrates this computation.\nWe derive an expression for calculating H(ψ1, ψ2) in the next Lemma by first intro-\nducing the set of ψ-forms within [M(ψ1)] (denoted Ψ1), all of which entail some part of\n[M(ψ2)], and showing how to combine them to calculate H(ψ1, ψ2).\n20 T. BABAIAN AND J. G. SCHMOLZE\nCompute H(ψ1, ψ2)\n– Requires that [M(ψ1)] |= [M(ψ2)]\nCompute Θ =MGU⊆(M(ψ1),M(ψ2),V(ψ1))\nSet Ψ = ∅\nFor each i from 1 to ‖Θ‖ do\nSet ψi\n1\n= [M(ψ1)θi]\nSet Ψ2 = ∅\nFor each j from 1 to ‖Σ(ψ1)‖ do\nSet Iij = (([Ej(ψ1)] ∩ ψ\ni\n1\n) ⊲ [M(ψ2)])\nSet Ψ2 = Ψ2 ∪ Iij\nIf Ψ2 = ∅ Then Return ∅\nIf i = 1 Then Set Ψ = Ψ2\nElse Set Ψ = Ψ ∩Ψ2.\nReturn Ψ\nComputeImg2(ψ1, ψ2)\n– Requires that [M(ψ1)] |= [M(ψ2)]\nSet ΨH = ComputeH(ψ1, ψ2), ψ = ψ2\nFor each simple ψ-form ψh ∈ ΨH\nSet ψ = ψ − ψh\nReturn {ψ}\nComputeEDiff2(ψ2, ψ1)\n– Requires that [M(ψ1)] |= [M(ψ2)]\nSet ΨH = ComputeH(ψ1, ψ2), Ψ = ∅\nFor each simple ψ-form ψh ∈ ΨH\nSet ψ = ψh\nFor each simple ψ-form ψe ∈ E(ψ)\nSet ψ = ψ − ψe\nSet Ψ = Ψ ∪ {ψ}\nReturn Ψ\nFigure 9: Computing the set of holes, image and e-difference operations in case ψ1 nearly\nentails ψ2.\nLemma 3.14. Let Θ =MGU⊆(M(ψ1),M(ψ2),V(ψ1)) be nonempty. Let Ψ1 be defined as\nfollows.\nΨ1 = {ψ\ni\n1 |ψ\ni\n1 = [M(ψ1)θi], θi ∈ Θ, 1 ≤ i ≤ ‖Θ‖}. (3.8)\nThen,\nH(ψ1, ψ2) =\n⋂\nψi\n1\n∈Ψ1\n‖Σ(ψ1)‖⋃\nj=0\n(([Ej(ψ1)] ∩ ψ\ni\n1) ⊲ [M(ψ2)]), (3.9)\nProof. Each ψi1 entails [M(ψ2)], i.e. ψ\ni\n1 ⊲ [M(ψ2)] = [M(ψ2)] for each 1 ≤ i ≤ ‖Θ‖, because\nthe main clause of each ψi1 equals some subset of literals of M(ψ2). However, each ψ\ni\n1 may\ncontain clauses that are exceptions of ψ1, namely\n⋃‖Σ‖(ψ1)\nj=0 ([Ej(ψ1)] ∩ ψ\ni\n1). We call these\nclauses ψ1’s exceptions in ψ\ni\n1 .\nThus, φ(Ψ1) contains all clauses of ψ1 that entail something in [M(ψ2)] and more,\nnamely ψ1’s exceptions in ψ\ni\n1. Therefore\nψ1 ⊲ [M(ψ2)] =\n⋃\nψi\n1\n∈Ψ1\n((ψi1 −∪\n‖Σ(ψ1)‖\nj=0 ([Ej(ψ1)] ∩ ψ\ni\n1)) ⊲ [M(ψ2)]).\nNote that for a given ψi1 ∈ Ψ1, each clause of [M(ψ2)] is entailed by exactly one clause of\nψi1, i.e. for every ψ\ni\n1 ∈ Ψ1, and every c ∈ [M(ψ2)], (ψ\ni\n1 |= c) ⇔ ∃!c1 ∈ ψ\ni\n1 . c1 |= c (existence\nof a clause M(ψ2)σ such that there are 2 different clauses c\n′, c′′ ∈ ψi1 that entail it, leads to\na contradiction to the fact that ψ2 and ψ1 are fixed length ψ-forms). The last observation\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 21\nallows to distribute the image operator and rewrite the last expression as follows\nψ1 ⊲ [M(ψ2)] =\n⋃\nψi\n1\n∈Ψ1\n(\n(ψi1 ⊲ [M(ψ2)])−\n‖Σ(ψ1)‖⋃\nj=0\n([Ej(ψ1)] ∩ ψ\ni\n1) ⊲ [M(ψ2)]\n)\n= [M(ψ2)]−\n⋂\nψi\n1\n∈Ψ1\n‖Σ(ψ1)‖⋃\nj=0\n(\n([Ej(ψ1)] ∩ ψ\ni\n1) ⊲ [M(ψ2)]\n)\n.\ni.e. the set of clauses of [M(ψ2)] not entailed by ψ1 is the intersection of images of ψ1’s\nexceptions in all of ψi1 ∈ Ψ1,\nThe formula for H(ψ1, ψ2) follows from substituting the derived expression for ψ1 ⊲\n[M(ψ2)] in the definition (3.4) and noticing that since ψ1 nearly entails ψ2, [M(ψ1)] ⊲\n[M(ψ2)] = [M(ψ2)].\nThe procedure for computing the set of holes H(ψ1, ψ2) in case ψ1 nearly entails ψ2\npresented in Figure 9 is based on Lemma 3.14. Procedures for computing the image and\ne-difference in case ψ1 nearly entails ψ2 are also presented in Figure 9.\nExample 3.15. Consider\nψ1 = [¬P (x, y, z) except {{x = B}, {x = C, y = D}, {x = A}}]\nψ2 = [¬P (w,E,A) ∨ ¬P (C,D,w) ∨ ¬Q(w) except {{w = G}}]\nHere, the clause c = ¬P (K,E,A) ∨ ¬P (C,D,K) ∨ ¬Q(K), for example, is entailed by\ntwo clauses of [M(ψ1)], namely, by ¬P (K,E,A) and ¬P (C,D,K). Although the second of\nthese clauses is not in ψ1 due to the second exception, the first one, ¬P (K,E,A) is in ψ1 and\ntherefore ψ1 |= ¬P (K,E,A) ∨ ¬P (C,D,K) ∨¬Q(K). Thus, even though c ∈ [E2(ψ1)] ⊲ ψ2,\nc ∈ ψ1 ⊲ ψ2.\nComputing the set of holes according to Lemma 3.14 yields\nH(ψ1, ψ2) = ((E(ψ1) ∩ ψ\n1\n1) ⊲ [M(ψ2)]) ∩ ((E(ψ1) ∩ ψ\n2\n1) ⊲ [M(ψ2)]) ={\n[¬P (B,E,A) ∨ ¬P (C,D,B) ∨ ¬Q(B)],\n[¬P (A,E,A) ∨ ¬P (C,D,A) ∨ ¬Q(A)]\n}\n.\nFurthermore, according to Theorem 3.12, e-difference ψ2\ne ψ1 equals the set of holes\nH(ψ1, ψ2) minus exceptions of ψ2\nψ2\ne ψ1 = H(ψ1, ψ2)− {[¬P (G,E,A) ∨ ¬P (C,D,G) ∨ ¬Q(G)]} ={\n[¬P (B,E,A) ∨ ¬P (C,D,B) ∨ ¬Q(B)],\n[¬P (A,E,A) ∨ ¬P (C,D,A) ∨ ¬Q(A)]\n}\n.\nThe image ψ1 ⊲ ψ2 equals [M(ψ2)] minus the set of holes, and minus exceptions of ψ2, i.e.\nψ1 ⊲ ψ2 = ψ2 −H(ψ1, ψ2)\n= [¬P (w,E,A) ∨ ¬P (C,D,w) ∨ ¬Q(w) except {{w = G}}]−H(ψ1, ψ2)\n= [¬P (w,E,A) ∨ ¬P (C,D,w) ∨ ¬Q(w) except {{w = G}, {w = A}, {w = B}}]\nRecall that the computation of e-difference in case [M(ψ1)] |= [M(ψ2)] comes up in\nverifying entailment (Theorem 3.6). The following Observation guarantees that each ψ-form\nin the e-difference ψ2\ne ψ1 is strictly “smaller” than ψ2. This observation plays a critical\nrole in establishing the complexity bounds on ψ-form reasoning.\n22 T. BABAIAN AND J. G. SCHMOLZE\nObservation 3.16. Assume [M(ψ1)] |= [M(ψ2)] and assume that none of the exception\nforms of ψ1 entails [M(ψ2)]. Then each ψ-form in ψ2\ne ψ1 uses strictly fewer variables than\nthere are in ψ2.\nProof. As follows from Theorem 3.12 and Lemma 3.14 the e-difference is a subset of a union\nof images of exceptions of ψ1 on [M(ψ2)]. Each such image is obtained by instantiating\nthe main clause M(ψ2) with a subset unifying substitution, call it σ. When σ does not\nbind any variables of ψ2 to constants, the image is equal to [M(ψ2)], which contradicts the\nconditions of the Observation. Thus, σ must bind some variables of M(ψ2) to constants,\nand therefore [M(ψ2)σ], and in turn, every subset of this ψ-form is expressed with a ψ-form\nthat contains strictly fewer variables than [M(ψ2)].\nWe now consider case 3. There is a non-empty image of the main part of ψ1 onto\nthe main part of ψ2 which occurs when MGU⊆(M(ψ1),M(ψ2)) 6= ∅. In this case we first\ncompute the image [M(ψ1)] ⊲ [M(ψ2)], denoted below by Ψ. Every ψ-form in Ψ is nearly\nentailed by ψ1, and thus we can compute the image of ψ1 on each of ψ-forms in Ψ using the\nmethods of Case 2. The image ψ1 ⊲ ψ2 equals the union of images of ψ1 onto each ψ-form\nin Ψ, minus exceptions of ψ2.\nTheorem 3.17. Let MGU⊆(M(ψ1),M(ψ2)) 6= ∅, and let Ψ denote the image [M(ψ1)] ⊲\n[M(ψ2)]. Then\nψ1 ⊲ ψ2 = (ψ1 ⊲Ψ)− E(ψ2) (3.10)\nψ2\ne ψ1 = (ψ2 −Ψ) ∪ [(Ψ− E(ψ2))\ne ψ1] (3.11)\nProof. By Theorem 3.7 MGU⊆(M(ψ1),M(ψ2)) 6= ∅ implies that the image Ψ is non-empty\nand thus consists of a set of simple ψ-forms. Each ψ-form in Ψ is nearly entailed by ψ1.\nThe image ψ1 ⊲ ψ2 is a subset of Ψ, and equals exactly the set of all clauses in Ψ that are\nnot exceptions of ψ2 and that are entailed by ψ1, i.e.\nψ1 ⊲ ψ2 = (ψ1 ⊲Ψ)− E(ψ2).\nSince each of ψ-forms in Ψ is nearly entailed by ψ1 the calculation of the image ψ1 ⊲ Ψ in\nthe above expression can be carried out according to Theorem 3.12.\nThe proof of (3.11) is similar. The part of ψ2 that is not entailed by ψ1 includes ψ2−Ψ\nplus parts of Ψ that are not exceptions of ψ2 and are not entailed by ψ1, i.e. (Ψ−E(ψ2))\ne ψ1.\nThe procedures for computing image and e-difference in case 3 are given in Figure 10.\nTheorem 3.18. Image and e-difference of two fixed length ψ-forms is equivalent to a finite\nset of fixed length ψ-forms.\nProof. The fact that all operations produce sets of ψ-forms follows from the fact that all\nof them produce subsets of operand ψ-forms. The fact that indeed this set is finite follows\nfrom the Theorems 3.11, 3.17.\nThe resulting ψ-forms are fixed length, because they contain clauses from argument ψ-\nforms, and each subset ψ-form of a fixed length ψ-form is obviously a fixed length ψ-form.\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 23\nComputeImg3(ψ1, ψ2)\n– Requires MGU⊆(M(ψ1),M(ψ2)) 6= ∅\nSet Ψ = ComputeSimpleImg([M(ψ1)], [M(ψ2)])\nSet Ψr = ∅\nFor each ψ ∈ Ψ\nSet ψr = ComputeImg2(ψ1, ψ)\nSet Ψr = Ψr ∪ ψr\nSet Ψr = Ψr − E(ψ2)\nReturn Ψr\nComputeEDiff3(ψ2, ψ1)\n– Requires MGU⊆(M(ψ1),M(ψ2)) 6= ∅\nSet Ψ = ComputeSimpleImg\n([M(ψ1)], [M(ψ2)])\nSet Ψr = ψ2 −Ψ\nFor each ψ ∈ Ψ\nSet ψ = ψ − E(ψ2)\nSet ψr = ComputeEDiff2(ψ, ψ1)\nSet Ψr = Ψr ∪ ψr\nReturn Ψr\nFigure 10: Procedures ComputeImg3(ψ1, ψ2) and ComputeEDiff3(ψ1, ψ2) compute ψ1 ⊲ ψ2\nand ψ2\ne ψ1 in case there is a non-empty image of the main part of ψ1 onto the\nmain part of ψ2, i.e. [M(ψ1)] ⊲ [M(ψ2)] 6= ∅\n3.6. Complexity of ψ-form operations. The recursive procedure for determining entail-\nment Ψ |= ψ based on Theorem 3.6 takes time O(n), where n is the number of ψ-forms in Ψ,\nwhen the maximum number of exceptions, and variables and literals in the main clause of a\nψ-form are fixed. We assume unification takes constant bounded time, which is guaranteed\nwhen the cardinality of predicate symbols is bounded by a constant. These assumptions\nare common in open world applications:\n• the number of variables and literals in the main clause and cardinality of predicate\nsymbols are always finite and bounded by the specification of the initial and goal\nstates and the action descriptions. Moreover, they are typically small.\n• the number of exceptions is limited by a function of the number of objects known in\nthe initial state and those objects created by the actions in a constructed plan. When\nthe length of the plan is constant bounded, the number of exceptions is therefore\nalso constant bounded. In general, the complexity of entailment is polynomially\nbounded in the maximum number of exceptions, as presented in Figure 11 and\ndiscussed briefly at the end of this section.\nTo obtain the linear bound on the complexity of entailment, notice that finding a ψ-\nform that nearly entails ψ requires a pass through at most n ψ-forms of Ψ spending constant\ntime at each, since checking nearly entailment takes constant time. Once a nearly entailing\nψ-form ψk is found, we calculate the difference ψ\ne ψk and apply Theorem (3.6) to each\nψ-form in the e-difference. This is a recursive procedure, which can be represented by a\nrecursion tree. In the tree, each node represents the non-recursive computation, i.e. finding\na nearly entailing ψ-form ψk, and computing the e-difference ψ\ne ψk; and each branch\nrepresents a recursive call to the same procedure for checking entailment of each ψ-form\nin the e-difference ψ e ψk. The complexity of the entire procedure equals the sum of the\ncomplexities at the nodes of the recursion tree. The time spent at each node is proportional\nto the number of ψ-forms in Ψ, which is n at the root of the tree, and decreases by one at\neach subsequent level. The branching factor β at each node equals the number of ψ-forms in\nthe e-difference ψ e ψk. β is constant bounded when we bound by constants the maximum\nnumber of exceptions, variables and literals in the ψ-forms. Therefore, at each level i of\n24 T. BABAIAN AND J. G. SCHMOLZE\nthe tree we have at most βi nodes, and computation at each node has time complexity\nproportional to (n− i).\nThe depth of the recursion tree is bounded by n. However, it is also bounded by\nmin(n, V + 1), where V is the maximum number of variables in a ψ-form. As follows from\nObservation 3.16 (page 22) unless ψ e ψk = {ψ}, each ψ-form in the difference ψ\ne ψk uses\nstrictly fewer variables than the original ψ, because when ψk nearly entails ψ, all ψ-forms in\nthe e-difference ψ e ψk are subsets of images from ψk’s exceptions, and unless an exception\nentails the whole ψ, this image is obtained by instantiating some variables of ψ. In the case\nwhere ψ e ψk = {ψ}, the branching factor out of the node equals one, and we can collapse\nthe parent and the child into one node. Thus, assuming V is less than n, the depth of\nrecursion in checking Ψ |= ψ is bounded by the maximum number of variables in a ψ-form,\nV . The overall time complexity bound is O(βV n) = O(n), since β and V are constants.\nFigure 11 shows time complexity bounds of the ψ-form calculus operations as functions\nof the number of participating ψ-forms n, maximum number of exceptions E, maximum\nnumber of variables V , and maximum number of literals in a ψ-form clause C. The complete\ntreatment of the time complexity issues of the calculus of ψ-forms can be found in [2].\n{ψ1, . . . , ψn} |= ψ ψ1 ⊲ ψ2 ψ2\ne ψ1\nSimple fixed length ψ-forms O(n) O(1) O(1)\nNon-simple fixed length ψ-forms O(nEV (t+1)+1) O(Et) O(Et+1)\nNon-simple limited form ψ-forms O(nEV+1) O(E) O(E2)\nSingleton ψ1 O(nE) O(E) O(E)\nFigure 11: Time complexity of computing ψ-form operations. Assumes unification takes\nconstant time and the maximum number of literals (C) and variables (V ) in\nthe main form of a ψ-form are constant. E denotes the maximum number of\nexceptions, t denotes the maximum number of possible subset matches between\nmain forms of two ψ-forms.t = O(eC/e), where e is the Euler’s number, for fixed\nlength ψ-forms.\nTo summarize: ψ-form entailment takes linear time in the number of participating\nψ-forms n, when the maximum length of clause, maximum number of variables in a ψ-\nform and maximum number of exceptions are all fixed. When the number of exceptions\nis proportional to n, computing entailment remains bounded by a polynomial of the order\nproportional to the maximum number of variables in a ψ-form times the number of possible\nsubset-matches between the main clauses. The complexity of ψ-form operations depends\non the number of subset-matches between the main clauses of two ψ-forms, which in case of\nunrestricted ψ-forms is CC . In fixed length ψ-forms the number of subset-matches between\nthe main forms of two ψ-forms is bounded by eC/e, as stated by the next Observation. To\nlimit the number of subset-matches to at most one, ψ-forms can be restricted to contain\nno duplicate occurrences of a predicate symbol in the main clause. We call such ψ-forms\nlimited form ψ-forms.\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 25\nLemma 3.19. Let ψ1 and ψ2 be fixed length ψ-forms. In two different subset-matches of\nM(ψ1) onto M(ψ2), no two different literals of M(ψ1) match the same literal of M(ψ2).\nProof. As always, we assume that there is no overlap between the variables in two different\nψ-forms. Suppose that the above statement is not true, i.e. there exist two different subset-\nmatches ofM(ψ1) ontoM(ψ2) that match two different literals d1 and d2 ofM(ψ1) on the\nsame literal d ofM(ψ2). Let σ1 and σ2 be substitutions corresponding to each subset-match,\ni.e. σ1 and σ2 are in MGU⊆(M(ψ1),M(ψ2),V(ψ1)).\nConsider the following substitution σ.\n• If d1 and d2 do not share variables, construct σ as follows: combine bindings on\nvariables in d1 from σ1 and bindings on variables in d2 from σ2. Then, obviously,\nd1σ = d2σ = d.\n• Otherwise, if d1 and d2 do share variables, rename variables in d2 so that there is\nno overlap with variables in d1. Modify σ2 by renaming the variables in the same\nway we did with d2, and construct σ as in the previous case. Again, d1σ = d2σ.\nThus, both cases produced a contradiction to the fact that no two literals of a fixed\nlength ψ1 unify.\nObservation 3.20. Let ψ1 and ψ2 be fixed length ψ-forms. The number of subset-matches\nof M(ψ1) onto M(ψ2) is bounded by e\nC/e, i.e. ‖MGU⊆(M(ψ1),M(ψ2),V(ψ1))‖ ≤ (e\nC/e).\nProof. Suppose there are ik literals inM(ψ2) that matched the k-th literal ofM(ψ1). Since\naccording to Lemma 3.19 (page 24) no two different matches can match two different literals\nof M(ψ1) to the same literal of M(ψ2), i1 + . . .+ ik ≤ C, otherwise two literals of M(ψ1)\nwould match the same literal of M(ψ2).\nThe maximum size ofMGU⊆(M(ψ1),M(ψ2),V(ψ1)) is bound by the product i1× . . .×\nik. By the Cauchy’s inequality\ni1 + . . .+ ik\nk\n≥ (i1 × . . .× ik)\n1/k.\nThe product (i1 × . . . × ik) is limited by (\nC\nk )\nk with the equality reachable only when i1 =\ni2 = . . . = ik =\nC\nk . Then the product i1× . . .× ik. The maximum of this product is reached\nwhen log(Ck )− 1 = 0, i.e. when\nC\nk = e, and equals e\nC/e.\nThus, when the maximum number of disjuncts in a ψ-form is C, number of possible\nsubset-matches is bounded by eC/e.\n4. PSIPLAN Representation\nAs in previous work on open world planning, we assume that the world evolves as a\nsequence of states, where the transitions occur only as the result of deliberate action taken\nby the single agent. Since the agent’s model of the world is incomplete, the actual state of\nthe world differs from the state of the agent’s knowledge of the world, which we call SOK.\nThe agent’s knowledge of the world is assumed to be correct.\n26 T. BABAIAN AND J. G. SCHMOLZE\n4.1. States of Knowledge. PSIPLAN propositions. A SOK is a set of propositions\nthat represents what the agent knows is true about the world. In PSIPLAN, a SOK is a\nfinite set of PSIPLAN domain propositions or, simply, propositions, which are defined\nto be either ground atoms or ψ-forms.\nSince the agent’s theory of the world is assumed to be correct, every proposition that a\nSOK entails is true in the actual world. Moreover, we make the following closed knowledge\nassumption (CKA): A literal L is known to be true in s if s |= L, known to be false if s |= ¬L\nand unknown if both s 6|= L and s 6|= ¬L. This assumption is closed because entailment is\ndecidable in PSIPLAN.\nA model of a PSIPLAN proposition is a world in which it is true. We refer to the set\nof all models of a SOK s, denoted I(s), as the set of possible worlds of s. It is the set of\nall worlds in which everything known by the agent is true. Correctness of the agents SOK\nimplies that the set of models of a SOK always contains the actual world.\nWe use symbols w,w′, w1 . . . wn to refer to worlds, W,W\n′ to refer to the sets of worlds,\nand s, s′, s1 . . . sn to refer to the agent’s SOK. W denotes a set of all worlds.\n4.2. Entailment in PSIPLAN. First observe that any consistent set s of atoms plus\nψ-forms does not entail any atoms but those in s. However, it may entail more ψ-forms\nthan are entailed by ψ-forms of s alone, because of the possibility of resolution between a\nground clause c, represented by a ψ-form, and some atom a, such that ¬a is a literal of\nc. However, once all resolutions are performed and the resolvents added to s, each ψ-form\nentailed by s is entailed by the set of only ψ-forms of s, and each atom entailed by s is in s.\nIn other words, s becomes saturated, i.e. for any ground proposition q entailed by s there\nis a single proposition p ∈ s that entails q (see also Theorem 3.2). A set of propositions s\nis saturated if and only if for any ground proposition q,\n(s |= q) =⇒ ∃p . (p ∈ s) ∧ (p |= q). (4.1)\nThus, when s is saturated, one need not combine elements of s (through resolution) in\norder to show entailment. To determine entailment of an atom, s |= a, a must be found\nin s. Entailment of any ground negated clause, or, in other words, singleton ψ, s |= ψ is\ncompletely determined by entailment from a single ψ-form in s.\nA saturated equivalent of s is obtained by computing all possible resolutions from the\nunit clause resolution rule\na,¬a ∨ ¬a1 ∨ . . . ∨ ¬an\n¬a1 ∨ . . . ∨ ¬an\nbetween domain atoms in s and clauses\nrepresented by ψ-forms.\nFor example, suppose\ns = {In(paper, /tex), ψ = [¬In(x, y) ∨ ¬T (x, PS) except {{y = /img}}]}.\nHere, In(x, y) states that file x is in directory y, and T (x, PS) states that file x has type\nPostscript.\ns is not saturated because, even though it entails that file paper is not a Postscript file,\nno single proposition of s alone entails ¬T (paper, PS).\nHowever, ψ in s contains a clause c = ¬In(paper, /tex) ∨ ¬T (paper, PS), and we can\nperform a resolution between c and the atom In(paper, /tex), resulting in ¬T (paper, PS).\nWe add the ψ-form [¬T (paper, PS)] to the initial SOK, s0.\nThe procedure Saturate(s), depicted in Figure 12, returns a saturated equivalent of s\nand consists of the following steps. Initially we set s0 = s. For every ψ-form ψ in s0 and for\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 27\nevery atom a, we compute (¬a) ⊲ ψ, as those are all and only clauses for which resolution\nis possible. If this image is empty, we go to the next ψ-form in s0. Otherwise, suppose\n(¬a)⊲ψ = ψ′. From the properties of the image, it follows that ¬a is a subclause ofM(ψ′).\nLet ψnew denote the ψ-form that is obtained from ψ\n′ by removing ¬a from its main clause,\ni.e. ψnew = [M(ψ\n′)−(¬a) except Σ(ψ′)]. We add ψnew to s0, and continue until all ψ-forms\nin s0, including the newly added, are processed in this way.\nSaturate(s)\n1. Set s0 = s\n2. For each ψ-form ψ ∈ s0\n3. For each atom a ∈ s0\n4. If [¬a] ⊲ ψ 6= ∅ Then\n5. Set ψ′ = [¬a] ⊲ ψ\n6. Let D denote the main clause of ψ′ without the literal ¬a\n7. If D = ∅, Then return fail.\n8. Else Set ψnew = [D except Σ(ψ)], Add ψnew to s0.\n9. End For\n10. End For\n11. Return s0.\nFigure 12: Procedure Saturate(s). Preprocessing the Initial SOK. If set s is unsatisfiable,\nreturns fail.\nNotice that, as a side effect, procedure Saturate() determines if s is consistent. If at\nany moment we obtain an empty clause as a main part of some ψnew, that indicates that\nM(ψ′)− (¬a) = ∅, i.e. M(ψ′) = ¬a, which means that we can derive both a and ¬a from\ns, and hence s is inconsistent.\nLemma 4.1. Procedure Saturate(s) returns a SOK s0 that is a saturated equivalent of\ns, if s is consistent, or fail, otherwise. Assuming s consists of n fixed length ψ-forms and\nm atoms, the time complexity of procedure Saturate(s) is O(nmC), where C denotes the\nmaximum length of a ψ-form clause.\nProof. The SOK returned by Saturate (Figure 12) contains the input set of propositions\ns and some additional propositions that are derived from s using unit clause resolution, i.e.\nthose that follow from s. Thus the returned SOK s0 is equivalent to the input.\nIt is saturated because we compute and add the results of all possible resolutions to s0.\nThus, for every ground proposition p such that s |= p, there is a proposition c ∈ s0 such\nthat c |= p.\nSaturate computes all possible resolutions in s and returns fail whenever an empty\ndisjunct is derived, as follows from the the known property of resolution deduction (see [20]\npage 87): If a set ∆ of ground clauses is unsatisfiable, then there is a resolution deduction\nof the empty clause from ∆.\nTo estimate the time complexity bound, we consider the following stages of the algo-\nrithm. During the first stage, for each of n ψ-forms in s the procedure will first compute\nthe resolution with every atom in s, when the resolution rule is applicable. Determining\nif resolution between an atom and a single fixed length ψ-form is applicable consists of\ncomputing an image of a single literal onto a ψ-form (step 4 in Figure 12), which takes\n28 T. BABAIAN AND J. G. SCHMOLZE\nconstant time, assuming the number of exceptions, clauses and variables in a ψ-form are\nconstant bounded. Thus, the first stage takes time O(mn).\nNote that the result of each resolution is another ψ-form (denoted ψnew in the algo-\nrithm), which is added to the saturation s0. For each ψ-form at most m new ψ-forms can\nbe added to s0 as a result of resolution with the atoms in s. Furthermore, each of the added\nψ-forms will have 1 fewer literals in the main clause than the original. Overall, nm new\nψ-forms with the maximum clause length C− 1 could be added to s0 during the first stage.\nDuring the second stage, resolutions are computed between the ψ-forms added in the\nprevious stage and m atoms of s. This process will take time O(m2n) and add no more\nthan m2n new ψ-forms with the maximum clause length of C − 2.\nThe third stage will take O(m3n) time and add no more than m3n new ψ-forms with\nthe clause length C − 3, and so on. Since the maximum possible length of the main clause\nin the ψ-forms added at each stage is decreasing by one at each stage of this process, it is\nevident that the number of stages is bounded by the size of the longest ψ-form clause C.\nOverall computation will thus take time O(mn+m2n+ . . .+mCn) = O(mCn).\nWhen a set of PSIPLAN propositions s consists of m atoms and n ψ-forms, checking\ns |= a, where a is an atom, takes time O(m) because a set of PSIPLAN propositions only\nentails those atoms that it contains. Checking s |= ψ takes time O(nmC + n), where\nO(nmC) is the time to saturate s. When s is saturated, checking s |= ψ is O(n).\n4.3. PSIPLAN Actions. Actions are deterministic and are represented via preconditions\nand effects. Actions are described using parameterized schemas, however, for the simplic-\nity of presentation, the examples in this section present instantiated, i.e. fully grounded,\nversions of actions.\nEach action a has a name, N (a), and a set of preconditions, P(a), which identify\nthe domain propositions necessary for executing the action. The propositions in P(a) can\ninclude literals and quantified ψ-forms 4.\nEach domain action has a set of domain literals called the assert list , A(a). The assert\nlist, also called the effects of the domain action, identifies the complete set of domain\npropositions whose value may change as a result of the action. We assume that an action\nis deterministic and can change the truth value (true or false) of only a finite number of\natoms, and thus any ψ-form in the assert list defines a single negated literal.\nConsider, for example, PSIPLAN encoding of the action of moving a file from one\ndirectory to another, as depicted in Figure 13. mv(F, S,D) moves file F from directory S\ninto the directory D. The single precondition requires that file F be in directory S. The\neffects are given by a ψ-form that denotes that file F is not in directory S, and an atom\nthat denotes that file F is in directory D. Action lif t(B,L), from our warehouse domain,\nlifts object B from location L. One of its preconditions is a quantified ψ-form and requires\nthat B contains no fragile goods.\n4Ruling out other forms of non-quantified disjunction is not a limitation, since any action schema with a\nnon-quantified disjunction as its precondition can be equivalently split into several actions.\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 29\nPSIPLAN action a = mv(F, S,D) PSIPLAN action a = lift(B,L)\nP(a) : In(F, S), F ile(F ), Dir(S), Dir(D) P(a) : [¬In(g,B) ∨ ¬Fragile(g)], At(B,L)\nA(a) : [¬In(F, S)], In(F,D) A(a) : [¬At(B,L)], Lifted(B)\nFigure 13: PSIPLAN domain actions\n4.4. Planning Problem. A planning problem is a three tuple 〈Λ,I,G〉 where Λ is the\nset of available PSIPLAN actions, I is the set of initial conditions – i.e., a set of PSIPLAN\npropositions – and G is the goal, which is also a set of PSIPLAN propositions.\nA solution plan is a sequence of actions, that is executable and transforms any world\nstate satisfying the initial conditions into a world state satisfying the goal.\nGiven a sequence of ground actions a1, . . . , an, let Wi denote the set of possible worlds\nobtained by executing the sequence up to the i-th action from any of the possible initial\nworlds. Let W0 denote the set of possible worlds corresponding to the initial conditions,\ni.e. I(I). Then, a sequence of actions a1, . . . , an is called a solution plan to a planning\nproblem 〈Λ,I,G〉, if and only if:\n(1) The goal G holds in all final worlds, i.e. for all w in Wn, w(G), and\n(2) Each action ai of the plan is executable in every possible world w in Wi, for all\nvalues of i, 0 ≤ i < n, i.e. for all w in Wi, w(P(ai)).\nSince our agent uses the SOK s to represent the set of possible worlds I(s), in order\nto plan, it must be able to progress the SOK in order to predict the set of worlds resulting\nfrom executing a sequence of actions. The function update() does exactly that.\nIf ~a is a sequence of actions executable from the SOK s0, update(~a, s0) denotes the SOK\nour agent uses to predict the set of possible worlds resulting from executing ~a in any of the\nworlds I(s0).\nIdeally, the SOK obtained by progression must include all and only worlds that are\nthe result of executing the sequence ~a in some world described by the initial SOK. Then,\nevery plan that is executable and achieves the goal in the agent’s knowledge of the world is\nindeed a solution plan for the real world. This requirement is satisfied when the update()\nfunction is correct and complete.\nThe next section formally defines the correctness and completeness properties, presents\nPSIPLAN’s update procedure and proves that it is correct and complete. Thus, a sequence\nof ground actions a1, . . . , an, is a solution to the planning problem 〈Λ,I,G〉 if and only if\n(1) The goal G is entailed by the final SOK, i.e. update(a1 . . . an,I) |= G, and\n(2) Each ai is executable, i.e. update(a1 . . . ai−1,I) |= P(ai).\nThus, goal achievement can be established by checking entailment from the updated SOK\nwithout considering the set of all possible worlds.\n4.5. SOK Update. Actions cause transitions between worlds. The agent’s SOK must\nevolve in parallel with the world, and must adequately reflect the changes in the world\nthat occur due to an action. Correctness of a SOK update guarantees that the SOK is\nalways consistent with the world model, given a consistent initial SOK. The other desirable\nproperty of the SOK update is completeness: we would like the agent to take advantage of\n30 T. BABAIAN AND J. G. SCHMOLZE\nall information that becomes available and not to discard what was previously known and\nhas not changed. The correctness and completeness properties of the SOK update, as well\nas soundness and completeness of entailment within the state language, are prerequisites\nfor a sound and complete planning algorithm. The correctness and completeness criteria\nare best formulated in the context of possible worlds. Let do(a,W ) denote the set of worlds\nobtained from performing action a in any of the worlds in W, and update(s, a) denote the\nSOK that results if the agent performs action a from SOK s. We say that the update\nprocedure is correct if and only if\nI(update(s, a)) ⊆ do(a, I(s)), (4.2)\ni.e. every possible world after performing the action a has to have a possible predecessor.\nThe update procedure is complete if and only if\ndo(a, I(s)) ⊆ I(update(s, a)), (4.3)\ni.e. every world obtained from a previously possible world is a model of the new SOK. This\nimplies that all changes to the world must be reflected in the new SOK.\nTo achieve correctness of SOK updates, the agent must remove from the SOK all\npropositions whose truth value might have changed as the result of the performed action.\nTo achieve completeness, the agent must add to the SOK all facts that become known. The\ncomplexity of the SOK update, therefore, depends critically on the process of identifying\nthe propositions that must be retracted to preserve correctness. In our language, this\ncomputation is reduced to computing e-difference, which has polynomial complexity.\nTo obtain the agent’s SOK s after performing an action a, we first remove all proposi-\ntions implied by the negation of the assert list, as only those propositions of s might change\ntheir values after a. Symbol A−(a) is used to denote the set of negations of propositions\nin the assert list of a. For example, the action a = mv(fig, /img, /tex) of moving file fig\nfrom directory /img into /tex has assert list\nA(a) = {[¬In(fig, /img)], In(fig, /tex)},\nand thus\nA−(a) = {In(fig, /img), [¬In(fig, /tex)]}.\nDuring the update we also remove from the SOK all redundant propositions, i.e. those\nthat follow from the effects of the action, and then add these effects to the new state. The\nagent’s SOK after executing action a in the SOK s is described below.\nupdate(s, a) = ((s e A−(a)) e A(a)) ∪ A(a) (4.4)\nThough it is not necessary, we include e A(a) in (4.4) to keep the SOK simple. e A(a)\nremoves all propositions entailed by A(a), not just those in A(a).\nExample 4.2. For an example, consider action a = mv(fig, /img, /tex) introduced above.\nLet P(a) = {In(fig, /img)}, which states that fig must be in /img. Recall that A(a) =\n{[¬In(fig, /img)], In(fig, /tex)} and A−(a) = {In(fig, /img), [¬In(fig, /tex)]}.\nWe begin with an SOK s, which states that fig and a.bmp are the only files in /img,\nand that a.ps is the only Postscript file in the system, except for possibly files in directory\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 31\n/img.\ns =\n\n\nIn(fig, /img), In(a.bmp, /img), T (a.ps, PS),\n[¬In(x, /img) except {{x = fig}, {x = a.bmp}}],\n[¬In(x, d) ∨ ¬T (x, PS) except {{x = a.ps}, {d = /img}}]\n\n\na = mv(fig, /img, /tex) is executable in s, and as a result of computing s e A−(a), the\natom In(fig, /img) is removed from s and the exception {x = fig, d = /tex} is added to\nthe second ψ-form. The first ψ-form is left intact, producing\ns e A−(a) =\n\n\nIn(a.bmp, /img), T (a.ps, PS),\n[¬In(x, /img) except {{x = fig}, {x = a.bmp}}],\n[¬In(x, d) ∨ ¬T (x, PS) except\n{{x = a.ps}, {d = /img}, {x = fig, d = /tex}}]\n\n .\nFurther e-difference with A(a) and union with A(a) yields the following SOK\ns′ =\n\n\nIn(fig, /tex), In(a.bmp, /img), T (a.ps, PS),\n[¬In(x, /img) except {{x = fig}, {x = a.bmp}}],\n[¬In(x, d) ∨ ¬T (x, PS) except {{x = a.ps}, {d = /img}, {x = fig, d = /tex}}]\n[¬In(fig, /img)]\n\n .\nNote that s contained ¬In(fig, /tex)∨¬T (fig, PS) and that we added In(fig, /tex) when\ndetermining s′. If our update rule retained ¬In(fig, /tex) ∨ ¬T (fig, PS) in s′, then in\ns′ we could perform resolution and conclude that ¬T (fig, PS). However, this would be\nwrong because we have no information on whether or not fig is a Postscript file. Instead,\nour update rule removes any clause that is entailed by ¬In(fig, /tex), and so s′ does not\ncontain ¬In(fig, /tex) ∨ ¬T (fig, PS).\nThis update rule (4.4) produces the same result as Winslett’s update operator [41]\nin the special case where actions are deterministic. Moreover, our rule accomplishes this\nwithout considering all possible worlds corresponding to SOK s explicitly, and thus is more\nefficient.\nWe show next how the same rule is used for updating the state of knowledge after the\nactions that create new objects5.\nExample 4.3. Consider an action of creating a new file named afile in directory /code with\neffect In(afile,/code) and no preconditions. When this action is executed in s′, the state\nupdate rule yields the new SOK s′′ that differs from s′ in the following way:\n(1) s′′ contains a new atom In(afile, /code), reflecting the effect of the action added by\nthe update rule,\n(2) the first ψ-form of s′ now has a new exception, reflecting the fact that it is un-\nknown whether afile has Postscript format. This is the result of the e-difference\ns′ e {[¬In(afile, /code)]}.\n5Note that discovering a new object is a different issue (since it assumes that the object always existed\nin the domain and is therefore included in the universal quantification even prior to being discovered, which\nis not true of a newly created object) that is also handled in PSIPLAN, however it is beyond the scope of\nthis paper.\n32 T. BABAIAN AND J. G. SCHMOLZE\ns′′ =\n\n\nIn(afile, /code), In(fig, /tex), In(a.bmp, /img), T (a.ps, PS),\n[¬In(x, d) ∨ ¬T (x, PS) except {{x = a.ps}, {d = /img}, {x = fig, d = /tex},\n{x = afile, d = /code}}]\n[¬In(x, /img) except {{x = fig}, {x = a.bmp}}],\n[¬In(fig, /img)]\n\n\nA factor that turns out to be critical for the use of PSIPLAN in planning is that the\nSOK resulting from updating a saturated SOK is also saturated. After the initial SOK of\nthe agent is saturated, there is no need to consider resolution of the initial conditions and\naction effects in satisfying a goal.\nWe call an SOK s minimal if it is saturated and it does not contain any ground clause\nentailed by some other clause in s, i.e. for any two ground clauses p, q from s\n(q |= p) =⇒ (q = p) (4.5)\nTheorem 4.4. Let a be a domain action and s be an agent’s SOK before executing a, where\ns is satisfiable and saturated, and a is executable in s. Then the following state of knowledge\nupdate rule\nupdate(s, a) = ((s e A−(a)) e A(a)) ∪ A(a), (4.6)\nis correct and complete. Moreover, the resulting SOK s′ = update(s, a) is saturated. It is\nminimal if s is minimal.\nProof. We start by proving that the result of updating a saturated SOK is a saturated\nSOK. We first show that if s is saturated then s1 = ((s\ne A−(a)) e A(a)) is also saturated\n( recall that A−(a) is used to denote the set of negations of propositions in the assert list\nof a). Suppose this is not true, i.e. there’s a ground PSIPLAN proposition p such that,\nwhile s1 |= p, ∀p\n′ ∈ s1 . p\n′ 6|= p. Let q be a smallest (non-empty) subclause of p such that\ns1 |= q (there might be several such q). Since s1 ⊆ s, q must also be entailed by s, but s is\nsaturated, so there exists a q′ in s such that q′ |= q, i.e. q′ ⊆ q.\nq′ 6∈ s1 (otherwise q\n′ as a subclause of p would entail p) so q′ must be entailed by either\nA(a) or A−(a). We abbreviate the union A(a) ∪ A−(a) by A◦(a). Since q′ ∈ (A(a) ⊲ s) ∪\n(A−(a) ⊲ s), there must be a literal e in A◦(a) such that e is a subclause of q′, and therefore,\ne is a subclause of q. Note, that since s1 |= q, in case q 6∈ s1 there must be a way of deriving\nq by resolution from some propositions of s1, which is only possible when there exists a\nproposition r in s1 such that r contains q as a subclause. This means that r has e as a\nsubclause, and consequently r is entailed by e. But according to the definition of s1, it does\nnot contain any clauses entailed by any clause in A◦(a). We arrive at a contradiction, so s1\nis saturated.\nAdding A(a), which consists of literals and is itself saturated, to s1 also produces a\nsaturated state. Assume there is a proposition q that is not entailed by either s1 or some\nelement of A(a), but is entailed by s1∪A(a). q cannot be an atom, therefore it is a negated\nclause. The only clauses that are not entailed by A(a) and s1 separately, but are entailed by\ntheir union are those obtained via resolution of some atom e ∈ A(a) with a ground clause\nof the form ¬e ∨ x in s1. But this is impossible, because A\n−(a) |= ¬e ∨ x, so such clause\nwould not be in s1. We arrive at a contradiction.\nIn case s was minimal, s′ is also minimal, because\n• removing clauses from a minimal set preserves minimality, so s1 is minimal, and\n• s′ = s1 ∪ A(a) is minimal because\n(1) s1 does not contain any propositions entailed by any literal in A\n◦(a),\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 33\n(2) A(a) is minimal, and\n(3) s1 does not entail anything in A(a).\nTo prove that the update function is correct and complete, we need to show that for\ns′ = update(s, a):\nI(s′) = do(a, I(s)). (4.7)\nCompleteness proof. We first show that do(a, I(s)) ⊆ I(s′), i.e. for every world w\nin I(s), its successor, w′ = do(a,w), is in I(s′). The set of possible worlds consists of\nall and only worlds that model the agent’s knowledge of the world, i.e. for any s I(s) =\n{w | (p ∈ s) =⇒ w(p) = true}. Thus, to show that w′ ∈ I(s′) we need to prove, that for\nevery proposition p in s′, w′(p) = true.\nNote that according to the world transition model, w′ = (w −A◦(a)) ∪ A(a).\nWe partition s′ into s′1 = ((s\ne A−(a)) e A(a)) and s′2 = A(a). We partition w\n′ into\nw′1 = w − A\n−(a) − A(a) and w′2 = A(a). Since w ∈ I(s), for every p1 such that p1 ∈ s\n′\n1,\nw′1(p1). Also, for each p2 ∈ s\n′\n2, we have w\n′\n2(p2). Therefore for every p such that p ∈ s, w(p).\nCorrectness proof. Now we need to show that I(s′) ⊆ do(a, I(s)), i.e. every possible\nworld w′ of s′ has a predecessor w that is a possible world of s. We need to show that for\nevery w′ ∈ I(s′) there is a world w such that w′ = (w −A◦(a)) ∪ A(a) where w is in I(s)\nand A◦(a) denotes the union A(a)∪A−(a). A possible world is a model of all propositions\np such that p ∈ s, i.e. w is in I(s) if and only if w models every such domain proposition p.\nThe proof is by construction. Since s is saturated, for every proposition p that is implied\nby s, there’s a single proposition q ∈ s such that q |= p.\nSTEP 1. Since w′ = do(a,w) we need to include in w all literals of w′ that are not in\nA(a), because those would not have changed as a result of the action. Let w0 = w\n′ −A(a)\nand w will include w0.\nSTEP 2. We also include in w those literals from A(a)◦ that are known in s, i.e. the\nliterals l ∈ A◦(a) such that l ∈ s.\nSTEP 3. At this point every literal or its complement is included in w except for\nl ∈ A◦(a) where neither l nor ¬l belongs to s. We now describe a procedure for choosing\neither the literal or its complement for inclusion in w from these “leftover” literals. Suppose\n¬l is an arbitrary negated literal from this set. Further, let C = ¬l ⊲ s. If there is a\nproposition in C that is not already implied by some p, where p ∈ w, then we must include\n¬l in w in order to keep it accessible from s. Otherwise we may include in w either l or ¬l.\nThe world w is now completely specified and is easy to verify that w ∈ I(s), as well as\nw′ = do(a,w).\nWe should note that although the state update procedure above is defined for fully\ngrounded actions, it does not mean that all PSIPLAN-based planners must work with fully\ngrounded representations. For example, the partial order planner PSIPOP operates using\naction schemas and grounds action parameters only as needed.\n5. Related Work\n5.1. Representations for conformant planning. Representations used for reasoning\nand planning in an open world can be broadly categorized as those that operate using the\nset of all possible worlds, and those that rely instead on reasoning using a specification of\nincomplete state of knowledge. Presented here PSIPLAN belongs to the second of these\ncategories.\n34 T. BABAIAN AND J. G. SCHMOLZE\nAmong the planning systems in the second category is a situation-calculus based planner\nby Finzi et al. [18], implemented in GOLOG [27]. The planning task is reduced to theorem\nproving in situtation calculus, and the authors present two approaches to theorem proving\nfrom the initial state. One approach invokes a Davis-Putnam based theorem prover every\ntime entailment from initial situation is checked. The other approach intends to minimize\nthe time spent checking entailment by precompiling the specification of the initial state\ninto its equivalent form containing all prime implicates of the original specification. (The\nreduction to prime implicates is akin to PSIPLAN’s saturation of the initial state.) From\nthe prime-implicate form further theorem proving is done by subsumption of clauses.\nThe foregoing is the only conformant planner that subsumes the state and goal language\nof PSIPLAN. However, the examples presented in the paper do not contain universally\nquantified disjunctive goals with exceptions that are handled by PSIPLAN. The generality of\nthe situation-calculus permits any first-order specification of the initial and goal situations,\nand actions, however, at a price of the complexity of planning. In PSIPLAN, we have\ndeliberately and significantly restricted the language for the sake of reduced complexity of\nreasoning.\nA subset of situation calculus with equality, which has tractable, sound and under\ncertain conditions complete action progression procedure from an incompletely specified\nstate is presented by Liu and Levesque in [30]. There are similarities between PSIPLAN\nand the language of Liu and Levesque, in particular in the use of universally quantified\nstatements in the knowledge base. However, neither language subsumes the other one in\nthe expressive power.\nShirazi and Amir ([35]) also address the problem of progressing a belief state encoded\nin first-order logical sentences over a sequence of actions. They present special purpose\nalgorithms for computing the progression, which they call logical filtering, in polynomial\ntime. The polynomial time complexity of belief update is achieved for STRIPS and also\npermuting actions. An action is called permuting, if for each world w′ there is at most one\nw such that do(a,w) = w′, i.e. for every world potentially resulting from execution of action\na, there is a unique ”original” world state. PSIPLAN’s actions are not permuting, however\nthey are similar to STRIPS actions in the sense that the assert list of an action includes\nonly those literals that change as the result of an action and there are no conditional effects.\nThus, the polynomial time complexity of PSIPLAN’s update procedure is consistent with\nthe findings of Shirazi and Amir.\nEiter et al. [14, 15] propose a (propositional) logic based planning language K for\nplanning with incomplete information as answer set programming. In this framework,\nproposed originally by Lifschitz [29], a plan is the answer set of a logical program formulated\nusing a specialized logical language. K represents lack of knowledge using negation as failure\nsemantics. It supports both knowledge state and possible world planning. The authors\nfurther distinguish between optimistic and secure (i.e. conformant) planning. Optimistic\nplans may not be executable, due to their assumptions on the missing information. K\nsupports conditional effects, but does not allow any universal quantification on goals or\nstate description.\nThielscher [37] presents FLUX - a logical programming framework for agent program\ndesign in the presence of incomplete information and sensing. FLUX is based on fluent\ncalculus and is implemented as a set of constraints, defining the domain, action update,\nagent’s knowledge and action execution. The syntax of the language is carefully restricted\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 35\nto provide linear time evaluation of the constraints. The constraint language includes uni-\nversally quantified negated clauses, similar to the simple ψ-forms of PSIPLAN. However,\nunlike PSIPLAN, the constraint solver assumes a finite domain, and does not represent\nexceptions to the universally quantified clauses. FLUX has nice computational properties,\nbut it is not complete. Also differently from PSIPLAN, the FLUX framework is designed\nfor programming the intended behavior of the agent via a designer-specified strategy, which\ndefines the set of agent control rules, rather than the problem of automatically constructing\na sequence of actions that will result in the achievement of the goal.\nConformant Graphplan [36] and its extension to planning with sensing, SGP [40] are\npropositional Graphplan [9] based open world planners that consider every possible world\nand thus rely on the domain of objects being sufficiently small. However, in small domains\nthese planners are able of generating remarkably long plans. Graphplan based planners\nperform a search in a space of graphs generated by forward-chaining in the state space,\nand their performance degrades when the initial state contains large number of irrelevant\natoms.\nCMBP planner [13] is a conformant planner based on model checking. Like Conformant\nGraphplan it performs a forward-chaining analysis, but relies on an effective way of encoding\nsets of possible worlds and its performance is less dependent on the amount of irrelevant\ninformation in the initial state. CMBP uses action representation in the form of non-\ndeterministic state transition relations.\nAn approach to conformant planning as a heuristic search in the space of belief states\nthat are sets of world states is presented by Bonet and Geffner ([10]). An admissible heuristic\nfunction is computed based on the distance to the goal state under the assumption of\ncomplete information. The search produces an optimal plan, however the algorithm relies\non the finiteness of the state space, which is not achievable when the domain of objects\nis infinite. The action language used is an extension of STRIPS that includes function\nsymbols, negation, disjunction, non-deterministic actions and conditional effects.\nIPE [1], SENSE-P [17], XII[22], PUCCINI [21] are causal link planners that interleave\nplanning with execution of incomplete plans. The action description language of PUCCINI,\nSADL [23] includes actions with conditional and informational effects. However, to the best\nof our knowledge there are no completeness results for conformant planning with SADL.\nPKS [33] is a forward chaining planner based on a representation of the agent’s knowl-\nedge that captures a set of possible worlds via a set of knowledge formulas similarly to\nPSIPLAN’s SOK. The representation of Petrick and Baccus includes functional symbols,\nconditional plans and actions with conditional effects, all of which are not represented in\nPSIPLAN for the reason of tractability. However, the PKS planner only admits ground\nliterals in its goal language and does not handle universally quantified negated goals. PKS\nis also incomplete, but the authors report that it is able to generate plans in many domains.\nThe LCW [16] (see introduction) language is extended in [28, 19] to handling exceptions.\nLevy in [28] uses extended LCW sentences, called Local Completeness (LC) sentences, to\nrepresent database completeness information and derive answer completeness property of\na conjunctive query. This is analogous to computing whether a SOK entails a universally\nquantified knowledge goal, where the SOK is given by the combination of relational tables\nand the knowledge goal is to know all individual objects that satisfy a given query. Friedman\nand Weld [19] extend on Levy’s work for the purpose of eliminating redundant information\ngathering from databases by an Internet agent. They present a method of determining\nsubsumption from LC sentences: whether a set of relational tables contains all information\n36 T. BABAIAN AND J. G. SCHMOLZE\navailable in another relational table. Thus, both of these works only consider the setting in\nwhich there are no actions that can change the world, do not address a changing world or\nplanning, and do not present any methods that would make these extensions amenable to\ntheir use in an open world planning algorithm, such as the image and e-difference operations\nof PSIPLAN that are critical in the computation of state update after an action, causal\nlinks and threat resolution.\n5.2. Complexity. Complexity of propositional planning with incomplete information, with\nand without sensing actions, has been addressed by many researchers (e.g. [24], [15], [6],\n[38]). Results presented in these works, although for different state and action languages,\ngenerally show that the complexity of constructing conformant plans of polynomial length\nis greater than planning with complete information, which is NP-complete. Though we\nhave not proven the following formally, from the results of this paper it appears that: (a)\nchecking whether a given plan (of polynomial length) solves a given problem in PSIPLAN\ncan be done in polynomial time and, thus, (b) determining whether there exists a plan (of\npolynomial length) that solves a given problem in PSIPLAN is NP-complete. These results\ndo not contradict the results of Baral et al., nor those of Turner [38], as we explain below;\nthe key to the reduced complexity of PSIPLAN-based planning compared to the analysis\nin these papers seems to be the absence of conditional effects.\nBaral et al. [6] present complexity results for a variety of problems related to open world\nplanning with action language A. In particular, they show the problem of finding all solution\nplans in presence of incomplete information and no sensing belongs to the class Σ2P . To\nkeep the complexity of planning with incomplete information within the NP-completeness\nbounds, they propose a 0-approximation, which sacrifices completeness.\nIn PSIPLAN as in 0-approximation of Baral et al., the set of possible worlds is rep-\nresented by a set of propositions that are known to be true. However, unlike the action\nlanguage A used in Baral et al.’s work, PSIPLAN’s action language does not allow for\nconditional effects, and so all of an action’s effects are guaranteed to be true after the (ex-\necutable) action is performed. In contrast, in action language A determining the effect of\nthe action and thus the resulting set of possible worlds sometimes requires an analysis of\npossible values of unknown propositions. 0-approximation does not involve such analysis\nand thus loses such plans.\nFor example having no information at all and an action a0 with conditional effect ”a0\ncauses p if ¬p”, a plan that consists of a single action a0 achieves p, but it will be missed by\n0-approximation. Without the analysis of the result of performing a0 in two possible initial\nstates (corresponding to the two different values of p), it is impossible to conclude that p is\ntrue after performing a0. That is the reason why 0-approximation will miss it. In PSIPLAN,\nthere are no conditional effects, and so action a0 from above cannot be represented. Once\nexecutability of an action is determined, all effects are guaranteed and the set of possible\nworlds is precisely described by the single updated state of knowledge. Thus, in PSIPLAN\ncompleteness of conformant planning is preserved without an increase of complexity over\nclassical planning.\nTurner [38] presents a comprehensive complexity analysis of a set of planning problems\nby using a very general framework for describing a planning problem. This framework rep-\nresents actions as state transition relations and integrates many action languages including\nthose with conditional effects, nondeterminism and concurrency. As in Baral et al. his\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 37\nresults on conformant planning consider actions that may have conditional effects, and are\nmore general than PSIPLAN’s.\nHaslum and Jonsson’s paper [24] states a PSPACE-completeness result for the problem\nof verifying existence of a conformant plan of unbounded length with STRIPS-like actions.\nThis result is presented without proof and thus it is difficult to analyze it for the case of\npolynomially bounded-length plans.\n6. Conclusions\nClassical planning presupposes that complete and correct information about the world\nis available at any point of planning (by having a completely specified initial situation, and\ndeterministic actions). However, in a more realistic setting, the knowledge about the initial\nstate may be incomplete, the effects of actions may be nondeterministic, or there may be\nother agents acting in the world. These are some sources of uncertainty in planning.\nIn this paper we dropped one of the assumptions of classical planning—the assumption\nof complete knowledge of the initial state of the world— thereby considering the problem\nof open world planning. We have presented PSIPLAN, a language for representing and\nreasoning in open world applications. PSIPLAN uses ψ-forms to represent infinite sets of\nclauses of negated literals. We have shown the following.\n• Our algorithm to determine entailment in PSIPLAN is sound and complete and has\npolynomial complexity in the number of propositions in the state of knowledge under\ncertain assumptions on the structure of ψ-forms common for open world planning\nproblems. Operations image and e-difference between ψ-forms, which are crucial to\nplanning with quantified propositions, also have polynomial complexity.\n• Updating the agent’s state of knowledge after performing an action has polynomial\ncomplexity in the number of propositions in the state of the agent’s knowledge. In\naddition, the update procedure correctly and completely describes the transition\nbetween possible worlds due to the action.\nThus, PSIPLAN representation efficiently handles domains with an incomplete specification\nof the initial state without considering the set of all possible worlds, and does not require\nthat the agent know the set of all objects. We implemented a partial order planning\nalgorithm PSIPOP [5] for open worlds that uses PSIPLAN representation of state and\nactions. PSIPOP uses PSIPLAN calculus for reasoning about goal achievement. Since all\nof the PSIPLAN operations used by PSIPOP have only polynomial complexity, we argue\ninformally that planning with PSIPLAN does not exceed the complexity of closed world\nSTRIPS style planning. PSIGraph [12] is a GraphPlan-based planning algorithm which\nuses PSIPLAN.\nFurther evidence of the applicability of PSIPLAN representation for planning in open\nworlds with a large or infinite number of objects, is the use of PSIPOP’s extension to\nplanning with sensing and interleaved execution at the core of the Writer’s Aid [3] – a\ncollaborative bibliography assistant. Completeness and tractability of PSIPLAN’s reason-\ning and its ability to effectively handle initial information and goal statements universally\nquantified over an infinite domain of objects ensured effective and non-redundant operation\nof the system, which were critically important in this application.\nExperimental results from the initial implementation of PSIPOP, as well as an ex-\nperimental assessment of the impact of various parameters of ψ-forms on the performance\nof the entailment algorithm are presented in [4]. Results from the initial implementation\n38 T. BABAIAN AND J. G. SCHMOLZE\nof PSIGraph planner are presented in [12]. We are currently working on optimizing the\nperformance of these two planners. A thorough experimental evaluation of PSIPOP and\nPSIGraph is under way and we are planning on reporting it in a future paper.\nIn the future, we will extend PSIPLAN to allow function symbols, and we will publish\nalready completed work that extends PSIPLAN to reasoning about the agent’s knowledge\ngoals and the effects of sensing actions.\n7. Acknowledgements\nWe thank Barbara Grosz, Wendy Lucas, Wheeler Ruml and the anonymous referees\nfor their helpful comments on an earlier draft of this paper.\nReferences\n[1] Ambros-Ingerson, J. A. and S. Steel: 1988, ‘Integrating Planning, Execution and Monitoring’. In: Pro-\nceedings of the Seventh National Conference on Artificial Intelligence (AAAI-88). St. Paul, Minnesota,\npp. 83–88.\n[2] Babaian, T.: 2000, ‘Knowledge Representation and Open World Planning Using ψ-forms’. Ph.D. thesis,\nTufts University, Medford, MA.\n[3] Babaian, T., B. J. Grosz, and S. M. Shieber: 2002, ‘A Writer’s Collablrative Assistant’. In: Proceedings\nof Intelligent User Interfaces’02. pp. 7–14.\n[4] Babaian, T. and J. Schmolze, ‘Efficient Open World Reasoning and Planning’. Unpublished paper,\navailable at http://cis.bentley.edy/tbabaian/papers/p1.ps.\n[5] Babaian, T. and J. Schmolze: 2000, ‘PSIPLAN: open world planning with ψ-forms’. In: Artificial\nIntelligence Planning and Scheduling: Proceedings of the Fifth International Conference (AIPS’00). pp.\n292–300.\n[6] Baral, C., V. Kreinovich, and R. Trejo: 2000, ‘Computational complexity of planning and approximate\nplanning in the presence of incompleteness’. Artificial Intelligence 122(1-2), 241–267.\n[7] Baral, C. and T. C. Son: 1997, ‘Approximate Reasoning about Actions in Presence of Sensing and\nIncomplete Information’. In: International Logic Programming Symposium. pp. 387–401.\n[8] Biundo, S. and M. Fox (eds.): 1999, ‘Proceedings of the 5-th European Conference on Planning’.\nDurham, England:, Springer-Verlag.\n[9] Blum, A. L. and M. L. Furst: 1995, ‘Fast Planning Through Planning Graph Analysis’. In: Proceedings\nof the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95). Nagoya, Japan,\npp. 1636–1642.\n[10] Bonet, B. and H. Geffner: 2000, ‘Planning with incomplete information as a heuristic search in be-\nlief space.’. In: Artificial Intelligence Planning and Scheduling: Proceedings of the Fifth International\nConference (AIPS’00). Breckenridge, Colorado.\n[11] Brafman, R. I. and J. Hoffmann: 2004, ‘Conformant Planning via Heuristic Forward Search: A New\nApproach.’. In: ICAPS. pp. 355–364.\n[12] Carlin, A., J. G. Schmolze, and T. Babaian: 2005, ‘Graphplan Based Conformant Planning with Limited\nQuantification’. Research on Computing Science. 5 (Special Issue: Advances in Artificial Intelligence\nTheory), 65–75.\n[13] Cimatti, A. and M. Roveri: 1999, ‘Conformant Planning via Model Checking’. in [8].\n[14] Eiter, T., W. Faber, N. Leone, G. Pfeifer, and A. Polleres: 2000, ‘Planning under Incomplete Knowl-\nedge’. In: Computational Logic. pp. 807–821.\n[15] Eiter, T., W. Faber, N. Leone, G. Pfeifer, and A. Polleres: 2004, ‘A logic programming approach to\nknowledge-state planning: Semantics and complexity’. ACM Trans. Comput. Logic 5(2), 206–263.\n[16] Etzioni, O., K. Golden, and D. Weld: 1997, ‘Sound and efficient closed-world reasoning for planning’.\nArtificial Intelligence 89(1–2), 113–148.\n[17] Etzioni, O., S. Hanks, D.Weld, D. Draper, N. Lesh, and M. Williamson: 1992, ‘An Approach to Planning\nwith Incomplete Information’. In: Proceedings of the Third International Conference on Principles of\nKnowledge Representation and Reasoning (KR-92). Cambridge, MA, pp. 115–125.\nEFFICIENT OPEN WORLD REASONING FOR PLANNING 39\n[18] Finzi, A., F. Pirri, and R. Reiter: 2000, ‘OpenWorld Planning in the Situation Calculus’. In: Proceedings\nof the Seventeenth National Conference on Artificial Intelligence (AAAI-00). Austin, Texas.\n[19] Friedman, M. and D. S. Weld: 1997, ‘Efficiently Executing Information-Gathering Plans’. In: Proceed-\nings of the Fifthteenth International Joint Conference on Artificial Intelligence (IJCAI-97).\n[20] Genesereth, M. R. and N. Nilsson: 1986, Logical Foundations of Artificial Intelligence. Morgan-\nKaufmann Publishers.\n[21] Golden, K.: 1997, ‘Planning and knowledge representation for Softbots.’. Ph.D. thesis, University of\nWashington.\n[22] Golden, K., O. Etzioni, and D. Weld: 1994, ‘Omnipotence Without Omniscience: Efficient Sensor\nManagement for Planning’. In: Proceedings of AAAI-94.\n[23] Golden, K. and D. Weld: 1996, ‘Representing Sensing Actions: The Middle Ground Revisited’. In: Pro-\nceedings of the Fifth International Conference on Principles of Knowledge Representation and Reasoning\n(KR-96). Cambridge, MA, pp. 174–185.\n[24] Haslum, P. and P. Jonsson: 1999, ‘Some results on the complexity of planning with incomplete infor-\nmation.’. in [8].\n[25] Krebsbach, K., D. Olawsky, and M. Gini: 1992, ‘An Empiracal Study of Sensing and Defaulting in Plan-\nning’. In: Artificial Intelligence Planning Systems: Proceedings of the First International Conference\n(AIPS-92). College Park, MD, pp. 136–144.\n[26] Levesque, H.: 1996, ‘What is planning in the presence of sensing?’. In: Proceedings of the Thirteenth\nNational Conference on Artificial Intelligence (AAAI-96).\n[27] Levesque, H. J., R. Reiter, Y. Lesperance, F. Lin, and R. B. Scherl: 1997, ‘GOLOG: A Logic Program-\nming Language for Dynamic Domains’. Journal of Logic Programming 31(1-3), 59–83.\n[28] Levy, A.: 1996, ‘Obtaining Complete Answers from Incomplete Databases’. In: Proceedings of the 22nd\nVLDB Conference. Mumbai (Bombay), India.\n[29] Lifschitz, V.: 1999, ‘Answer Set Planning.’. In: ICLP. pp. 23–37.\n[30] Liu, Y. and H. J. Levesque: 2005, ‘Tractable reasoning with incomplete first-order knowledge in dynamic\nsystems with context-dependent actions’. In: IJCAI. pp. 639–644.\n[31] McAllester, D. and D. Rosenblitt: 1991, ‘Systematic Nonlinear Planning’. In: Proceedings of the Ninth\nNational Conference on Artificial Intelligence (AAAI-91). Anaheim, California, pp. 634–639.\n[32] Peot, M. A. and D. E. Smith: 1992, ‘Conditional Nonlinear Planning’. In: Artificial Intelligence Plan-\nning Systems: Proceedings of the First International Conference (AIPS-92). College Park, MD, pp.\n189–197.\n[33] Petrick, R. and F. Bacchus: 2002, ‘A Knowledge-Based Approach to Planning with Incomplete Infor-\nmation and Sensing’. In: AI Planning and Scheduling (AIPS2002). pp. 212–222.\n[34] Scherl, R. B. and H. J. Levesque: 1993, ‘The Frame Problem and Knowledge-Producing Actions’. In:\nProceedings of the Eleventh National Conference on Artificial Intelligence (AAAI-93).\n[35] Shirazi, A. and E. Amir: 2005, ‘First Order Logical Filtering’. In: IJCAI. pp. 589–595.\n[36] Smith, D. and D. S. Weld: 1998, ‘Conformant Graphplan’. In: Proceedings of the Fifteenth National\nConference on Artificial Intelligence (AAAI-98).\n[37] Thielscher, M.: 2005, ‘FLUX: A logic programming method for reasoning agents’. Theory and Practice\nof Logic Programming 5(4-5), 533–565.\n[38] Turner, H.: 2002, ‘Polynomial-length planning spans the polynomial hierarchy’. In: Proceedings of\nEighth European Conf. on Logics in Artificial Intelligence (JELIA’02).\n[39] Weld, D. S.: 1999, ‘Recent Advances in AI Planning’. AI Magazine 20(2), 93–123.\n[40] Weld, D. S., C. R. Anderson, and D. E. Smith: 1998, ‘Extending Graphplan to Handle Uncertainty\nand Sensing Actions’. In: Proceedings of the Fifteenth National Conference on Artificial Intelligence\n(AAAI-98). Madison, Wisconsin.\n[41] Winslett, M.: 1988, ‘Reasoning about action using a possible models approach’. In: Proceedings of the\nSeventh National Conference on Artificial Intelligence (AAAI-88).\nThis work is licensed under the Creative Commons Attribution-NoDerivs License. To view\na copy of this license, visit http://creativecommons.org/licenses/by-nd/2.0/ or send a\nletter to Creative Commons, 559 Nathan Abbott Way, Stanford, California 94305, USA.\n",
            "id": 946054,
            "identifiers": [
                {
                    "identifier": "cs/0601032",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "188983842",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "1977644491",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "10.2168/lmcs-2(3:5)2006",
                    "type": "DOI"
                },
                {
                    "identifier": "2421572",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:arxiv.org:cs/0601032",
                    "type": "OAI_ID"
                }
            ],
            "title": "Efficient Open World Reasoning for Planning",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "1977644491",
            "oaiIds": [
                "oai:arxiv.org:cs/0601032"
            ],
            "publishedDate": "2006-09-26T00:00:00",
            "publisher": "'Logical Methods in Computer Science e.V.'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "https://lmcs.episciences.org/2247/pdf",
                "http://arxiv.org/abs/cs/0601032"
            ],
            "updatedDate": "2021-04-27T05:05:59",
            "yearPublished": 2006,
            "journals": [
                {
                    "title": "Logical Methods in Computer Science (LMCS)",
                    "identifiers": [
                        "1860-5974"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/cs/0601032"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/946054"
                }
            ]
        },
        {
            "acceptedDate": "2007-10-23T00:00:00",
            "arxivId": "cs/0612083",
            "authors": [
                {
                    "name": "Zhao, Wenbing"
                }
            ],
            "contributors": [
                "The Pennsylvania State University CiteSeerX Archives"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/24505611",
                "https://api.core.ac.uk/v3/outputs/206682799"
            ],
            "createdDate": "2012-04-13T14:21:51",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 145,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/145",
                    "logo": "https://api.core.ac.uk/data-providers/145/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2007-09-01T00:00:00",
            "abstract": "In this paper, we present a Byzantine fault tolerant distributed commit\nprotocol for transactions running over untrusted networks. The traditional\ntwo-phase commit protocol is enhanced by replicating the coordinator and by\nrunning a Byzantine agreement algorithm among the coordinator replicas. Our\nprotocol can tolerate Byzantine faults at the coordinator replicas and a subset\nof malicious faults at the participants. A decision certificate, which includes\na set of registration records and a set of votes from participants, is used to\nfacilitate the coordinator replicas to reach a Byzantine agreement on the\noutcome of each transaction. The certificate also limits the ways a faulty\nreplica can use towards non-atomic termination of transactions, or semantically\nincorrect transaction outcomes.Comment: To appear in the proceedings of the 3rd IEEE International Symposium\n  on Dependable, Autonomic and Secure Computing, 200",
            "documentType": "research",
            "doi": "10.1109/dasc.2007.10",
            "downloadUrl": "http://arxiv.org/abs/cs/0612083",
            "fieldOfStudy": "computer science",
            "fullText": "ar\nX\niv\n:c\ns/0\n61\n20\n83\nv3\n  [\ncs\n.D\nC]\n  1\n A\nug\n 20\n07\nA Byzantine Fault Tolerant Distributed Commit Protocol ∗\nWenbing Zhao\nDepartment of Electrical and Computer Engineering\nCleveland State University, 2121 Euclid Ave, Cleveland, OH 44115\nwenbing@ieee.org\nAbstract\nIn this paper, we present a Byzantine fault tolerant dis-\ntributed commit protocol for transactions running over un-\ntrusted networks. The traditional two-phase commit proto-\ncol is enhanced by replicating the coordinator and by run-\nning a Byzantine agreement algorithm among the coordi-\nnator replicas. Our protocol can tolerate Byzantine faults\nat the coordinator replicas and a subset of malicious faults\nat the participants. A decision certificate, which includes a\nset of registration records and a set of votes from partici-\npants, is used to facilitate the coordinator replicas to reach\na Byzantine agreement on the outcome of each transaction.\nThe certificate also limits the ways a faulty replica can use\ntowards non-atomic termination of transactions, or seman-\ntically incorrect transaction outcomes.\nKeywords: Distributed Transaction, Two Phase Commit,\nFault Tolerance, Byzantine Agreement, Web Services\n1. Introduction\nThe two-phase commit (2PC) protocol [8] is a standard\ndistributed commit protocol [12] for distributed transac-\ntions. The 2PC protocol is designed with the assumptions\nthat the coordinator and the participants are subject only to\nbenign faults, and the coordinator can be recovered quickly\nif it fails. Consequently, the 2PC protocol does not work\nif the coordinator is subject to arbitrary faults (also known\nas Byzantine faults [10]) because a faulty coordinator might\nsend conflicting decisions to different participants. Unfor-\ntunately, with more and more distributed transactions run-\nning over the untrusted Internet, driven by the need for busi-\nness integration and collaboration, and enabled by the latest\nWeb-based technologies such as Web services, it is a realis-\ntic threat that cannot be ignored.\nThis problem is first addressed by Mohan et al. in [11] by\nintegrating Byzantine agreement and the 2PC protocol. The\nbasic idea is to replace the second phase of the 2PC pro-\n∗This work was supported in part by Department of Energy Contract\nDE-FC26-06NT42853, and by a Faculty Research Development award\nfrom Cleveland State University.\ntocol with a Byzantine agreement among the coordinator,\nthe participants, and some redundant nodes within the root\ncluster (where the root coordinator resides). This prevents\nthe coordinator from disseminating conflicting transaction\noutcomes to different participants without being detected.\nHowever, this approach has a number of deficiencies. First,\nit requires all members of the root cluster, including partici-\npants, to reach a Byzantine agreement for each transaction.\nThis would incur very high overhead if the size of the cluster\nis large. Second, it does not offer Byzantine fault tolerance\nprotection for subordinate coordinators or participants out-\nside the root cluster. Third, it requires the participants in the\nroot cluster to know all other participants in the same clus-\nter, which prevents dynamic propagation of transactions. In\ngeneral, only the coordinator should have the knowledge of\nthe participants set for each transaction. These problems\nprevent this approach from being used in practical systems.\nRothermel et al. [13] addressed the challenges of en-\nsuring atomic distributed commit in open systems where\nparticipants (which may also serve as subordinate coor-\ndinators) may be compromised. However, [13] assumes\nthat the root coordinator is trusted, which limits its useful-\nness. Garcia-Molina et al. [6] discussed the circumstances\nwhen Byzantine agreement is needed for distributed trans-\naction processing. Gray [7] compared the problems of dis-\ntributed commit and Byzantine agreement, and provided in-\nsight on the commonality and differences between the two\nparadigms.\nIn this paper, we carefully analyze the threats to atomic\ncommitment of distributed transactions and evaluate strate-\ngies to mitigate such threats. We choose to use a Byzan-\ntine agreement algorithm only among the coordinator repli-\ncas, which avoids the problems in [11]. An obvious candi-\ndate for the Byzantine agreement algorithm is the Byzantine\nfault tolerance (BFT) algorithm described in [5] because\nof its efficiency. However, the BFT algorithm is designed\nto ensure totally ordered atomic multicast for requests to a\nreplicated stateful server. We made a number of modifica-\ntions to the algorithm so that it fits the problem of atomic\ndistributed commit. The most crucial change is made to the\nfirst phase of the BFT algorithm, where the primary coordi-\nnator replica is required to use a decision certificate, which\nis a collection of the registration records and the votes it\nhas collected from the participants, to back its decision on a\ntransaction’s outcome. The use of such a certificate is essen-\ntial to enable a correct backup coordinator replica to verify\nthe primary’s proposal. This also limits the methods that a\nfaulty replica can use to hinder atomic distributed commit\nof a transaction.\nWe integrated our Byzantine fault tolerant distributed\ncommit (BFTDC) protocol with Kandula, a well-known\nopen source distributed commit framework for Web ser-\nvices [2]. The framework is an implementation of the Web\nServices Atomic Transaction Specification (WS-AT) [4].\nThe measurements show that our protocol incurs only mod-\nerate runtime overhead during normal operations.\n2. Background\n2.1. Distributed Transactions\nA distributed transaction is a transaction that spans\nacross multiple sites over a computer network. It should\nmaintain the same ACID properties [8] as a local transac-\ntion does. One of the most interesting issues for distributed\ntransactions is how to guarantee atomicity, i.e., either all op-\nerations of the transaction succeed in which case the trans-\naction commits, or none of the operations is carried out in\nwhich case the transaction aborts.\nThe middleware supporting distributed transactions is\noften called transaction processing monitors (or TP moni-\ntors in short). One of the main services provided by a TP\nmonitor is a distributed commit service, which guarantees\nthe atomic termination of distributed transactions. In gen-\neral, the distributed commit service is implemented by the\n2PC protocol, a standard distributed commit protocol [12].\nAccording to the 2PC protocol, a distributed transaction\nis modelled to contain one coordinator and a number of par-\nticipants. A distributed transaction is initiated by one of the\nparticipants, which is referred to as the initiator. The coor-\ndinator is created when the transaction is activated by the\ninitiator. All participants are required to register with the\ncoordinator when they get involved with the transaction. As\nthe name suggests, the 2PC protocol commits a transaction\nin two phases. During the first phase (also called prepare\nphase), a request is disseminated by the coordinator to all\nparticipants so that they can prepare to commit the trans-\naction. If a participant is able to commit the transaction, it\nprepares the transaction for commitment and responds with\na “prepared” vote. Otherwise, it votes “aborted”. When\na participant responded with a “prepared” vote, it enters a\n“ready” state. Such a participant must be prepared to ei-\nther commit or abort the transaction. A participant that has\nnot sent a “prepared” vote can unilaterally abort the trans-\naction. When the coordinator has received votes from every\nparticipant, or a pre-defined timeout has occurred, it starts\nthe second phase by notifying the outcome of the transac-\ntion. The coordinator decides to commit a transaction only\nif it has received the “prepared” vote from every participant\nduring the first phase. It aborts the transaction otherwise.\n2.2. Byzantine Fault Tolerance\nByzantine fault tolerance refers to the capability of a sys-\ntem to tolerate Byzantine faults. It can be achieved by repli-\ncating the server and by ensuring all server replicas receive\nthe same input in the same order. The latter means that the\nserver replicas must reach an agreement on the input despite\nByzantine faulty replicas and clients. Such an agreement is\noften referred to as Byzantine agreement [10].\nByzantine agreement algorithms had been too expensive\nto be practical until Castro and Liskov invented the BFT\nalgorithm mentioned earlier [5]. The BFT algorithm is ex-\necuted by a set of 3f + 1 replicas to tolerate f Byzantine\nfaulty replicas. One of the replicas is designated as the pri-\nmary while the rest are backups. The normal operation of\nthe BFT algorithm involves three phases. During the first\nphase (called pre-prepare phase), the primary multicasts a\npre-prepare message containing the client’s request, the cur-\nrent view and a sequence number assigned to the request to\nall backups. A backup verifies the request message and the\nordering information. If the backup accepts the message, it\nmulticasts to all other replicas a prepare message contain-\ning the ordering information and the digest of the request\nbeing ordered. This starts the second phase, i.e., the pre-\npare phase. A replica waits until it has collected 2f match-\ning prepare messages from different replicas before it mul-\nticasts a commit message to other replicas, which starts the\nthird phase (i.e., commit phase). The commit phase ends\nwhen a replica has received 2f matching commit messages\nfrom other replicas. At this point, the request message has\nbeen totally ordered and it is ready to be delivered to the\nserver application.\nIf the primary or the client is faulty, a Byzantine agree-\nment on the order of a request might not be reached, in\nwhich case, a new view is initiated, triggered by a time-\nout on the current view. A different primary is designated\nin a round-robin fashion for each new view installed.\n3. BFT Distributed Commit\n3.1. System Models\nWe consider transactional client/server applications sup-\nported by an object-based TP monitor such as the WS-AT\nconformant framework [2] used in our implementation. For\nsimplicity, we assume a flat distributed transaction model.\nWe assume that for each transaction, a distinct coordinator\nis created. The lifespan of the coordinator is the same as the\ntransaction it coordinates.\nAll transactions are started and terminated by the initia-\ntor. The initiator also propagates the transaction to other\nparticipants. The distributed commit protocol is started for\na transaction when a commit/abort request is received from\nthe initiator. The initiator is regarded as a special partici-\npant. In later discussions we do not distinguish the initiator\nand other participants unless it is necessary to do so.\nWhen considering the safety of our distributed com-\nmit protocol, we use an asynchronous distributed system\nmodel. However, to ensure liveness, certain synchrony must\nbe assumed. Similar to [5], we assume that the message\ntransmission and processing delay has an asymptotic upper\nbound. This bound is dynamically explored in the adapted\nByzantine agreement algorithm in that each time a view\nchange occurs, the timeout for the new view is doubled.\nWe assume that the transaction coordinator runs sepa-\nrately from the participants, and it is replicated. For sim-\nplicity, we assume that the participants are not replicated.\nWe assume that 3f + 1 coordinator replicas are available,\namong which at most f can be faulty during a transaction.\nThere is no limit on the number of faulty participants. Simi-\nlar to [5], each coordinator replica is assigned a unique id i,\nwhere i varies from 0 to 3f . For view v, the replica whose id\ni satisfies i = v mod (3f + 1) would serve as the primary.\nThe view starts from 0. For each view change, the view\nnumber is increased by one and a new primary is selected.\nIn this paper, we call a coordinator replica correct if it\ndoes not fail during the distributed commit for the trans-\naction under consideration, i.e., it faithfully executes ac-\ncording to the protocol prescribed from the start to the end.\nHowever, we call a participant correct if it is not Byzantine\nfaulty, i.e., it may be subject to typical non-malicious faults\nsuch as crash faults or performance faults.\nThe coordinator replicas are subject to Byzantine faults,\ni.e., a Byzantine faulty replica can fail arbitrarily. For par-\nticipants, however, only a subset of faulty behaviors are tol-\nerated, such as a faulty participant sending conflicting votes\nto different coordinator replicas. Some forms of participant\nByzantine behaviors cannot be addressed by the distributed\ncommit protocol.1\nFor the initiator, we further limits its Byzantine faulty\nbehaviors. In particular, it does not exclude any correct par-\nticipant from the scope of the transaction, or include any\nparticipant that has not registered properly with the coordi-\nnator replicas, as discussed below.\nTo ensure atomic termination of a distributed transaction,\nit is essential that all correct coordinator replicas agree on\nthe set of participants involved in the transaction. In this\nwork, we defer the Byzantine agreement on the participants\nset until the distributed commit stage and combine it with\nthat for the transaction outcome. To facilitate this optimiza-\ntion, we need to make the following additional assumptions.\nWe assume that there is proper authentication mecha-\nnism in place to prevent a Byzantine faulty process from\nillegally registering itself as a participant at correct coor-\ndinator replicas. Furthermore, we assume that a correct\n1 For example, a Byzantine faulty participant can vote to commit a\ntransaction while actually aborting it, and vice versa.\nparticipant registers with f + 1 or more correct coordina-\ntor replicas before it sends a reply to the initiator when the\ntransaction is propagated to this participant with a request\ncoming from the initiator. If a correct participant crashes\nbefore the transaction is propagated to itself, or before it\nfinishes registering with the coordinator replicas, either no\nreply is sent back to the initiator, or an exception is thrown\nback to the initiator. As a result, the initiator should decide\nto abort the transaction. The interaction pattern among the\ninitiator, participants and the coordinator is identical to that\ndescribed in the WS-AT specification [4], except that the\ncoordinator is replicated in this work.\nAll messages between the coordinator and the partici-\npants are digitally signed. We assume that the coordinator\nreplicas and the participants each has a public/secret key\npair. The public keys of the participants are known to all\ncoordinator replicas, and vice versa, while the private key is\nkept secret to its owner. We assume that the adversaries\nhave limited computing power so that they cannot break\nthe encryption and digital signatures of correct coordinator\nreplicas.\n3.2. BFTDC Protocol\nFigure 1 shows the pseudo-code of the our Byzantine\nfault tolerant distributed commit protocol. Comparing with\nthe 2PC protocol, there are two main differences:\n– At the coordinator side, an additional phase of Byzan-\ntine agreement is needed for the coordinator replicas\nto reach a consensus on the outcome of the transaction,\nbefore they notify the participants.\n– At the participant side, a decision (commit or abort\nrequest) from a coordinator replica is queued until at\nleast f+1 identical decision messages have been re-\nceived, unless the participant unilaterally aborts the\ntransaction. This is to make sure that at least one of\nthe decision messages come from a correct coordina-\ntor replica.\nThe distributed commit for a transaction starts when a\ncoordinator replica receives a commit request from the ini-\ntiator. If the coordinator replica receives an abort request\nfrom the initiator, it skips the first phase of the distributed\ncommit. In any case, a Byzantine agreement is conducted\non the decision regarding the transaction’s outcome.\nThe operations of each coordinator replica is defined in\nthe BFTDistributedCommit() method in Fig. 1. During the\nprepare phase, a coordinator replica sends a prepare request\nto every participant in the transaction. The prepare request\nis piggybacked with a prepare certificate, which contains\nthe commit request sent (and signed) by the initiator.\nWhen a participant receives a prepare request from a co-\nordinator replica, it verifies the correctness of the signature\nof the message and the prepare certificate (if the partici-\npant does not know the initiator’s public key, this step is\nMethod: BFTDistributedCommit(CommitRequest)\nbegin\nPrepareCert := CommitRequest;\nAppend PrepareCert to PrepareRequest;\nMulticast PrepareRequest;\nVoteLog := CollectVotes();\nAdd VoteLog to DecisionCert;\ndecision := ByzantineAgreement(DecisionCert);\nif decision = Commit then Multicast CommitRequest;\nelse Multicast AbortRequest;\nReturn decision;\nend\nMethod: PrepareTransaction(PrepareRequest)\nbegin\nif VerifySignature(PrepareRequest) = false then\nDiscard PrepareRequest and return;\nif HasPrepareCert(PrepareRequest) = false then\nDiscard PrepareRequest and return;\nif P is willing to commit T then\nLog(<Prepared T>) to stable storage;\nSend ‘‘prepared’’ to coordinator;\nelse\nLog(<Abort T>); Send ‘‘aborted’’ to coordinator;\nend\nMethod: CommitTransaction(CommitRequest)\nbegin\nif VerifySignature(CommitRequest) = false then\nDiscard CommitRequest and return;\nAppend CommitRequest to DecisionLog;\nif CanMakeDecision(commit, DecisionLog) then\nLog(<Commit T>) to stable storage;\nSend ‘‘committed’’ to coordinator;\nend\nMethod: AbortTransaction(AbortRequest)\nbegin\nif VerifySignature(AbortRequest) = false then\nDiscard AbortRequest and return;\nAppend AbortRequest to DecisionLog;\nif CanMakeDecision(abort, DecisionLog) then\nLog(<Abort T>); Abort T locally;\nSend ‘‘aborted’’ to coordinator;\nend\nMethod: CanMakeDecision(decision, DecisionLog)\nbegin\nNumOfDecisions := 0;\nforeach Message in DecisionLog do\nif GetDecision(Message) = decision then\nNumOfDecisions++;\nif NumOfDecisions >= f+1 then Return true;\nelse Return false;\nend\nFigure 1. Pseudo-code for our Byzantine fault\ntolerant distributed commit protocol.\nskipped). The prepare request is discarded if any of the veri-\nfication steps fails. Even though the check for a prepare cer-\ntificate is not essential to the correctness of our distributed\ncommit protocol, it nevertheless can prevent a faulty coordi-\nnator replica from instructing some participants to prepare\na transaction, even after the initiator has requested to abort\nthe transaction.\nAt the end of the prepare phase, all correct coordinator\nreplicas engage in an additional round for them to reach\na Byzantine agreement on the outcome of the transaction.\nThe Byzantine agreement algorithm used in this phase is\nelaborated in Section 3.3.\nWhen a participant receives a commit request from a co-\nordinator replica, it commits the transaction only if it has\nreceived the same decision from f other replicas so that at\nleast one of them comes from a correct replica. The han-\ndling of an abort request is similar.\n3.3. Byzantine Agreement Algorithm\nThe Byzantine agreement algorithm used in the BFTDC\nprotocol is adapted from the BFT algorithm by Castro and\nLiskov [5]. To avoid possible confusion with the terms used\nto refer to the distributed commit protocol, the three phases\nduring normal operations are referred to as ba-pre-prepare,\nba-prepare, and ba-commit. Our algorithm differs from the\nBFT algorithm in a number of places due to different objec-\ntives. The BFT algorithm is used for server replicas to agree\non the total ordering of the requests received, while our al-\ngorithm is used for the coordinator replicas to agree on the\noutcome (and participants set) of each transaction. In our al-\ngorithm, the ba-pre-prepare message is used to bind a deci-\nsion (to commit or abort) with the transaction under concern\n(represented by a unique transaction id). In [5], the ba-pre-\nprepare message is used to bind a request with an execution\norder (represented by a unique sequence number). Further-\nmore, for distributed commit, an instance of our algorithm\nis created and executed for each transaction. When there are\nmultiple concurrent transactions, multiple instances of our\nalgorithm are running concurrently and independently from\neach other (the relative ordering of the distributed commit\nfor different transactions is not important). In [5], however,\na single instance of the BFT algorithm is used for all re-\nquests to be ordered.\nWhen a replica completes the prepare phase of the dis-\ntributed commit for a transaction, an instance of our Byzan-\ntine agreement algorithm is created. The algorithm starts\nwith the ba-pre-prepare phase. During this phase, the pri-\nmary p sends a ba-pre-prepare message including its de-\ncision certificate to all other replicas. The ba-pre-prepare\nmessage has the form <BA-PRE-PREPARE, v, t, o, C>σp ,\nwhere v is the current view number, t is the transaction\nid, o is the proposed transaction outcome (i.e., commit or\nabort), C is the decision certificate, and σp is the signature\nof the message signed by the primary. The decision certifi-\ncate contains a collection of records, one for each partici-\npant. The record for a participant j contains a signed reg-\nistration Rj = (t, j)σj and a signed vote Vj = (t, vote)σj\nfor the transaction t, if a vote from j has been received by\nthe primary. The transaction id is included in each registra-\ntion and vote record so that a faulty primary cannot reuse\nan obsolete registration or vote record to force a transac-\ntion outcome against the will of some correct participants\n(which may lead to non-atomic transaction commit).\nA backup accepts a ba-pre-prepare message provided:\n– The message is signed properly by the primary. The\nreplica is in view v, and it is handling transaction t.\n– It has not accepted a ba-pre-prepared message for\ntransaction t in view v.\n– The registration records in C are identical to, or form\na superset of, the local registration records.\n– Every vote record in C is properly signed by its send-\ning participant and the transaction identifier in the\nrecord matches that of the current transaction, and the\nproposed decision o is consistent with the registration\nand vote records.\nNote that a backup does not insist on receiving a decision\ncertificate identical to its local copy. This is because a cor-\nrect primary might have received a registration from a par-\nticipant which the backup has not, or the primary and back-\nups might have received different votes from some Byzan-\ntine faulty participants, or the primary might have received\na vote that a backup has not received if the sending partici-\npant crashed right after it has sent its vote to the primary.\nIf the registration records in C form a superset of the lo-\ncal registration records, the backup updates its registration\nrecords and asks the primary replica for the endpoint ref-\nerence2 of each missing participant (so that it can send its\nnotification to the participant).\nA backup suspects the primary and initiates a view\nchange immediately if the ba-pre-prepare message fails the\nverification. Otherwise, the backup accepts the ba-pre-\nprepare message. At this point, we say the replica has ba-\npre-prepared for transaction t. It then logs the accepted ba-\npre-prepare message and multicasts a ba-prepare message\nwith the same decision o as that in the ba-pre-prepare mes-\nsage (this starts the ba-prepare phase). The ba-prepare mes-\nsage takes the form <BA-PREPARE, v, t, d, o, i>σi , where d\nis the digest of the decision certificate C.\nA coordinator replica j accepts a ba-prepare message\nprovided:\n– The message is correctly signed by replica i, and\nreplica j is in view v and the current transaction is t;\n– The decision o matches that in the ba-pre-prepare mes-\nsage;\n– The digest d matches the digest of the decision certifi-\ncate in the accepted ba-pre-prepare message.\nIf a replica has collected 2f matching ba-prepare mes-\nsages from different replicas (including the replica’s own\nba-prepare message if it is a backup), the replica is said to\nhave ba-prepared to make a decision on transaction t. This\nis the end of the ba-prepare phase.\nA ba-prepared replica enters the ba-commit phase by\nmulticasting a ba-commit message to all other repli-\ncas. The ba-commit message has the form <BA-\nCOMMIT, v, t, d, o, i>σi . The replica i is said to have ba-\ncommitted, if it has obtained 2f + 1 matching ba-commit\n2The term endpoint reference refers to the physical contact information\nsuch as host and port of a process. In Web services, an endpoint reference\ntypically contains a URL to a service and an identifier used by the service\nto locate the specific handler object [9].\nmessages from different replicas (including the message it\nhas sent). When a replica is ba-committed for transaction t,\nit sends the decision o to all participants of transaction t.\nIf a replica i could not advance to the ba-committed state\nuntil a timeout, it initiates a view change by sending a view\nchange message to all other replicas. The view change mes-\nsage has the form<VIEW-CHANGE, v+1, t, P, i>σi , where\nP contains information regarding its current state. If the\nreplica has ba-pre-prepared t in view v, it includes a tuple\n<v, t, o, C>. If it has ba-prepared t in view v, it includes\nboth the tuple <v, t, o, C> and 2f matching ba-prepared\nmessages from different replicas for t obtained in view v.\nIf the replica has not ba-pre-prepared t, it includes its own\ndecision certificate C.\nA correct replica that has not timed out the current view\nmulticasts a view change message only if it is in view v and\nit has received valid view change messages for view v + 1\nfrom f + 1 different replicas. This is to prevent a faulty\nreplica from inducing unnecessary view changes. A view\nchange message is regarded as valid if it is for view v + 1\nand the ba-pre-prepare and ba-prepare information included\nin P , if any, is for transaction t in a view up to v.\nWhen the primary for view v + 1 receives 2f + 1\nvalid view change messages for v + 1 (including the one\nit has sent or would have sent), it installs the new view,\nand multicasts a new view message, in the form <NEW-\nVIEW, v + 1, V, t, o, C> for view v + 1, where V contains\n2f + 1 tuples for the view change messages received for\nview v + 1. Each tuple has the form <i, d>, where i is\nthe sending replica, and d is the digest of the view change\nmessage. The proposed decision o for t and the decision\ncertification C are determined according to the following\nrules:\n1. If the new primary has received a view change message\ncontaining a valid ba-prepare record for t, and there is\nno conflicting ba-prepare record, it uses that decision.\n2. Else, the new primary rebuilds a set of registration\nrecords from the received view change messages. This\nnew set may be identical to, or a superset of, the regis-\ntration set known to the new primary prior to the view\nchange. The new primary then rebuilds a set of vote\nrecords in a similar manner. It is possible that conflict-\ning vote records are found from the same participant\n(i.e., , a participant sent a “prepared” vote to one co-\nordinator replica, while sending an “aborted” vote to\nsome other replicas), in which case, a decision has to\nbe made on the direction of the transaction t. In this\nwork, we choose to take the “prepared” vote to maxi-\nmize the commit rate. A new decision certificate will\nbe constructed and a decision for t’s outcome is pro-\nposed accordingly. They will be included in the new\nview message for view v + 1.\nWhen a backup receives the new view message, it veri-\nfies the message basically by following the same steps used\nby the primary. If the replica accepts the new view message,\nit may need to retrieve the endpoint references for some par-\nticipants that it did not receive from other correct replicas.\nWhen a backup replica has accepted the new view message\nand obtained all missing information, it sends a ba-prepare\nmessage to all other replicas. The algorithm then proceeds\naccording to its normal operations.\n3.4. Informal Proof of Correctness\nWe now provide an informal proof of the safety of our\nByzantine agreement algorithm and the distributed commit\nprotocol. Due to space limitation, the proof for liveness is\nomitted.\nClaim 1: If a correct coordinator replica ba-commits\na transaction t with a commit decision, the registration\nrecords of all correct participants must have been included\nin the decision certificate, and all such participants must\nhave voted to commit the transaction.\nWe prove by contradiction. Assume that there exists a\ncorrect participant p whose registration is left out of the de-\ncision certificate. Since a correct coordinator replica has ba-\ncommitted t with a commit decision, it must have accepted\na ba-pre-prepare message and 2f matching ba-prepare mes-\nsage from different replicas. This means that a set R1 of\n2f + 1 replicas have all accepted the same decision cer-\ntificate without the participant p, the initiator has requested\nthe coordinator replicas to commit t, and every participant\nin the registration set has voted to commit the transaction.\nThis further implies that the initiator has received normal\nreplies from all participants, including p, to which it has\npropagated the current transaction. Because the participant\np is correct and responded to the initiator’s request prop-\nerly, it must have registered with at least 2f +1 coordinator\nreplicas prior to sending its reply to the initiator. Among the\n2f+1 coordinator replicas, at least a set R2 of f+1 replicas\nare correct, i.e., all replicas in R2 are correct and have the\nregistration record for p prior to the start of the distributed\ncommit for t. Because the total number of replicas is 3f+1,\nthe two sets R1 and R2 must intersect in at least one correct\nreplica. The correct replica in the intersection either did not\nreceive the registration from p, or it has accepted a decision\ncertificate without the registration record for p despite the\nfact that it has received the registration from p, which is im-\npossible. Therefore, all correct participants must have been\nincluded in the decision certificate if any correct replica ba-\ncommitted a transaction with a commit decision.\nWe next prove that if any correct replica ba-committed\na transaction with a commit decision, all correct partici-\npants must have voted to commit the transaction. Again,\nwe prove by contradiction. Assume that the above state-\nment is not true, and a correct participant q has voted to\nabort the transaction t. Since we have proved above that\nq’s registration record must have been included in the de-\ncision certificate, its vote cannot be ignored. Furthermore,\nsince a correct replica ba-committed t with a commit de-\ncision, the set R1 of 2f + 1 replicas have all accepted the\ncommit decision. Again, since R1 and R2 must intersect\nby at least one correct replica, that replica both accepted the\ncommit decision and has received the “aborted” vote from\nq. This is possible only if the ba-pre-prepare message that\nthe replica has accepted contains a “prepared” vote from q.\nThis contradict to the fact that q is a correct participant. A\ncorrect participant never sends conflicting votes to different\ncoordinator replicas. This concludes our proof for claim 1.\nClaim 2: Our Byzantine agreement algorithm ensures\nthat all correct coordinator replicas agree on the same de-\ncision regarding the outcome of a transaction.\nWe prove by contradiction. Assume that two correct\nreplicas i and j reach different decisions for t, without loss\nof generality, assume i decides to abort t in a view v and j\ndecides to commit t in a view u.\nFirst, we consider the case when v = u. According to\nour algorithm, i must have accepted a ba-pre-prepare mes-\nsage with an abort decision supported by a decision certifi-\ncate, and 2f matching ba-prepare messages from different\nreplicas, all in view v, this means a set R3 of at least 2f +1\nreplicas have ba-prepared t with an abort decision in view\nv. Similarly, replica j must have accepted a ba-pre-prepare\nmessage with a commit decision supported by a decision\ncertificate, and 2f matching ba-prepare messages from dif-\nferent replicas for transaction t in the same view v, which\nmeans a set R4 of at least 2f + 1 replicas have ba-prepared\nt with a commit decision in view v. Since there are only\n3f + 1 replicas, the two sets R3 and R4 must intersect in at\nleast f + 1 replicas, among which, at least one is a correct\nreplica. It means that this replica must have accepted two\nconflicting ba-pre-prepare messages (one to commit and the\nother to abort) in the same view. This contradicts the fact\nthat it is a correct replica.\nNext, we consider the case when view u > v. Since\nreplica i ba-committed with an abort decision for t in view\nv, it must have received 2f + 1 matching ba-commit mes-\nsages from different replicas (including the one sent by it-\nself). This means that a set R5 of 2f + 1 replicas have\nba-prepared t in view v, all with the same decision to abort\nt. To install a new view, the primary of the new view must\nhave received view change messages (including the one it\nhas sent or would have sent) from a set R6 of 2f + 1 repli-\ncas. Similar to the previous argument, the two sets R5 and\nR6 intersect in at least f +1 replicas, among which, at least\none must be a correct replica. This replica would have in-\ncluded the decision and the decision certificate backed by\nthe ba-pre-prepare message and the 2f matching ba-prepare\nmessages it has received from other replicas, in its view\nchange message. The primary in the new view, if it is cor-\nrect, must have used the decision and decision certificate\nfrom this replica. This should have led all correct replicas to\nba-commit transaction t with an abort decision, which con-\ntradicts to the assumption that a correct replica committed\nt. If the primary is faulty and did not obey the new view\nconstruction rule, we argue that no correct replica could\nhave accepted the new view message, let alone to have ba-\ncommitted t with a commit decision. Recall that a correct\nreplica should verify the new view message by following\nthe new view construction rules, just as a correct primary\nwould do. We have proved above that the 2f + 1 view\nchange messages must contain one sent by a correct replica\nwith ba-prepare information for an abort decision. A correct\nreplica cannot possibly have accepted the new view mes-\nsage sent by the faulty primary, which contains a conflict-\ning decision. This contradicts to the initial assumption that\na correct replica j committed transaction t in view u. The\nproof for the case when v > u is similar. Therefore, claim\n2 is correct.\nClaim 3: The BFTDC protocol guarantees atomic termi-\nnation of transactions at all correct participants.\nWe prove by contradiction. Assume that a transaction t\ncommits at a participant p but aborts at another participant\nq. According to the criteria indicated in the CommitTrans-\naction() method shown in Fig. 1, p commits the transaction\nt only if it has received the commit request from at least\nf+1 different coordinator replicas. Since at most f replicas\nare faulty, at least one request comes from a correct replica.\nDue to claim 1, if any correct replica ba-committed a trans-\naction with a commit decision, then the registration records\nof all correct participants must have been included in the\ndecision certificate, and all correct participants must have\nvoted to commit the transaction.\nOn the other hand, since q aborted t, one of the following\ntwo scenarios must be true: (1) q unilaterally aborted t, in\nwhich case, it must not have sent a “prepared” vote to any\ncoordinator replica. (2) q received a prepare request, pre-\npared t, sent a “prepared” vote to one or more coordinator\nreplicas. But it received an abort request from at least f +1\ndifferent coordinator replicas.\nIf the first scenario is true, q might or might not have\nfinished its registration process. If it did not, the initiator\nwould have been notified by an exception, or would have\ntimed out q. In any case, the initiator should have decided\nto abort t. This conflicts with the fact that p has committed\nt because it implies that the initiator has asked the coordi-\nnator replicas to commit t. If q completed the registration\nprocess, its registration record should have been aware by a\nset R7 of at least f+1 correct replicas. Since p has commit-\nted t, at least one correct replica has ba-committed t with a\ncommit decision, which in turn implies that a set R8 of at\nleast 2f + 1 coordinator replicas have accepted a ba-pre-\nprepare message with a decision certificate either has no q\nin its registration records, or without q’s “prepared” vote.\nSince there are 3f + 1 replicas, R7 and R8 must intersect\nin at least one replica. This correct replica could not possi-\nbly have accepted a ba-pre-prepare message with a decision\ncertificate described above.\nFor the second scenario, at least one correct replica has\ndecided to abort t. Since another participant p committed\nt, at least one correct replica has decided to commit t. This\ncontradicts to claim 2, which we have proved to be true.\nTherefore, claim 3 is correct.\n4. Implementation and Performance\nWe have implemented the BFTDC protocol (with the ex-\nception of the view change mechanisms) and integrated it\ninto a distributed commit framework for Web services in\nJava programming language. The extended framework is\nbased on a number of Apache Web services projects, includ-\ning Kandula (an implementation of WS-AT) [2], WSS4J\n(an implementation of the Web Services Security Specifi-\ncation) [3], and Apache Axis (SOAP Engine) [1]. Most of\nthe mechanisms are implemented in terms of Axis handlers\nthat can be plugged into the framework without affecting\nother components. Some of the Kandula code is modified\nto enable the control of its internal state, to enable a Byzan-\ntine agreement on the transaction outcome, and to enable\nvoting. Due to space constraint, the implementation details\nare omitted.\nFor performance evaluation, we focus on assessing the\nruntime overhead of our BFTDC protocol during normal\noperations. Our experiment is carried out on a testbed con-\nsisting of 20 Dell SC1420 servers connected by a 100Mbps\nEthernet. Each server is equipped with two Intel Xeon\n2.8GHz processors and 1GB memory running SuSE 10.2\nLinux.\nThe test application is a simple banking Web services\napplication where a bank manager (i.e., initiator) transfers\nfunds among the participants within the scope of a dis-\ntributed transaction for each request received from a client.\nThe coordinator-side services are replicated on 4 nodes to\ntolerate a single Byzantine faulty replica. The initiator and\nother participants are not replicated, and run on distinct\nnodes. The clients are distributed evenly (whenever pos-\nsible) among the remaining nodes. Each client invokes a\nfund transfer operation on the banking Web service within\na loop without any “think” time between two consecutive\ncalls. In each run, 1000 samples are obtained. The end-\nto-end latency for the fund transfer operation is measured\nat the client. The latency for the distributed commit and\nthe Byzantine agreement is measured at the coordinator\nreplicas. Finally, the throughput of the distributed commit\nframework is measured at the initiator for various number\nof participants and concurrent clients.\nTo evaluate the runtime overhead of our protocol, we\ncompare the performance of our BFTDC protocol with the\n2PC protocol as it is implemented in the WS-AT framework\nwith the exception that all messages exchanged over the net-\nwork are digitally signed.\nIn Figure 2(a), we included the distributed commit la-\ntency and the end-to-end latency for both our protocol (in-\ndicated by “with bft”) and the original 2PC protocol (indi-\ncated by “no bft”). The Byzantine agreement latency is also\nshown. Figure 2(b) shows the throughput measurement re-\n 0\n 500\n 1000\n 1500\n 2000\n 2500\n 2  3  4  5  6  7  8  9  10\nLa\nte\nnc\ny \n(m\nilli\nsec\non\nds\n)\nNumber of Participants in Each Transaction\nByzantine Agreement Latency\nDistributed Commit Latency (no bft)\nDistributed Commit Latency (with bft)\nEnd-to-End Latency (no bft)\nEnd-to-End Latency (with bft)\n(a)\n0.0\n1.0\n2.0\n3.0\n4.0\n5.0\n 1  2  3  4  5  6  7  8  9  10\nTh\nro\nug\nhp\nut\n (T\nran\nsac\ntio\nns\n/Se\nco\nnd\n)\nNumber of Concurrent Clients\n2 Participants (no bft)\n2 Participants\n4 Participants\n6 Participants\n8 Participants\n10 Participants\n(b)\nFigure 2. (a) Various latency measurements for transactions with different number of participants\nunder normal operations (with a single client). (b) Throughput of the distributed commit service\nin terms of transactions per second for transactions with different number of participants under\ndifferent load.\nsults for transactions using our protocol with up to 10 con-\ncurrently running clients and 2-10 participants in each trans-\naction. For comparison, the throughput for transactions us-\ning the 2PC protocol for 2 participants is also included.\nAs can be seen in Figure 2(a), the latency for the dis-\ntributed commit and the end-to-end latency both are in-\ncreased by about 200-400 ms when the number of partici-\npants varies from 2 to 10. This increase is mostly attributed\nto the introduction of the Byzantine agreement phase in our\nprotocol. Percentage-wise, the end-to-end latency, as per-\nceived by an end user, is increased by only 20% to 30%,\nwhich is quite moderate. We observe a similar range of\nthroughput reductions for transactions using our protocol,\nas shown in Figure 2(b).\n5. Conclusions\nIn this paper, we presented a Byzantine fault tolerant dis-\ntributed commit protocol. We carefully studied the types\nof Byzantine faults that might occur to a distributed trans-\nactional systems and identified the subset of faults that a\ndistributed commit protocol can handle. We adapted Cas-\ntro and Liskov’s BFT algorithm to ensure Byzantine agree-\nment on the outcome of transactions. We also proved infor-\nmally the correctness of our BFTDC protocol. A working\nprototype of the protocol is built on top of an open source\ndistributed commit framework for Web services. The mea-\nsurement results of our protocol show only moderate run-\ntime overhead. We are currently working on the implemen-\ntation of the view change mechanisms and exploring addi-\ntional mechanisms to protect a TP monitor against Byzan-\ntine faults, not only for distributed commit, but for activa-\ntion, registration, and transaction propagation as well.\nReferences\n[1] Apache Axis project. http://ws.apache.org/axis/.\n[2] Apache Kandula project. http://ws.apache.org/kandula/.\n[3] Apache WSS4J project. http://ws.apache.org/wss4j/.\n[4] L. Cabrera et al. WS-AtomicTransaction Specification, Au-\ngust 2005.\n[5] M. Castro and B. Liskov. Practical Byzantine fault toler-\nance and proactive recovery. ACM Transactions on Com-\nputer Systems, 20(4):398–461, November 2002.\n[6] H. Garcia-Molina, F. Pittelli, and S. Davidson. Applications\nof Byzantine agreement in database systems. ACM Transac-\ntions on Database Systems, 11(1):27–47, March 1986.\n[7] J. Gray. A comparison of the Byzantine agreement prob-\nlem and the transaction commit problem. Springer Verlag\nLecture Notes in Computer Science, 448:10–17, 1990.\n[8] J. Gray and A. Reuter. Transaction Processing: Concepts\nand Techniques. Morgan Kaufmann Publishers, San Mateo,\nCA, 1983.\n[9] M. Gudgin and M. Hadley. Web Services Addressing 1.0 -\nCore. W3C working draft, February 2005.\n[10] L. Lamport, R. Shostak, and M. Pease. The Byzantine gen-\nerals problem. ACM Transactions on Programming Lan-\nguages and Systems, 4(3):382–401, July 1982.\n[11] C. Mohan, R. Strong, and S. Finkelstein. Method for dis-\ntributed transaction commit and recovery using Byzantine\nagreement within clusters of processors. In Proceedings of\nthe ACM symposium on Principles of Distributed Comput-\ning, pages 89–103, Montreal, Quebec, Canada, 1983.\n[12] The Open Group. Distributed Transaction Processing: The\nXA Specification, February 1992.\n[13] K. Rothermel and S. Pappe. Open commit protocols toler-\nating commission failures. ACM Transactions on Database\nSystems, 18(2):289–332, June 1993.\n",
            "id": 947554,
            "identifiers": [
                {
                    "identifier": "10.1109/dasc.2007.10",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:citeseerx.psu:10.1.1.70.2650",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "10.1109/isdasc.2007.4351387",
                    "type": "DOI"
                },
                {
                    "identifier": "cs/0612083",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "24505611",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2423157",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:arxiv.org:cs/0612083",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "206682799",
                    "type": "CORE_ID"
                }
            ],
            "title": "A Byzantine Fault Tolerant Distributed Commit Protocol",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:citeseerx.psu:10.1.1.70.2650",
                "oai:arxiv.org:cs/0612083"
            ],
            "publishedDate": "2007-01-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/cs/0612083",
                "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.2650"
            ],
            "updatedDate": "2021-07-20T15:54:56",
            "yearPublished": 2007,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/cs/0612083"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/947554"
                }
            ]
        },
        {
            "acceptedDate": "2004-03-03T00:00:00",
            "arxivId": "cond-mat/0309488",
            "authors": [
                {
                    "name": "C. Castellano"
                },
                {
                    "name": "Camacho"
                },
                {
                    "name": "D. Parisi"
                },
                {
                    "name": "F. Cecconi"
                },
                {
                    "name": "F. Radicchi"
                },
                {
                    "name": "Garlaschelli"
                },
                {
                    "name": "Girvan"
                },
                {
                    "name": "Holme"
                },
                {
                    "name": "Jeong"
                },
                {
                    "name": "Montoya"
                },
                {
                    "name": "Pastor-Satorras"
                },
                {
                    "name": "Ravasz"
                },
                {
                    "name": "V. Loreto"
                },
                {
                    "name": "Wagner"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/54291555",
                "https://api.core.ac.uk/v3/outputs/208983423"
            ],
            "createdDate": "2012-04-13T14:21:09",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 150,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/150",
                    "logo": "https://api.core.ac.uk/data-providers/150/logo"
                },
                {
                    "id": 1084,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/1084",
                    "logo": "https://api.core.ac.uk/data-providers/1084/logo"
                }
            ],
            "depositedDate": "2004-02-23T00:00:00",
            "abstract": "The investigation of community structures in networks is an important issue\nin many domains and disciplines. This problem is relevant for social tasks\n(objective analysis of relationships on the web), biological inquiries\n(functional studies in metabolic, cellular or protein networks) or\ntechnological problems (optimization of large infrastructures). Several types\nof algorithm exist for revealing the community structure in networks, but a\ngeneral and quantitative definition of community is still lacking, leading to\nan intrinsic difficulty in the interpretation of the results of the algorithms\nwithout any additional non-topological information. In this paper we face this\nproblem by introducing two quantitative definitions of community and by showing\nhow they are implemented in practice in the existing algorithms. In this way\nthe algorithms for the identification of the community structure become fully\nself-contained. Furthermore, we propose a new local algorithm to detect\ncommunities which outperforms the existing algorithms with respect to the\ncomputational cost, keeping the same level of reliability. The new algorithm is\ntested on artificial and real-world graphs. In particular we show the\napplication of the new algorithm to a network of scientific collaborations,\nwhich, for its size, can not be attacked with the usual methods. This new class\nof local algorithms could open the way to applications to large-scale\ntechnological and biological applications.Comment: Revtex, final form, 14 pages, 6 figure",
            "documentType": "research",
            "doi": "10.1073/pnas.0400054101",
            "downloadUrl": "http://arxiv.org/abs/cond-mat/0309488",
            "fieldOfStudy": "computer science",
            "fullText": "ar\nX\niv\n:c\non\nd-\nm\nat\n/0\n30\n94\n88\nv2\n  [\nco\nnd\n-m\nat.\nsta\nt-m\nec\nh]\n  2\n7 F\neb\n 20\n04\nDefining and identifying communities in networks\nFilippo Radicchi,1 Claudio Castellano,2 Federico Cecconi,3 Vittorio Loreto,2, ∗ and Domenico Parisi3\n1Dipartimento di Fisica, Universita` di Roma “Tor Vergata”,\nVia della Ricerca Scientifica 1, 00133 Roma, Italy\n2Dipartimento di Fisica, Universita` di Roma “La Sapienza” and INFM-SMC,\nUnita` di Roma 1, P.le A. Moro 5, 00185 Roma, Italy\n3Institute of Cognitive Sciences and Technologies C.N.R., Viale Marx, 15, 00137, Roma, Italy\nThe investigation of community structures in networks is an important issue in many domains\nand disciplines. This problem is relevant for social tasks (objective analysis of relationships on the\nweb), biological inquiries (functional studies in metabolic and protein networks) or technological\nproblems (optimization of large infrastructures). Several types of algorithms exist for revealing\nthe community structure in networks, but a general and quantitative definition of community is\nnot implemented in the algorithms, leading to an intrinsic difficulty in the interpretation of the\nresults without any additional non-topological information. In this paper we deal with this problem\nby showing how quantitative definitions of community are implemented in practice in the existing\nalgorithms. In this way the algorithms for the identification of the community structure become\nfully self-contained. Furthermore, we propose a new local algorithm to detect communities which\noutperforms the existing algorithms with respect to computational cost, keeping the same level of\nreliability. The new algorithm is tested on artificial and real-world graphs. In particular we show\nhow the new algorithm applies to a network of scientific collaborations, which, for its size, can\nnot be attacked with the usual methods. This new type of local algorithm could open the way to\napplications to large-scale technological and biological applications.\nI. INTRODUCTION\nEvidence has rapidly grown in the past few years that very diverse systems in many different fields can be described\nas complex networks, i. e. assemblies of nodes and edges with nontrivial topological properties [1, 2]. Examples\nrange from technological systems (the Internet and the web [3, 4]) to biological (epidemiology [5, 6], metabolic\nnetworks [7, 8, 9], ecological webs [10, 11, 12, 13]) and social systems [14, 15] (scientific collaborations, structure of\nlarge organizations).\nIn this paper we deal with a topological property of networks, the community structure, that has attracted a great\ndeal of interest very recently. The concept of community is very common and it is linked to the classification of\nobjects in categories for the sake of memorization or retrieval of information. From this point of view the notion of\ncommunity is very general and, depending on the context, can be synonymous of module, class, group, cluster, etc.\nAmong the many contexts where this notion is relevant it is worth mentioning the problem of modularity in metabolic\nor cellular networks [9, 16] or the problem of the identification of communities in the web [17]. This last issue is\nrelevant for the implementation of search engines of new generation, content filtering, automatic classification or the\nautomatic realization of ontologies.\nGiven the relevance of the problem it is crucial to construct efficient procedures and algorithms for the identification\nof the community structure in a generic network. This, however, is a highly nontrivial task.\nQualitatively, a community is defined as a subset of nodes within the graph such that connections between the nodes\nare denser than connections with the rest of the network. The detection of the community structure in a network\nis generally intended as a procedure for mapping the network into a tree (Fig. 1). In this tree (called dendrogram\nin social sciences) the leaves are the nodes while the branches join nodes or (at higher level) groups of nodes, thus\nidentifying a hierarchical structure of communities nested within each other.\nSeveral algorithms to perform this mapping are known in the literature. The traditional method is the so-called\nhierarchical clustering [18]. For every pair i, j of nodes in the network one calculates a weightWi,j , which measures how\nclosely connected the vertices are. Starting from the set of all nodes and no edges, links are iteratively added between\npairs of nodes in order of decreasing weight. In this way nodes are grouped into larger and larger communities and\nthe tree is built up to the root, which represents the whole network. Algorithms of this kind are called agglomerative.\n∗Electronic address: loreto@roma1.infn.it\n2For the other class of algorithms, called divisive, the order of construction of the tree is reversed: one starts with\nthe whole graph and iteratively cuts the edges, thus dividing the network progressively into smaller and smaller\ndisconnected sub-networks identified as the communities. The crucial point in a divisive algorithm is the selection of\nthe edges to be cut, which have to be those connecting communities and not those within them. Very recently, Girvan\nand Newman (GN) have introduced a divisive algorithm where the selection of the edges to be cut is based on the\nvalue of their “edge betweenness” [19], a generalization of the centrality betweenness introduced by Anthonisse [20]\nand Freeman [21]. Consider the shortest paths between all pairs of nodes in a network. The betweenness of an edge\nis the number of these paths running through it. It is clear that when a graph is made of tightly bound clusters,\nloosely interconnected, all shortest paths between nodes in different clusters have to go through the few inter-clusters\nconnections, which therefore have a large betweenness value. The single step of the GN detection algorithm consists in\nthe computation of the edge betweenness for all edges in the graph and in the removal of those with the highest score.\nThe iteration of this procedure leads to the splitting of the network into disconnected subgraphs that in their turn\nundergo the same procedure, until the whole graph is divided in a set of isolated nodes. In this way the dendrogram\nis built, from the root to the leaves.\nThe GN algorithm represents a major step forward for the detection of communities in networks, since it avoids\nmany of the shortcomings of traditional methods [19, 22]. This explains why it has been quickly adopted in the past\nyear as a sort of standard for the analysis of community structure in networks [23, 24, 25, 26, 27].\nThis paper follows a different track by proposing an alternative strategy for the identification of the community\nstructure. This complementary approach follows from the need of addressing the two following issues.\n1. In general algorithms define communities operationally as what the they finds. A dendrogram, i. e. a community\nstructure, is always produced by the algorithms down to the level of single nodes, independently from the type\nof graph analyzed. This is due to the lack of explicit prescriptions to discriminate between networks that\nare actually endowed with a community structure and those that are not. As a consequence, in practical\napplications one needs additional, non topological, information on the nature of the network to understand\nwhich of the branches of the tree have a real significance. Without such information it is not clear at all whether\nthe identification of a community is reliable or not. There have been two noticeable proposals to solve this\nproblem. In particular it is worth mentioning the approach proposed by Wilkinson and Huberman [24], which\nis limited to the lowest level of the community structure and specific to algorithms based on betweenness. More\nrecently [22], Newman and Girvan have introduced an a posteriori measure of the strength of the community\nstructure, which they called modularity. More precisely, the modularity estimates the fraction of inward links in\na community minus the expectation value of the same quantity in a network with the same community divisions\nbut random connections between the nodes. This quantity definitely gives an indication of the strength of the\ncommunity structure, even though the lack of the implementation of a quantitative definition of community\ndoes not allow to discriminate in an objective way meaningful communities.\n2. The “edge betweenness algorithm” is computationally costly, as already remarked by Girvan and Newman [19,\n22]. Evaluating the score for all edges requires a time of the order of MN , where M is the number of edges and\nN the number of nodes. The iteration of the procedure for all M edges leads in the worst case to a total scaling\nof the computational time asM2N , which makes the analysis practically unfeasible already for moderately large\nnetworks (of the order of N = 10000 [22]).\nIn this paper we propose solutions to both these problems. First we introduce a general criterion for deciding\nwhich of the subgraphs singled out by the detection algorithms are actual communities. We discuss in detail the case\nof two quantitative definitions of community. In this way we transform the GN algorithm in a self-contained tool.\nSecondly we present an alternative algorithm, based on the computation of local quantities, which gives, in controlled\ncases, results of accuracy comparable to the GN method, while largely outperforming it from the point of view of the\ncomputational speed.1\nThe outline of the paper is as follows. In section II we introduce the definitions of community. In section III we\nshow how these definitions can be implemented in a generic divisive algorithm in order to make it self-contained and\nwe present tests on some computer-generated and real networks. In section IV we present a new and fast algorithm\nfor the detection of communities and we compare its performance with the GN algorithm. Section V is devoted to the\napplication of the new algorithm to a network of scientific collaborations which, because of its size, is hard to analyze\nwith the Girvan-Newman method. We finally draw some conclusions and discuss the perspectives of this work.\n1 It is worth mentioning that after the completion of this work Newman has proposed a new agglomerative algorithm to address the issue\nof the computational efficiency [28].\n3II. QUANTITATIVE DEFINITIONS OF COMMUNITY\nThe idea to solve the first of the problems discussed above is very simple: the algorithm that builds the tree just\nselects subgraphs that are candidate to be considered communities. One has then to check whether they are actually\nsuch by using a precise definition. If the subgraph does not meet the criterion, the subgraph isolated from the network\nis not a community and the corresponding branch in the dendrogram should not be drawn.\nAs mentioned above, a community is generally thought as a part of a network where internal connections are denser\nthan external ones. To sharpen the use of detection algorithms a more precise definition is needed. Many possible\ndefinitions of communities exist in the literature [18]. Here we consider explicitly the implementation in the algorithms\nof two plausible definitions of community which translate into formulas the sentence above.\nThe basic quantity to consider is ki, the degree of a generic node i, which in terms of the adjacency matrix Ai,j of\nthe network G is ki =\n∑\nj Ai,j\n2. If we consider a subgraph V ⊂ G, to which node i belongs, we can split the total\ndegree in two contributions: ki(V ) = k\nin\ni (V ) + k\nout\ni (V ). k\nin\ni (V ) =\n∑\nj∈V Ai,j is the number of edges connecting node\ni to other nodes belonging to V . kouti (V ) =\n∑\nj /∈V Ai,j is clearly the number of connections toward nodes in the rest\nof the network.\nDefinition of community in a strong sense\nThe subgraph V is a community in a strong sense if\nkini (V ) > k\nout\ni (V ), ∀i ∈ V. (1)\nIn a strong community each node has more connections within the community than with the rest of the graph.\nThis definition coincides with the one proposed in [17] in the framework of the identification of web communities.\nDefinition of community in a weak sense\nThe subgraph V is a community in a weak sense if\n∑\ni∈V\nkini (V ) >\n∑\ni∈V\nkouti (V ). (2)\nIn a weak community the sum of all degrees within V is larger than the sum of all degrees toward the rest of the\nnetwork.\nClearly a community in a strong sense is also a community in a weak sense, while the converse is not true.\nIt is worth mentioning that our definitions of community, though very natural, do not represent the only possible\nchoice. Several other possible definitions, possibly more appropriate in some cases, exist and are described in [18].\nAmong them for instance the definition of the so-called LS − set goes in the direction of our strong definition even\nthough extremely more stringent. An LS − set is a set of nodes such that each of its proper subsets has more ties\nto its complement within the set than outside. On the other hand the definition of k-core is roughly, although not\nexactly, equivalent to our weak definition. A k-core is defined as a subgraph in which each node is adjacent to at least\na minimum number, k, of the other nodes in the subgraph.\nIII. SELF-CONTAINED ALGORITHMS\nFrom the definitions given above, it is apparent that, if a network is randomly split in two parts, one very large\nand the other with only few nodes, the very large part almost always fulfills the definition of community. In order\nto deal with this problem, let us consider the Erdo¨s-Renyi random graph [29]. If we cut at random the graph in two\nparts containing αN and (1 − α)N nodes, respectively, it is easy to evaluate analytically the probability P (α) that\nthe subgraph containing αN nodes fulfills the weak or the strong definition (details will be given in a forthcoming\npublication). It turns out that, as soon as N is sufficiently large, the probability is very close to a step function\naround α = 0.5. Hence it is extremely likely that, in a random graph randomly cut in two parts, the largest one\n2 The adjacency matrix fully specifies the topology of the network. In the simplest case of an unweighted, undirected network, it is equal\nto 1 if i and j are directly connected, zero otherwise.\n4is a community according to the previous definitions. However it is extremely unlikely that both subgraphs fulfill\nsimultaneously the definitions: therefore if we accept divisions only if both groups fulfill the definition of community,\nwe correctly find that a random graph has no community structure. We extend this criterion to the general case: if\nless than two subgraphs obtained from the cut satisfy the definitions, then the splitting is considered to be an artifact\nand disregarded.\nWe can now summarize the improved self-contained version of the GN algorithm.\n1. Choose a definition of community;\n2. Compute the edge betweenness for all edges and remove those with the highest score;\n3. If the removal does not split the (sub-)graph go to point 2;\n4. If the removal splits the (sub-)graph, test if at least two of the resulting subgraphs fulfill the definition. If they\ndo, draw the corresponding part of the dendrogram;\n5. Iterate the procedure (going back to point 2) for all the sub-graphs until no edges are left in the network.\nIt is important to remark that the quantities appearing in Eqs. (1) and (2) must always be evaluated with respect\nto the full adjacency matrix. The application of this procedure to a network produces a tree, where every branch\nsplitting represents a meaningful (with respect to the definition) separation in communities.\nIt is now possible to blindly test the effectiveness of the GN algorithm. We have considered the “artificial” graph\nalready discussed by Girvan and Newman. It is a simple network with N nodes divided into four groups: connections\nbetween pairs within a group are present with probability pin, while pairs of nodes in different groups are connected\nwith probability pout. As the probability pout grows from zero, the community structure in the network becomes less\nwell defined.\nFor every realization of the artificial graph the application of the detection algorithm generates a tree. We consider\nthe algorithm to be successful if the four communities are detected, each node is classified in the right community\nand the communities are not further subdivided.\nFig. 2 presents the comparison of the fraction of successes for the modified GN algorithm with the expected value\ncomputed analytically. We see that the GN algorithm captures very well the existence of communities in a strong\nsense, while it performs less well for the weak definition. However, one should not be misguided by the quantity\npresented in Fig. 2. By looking at a softer measure of success, the fraction f of nodes not correctly classified, one\nrealizes that, when the algorithm with weak definition seems to fail, it correctly identifies the four communities and\nit misclassifies only a few nodes up to much higher values of pout. The deviations from the theoretical behaviour\nobserved for small values of pout are due to the possibility that one or more of the four communities are further split\nin smaller sub-communities. This event, not taken into account in the analytical calculation, becomes very unlikely\nas the size of the system increases.\nIV. A NEW FAST ALGORITHM\nThe Girvan-Newman algorithm is computationally expensive because it requires the repeated evaluation, for each\nedge in the system, of a global quantity, the betweenness, whose value depends on the properties of the whole system.\nDespite smart methods to compute the edge betweenness simultaneously for all edges [30, 31], the evaluation of such\nquantity is the time consuming part of the procedure. As a consequence the time to analyze completely a network\nturns out to grow fast with its size, making the analysis unfeasible for networks of size larger than around 10000\nnodes [22].\nIn order to overcome this problem we introduce a new kind of divisive algorithm which requires the consideration\nof local quantities only, and is therefore much faster than the GN algorithm. The fundamental ingredient of a divisive\nalgorithm is a quantity which can single out edges connecting nodes belonging to different communities. We consider\nthe edge clustering coefficient, defined, in analogy with the usual node clustering coefficient, as the number of triangles\nto which a given edge belongs, divided by the number of triangles that might potentially include it, given the degrees\nof the adjacent nodes. More formally, for the edge connecting node i to node j, the edge clustering coefficient is\nC\n(3)\ni,j =\nz\n(3)\ni,j\nmin[(ki − 1), (kj − 1)]\n, (3)\nwhere z\n(3)\ni,j is the number of triangles built on that edge and min[(ki − 1), (kj − 1)] is the maximal possible number of\nthem.\n5The idea behind the use of this quantity in a divisive algorithm is that edges connecting nodes in different commu-\nnities are included in few or no triangles, and tend to have small values of C\n(3)\ni,j . On the other hand many triangles\nexist within clusters. Hence the coefficient C\n(3)\ni,j is a measure of how inter-communitarian a link is. A problem arises\nwhen the number of triangles is zero, because C\n(3)\ni,j = 0, irrespective of ki and kj , (or even C\n(3)\ni,j is indeterminate,\nwhen min[(ki− 1), (kj − 1)] = 0). To remove this degeneracy we consider a slightly modified quantity by using, at the\nnumerator, the number of triangles plus one:\nC˜\n(3)\ni,j =\nz\n(3)\ni,j + 1\nmin[(ki − 1), (kj − 1)]\n. (4)\nBy considering higher order cycles we can define, in much the same way, coefficients of order g as:\nC˜\n(g)\ni,j =\nz\n(g)\ni,j + 1\ns\n(g)\ni,j\n, (5)\nwhere z\n(g)\ni,j is the number of cyclic structures of order g the edge (i, j) belongs to, while s\n(g)\ni,j is the number of possible\ncyclic structures of order g that can be built given the degrees of the nodes.\nWe can now define, for every g, a detection algorithm that works exactly as the GN method with the difference\nthat, at every step, the removed edges are those with the smallest value of C˜\n(g)\ni,j . By considering increasing values\nof g, one can smoothly interpolate between a local and a non local algorithm. Notice that the definition of C˜\n(g)\ni,j\nguarantees that nodes with only one connection are not considered as isolated communities by the algorithm, since\nfor their unique edge C˜\n(g)\ni,j is infinite.\nWe have checked the accuracy of this new algorithm by comparing its performance with the GN method. Fig. 2\nreports the results for the artificial test graph with four communities. It turns out that, with respect to the strong\ndefinition, the new algorithm is as accurate as the GN one, both in the case of cycles of order g = 3 (triangles) and\ng = 4 (squares).\nOn the other hand, for the weak definition the best accuracy is achieved with the new algorithm with g = 4.\nAnother test is performed by considering the examples of social networks already studied by GN. Fig. 3 shows the\ntrees resulting from the application of the GN algorithm and of the g = 4 algorithm to the network of college football\nteams. Again the results are very similar, indicating that the local algorithm captures well the presence of communities\nin that network.\nAdditional insight into the relationship between the GN algorithm and this new algorithm based on edge clustering\nis provided by Fig. 4, where the edge betweenness is plotted versus C˜\n(4)\ni,j for each edge of the graph of scientific\ncollaborations studied in Ref. [15]. It is clear that an anticorrelation exists between the two quantities: edges with low\nvalues of C˜\n(4)\ni,j tend to have high values of betweenness. The anticorrelation is not perfect: the edge with minimum\nC˜\n(4)\ni,j is not the one with maximal betweenness. Therefore we expect the two algorithms to yield similar community\nstructures though not perfectly coinciding.\nIn this framework it is important to recall the parallel drawn by Ronald S. Burt [32] between betweenness centrality\nand the so-called redundancy. The definition of redundancy is very close to that of node clustering and, much in the\nspirit of our work, Burt first pointed out that nodes that belong to few loops are central in the betweenness sense.\nLet us now turn our attention to the question of the computational efficiency of our local algorithm. One can roughly\nestimate the scaling of the computational time as follows. When an edge is removed one has to check whether the\nwhole system has been separated in disconnected components and update the value of C˜\n(g)\ni,j in a small neighborhood\nof the removed edge. The first operation requires a time of the order of M , the total number of edges present in\nthe network, while the time required by the second operation does not scale with M . Since this operation has to be\nrepeated for all edges we can estimate the scaling of the total time as aM + bM2. We thus expect computational time\nto be linearly dependent on M for small systems and to cross over to an M2 regime for large sizes.\nWe have measured the velocity of the new algorithm by computing the time needed to generate the whole tree for\na random graph of increasing size N and fixed average degree (M ∼ N). Results, reported in Fig. 5, confirm that\nthe algorithm based on the computation of the edge clustering coefficient is much faster than the one based on edge\nbetweenness, both for g = 3 and g = 4. The crossover between the initial linear dependence on N to the N2 growth\nfor the local algorithm is evident for g = 3.\n6V. COMMUNITY STRUCTURE IN A NETWORK OF SCIENTIFIC COLLABORATIONS\nIn this section we consider an application of the fast algorithm discussed in the previous section to a\nnetwork of scientific collaborations. In particular we have considered the network of scientists who signed\nat least one paper submitted to the E-print Archive relative to Condensed Matter in the period 1995-1999\n(http://xxx.lanl.gov/archive/cond-mat). Data have been kindly provided by Mark Newman.\nThe network includes 15616 nodes (scientists) but it has a giant connected component which includes only N =\n12722 scientists. We have focused our attention on this giant component and we have applied to it our algorithm for\ng = 3 and g = 4. The time needed for the generation of the dendrogram for g = 3 is about 3 minutes on a desktop\ncomputer with 800 MHz CPU. The algorithm detects at the same time the communities satisfying the weak and the\nstrong criteria. At the end of the procedure one obtains, for every value of g, a list of communities identified in a\nweak sense and another list of communities identified in a strong sense. By definition the second list is a subset of\nthe first one. Figure 6 reports the size distributions (for g = 3 and g = 4) of all the communities identified in a weak\nsense. These distributions feature a power-law behaviour P (S) ∼ S−τ with τ ≃ 2, an indication of the self-similar\ncommunity structure of this network [33]. It is worth mentioning that the exponent we observe coincides with the one\nobtained, using the GN algorithm, in the framework of a recently proposed model of social network formation [34].\nFor what concerns a more detailed analysis of the communities found, validation of the results is far from trivial,\nsince there is no quantitative criterion to assess their accuracy. One may directly inspect the dendrogram in order to\nanswer questions like: are the communities representative of real collaborations between the corresponding scientists?\nDo they identify specific research areas? Would a generic scientist agree about his or her belonging to a given\ncommunity? Obviously all these questions cannot be answered in a definitive and quantitative way. We have partially\nfollowed this path and we have checked several subsets of the network at different levels in the hierarchy. To the best\nof our knowledge the results seem to us reasonable. Of course this does not represent a proof of the efficiency of the\nalgorithm. We refer the reader to the detailed results of our analysis that we make available as additional supporting\ninformation 3.\nVI. CONCLUSIONS\nThe detection of the community structure in large complex networks is a promising field of research with many\nopen challenges. The concept of community is qualitatively intuitive. However, to analyze a network it is necessary to\nspecify quantitatively and unambiguously what a community is. Once a definition is given it is in principle possible to\ndetermine all subgraphs of a given network that fulfill the definition. However this task is in practice computationally\nout of reach even for small systems. Therefore the search for the community structure has generally a more limited\ngoal: selecting, among all possible communities, a subset of them organized hierarchically, a dendrogram. Divisive\nand agglomerative algorithms carry out this task. A comparison of the performances of such algorithms is non trivial.\nIn some simple cases, as the artificial graph with four subsets considered above, it is possible to assess quantitatively\nthe validity of the results. In other cases, like the network of scientific collaborations, no quantitative measure exists\nto decide, given a precise definition of community, how good a dendrogram is. Typically, one may check whether the\nresults appear sensible. However this is far from objective, being mediated by the observer’s own perception and by\nhis/her intuitive concept of community.\nIn this work we have proposed two improvements in the construction of the dendrogram. First, we have devised\na way to implement, in a generic divisive algorithm, a quantitative definition of community. In this way algorithms\nbecome fully self-contained, i.e. they do not need non-topological input to generate the dendrogram. Then we\nhave introduced a new divisive algorithm, which is based on local quantities, and therefore extremely fast. Both\nthese improvements have been tested successfully in controlled cases. The analysis of the large network of scientific\ncollaborations gives results that appear reasonable. However, it is clear that, as discussed above, this statement\nis subjective and cannot be made at present more precise. Definitely, a quantitative measure for the evaluation of\ndendrograms would be a major step forward in this field.\nAt this point a remark is in order. So far we have only discussed examples of the so-called social networks. It\nhas been shown [35] that social networks substantially differ from other types of networks, namely technological or\nbiological networks. Among other differences, they exhibit a positive correlation between the degree of adjacent\nvertices (assortativity) while most non-social networks are disassortative. While these results are consistent with our\n3 The list of all the communities found with our algorithm for the scientific collaboration network is available on request\n(castella@pil.phys.uniroma1.it)\n7and other’s findings about community structures in social networks, they put into question the very existence of a\ncommunity structure in non-social networks and the possibility of detecting it with the existing algorithms. From\nthe perspective of our local algorithm, which relies on the existence of closed loops, disassortative networks could\nbe in principle problematic, due to the small number of short cycles. However, interesting insight comes from the\nstudy of the loops of arbitrary order [36]. In particular for four different types of networks (two of them social\nand assortative and two non-social and disassortative) measured values for the so-called average grid coefficient (the\nextension of the concept of clustering coefficient to cycles of order four) are two to four order of magnitude larger\nthan the corresponding coefficients of a random graph with the same average degree and size N . This argues in favor\nof the presence of some sort of hierarchical structure and well-defined communities also in disassortative networks. It\nalso hints that our algorithm could be fruitfully applied also to non-social (disassortative) networks, although future\nwork will be needed in this direction.\nWe believe that the new elements presented in this paper can be of great help in the analysis of networks. On the\none hand, the implementation of a quantitative definition of community makes algorithms self-contained and allows\nthe analysis of the community structure based only on the network topology. On the other hand, the introduction of\na new class of local and fast algorithms could open the way for applications to large-scale systems.\nAcknowledgments: The authors are very grateful to Mark Newman for providing data on the networks of college\nfootball teams and of scientific collaborations. They also wish to thank Alain Barrat for useful suggestions and\ndiscussions.\n[1] Albert, R. & Baraba´si, A.-L. (2002), Rev. Mod. Phys. 74, 47-97.\n[2] Newman, M. E. J. (2003), SIAM Review 45, 167-256.\n[3] Albert, R.,Jeong, H. & and Baraba´si, A.-L. (1999), Nature 401, 130-131.\n[4] Broder, A., Kumar, R., Maghoul, F., Raghavan, P., Rajagopalan, S., Stata, R., Tomkins, A. & Wiener, J. (2000), Computer\nNetworks 33, 309-320.\n[5] Moore, C. & Newman, M. E. J. (2000), Phys. Rev. E 61, 5678-5682.\n[6] Pastor-Satorras, R. & Vespignani, A. (2001), Phys. Rev. Lett. 86, 3200-3203.\n[7] Jeong, H., Tombor, B., Albert, R., Oltvai, Z.N. & Baraba´si, A.-L. (2000), Nature 407, 651-654.\n[8] Wagner, A. & Fell, D. A. (2001), Proc. R. Soc. London B 268, 1803-1810.\n[9] Ravasz, E., Somera, A., Mongru, D. A., Oltvai, Z. N. & Baraba´si A.-L., (2002), Science 297, 1551-1555.\n[10] Montoya, J. M., & Sole´, R. V. (2002), J. Theor. Biol. 214, 405-412.\n[11] Dunne, J. A., Williams, R. J. & Martinez, N. D. (2002), Proc. Natl. Acad. Sci. USA 99, 12917-12922.\n[12] Camacho, J., Guimera`, R. & Amaral, L. A. N. (2002), Phys. Rev. Lett. 88, 228102.\n[13] Garlaschelli, D., Caldarelli, G. & Pietronero, L. (2003), Nature 423, 165-168.\n[14] Redner, S. (1998), Eur. Phys. J. B 4, 131-134.\n[15] Newman, M. E. J. (2001), Proc. Natl. Acad. Sci. USA 98, 404-409.\n[16] Rives, A. & Galitski, T. (2003), Proc. Natl. Acad. Sci. USA 100, 1128-1133.\n[17] Flake, G. W., Lawrence, S. R., Giles, C. L. & Coetzee, F. M. (2002), IEEE Computer 35(3), 66-71.\n[18] Wasserman, S. & Faust, K. (1994), Social Network Analysis (Cambridge Univ. Press, Cambridge, U.K.).\n[19] Girvan, M. & Newman, M. E .J. (2002), Proc. Natl. Acad. Sci. USA 99, 7821-7826.\n[20] Anthonisse, J. M. (1971), Technical Report BN 9/71, Stichting Mathematisch Centrum, Amsterdam.\n[21] Freeman, L. (1977), Sociometry 40, 35-41.\n[22] Newman, M. E .J. & Girvan, M. (2003), cond-mat/0308217.\n[23] Holme, P., Huss, M. & Jeong, H. (2003), Bioinformatics 19, 532-538.\n[24] Wilkinson, D. & Huberman, B. A. (2002), cond-mat/0210147.\n[25] Gleiser, P. & Danon, L. (2003), cond-mat/0307434.\n[26] Guimera`, R., Danon, L., Di´az-Guilera, A., Giralt, F. & Arenas, A. (2002), cond-mat/0211498.\n[27] Tyler, J. R., Wilkinson, D. M. & Huberman, B. A. (2003), cond-mat/0303264.\n[28] Newman, M. E .J. (2003), cond-mat/0309508.\n[29] Bollobas, B. (1985), Random Graphs (Academic Press, New York).\n[30] Newman, M. E. J. (2001), Phys. Rev. 64, 016131.\n[31] Brandes U. (2001), Journal of Mathematical Sociology 25, 163-177.\n[32] Burt R.S. (1992), Structural Holes (Harward University Press, Cambridge, MA).\n[33] Caldarelli G., Caretta Cartozo C., De Los Rios P. & Servedio V.D.P., cond-mat/0311486.\n[34] Bogun˜a M., Pastor-Satorras R., Di´az-Guilera A. & Arenas A. (2003), cond-mat/0309263.\n[35] Newman, M.E.J. & Park, J. (2003) Phys. Rev. E 68, 036122.\n[36] Caldarelli, G., Pastor-Satorras, R. & Vespignani A. (2002), cond-mat/0212026.\n8Figure legends\nFig.1 A simple network (left) and the corresponding dendrogram (right).\nFig.2 Test of the efficiency of the different algorithms in the analysis of the artificial graph with four communities.\nThe construction of the graph is described in text. Here N = 128 and pin is changed with pout in order to\nkeep the average degree equal to 16. (Left) Strong definition: fraction of successes for the different algorithms\ncompared with the analytical probability that four communities are actually defined. (Right) Weak definition:\nin addition to the same quantities plotted in the left graph, here we report, for every algorithm, the fraction f\nof nodes not correctly classified.\nFig.3 Plot of the dendrograms for the network of college football teams, obtained using the Girvan-Newman algorithm\n(left) and our new algorithm with g = 4 (right). Different symbols denote teams belonging to different confer-\nences. In both cases the observed communities perfectly correspond to the conferences, with the exception of\nthe 6 members of the Independent conference, which are misclassified.\nFig.4 Edge betweenness vs. the modified edge clustering coefficient C˜\n(4)\ni,j , for the network of scientific collaborations\nconsidered in section V. Each dot represents an edge in the network. See section V for details.\nFig.5 Plot of the average time (in seconds) needed to analyze a random graph of N nodes and fixed average degree\n〈k〉 = 5. The time refers to the construction of the full tree down to single nodes (Fig. 1). No criterion to\nvalidate the communities was imposed. The runs are performed on a desktop computer with a 800 MHz CPU.\nFig.6 Normalized size distribution of all the communities of scientists identified in a weak sense by the algorithm\ndescribed in section IV for g = 3 (circles) and g = 4 (squares). In both cases the behaviour is well reproduced\nby a power law with exponent −2.\n9FIG. 1:\n1\n0\n0 0.02 0.04 0.06 0.08 0.1p\nout\n0\n0.2\n0.4\n0.6\n0.8\n1\nR\n0\nAnalytical\ng = 3\ng = 4\nGN\nStrong definition\n0 0.02 0.04 0.06 0.08 0.1p\nout\n0\n0.2\n0.4\n0.6\n0.8\n1\nR\n0\n,\n \nf\nR0 Analytical\nR0   g = 3\nR0   g = 4\nR0   GN\nf   g = 3\nf   g = 4\nf   GN\nWeak definition\nF\nIG\n.\n2\n:\n11\nFIG. 3:\n1\n2\n0.01 0.1 1\nModified Edge clustering coefficient (g=4) \n100\n102\n104\n106\nE\nd\ng\ne\n \nb\ne\nt\nw\ne\nn\nn\ne\ns\ns\nF\nIG\n.\n4\n:\n1\n3\n10 100 1000 10000\nN\n0.01\n1\n100\nT\ni\nm\ne\n \n(\ns\ne\nc\n)\nGN\nLocal, g=3\nLocal, g=4\nF\nIG\n.\n5\n:\n1\n4\n100 101 102 103 104\nS\n10-6\n10-4\n10-2\n100\nP\n(\nS\n)\nDistribution of (weak) community sizes (g=3)\nDistribution of (weak) community sizes (g=4)\nFit with a power law with exponent -2\nF\nIG\n.\n6\n:\n",
            "id": 898140,
            "identifiers": [
                {
                    "identifier": "10.1073/pnas.0400054101",
                    "type": "DOI"
                },
                {
                    "identifier": "208983423",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2364828",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "3432462",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "54291555",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "cond-mat/0309488",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "365677",
                    "type": "PUBMED_ID"
                },
                {
                    "identifier": "oai:arxiv.org:cond-mat/0309488",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "2038920443",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "oai:pubmedcentral.nih.gov:365677",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "oai:iris.uniroma1.it:11573/106703",
                    "type": "OAI_ID"
                }
            ],
            "title": "Defining and identifying communities in networks",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:arxiv.org:cond-mat/0309488",
                "oai:pubmedcentral.nih.gov:365677",
                "oai:iris.uniroma1.it:11573/106703"
            ],
            "publishedDate": "2004-01-01T00:00:00",
            "publisher": "'Proceedings of the National Academy of Sciences'",
            "pubmedId": "365677",
            "references": [],
            "sourceFulltextUrls": [
                "http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=365677",
                "http://arxiv.org/abs/cond-mat/0309488"
            ],
            "updatedDate": "2021-05-18T22:41:31",
            "yearPublished": 2004,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "0027-8424"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/cond-mat/0309488"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/898140"
                }
            ]
        },
        {
            "acceptedDate": "2010-12-01T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Brackley, Chris A."
                },
                {
                    "name": "de Moura, Alessandro"
                },
                {
                    "name": "Ebenhoeh, Oliver"
                },
                {
                    "name": "Grebogi, Celso"
                },
                {
                    "name": "Kurths, Juergen"
                },
                {
                    "name": "Romano, M. Carmen"
                },
                {
                    "name": "Thiel, Marco"
                }
            ],
            "contributors": [
                "University of Aberdeen.Physics",
                "University of Aberdeen.Institute for Complex Systems and Mathematical Biology (ICSMB)",
                "University of Aberdeen.Medical Sciences",
                "University of Aberdeen.Natural & Computing Sciences",
                "University of Aberdeen.Environment and Food Security",
                "University of Aberdeen.Institute of Medical Sciences"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/9597638",
                "https://api.core.ac.uk/v3/outputs/192504841"
            ],
            "createdDate": "2012-12-16T02:51:11",
            "dataProviders": [
                {
                    "id": 1,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/1",
                    "logo": "https://api.core.ac.uk/data-providers/1/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2010-12-01T00:00:00",
            "abstract": "Peer reviewedPublisher PD",
            "documentType": "research",
            "doi": "10.1063/1.3530126",
            "downloadUrl": "https://core.ac.uk/download/9597638.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "Introduction to Focus Issue: Dynamics in Systems Biology\nChris A. Brackley,1,a\u0002 Oliver Ebenhöh,2 Celso Grebogi,2 Jürgen Kurths,2\nAlessandro de Moura,2 M. Carmen Romano,2 and Marco Thiel2\n1Institute for Complex Systems and Mathematical Biology, SUPA King’s College, University of Aberdeen,\nAberdeen AB24 3UE, United Kingdom\n2Institute for Complex Systems and Mathematical Biology, SUPA, University of Aberdeen,\nAberdeen AB24 3UE, United Kingdom\n\u0002Received 1 December 2010; published online 30 December 2010\u0003\nThe methods of nonlinear systems form an extensive toolbox for the study of biology, and systems\nbiology provides a rich source of motivation for the development of new mathematical techniques\nand the furthering of understanding of dynamical systems. This Focus Issue collects together a large\nvariety of work which highlights the complementary nature of these two fields, showing what each\nhas to offer the other. While a wide range of subjects is covered, the papers often have common\nthemes such as “rhythms and oscillations,” “networks and graph theory,” and “switches and deci-\nsion making.” There is a particular emphasis on the links between experimental data and modeling\nand mathematical analysis. © 2010 American Institute of Physics. \u0004doi:10.1063/1.3530126\u0005\nNonlinear dynamical systems can be found throughout\nbiology. This Focus Issue showcases examples of work\nwhere mathematical approaches have successfully been\napplied to a myriad of biological problems. From the pro-\ncessing and understanding of the vast amounts of experi-\nmental data which are the fruits of recent advances in\nmeasurement techniques, to modeling exploits which mo-\ntivate developments in mathematics, presented here are\nsome highly interesting contributions to this blossoming\nfield of systems biology.\nThe biological sciences have experienced a dramatic\nchange over the past decades. With the rapid advance of\nhigh-throughput technologies, it has now become possible to\nsimultaneously monitor many molecular components of a\ncell with an unprecedented temporal and spatial resolution—\nthus rendering molecular biology a truly quantitative science.\nDespite this technological advance, the interpretation and\ncondensation of the immense amount of available data still\npose major challenges, and overarching theories of general\nvalidity are still sparse.\nThe necessity to develop new theories and to describe\nbiological systems with mathematical methods is reflected in\nthe rapidly growing scientific field of systems biology. Many\ndifferent approaches are being developed and the diversity of\ninvestigated systems mirrors the versatility of the life sci-\nences. From a physicist’s point of view, living organisms can\nbe understood as highly complex dynamic systems with vari-\nous levels of organization, all of which influence each other.\nHowever, to view life as a special instance of physics does\nnot fully account for the true complexity of biology. Most\nimportantly, the biological systems that we observe today\nhave been shaped \u0002and are still being shaped\u0003 by billions of\nyears of evolutionary pressure. The mere fact that these sys-\ntems exist today reflects their success in the everlasting\nstruggle for survival. Indeed, survival in this constantly\nchanging, harsh, and competitive environment requires spe-\ncial features which one would not expect to have appeared\npurely by chance.\nDespite the advances in biological measurement technolo-\ngies, the available data are by far less complete than for\nphysical systems, such as astronomical bodies or electronic\ndevices. This necessitates a completely different approach to\ndata analysis. To unravel the mysteries of biology and give\nour myriad of observations a unifying theoretical superstruc-\nture, it is clearly required that life is investigated on all spa-\ntial, temporal, and organizational levels and that common\npatterns, or evolutionary design principles, are identified.\nConsidering our limited knowledge, it is evident that we are\npresently still at the very beginning of a new and fascinating\nera of theoretical biological research with unpredictable out-\ncome.\nAn important contribution to the development of general\nbiological theories is certainly the study of the dynamics of\nbiological systems, on scales ranging from short-term adap-\ntive processes which occur in seconds or faster to evolution-\nary dynamics which extend over billions of years. The pur-\npose of this Focus Issue is to bring together work from the\nmany diverse areas where the techniques of dynamical sys-\ntem research are applied to biology. Nonlinear dynamics of-\nfers the possibility to structure the data and to make sense of\nthe rich experimental results. The complexity of biological\norganisms and the intricate interactions between different\nsubsystems make it necessary to use cutting-edge mathemat-\nics to unravel the mechanisms that lie at the heart of life.\nDifferent areas of dynamical systems theory, such as syn-\nchronization and network theory, offer suitable tools for the\nanalysis and description of biological systems and cast new\nlight on experimental data and their interpretation. This sys-\ntematic, quantitative, and predictive approach characterizes\nthe increasingly important interplay between the biological\nand mathematical sciences. Crossing the border between\nthese formerly separated branches, systems biology fosters a\ncross-fertilization of ideas and stimulates new thinking,a\u0003Electronic mail: c.a.brackley@abdn.ac.uk.\nCHAOS 20, 045101 \u00022010\u0003\n1054-1500/2010/20\u00034\u0002/045101/3/$30.00 © 2010 American Institute of Physics20, 045101-1\nDownloaded 11 Jan 2011 to 139.133.7.237. Redistribution subject to AIP license or copyright; see http://chaos.aip.org/about/rights_and_permissions\nopening a treasure trove of novel problems which have been\nunnoticed until recently.\nIt is becoming more and more obvious that the theory of\ndynamical systems and chaos is key for the growth of bio-\nlogical knowledge in the coming years. This Focus Issue\nshows some examples of a successful application of various\nmathematical concepts to diverse biological systems. It sug-\ngests how the expected revolution in our understanding of\nbiological systems might, like other scientific developments\nbefore, be triggered by new mathematics and the interaction\nof scientists from different disciplines.\nAlthough the range of subjects is deliberately wide, there\nare several core themes which are repeatedly addressed from\ndifferent angles.\nOne vital facet of all biological systems is the ability to\ngenerate rhythmicity and oscillations: from the smallest\nsingle celled organism to the complexity of our own bodies,\nlife relies on cycles and timing. The beating of the human\nheart is analyzed by Thul and Coombes,1 who studied a\nmodel of intercellular calcium dynamics in order to under-\nstand cardiac alternans—a precursor to fibrillation. Also con-\ncerned with fibrillation, Petrov et al.2 examined spiral wave\nstability and the influence of fibroblasts. Harvey et al.3 con-\nsidered calcium dynamics on an intracellular length scale.\nThey use recently developed canard theory for systems with\nthree or more slow variables to study models of calcium\ndynamics. Raue et al.4 studied identifiability and observabil-\nity analysis of experimental data, considering how such tech-\nniques can inform experimental and model design.\nSignaling and oscillations are also at the heart of neural\nsystems; Chandrasekar et al.5 represented neuronal popula-\ntions as populations of coupled oscillators in order to under-\nstand the mechanisms of synchronization—an often patho-\nlogical phenomena associated with seizures. Finke6 revisited\nthe classic Hodgekin–Huxley model of a neuron to study the\nmechanism of cold reception in mammals. On smaller length\nscale oscillators can be used as cellular clocks: Morant et al.7\nexamined the circadian clock in green algae, analyzing time\nseries data which shows remarkable agreement with models.\nGérard et al.8 studied a network of kinases which control the\ndynamics of the mammalian cell cycle. They showed that as\na result of multiple oscillatory network circuits, there exists\nrich dynamics such as complex periodic orbits, quasiperiodic\noscillations, and chaos.\nNetworks and graph theory are themselves important\ntools in systems biology. In particular, protein interaction\nnetworks can be used to understand the many complex inter-\nactions between different proteins in a cell, for example, in\nhow a cell responds to extracellular signals. Rué et al.9 stud-\nied the signal transduction network found in human fibro-\nblasts; they use a simple Boolean representation of the net-\nwork with experimentally motivated parameters to show that\nthe relaxation to an attractor is robust to noise. Koseska and\nKurths10 considered network subunits \u0002such as switches and\noscillators\u0003 in synthetic biological systems. They studied\nhow the addition of such circuits in a network can enhance\nthe presence of different dynamical regimes. Interesting net-\nwork structures arise in many biological applications:\nZhandov11 considered a layered network model of mRNA-\nprotein interactions, including noncoding RNAs. Wan et al.12\nstudied a network growth model, looking at how a network\nevolves based on simple rules, with the aim of producing a\nstructure which reflects the topology of a real protein net-\nwork. Wang et al.13 examined the evolution of functional\nsubnetworks. Network theory not only allows the study of\nthe evolution of networks themselves but can also be used to\nfurther the understanding of evolution in biology. Schütte et\nal.14 studied evolutionary dynamics of metabolic pathways;\nthey show, by considering evolutionary walks on the meta-\nbolic network, that new enzymes appear in clusters rather\nthan gradually.\nEvolution is another area where mathematical methods\nare crucial to furthering our understanding. Ni et al.15 de-\nscribed how the traditional “rock-paper-scissors” game can\nbe used to understand the dynamics of species coexistence\non evolutionary time scales. They showed that from simple\ncompetition rules, different basins of attraction \u0002e.g., coexist-\nence or extinction\u0003 can emerge. Slipantschuk et al.16 showed\nthat not only can dynamical systems teach us about biology\nbut also the inverse is true: they gain understanding of cha-\notic systems from a study of a grouse population dynamics\nmodel with delay differential equations. They find that the\nformation of shrimp shaped periodic regimes in parameter\nspace is triggered by a homoclinic bifurcation.\nEvolutionary decisions are not the only decisions that bi-\nology has to make. On shorter length and time scales, cells\nand organisms have many choices to make based on sensory\ninput and previous experience. Neri17 mathematically treated\nhuman sensory processing as a mapping between a stimulus\nvector and a decision variable. Learning and adaption are\nexamined by Komarov et al.18 who studied a model consist-\ning of a network of nonlinear dynamical elements which\nproduce sequences of goal directed actions.\nAnother feature of some dynamical systems which leads\nto decision-making-like functionality is bistability; bistable\nswitches are very prevalent in biology. Cellular decision\nmaking is studied by Domingo-Sananes and Novak,19 who\nfocused on a biochemical regulatory network which controls\na transition in the cell cycle. They highlighted the difference\nbetween positive and double negative feedbacks and how\ndifferences in architecture can change the position of the\nsaddle node bifurcation which determines where the system\nswitches. Schittler et al.20 considered a model of a two\nswitch regulatory genetic network applied to differentiation\nof stem cells. They are able to reproduce three experimen-\ntally observed equilibrium states.\nDynamical systems also play a role in understanding me-\nchanical processes in biology. Günther and Kruse21 studied\nforce generation in muscles, focusing on the dynamics of\nsarcomeres—the basic force generating subunits. They ana-\nlyzed Hopf bifurcations, canard explosions, and gluing bifur-\ncations and considered the implications for experiments.\nBlood flow is studied by Geddes et al.,22 who examined the\ntopology of microvascular networks. They found multiple\nsteady states due to the nonlinear dependence of viscosity on\nblood cell concentration and presented evidence of how the\npredicted phenomena can be observed experimentally. Suhr-\nbier et al.23 examined the role of the cardiovascular system\nin sleep. They present a method for the detection of time-\ndelayed coupling in time series, applied to heat rate and\nblood pressure data during different stages of sleep.\n045101-2 Brackley et al. Chaos 20, 045101 \u00032010\u0002\nDownloaded 11 Jan 2011 to 139.133.7.237. Redistribution subject to AIP license or copyright; see http://chaos.aip.org/about/rights_and_permissions\nDynamical systems also have a role to play in medical\napplications. Hirata et al.24 presented a mathematical model\nfor the treatment of cancer. The method aims to find an op-\ntimal protocol for intermittent drug delivery, using math-\nematics to inform physicians on how best to treat individual\npatients.\nThe guest editors would like to thank all of the authors\nwho contributed to this Focus Issue. Also, we are grateful to\nJanis Bennett \u0002Assistant Editor, Chaos\u0003 for much help in the\nlogistics of putting together this issue.\n1R. Thul and S. Coombes, “Understanding cardiac alternans: A piecewise\nlinear modelling framework,” Chaos 20, 045102 \u00022010\u0003.\n2V. S. Petrov, G. V. Osipov, and J. Kurths, “Fibroblasts alter spiral wave\nstability,” Chaos 20, 045103 \u00022010\u0003.\n3E. Harvey, V. Kirk, H. M. Osinga, J. Sneyd, and M. Wechselberger, “Un-\nderstanding anomalous delays in a model of intracellular calcium dynam-\nics,” Chaos 20, 045104 \u00022010\u0003.\n4A. Raue, V. Becker, U. Klingmüller, and J. Timmer, “Identifiability and\nobservability analysis for experimental design in nonlinear dynamical\nmodels,” Chaos 20, 045105 \u00022010\u0003.\n5V. K. Chandrasekar, J. H. Sheeba, and M. Lakshmanan, “Pathological\nmass synchronization in the brain: Occurrence and its control,” Chaos 20,\n045106 \u00022010\u0003.\n6C. Finke, J. A. Freund, E. Rosa, H. Braun, and U. Feudel, “On the role of\nsubthreshold currents in the Huber-Braun cold receptor model,” Chaos 20,\n045107 \u00022010\u0003.\n7P. Morant, Q. Thommen, B. Pfeuty, C. Vandermoere, F. Corellou, F-Y\nBouget, and M. Lefranc, “A robust two-gene oscillator at the core of\nOstreococcus tauri circadian clock,” Chaos 20, 045108 \u00022010\u0003.\n8C. Gérard and A. Goldbeter, “From simple to complex patterns of oscil-\nlatory behavior in a model for the mammalian cell cycle containing mul-\ntiple oscillatory circuits,” Chaos 20, 045109 \u00022010\u0003.\n9P. Rué, T. Pons, N. Domedel-Puig, and J. Garcia-Ojalvo, “Relaxation dy-\nnamics and frequency response of a noisy cell signaling network,” Chaos\n20, 045110 \u00022010\u0003.\n10A. Koseska and J. Kurths, “Topological structures enhance the presence of\ndynamical regimes in synthetic networks,” Chaos 20, 045111 \u00022010\u0003.\n11V. P. Zhdanov, “Hierarchical genetic networks and non-coding RNAs,”\nChaos 20, 045112 \u00022010\u0003.\n12X. Wan, S. M. Cai, J. Zhou, and Z. Liu, “Emergence of modularity and\ndisassortativity in protein-protein interaction networks,” Chaos 20,\n045113 \u00022010\u0003.\n13M. Li, X. Wang, and C. Lai, “Evolution of functional subnetworks in\ncomplex systems,” Chaos 20, 045114 \u00022010\u0003.\n14M. Schütte, A. Skupin, D. Segre, and O. Ebenhöh, “Modeling the complex\ndynamics of enzyme-pathway coevolution,” Chaos 20, 045115 \u00022010\u0003.\n15X. Ni, W. Wang, R. Yang, Y.-C. Lai, and C. Grebogi, “Basins of coexist-\nence and extinction in spatially extended ecosystems of cyclically com-\npeting species,” Chaos 20, 045116 \u00022010\u0003.\n16J. Slipantschuk and E. Ullner, M. da silva Baptista, M. Zeineddine, and M.\nThiel, “Abundance of stable periodic behavior in a Red Grouse population\nmodel with delay: A consequence of homoclinicity,” Chaos 20, 045117\n\u00022010\u0003.\n17P. Neri, “Stochastic characterization of small-scale algorithms for human\nsensory processing,” Chaos 20, 045118 \u00022010\u0003.\n18M. Komarov, G. V. Osipov, and M. S. Burtsev, “Adaptive functional\nsystems: Learning with chaos,” Chaos 20, 045119 \u00022010\u0003.\n19M. R. Domingo-Sananes and B. Novak, “Different effects of redundant\nfeedback loops on a bistable switch,” Chaos 20, 045120 \u00022010\u0003.\n20D. Schittler and J. Hasenauer, F. AllgÃwer, and S. Waldherr, “Cell differ-\nentiation modeled via a coupled two-switch regulatory network,” Chaos\n20, 045121 \u00022010\u0003.\n21S. Günther and K. Kruse, “Spontaneous sarcomere dynamics,” Chaos 20,\n045122 \u00022010\u0003.\n22J. Geddes, R. Carr, F. Wu, Y. Lao, and M. Maher, “Blood flow in mi-\ncrovascular networks: A study in nonlinear biology,” Chaos 20, 045123\n\u00022010\u0003.\n23A. Suhrbier, M. Riedl, H. Malberg, T. Penzel, G. Bretthauer, J. Kurths,\nand N. Wessel, “Cardiovascular regulation during sleep quantified by sym-\nbolic coupling traces,” Chaos 20, 045124 \u00022010\u0003.\n24Y. Hirata, M. Di Bernardo, N. Bruchovsky, and K. Aihara, “Hybrid opti-\nmal scheduling for intermittent androgen suppression of prostate cancer,”\nChaos 20, 045125 \u00022010\u0003.\n045101-3 Introduction Chaos 20, 045101 \u00032010\u0002\nDownloaded 11 Jan 2011 to 139.133.7.237. Redistribution subject to AIP license or copyright; see http://chaos.aip.org/about/rights_and_permissions\n",
            "id": 4364269,
            "identifiers": [
                {
                    "identifier": "10.1063/1.3530126",
                    "type": "DOI"
                },
                {
                    "identifier": "9597638",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:aura.abdn.ac.uk:2164/2527",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "2112455704",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "192504841",
                    "type": "CORE_ID"
                }
            ],
            "title": "Introduction to Focus Issue : Dynamics in Systems Biology",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2112455704",
            "oaiIds": [
                "oai:aura.abdn.ac.uk:2164/2527"
            ],
            "publishedDate": "2010-12-31T00:00:00",
            "publisher": "'AIP Publishing'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "https://aura.abdn.ac.uk/bitstream/2164/2527/1/Chaos_IntroductionToFocusIssue2010.pdf"
            ],
            "updatedDate": "2022-03-16T07:23:22",
            "yearPublished": 2010,
            "journals": [
                {
                    "title": "Chaos An Interdisciplinary Journal of Nonlinear Science",
                    "identifiers": [
                        "1054-1500",
                        "issn:1054-1500"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/9597638.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/9597638"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/9597638/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/9597638/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/4364269"
                }
            ]
        },
        {
            "acceptedDate": "2012-09-03T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Bellotto, Nicola"
                },
                {
                    "name": "Cielniak, Grzegorz"
                },
                {
                    "name": "Duckett, Tom"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/187037675",
                "https://api.core.ac.uk/v3/outputs/9052390"
            ],
            "createdDate": "2012-08-09T02:58:00",
            "dataProviders": [
                {
                    "id": 128,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/128",
                    "logo": "https://api.core.ac.uk/data-providers/128/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2012-08-07T06:17:00",
            "abstract": "This paper describes the integration of robotics education into an undergraduate Computer Science curriculum. The proposed approach delivers mobile robotics as well as covering the closely related field of Computer Vision, and is directly linked to the research conducted at the authors’ institution. The paper describes the most relevant details of the module content and assessment strategy, paying particular attention to the practical sessions using Rovio mobile robots. The specific choices are discussed that were made with regard to the mobile platform, software libraries and lab environment. The paper also presents a detailed qualitative and quantitative analysis of student results, including the correlation between student engagement and performance, and discusses the outcomes of this experience",
            "documentType": "research",
            "doi": "10.1109/te.2012.2213822",
            "downloadUrl": "https://core.ac.uk/download/9052390.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "IEEE TRANSACTIONS ON EDUCATION 1\nIntegrating Mobile Robotics and Vision\nwith Undergraduate Computer Science\nGrzegorz Cielniak, Nicola Bellotto and Tom Duckett\nAbstract—This paper describes the integration of robotics edu-\ncation into an undergraduate Computer Science curriculum. The\nproposed approach delivers mobile robotics as well as covering\nthe closely related field of Computer Vision, and is directly\nlinked to the research conducted at the authors’ institution.\nThe paper describes the most relevant details of the module\ncontent and assessment strategy, paying particular attention to\nthe practical sessions using Rovio mobile robots. The specific\nchoices are discussed that were made with regard to the mobile\nplatform, software libraries and lab environment. The paper also\npresents a detailed qualitative and quantitative analysis of student\nresults, including the correlation between student engagement\nand performance, and discusses the outcomes of this experience.\nIndex Terms—Robotics education, robot vision, robot pro-\ngramming, Student as Producer.\nI. INTRODUCTION\nThis paper describes the integration of robotics education\ninto the undergraduate curriculum at the School of Computer\nScience, University of Lincoln, UK. Rather than teaching\nrobotics in isolation, the subject is tightly integrated with\nother disciplines, in particular computer vision, as well as\nthe ongoing research activities in the School. The proposed\napproach to robotics teaching features a holistic combination\nof theory and practical work, including the programming of\nvision-guided mobile robots in an open-ended assignment\nloosely based on the popular RoboCup football tournament.\nThis strategy also reflects the current policies of the institution\non research-informed teaching and the Student as Producer [1],\na university-wide initiative supported by the UK Higher Edu-\ncation Academy.\nStudent as Producer “restates the meaning and purpose of\nhigher education by reconnecting the core activities of univer-\nsities, i.e., research and teaching, in a way that consolidates\nand substantiates the values of academic life” [1]. It does not\ndictate strict rules or policies but rather acts as a framework\nthat encourages thinking about educational processes with\nthe student engagement and active participation in mind. The\ncore idea of the initiative is rooted in constructivism, which\nalso inspired Papert’s constructionism [2]; it was Papert who\nadvocated the use of technology in the learning process. This\ndirect link makes Student as Producer an especially attractive\nframework in the current context, inspiring some of the choices\nmade and the decision to involve students at various stages\nof preparation of the Robotics module (e.g., the selection\nG. Cielniak, N. Bellotto and T. Duckett are with the School of Com-\nputer Science, University of Lincoln, UK. e-mail: {gcielniak,nbellotto,\ntduckett}@lincoln.ac.uk\nof software and hardware platform, creation of assessment\nscenarios, etc.).\nThis paper presents the experience gained from conducting\nthese activities, the choices made during preparation of the\ntheoretical and practical work, the design of assignments, and\nvarious ways of assessing student performance.\nII. RELATED WORK\nIn recent years, the use of robotics in education has gained\na significant interest among robotics experts and educators,\nillustrated by a number of funded projects (e.g., TereCop [3],\nRoberta [4]), workshops and conferences (e.g., [5], [6]). Due\nto its interdisciplinary nature, robotics is often delivered in\ncombination with other complementary subjects like electron-\nics, mechanical engineering, computer vision [7], and the like.\nHowever, robotics is not only being taught as a specialist sub-\nject to a narrow audience of future robotics specialists [8], but\nis being increasingly used as a tool for improving knowledge\nof other subjects, developing transferable skills, enhancing\nstudent engagement in science, and more. This is closely\nrelated to the recent increase in the availability of affordable\nrobotics platforms but is also due to the potential attractiveness\nof robotics technology to younger generations.\nPrevious work in educational robotics reports teaching activ-\nities at different educational stages, including the primary, sec-\nondary [9] and tertiary level [10]. These activities, depending\non the level, address various learning aspects including manual\nand communication skills [11], and scientific methods such as\nmeasurement, calculations, problem-solving, etc. Robotics is\nalso being used to improve understanding of other subjects\nlike maths, physics and engineering (e.g., [12]) and in general\nfor improving student engagement in science subjects, [4].\nHowever, the efficient use of robotics tools requires specialist\ntraining for teachers themselves, especially those involved in\nthe early stages of education. While for more technically-\noriented teachers this comes naturally, many others feel intim-\nidated by the complexity and the practical skills required [13].\nEducational robotics is also being used to address social issues\nincluding gender balance, disabilities or disadvantaged social\nbackgrounds (e.g., [11], [14]).\nWhile there are many comprehensive sources covering theo-\nretical aspects of robotics (e.g., [15], [16], [17]), the main chal-\nlenge lies in the preparation of appropriate practical activities.\nThe main difficulty comes from the fact that both pedagogical\n(e.g., specific age, or learning objectives) and technical aspects\nhave to be considered. A key decision related to the latter is\nthe choice of robotic platforms, taking into account reliability,\nIEEE TRANSACTIONS ON EDUCATION 2\nLecture Topic C1 C2 C3\nA1 Intro. to Computer Vision ◦ ◦\nA2 Intro. to Linear Algebra and MATLAB •\nA3 An Overview of Pattern Recognition • ◦\nA4 Spatial Processing and Filtering • ◦ ◦\nA5 Color Image Processing • ◦ ◦\nA6 Morphological Image Processing ◦ ◦ •\nA7 Image Segmentation I ◦ ◦ •\nA8 Image Segmentation II ◦ ◦ •\nA9 Image Representation and Description •\nA10 Pattern Classification •\nB1 Introduction to Robotics ◦ ◦\nB2 Robot Programming in C# •\nB3 Actuators and Sensors • •\nB4 Robot Vision • ◦\nB5 Robot Control • •\nB6 Robot Behaviors • •\nB7 Control Architectures I ◦ •\nB8 Control Architectures II ◦ •\nB9 Navigation Strategies •\nB10 Robotic Map Building •\nTABLE I\nMAIN TOPICS IN SEMESTER A (COMPUTER VISION) AND SEMESTER B\n(MOBILE ROBOTICS), WITH INDICATIVE COVERAGE BY ASSESSMENT\n(• = HIGH, ◦ = LOW), DETAILED IN TABLE II.\nAssessment Weighting Description Semester\nC1 30% Image Processing in MATLAB A\nC2 30% Vision-based Robot Control B\nC3 40% Written Examination A + B\nTABLE II\nTHE THREE ASSESSMENTS FOR THE MODULE INCLUDING WEIGHTINGS.\nease of use, maintenance, and so on. Thanks to the increasing\nnumber of affordable robots available on the market, this\nchoice is somewhat easier (see [18] for an extensive review\nof the available robotics platforms). In addition, affordable\nrobot kits have become popular toys and gadgets, further\nstimulating interest in robotics among younger generations.\nThere are several popular scenarios that are employed for\npractical activities including robotic football, home services,\nrescue missions [19], [20], or Braitenberg vehicles [21]. The\nscenarios are acting in this case as micro-worlds [2] in which\nthere are well understood objectives and requirements, as well\nas potential for experimentation and discovery.\nIII. MODULE DESCRIPTION\nRobotics at the Lincoln School of Computer Science is\ndelivered primarily in a study module called Computer Vision\nand Robotics. The module is targeted at the third-year (i.e.,\nfinal-year) undergraduate students due to its advanced content,\nits assumption of significant programming skills and its focus\non mobile robots as complete systems, as well as on their\ncomponents. The module is compulsory for students on the\nComputer Science program, and optional for students studying\nother programs at the School (Games Computing, Computer\nInformation Systems, and Web Technology). There are several\nrelevant modules that students undertake in earlier years of\nstudy which provide the necessary background in program-\nming and basic knowledge of Artificial Intelligence (AI),\nS\nto\nra\ng\ne\n C\na\nb\nin\ne\nts\nP\nC\nP\nC\nP\nC\nP\nC\nP\nC\nP\nC\nP\nC\nP\nC\nP\nC\nP\nC\nP\nC\nP\nC\nP\nC\nP\nC\nP\nC\nP\nC\nP\nC\nP\nC\nP\nC\nP\nC\nNET\nE\nx\np\ne\nri\nm\ne\nn\nta\nl A\nre\na\nC\no\nm\np\nu\nte\nrs\n +\n \nN\ne\ntw\no\nrk\nin\ng\nRobots\nDocking Stations\nFootball Pitch\nFig. 1. The layout of the computer lab used for practical sessions (left) and\na snapshot taken during one of the sessions (right).\nincluding Software Development in year 1, and Advanced\nSoftware Development and AI in year 2. In addition, students\nhave the option to study robotics as part of their self-guided\nIndividual Study Project in year 3, in parallel with this taught\nmodule.\nComputer Vision and Robotics consists of two distinct parts:\ncomputer vision fundamentals are covered in the first semester\n(A) and robotics is covered in the second semester (B) – see\nTable I for a detailed list of topics, and Table II for their\nweightings. The computer vision part has a particular emphasis\non pattern recognition applications, hence the early overview\non this topic, but otherwise follows a fairly standard selection\nof topics in digital image processing from a well-known\ntextbook [22]. The weekly one-hour lectures, with additional\nsupporting materials, are accompanied by a weekly two-hour\nworkshop in which the students learn to program in MATLAB\nusing the Image Processing Toolbox (IPT) and the correspond-\ning textbook [23]. Many techniques and concepts learned in\nthe computer vision part of the module are directly relevant\nand applicable to the robotics content. Indeed, robot vision is a\nprimary focus of the practical sessions and the assignment task\nin the second semester. Each semester is individually assessed\nthrough a practical assignment (see Section IV) and there is\na final written examination covering the theoretical content of\nboth semesters. The robotics part of the module includes 12\nweeks of lecture delivery (comprising ten major topics plus\ntwo weeks for supporting activities) and practical workshop\nsessions. The topics covered include the main challenges of\nrobotics, robotic components, relevant software libraries, robot\nvision and control, robotic architectures, navigation strategies\nand map building. The first set of lectures provides many\npractical examples related to the robotic platform used, so that\nthe students can experience a direct link between theory and\npractice. The material covered also includes examples from\nresearch conducted in the School to illustrate fundamental\nproblems of perception (e.g., detecting people) and control\n(e.g., navigation), though the focus is mainly on “textbook”\nscience where possible [15], [16], [17].\nA. Practical Sessions\nThe robotics part of the module features 12 two-hour long\nworkshop sessions where the students have access to the\nrobotic equipment. The sessions take place in a dedicated\ncomputer lab with storage facilities and necessary space for\nexperimentation with the robots, Fig. 1. With this arrangement,\nall frequent tasks such as unpacking, setting-up, charging, and\nIEEE TRANSACTIONS ON EDUCATION 3\nFig. 2. Rovio, the mobile robot (left) and the robot’s web interface (right).\nso on can be carried out efficiently and smoothly without\nlimiting the amount of time for interaction and work with\nthe robots. In addition, the students can access the lab out-\nside the workshop hours, when it is not reserved for other\nactivities. This arrangement appeared to be very popular in\nthe most recent offering of the course, especially towards the\nassignment hand-in date (!). The first four workshop sessions\nare designed as introductory tutorials where the students learn\nabout the basic components of a robot, relevant software\nlibraries and programming principles. The tasks include stu-\ndents writing their own object detection algorithms, developing\nvisual feedback controllers, implementing simple behaviors\nor using Finite State Machines for sequencing more complex\nbehaviors. The tutorial tasks were designed to provide a solid\nknowledge base as well as preparing necessary components\nneeded for developing the following assignment task (see\nSection IV-B). The preparation of these teaching materials\nwas assisted by a Master’s degree student who had studied\nthe module in the previous academic year, as a ten-week\nsummer project supported by the university in connection with\nthe Student as Producer program [1]. The university’s policy\nof research-informed teaching is also aided by the part-time\nemployment of current Ph.D. students in the area of vision\nand robotics as demonstrators during the workshop sessions.\nB. Choice of Robot Platform\nA number of choices had to be made regarding the platform\nand software libraries. Since the targeted students are exposed\nmostly to programming in C# in their previous years of study,\nit was decided to adopt this programming language for the\npractical sessions. The students are provided with a simple\nsoftware wrapper directly implementing robot commands as\nspecified by the Rovio API document. An alternative would\nbe to rely on existing robotic suites such as Player/Stage,\nROS or Microsoft Robotics Developer Studio; however, such a\nchoice is perhaps more preferable for larger scale, long-term\nprojects as it requires substantial effort and time to become\nfamiliar with these frameworks. At the same time, the students\nlearn what to expect from a commercial platform in terms\nof software support and how to extend the functionality to\nmeet their own needs. The wrapper library [24] was developed\nfollowing object-oriented programming guidelines so that the\nstudents can apply the concepts learned in previous years to\na real physical platform. The recommended image processing\nlibrary is AForge [25], which provides a well structured and\ndocumented functionality including a variety of image filters\nand object detection algorithms. Ideally, OpenCV [26] would\nFeature Rovio Roomba NXT\nAffordability • • ?\nMaintenance/charging • ? ◦\nSet-up difficulty • ? ?\nDocumentation ? ? ?\nCommunity Support ? ◦ •\nSoftware components/libraries • • •\nQuality of components ? ? •\nVision support • ◦ ◦\nTABLE III\nA COMPARISON OF ROBOTIC PLATFORMS USED FOR TEACHING ROBOTICS\nIN THE CLASSROOM (SUPPORT: • = GOOD, ? = FAIR, ◦ = POOR).\nbe preferable due to its overall functionality, maturity and\nsupport. However, the existing C# wrappers are inefficient and\nrely on non-safe use of the language that could hinder the\nlearning process.\nRovio by WowWee [27] is an affordable mobile platform\nequipped with a set of sensors including a color camera\n(640x480 pixel resolution, 30 fps, RGB) mounted on a moving\nhead, odometry, infrared global navigation sensor and an\ninfrared obstacle detector (see Fig. 2). The omni-directional\ndrive configuration with omni-directional wheels enables holo-\nnomic movement in all directions. The communication with\nthe robot is realized through wireless Ethernet; the on-board\ncomputer (ARM-based architecture) runs a web-server that\naccepts requests from and sends information to the remote\nPC, which can be programmed to realize different custom\nbehaviors. There are several alternative robotic platforms that\nare popular in delivering robotics courses, including Roomba\nand LEGO NXT. Table III presents a subjective comparison\nof the popular robotic platforms based on the delivery team’s\nexperience of using them for teaching robotics in the previous\nyear.\nWhile Roomba and LEGO NXT’s many attractive features\n(e.g. reconfigurability, direct control of wheels) have made\nthese platforms very popular at other institutions (e.g., [28],\n[29]), there are several reasons that make Rovio an ideal\nplatform for the delivery of robotics in the proposed context.\nFirst, the robot is equipped with a color camera that is\ndirectly accessible from the software and can be used as the\nprimary source of sensory information in the workshop tasks.\nOther platforms (Roomba, NXT) require additional hardware\ncomponents and solutions to enable video streaming. The\nplatform is affordable – currently, it costs less than a single\nRoomba robot or a standard Lego NXT set. This makes it\npossible to purchase a large number of units that can be used\nindependently by individual students in relatively large groups,\nwhich is very important for maintaining student engagement\nin the workshops. This also makes Rovio a more expendable\nplatform, as a broken or faulty unit can be easily replaced. A\nsimple and sturdy design, together with a recharging station,\nresult in a straightforward and easily maintained set-up that\nminimizes the time overhead for the preparatory tasks required\nbefore each session. The robot features a convenient com-\nmunication interface through wireless Ethernet, enabling easy\nsetup in existing computer laboratories (there are no drivers\nrequired, and no additional dongles, connections or the like)\nIEEE TRANSACTIONS ON EDUCATION 4\nand a straightforward API that simplifies development of the\nstudents’ own robot behaviors, with minimal dependencies on\nother software libraries. Each Rovio robot in the lab is assigned\nits own IP address, and can be accessed directly through an\nordinary web browser (an important consideration when trying\nto work with large groups of students).\nThe frequent and intensive use of the Rovio robots also\nrevealed some limitations that might have not been immedi-\nately obvious. Among them, the odometry sensor is unreliable,\nthe global navigation sensor is unusable in a multi-robot\nenvironment, the camera image is of low quality, there can be\nconnection and bandwidth problems which are partially caused\nby the existing network facilities, and so on. On the other hand,\nthese limitations helped the students to understand some of\nthe fundamental issues and trade-offs in robotics and other\nembedded systems, including networking, real-time control,\nsensor noise, interaction of multiple complex systems, etc.\nIV. ASSESSMENT\nThe assessment strategy involves three separate compo-\nnents. Details of the topics covered by these three assessments\nare shown in Table I. The main objective of the first and second\ncomponents, “Digital Image Processing in MATLAB” (C1)\nand “Vision-based Robot Control” (C2), is to assess student\npractical work during Semester A and B, respectively; the\nmarks for these two components mostly reflect the functional-\nity of the developed systems. The last component (C3) is the\nfinal (closed-book) written exam, designed to assess student\ncomprehension of theoretical topics from the whole academic\nyear. Each component carries a weighting, Table II, which is\nused at the end of year to compute the overall module mark.\nA. Digital Image Processing in MATLAB (C1)\nThe scope of this component is to assess knowledge and\nunderstanding of various aspects of computer vision, in par-\nticular those related to image processing. This component is\nsubdivided into three workshop tasks with increasing levels of\ndifficulty, assessed at regular intervals throughout the semester,\ntitled “Binary Images & Introduction to Pattern Recognition”,\n“Intensity Transforms & Spatial Filtering” and “Color Image\nProcessing & Face Detection”. The deliverables for each task\nconsist of a brief report describing the approach used to solve\nthe problem and the results obtained, accompanied by the\ncorresponding MATLAB source code. The latter is further\ndemonstrated in a follow-up workshop session, where students\nare required to answer specific questions about their own\nMATLAB implementations.\nB. Vision-Based Robot Control (C2)\nThe aim of this assignment is to evaluate competence in two\nmajor learning outcomes: the application of computer vision\ntechniques to solve practical problems, and the application of\nAI control methods to mobile robotics. The assignment, which\nbuilds upon computer vision expertise from the first semester\nand some knowledge of AI from the previous year, is inspired\nby the RoboCup competition [19].\nThe students are asked to design and develop a simplified\nversion of robotic football. The students can select one type\nof player from a given list of striker, midfielder, defender\nand goal keeper. The game features a uniformly colored ball\nand a playing field consisting of a rectangular enclosure with\ndistinctively colored goals installed in the computer lab, Fig. 1.\nThe minimum requirement for player functionality includes\na ball searching behavior and a striking/defending behavior\n(which could be implemented as a reactive steering behavior\nusing a proportional controller), depending on the player type.\nStudents are free to choose the vision and robot control\nalgorithms. Extra credit is given for developing additional\ncomponents, including, but not limited to: 1) an enhanced\nobject detection system for learning object appearance or\nusing multiple cues; 2) behavior coordination for deriving\nsophisticated game strategies and implementing awareness of\nother game objects, like goals and opponents; and 3) incorpo-\nration of the above information into the player behavior. The\nsubmission of this assignment includes a short technical report\ndocumenting the design, implementation and evaluation of the\nproposed functionality as well as the developed source code. In\naddition, it is compulsory for the students to demonstrate their\nwork during a separate workshop session after the submission,\nso the technical achievement and originality of the submitted\nwork can be assessed and graded.\nC. Exam (C3)\nThe exam covers theoretical topics from both parts of the\nmodule, and is particularly designed to evaluate students’\ncritical assessment of the major topics covered. The exam is\ntaken at a predetermined time and location, a few weeks after\nthe end of Semester B, and must be completed within three\nhours. The students have to answer four questions, two from\nthe three Computer Vision section questions, and two from the\nthree Robotics section questions. There are no programming\ntasks in this final assessment. Instead, students are required to\ndemonstrate their understanding of the systems and algorithms\ncovered in the two semesters (see Table I for an overview), and\nto compare various solutions to computer vision and robotics\nproblems.\nV. STUDENT PERFORMANCE\nIn the academic year 2010/11, the student cohort consisted\nof 18 Computer Science students for whom the module was\ncompulsory, four Games Computing students and one Web\nTechnology student who chose the module as an option, giving\n23 students in total. This is a relatively low sample and\ntherefore the reported results are anecdotal to some extent and\nmight not capture all characteristics of the course delivery.\nA. Overall Results\nFig. 3 presents the comparative distribution of marks for\neach assessment component, and sample correlation coeffi-\ncients between marks for each pair of assessment components.\nWhile the practical sessions proved to be popular and the stu-\ndents received relatively good marks for both assignments (C1\nIEEE TRANSACTIONS ON EDUCATION 5\n0 10 20 30 40 50 60 70 80 90 1000\n1\n2\n3\n4\n5\n6\n7\nmarks\nf r e\nq u\ne\nn\nc y\n \n \nC1\nC2\nC3 C2\nC30.62  C1\nFig. 3. Distribution of marks obtained for different assessment components\n(left) and mark correlation between each pair of assignment components\n(right).\nC1 C2 C3\nLectures 0.82 0.63 0.52\nWorkshops 0.70 0.55 0.37\nTABLE IV\nSAMPLE CORRELATION COEFFICIENTS BETWEEN THE MARKS OBTAINED\nFOR THE DIFFERENT COMPONENTS AND THE ATTENDANCE RECORDED\nFOR THE LECTURES AND WORKSHOP SESSIONS.\nand C2), the theoretical examination (C3) results were lower\nthan expected – see the low correlation coefficients between\nthe exam and both practical assignments. These figures might\nindicate a better engagement of the students in the practical\npart of the course, but perhaps also a wider problem of\nstudent comprehension of theoretical material observed across\nall programs at the School.\nTo analyze the results further, attendance data was consid-\nered as a basic indicator of student engagement, Table IV.\nThe results indicate that attendance at lectures and workshops\nshowed a strong correlation with performance on the Com-\nputer Vision assignment (C1), although less so in the second\nsemester. Perhaps surprisingly, the attendance data showed the\nlowest correlation with the marks obtained in the exam.\nFurther analysis included the correlation between marks\nobtained in Computer Vision and Robotics and other rele-\nvant modules taken by students during their course of study.\nTable V presents sample correlation coefficients in the form\nof a matrix between different modules including a first-year\nmodule, Software Development (SD); second-year modules,\nAdvanced Software Development (ASD) and Artificial Intel-\nligence (AI); and third-year modules Computer Vision and\nRobotics (CVR), Advanced Software Engineering (ASE) and\nIndividual Study Project (ISP). While it can be seen that\nthe correlation between CVR and other third-year modules is\nstronger than with modules from earlier years, it is interesting\nto note that CVR is ranked as the module most correlated with\nASE, ISP and ASD, i.e., the modules where programming\nskills play a prominent role.\nB. Robotics Assignment – Observations\nThe robotics assignment had clearly defined minimum re-\nquirements but fairly open goals, which encouraged exper-\nimentation and exploration. This resulted in a number of\nexceptionally good submissions which included many features\nbeyond the standard specifications. All the best submissions\nCVR ASE ISP ASD AI SD\nSD 0.34 0.41 0.38 0.37 0.53 1.00\nAI 0.49 0.40 0.49 0.62 1.00\nASD 0.62 0.54 0.54 1.00\nISP 0.87 0.87 1.00\nASE 0.94 1.00\nCVR 1.00\nTABLE V\nSAMPLE CORRELATION COEFFICIENTS BETWEEN COMPUTER VISION AND\nROBOTICS AND OTHER RELEVANT MODULES (SEE SECTION V-A FOR\nFULL LABEL DETAILS).\n(marks greater than 70%) had a clearly defined individual\nfocus and explored different issues and directions. Some\nexamples of the outstanding achievements presented by the\nstudents included:\n• object detection: a multi-stage image processing pipeline\nincluding cascaded segmentation in different color\nspaces, morphological operators for noise filtering, the\nuse of edges as additional features, and histogram-based\ntuning and learning of image filter parameters;\n• control architectures: a hybrid architecture combining\nreactive and deliberative approaches, complex behavior\nsequencing models (e.g., hierarchical FSM), a predictive\nball search behavior, multi-threading and synchronization\nfeaturing customized threading queue mechanisms and\noff-line system development using prerecorded data sets;\n• system evaluation: rigorous quantitative evaluation in-\ncluding switching time analysis of behavioral models and\ndedicated testing scenarios, consideration of trade-offs\n(e.g., speed vs. accuracy) for robot controllers.\nThe above list of topics indicates that the students were\nable to apply not only knowledge learned in the previous\nsemester (for example, object detection) but also that acquired\nin other modules including AI, Software Development and\nSoftware Engineering. Many of the listed techniques required\nthe students to research sources other than the recommended\nreading material. On average, the students with the highest\ngrade referenced two items from the recommended reading\nlist, three items citing other books, journal publications and\nconference papers discovered through individual research, and\none item citing software or hardware.\nVI. DISCUSSION AND CONCLUSIONS\nIt has been pointed out previously that the goal of “educa-\ntional robotics in general, is not precisely to teach learners to\nbe robotics experts but to develop the essential competences to\nbe successful in the present world” [28]. The authors believe\nthat the module presented here gives students vital expertise in\nareas that are otherwise not strongly covered in the “standard”\ncomputer science topics, such as dealing with complex sys-\ntems at a systems (and systems of systems) level, combining\nhardware with sensing and control software, understanding the\npracticalities of real-time systems, understanding the inherent\nuncertainty in the real-world as perceived through sensors\n(sensor noise), and applying programming methodologies in\npractice. The students demonstrated a very high level of\nIEEE TRANSACTIONS ON EDUCATION 6\nengagement, spending a significant amount of time solving the\nrobot football task required for the assignment. This resulted\nin a number of exceptional submissions with very original\nfunctionality that went beyond the assignment brief (incor-\nporating such features as threading queues, speech synthesis,\netc.). While many students struggled with the evaluation part\nof the assignment (most likely due to their time manage-\nment), many of the technical issues and platform shortcomings\nwere identified by the students (connection and bandwidth\nproblems, granularity of the movement commands, limited\nodometry, changing light conditions, etc.). The laboratory\nspace encouraged cooperation, support and competition in\ndeveloping individual solutions. The module proved to be\npopular among the students and there was an increased intake\n(44 students) in the academic year 2011/12. Some pedagogical\nissues to be addressed in future extensions of this work\ninclude, for example, time requirements for solving practical\nassignments and scenarios alternative to RoboCup. Further\ndevelopment plans for the module include: a common software\nrepository to teach code maintenance and development in\nteams, extensions to the software environment, multi-robot\nscenarios, and a greater involvement in the Student as Producer\ninitiative [1], including recruiting more student helpers and\nbuilding stronger links with the student Computing Society.\nA comparative study with the previous year and with other\ninstitutions delivering similar content is also envisaged.\nREFERENCES\n[1] M. Neary and J. Winn, “The student as producer: reinventing the student\nexperience in higher education,” in The future of higher education:\npolicy, pedagogy and the student experience. Continuum, 2009.\n[2] S. Papert, “What is Logo? And who needs it?” in Logo philosophy and\nimplementation. Logo Computer Systems Inc., 1999.\n[3] D. Alimisis, M. Moro, J. Arlegui, A. Pina, S. Frangou, and K. Papaniko-\nlaou, “Robotics & Constructivism in Education: the TERECoP project,”\nin Proc. of the 11th European Logo Conf., Bratislava, Slovakia, 2007.\n[4] Roberta Project. [Online]. Available: http://www.roberta-home.de/en\n[5] International workshop: Teaching robotics teaching with robotics.\n[Online]. Available: http://www.terecop.eu/TRTWR2012.htm\n[6] International robotics in education conference. [Online]. Available:\nhttp://www.rie2012.eu/\n[7] G. Bebis, D. Egbert, and M. Shah, “Review of computer vision educa-\ntion,” IEEE Transactions on Education, vol. 46, pp. 2–21, 2003.\n[8] B. A. Maxwell and L. A. Meeden, “Integrating robotics research with\nundergraduate education,” in IEEE Intelligent Systems Magazine, 2000.\n[9] M. Kaba´tova´, L. Jasˇkova´, P. Lecky´, and V. Lasˇsˇa´kova´, “Robotic activities\nfor visually impaired secondary school children,” in Proc. Int. Workshop\nTeaching Robotics Teaching with Robotics: Integrating Robotics in\nSchool Curriculum, Riva del Garda, Italy, April 2012.\n[10] J. Carter, S. Matthews, and S. Coupland, “Teaching robotics at the\npostgraduate level: Assessment and feedback for on site and distance\nlearning,” in Proc. Int. Conf. on Robotics in Education, 2011.\n[11] T. Urschitz and T. Carl, “Mission on Mars. Interactive robotics session,\non video conference, through the use of Interactive White Board (IWB),”\nin Proc. Int. Workshop Teaching Robotics Teaching with Robotics:\nIntegrating Robotics in School Curriculum, Riva del Garda, Italy, 2012.\n[12] L. Riano and M. McGinnity, “Design and validation of a robotic system\nto interactively teach geometry,” in Proc. Int. Conf. on Robotics in\nEducation. FEI STU, Slovakia, 2010, pp. 5–9.\n[13] D. Alimisis, “Integrating robotics in science and technology teacher\ntraining curriculum,” in Proc. Int. Workshop Teaching Robotics Teaching\nwith Robotics: Integrating Robotics in School Curriculum, Riva del\nGarda, Italy, April 2012.\n[14] D. Catlin and S. Robertson, “Using educational robots to enhance the\nperformance of minority students,” in Proc. Int. Workshop Teaching\nRobotics Teaching with Robotics: Integrating Robotics in School Cur-\nriculum, Riva del Garda, Italy, April 2012.\n[15] G. Bekey, Autonomous Robots: From Biological Inspiration To Im-\nplementation And Control, ser. Intelligent Robotics and Autonomous\nAgents. Mit Press, 2005.\n[16] R. Siegwart and I. R. Nourbakhsh, Introduction to Autonomous Mobile\nRobots. Bradford Company, 2004.\n[17] R. R. Murphy, Introduction to AI Robotics, 1st ed. MIT Press, 2000.\n[18] M. Ruzzenente, M. Koo, K. Nielsen, L. Grespan, and P. Fiorini, “A\nreview of robotics kits for tertiary education,” in Proc. Int. Workshop\nTeaching Robotics Teaching with Robotics: Integrating Robotics in\nSchool Curriculum, Riva del Garda, Italy, April 2012.\n[19] M. Asada, R. D’Andrea, A. Birk, H. Kitano, and M. Veloso, “Robotics\nin edutainment,” in Proc. IEEE Int. Conf. on Robotics and Automation,\n2000, pp. 795–800.\n[20] A. D. Tonkonogui, P. Bell, A. R. Linse, and A. V. Mamishev, “Integrating\nFIRST robotics program with university curriculum,” in Proc. Int. Conf.\non Robotics and Applications. Anaheim, CA, USA: ACTA Press, 2007.\n[21] E. Datteri, L. Zecca, F. Laudisa, and M. Castiglioni, “Explaining robotic\nbehaviors: a case study on science education,” in Proc. Int. Workshop\nTeaching Robotics Teaching with Robotics: Integrating Robotics in\nSchool Curriculum, Riva del Garda, Italy, April 2012.\n[22] R. C. Gonzalez and R. E. Woods, Digital Image Processing (3rd\nEdition). Prentice Hall, 2008.\n[23] R. C. Gonzalez, R. E. Woods, and S. L. Eddins, Digital Image Process-\ning Using MATLAB (2nd Edition). Gatesmark Publishing, 2009.\n[24] RobotLib project. [Online]. Available: http://robotlib.codeplex.com\n[25] AForge.NET. [Online]. Available: http://code.google.com/p/aforge\n[26] G. Bradski, A. Kaehler, and V. Pisarevsky, “Learning-based computer\nvision with OpenCV,” Intel Technology Journal, vol. 9, 2005.\n[27] WowWee Rovio. [Online]. Available: http://www.wowwee.com/\nen/products/tech/telepresence/rovio\n[28] K. Pitti, B. Curto, J. Garcia, and V. Moreno, “NXT workshops: Con-\nstructionist learning experiences in rural areas,” in Proc. Int. Work-\nshop ”Teaching robotics, teaching with robotics”, Darmstadt, Germany,\nNovember 2010.\n[29] E. Menegatti and M. Moro, “Educational robotics from high-school to\nmaster of science,” in Proc. Int. Workshop ”Teaching robotics, teaching\nwith robotics”, Darmstadt, Germany, November 2010.\nGrzegorz Cielniak is a Senior Lecturer at the School of Computer Science,\nUniversity of Lincoln, UK. He obtained his Ph.D. in Computer Science from\nO¨rebro University, Sweden in 2007 and his M.Sc. in Robotics from Wrocław\nUniversity of Technology, Poland in 2000. His research interests include\nmobile robotics, machine perception, estimation and tracking techniques,\nmonitoring of humans and other species. Grzegorz is currently involved in\nteaching a number of undergraduate computer science subjects at Lincoln,\nincluding the module Computer Vision and Robotics.\nNicola Bellotto is a Senior Lecturer in Computer Science at the University\nof Lincoln, where he teaches a number of undergraduate and postgraduate\nmodules, including Computer Vision and Robotics and Operating Systems.\nHis research interests range from mobile robotics to cognitive perception,\nincluding sensor fusion, Bayesian estimation and embedded AI. He holds a\nPh.D. in Computer Science from the University of Essex and a Laurea in\nElectronic Engineering from the University of Padua. Before joining Lincoln,\nhe was a post-doctoral research assistant at the University of Oxford. Nicola\nhas also several years of professional experience working in the industry as\nsoftware developer and embedded system programmer.\nTom Duckett is a Reader in Computer Science at the University of Lincoln,\nUK, where he is also Director of the Center for Vision and Robotics Research.\nHe was formerly a docent (Associate Professor) at O¨rebro University, Sweden,\nwhere he was leader of the Learning Systems Laboratory within the Center\nfor Applied Autonomous Sensor Systems. He obtained his Ph.D. in Computer\nScience from Manchester University in 2001, M.Sc. with distinction in\nKnowledge-Based Systems from Heriot-Watt University in 1995 and B.Sc.\n(Hons.) in Computer and Management Science from Warwick University in\n1991, and also studied at Karlsruhe and Bremen Universities. He currently\nteaches artificial intelligence, computer vision and mobile robotics as part of\nthe B.Sc. program in Computer Science at Lincoln.\n",
            "id": 4212729,
            "identifiers": [
                {
                    "identifier": "2144503148",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "187037675",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1109/te.2012.2213822",
                    "type": "DOI"
                },
                {
                    "identifier": "9052390",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:eprints.lincoln.ac.uk:6031",
                    "type": "OAI_ID"
                }
            ],
            "title": "Integrating mobile robotics and vision with undergraduate computer science",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2144503148",
            "oaiIds": [
                "oai:eprints.lincoln.ac.uk:6031"
            ],
            "publishedDate": "2013-02-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 18449240,
                    "title": "A review of robotics kits for tertiary education,” in",
                    "authors": [],
                    "date": "2012",
                    "doi": null,
                    "raw": "M. Ruzzenente, M. Koo, K. Nielsen, L. Grespan, and P. Fiorini, “A review of robotics kits for tertiary education,” in Proc. Int. Workshop Teaching Robotics Teaching with Robotics: Integrating Robotics in School Curriculum, Riva del Garda, Italy, April 2012.",
                    "cites": null
                },
                {
                    "id": 18449237,
                    "title": "Autonomous Robots: From Biological Inspiration To Implementation And Control, ser. Intelligent Robotics and Autonomous Agents.",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1017/s0269888905210512",
                    "raw": "G. Bekey, Autonomous Robots: From Biological Inspiration To Implementation And Control, ser. Intelligent Robotics and Autonomous Agents. Mit Press, 2005.",
                    "cites": null
                },
                {
                    "id": 18449234,
                    "title": "Design and validation of a robotic system to interactively teach geometry,” in",
                    "authors": [],
                    "date": "2010",
                    "doi": null,
                    "raw": "L. Riano and M. McGinnity, “Design and validation of a robotic system to interactively teach geometry,” in Proc. Int. Conf. on Robotics in Education. FEI STU, Slovakia, 2010, pp. 5–9.",
                    "cites": null
                },
                {
                    "id": 18449246,
                    "title": "Educational robotics from high-school to master of science,” in",
                    "authors": [],
                    "date": "2010",
                    "doi": null,
                    "raw": "E. Menegatti and M. Moro, “Educational robotics from high-school to master of science,” in Proc. Int. Workshop ”Teaching robotics, teaching with robotics”, Darmstadt, Germany, November 2010. Grzegorz Cielniak is a Senior Lecturer at the School of Computer Science, University of Lincoln, UK. He obtained his Ph.D. in Computer Science from ¨ Orebro University, Sweden in 2007 and his M.Sc. in Robotics from Wrocław University of Technology, Poland in 2000. His research interests include mobile robotics, machine perception, estimation and tracking techniques, monitoring of humans and other species. Grzegorz is currently involved in teaching a number of undergraduate computer science subjects at Lincoln, including the module Computer Vision and Robotics. Nicola Bellotto is a Senior Lecturer in Computer Science at the University of Lincoln, where he teaches a number of undergraduate and postgraduate modules, including Computer Vision and Robotics and Operating Systems. His research interests range from mobile robotics to cognitive perception, including sensor fusion, Bayesian estimation and embedded AI. He holds a Ph.D. in Computer Science from the University of Essex and a Laurea in Electronic Engineering from the University of Padua. Before joining Lincoln, he was a post-doctoral research assistant at the University of Oxford. Nicola has also several years of professional experience working in the industry as software developer and embedded system programmer. Tom Duckett is a Reader in Computer Science at the University of Lincoln, UK, where he is also Director of the Center for Vision and Robotics Research. He was formerly a docent (Associate Professor) at ¨ Orebro University, Sweden, where he was leader of the Learning Systems Laboratory within the Center for Applied Autonomous Sensor Systems. He obtained his Ph.D. in Computer Science from Manchester University in 2001, M.Sc. with distinction in Knowledge-Based Systems from Heriot-Watt University in 1995 and B.Sc. (Hons.) in Computer and Management Science from Warwick University in 1991, and also studied at Karlsruhe and Bremen Universities. He currently teaches artiﬁcial intelligence, computer vision and mobile robotics as part of the B.Sc. program in Computer Science at Lincoln.",
                    "cites": null
                },
                {
                    "id": 18449243,
                    "title": "Explaining robotic behaviors: a case study on science education,” in",
                    "authors": [],
                    "date": "2012",
                    "doi": null,
                    "raw": "E. Datteri, L. Zecca, F. Laudisa, and M. Castiglioni, “Explaining robotic behaviors: a case study on science education,” in Proc. Int. Workshop Teaching Robotics Teaching with Robotics: Integrating Robotics in School Curriculum, Riva del Garda, Italy, April 2012.",
                    "cites": null
                },
                {
                    "id": 18449242,
                    "title": "Integrating FIRST robotics program with university curriculum,” in",
                    "authors": [],
                    "date": "2007",
                    "doi": null,
                    "raw": "A. D. Tonkonogui, P. Bell, A. R. Linse, and A. V. Mamishev, “Integrating FIRST robotics program with university curriculum,” in Proc. Int. Conf. on Robotics and Applications. Anaheim, CA, USA: ACTA Press, 2007.",
                    "cites": null
                },
                {
                    "id": 18449235,
                    "title": "Integrating robotics in science and technology teacher training curriculum,” in",
                    "authors": [],
                    "date": "2012",
                    "doi": "10.4156/ijrea.vol2.issue2.2",
                    "raw": "D. Alimisis, “Integrating robotics in science and technology teacher training curriculum,” in Proc. Int. Workshop Teaching Robotics Teaching with Robotics: Integrating Robotics in School Curriculum, Riva del Garda, Italy, April 2012.",
                    "cites": null
                },
                {
                    "id": 18449230,
                    "title": "Integrating robotics research with undergraduate education,”",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1109/5254.895854",
                    "raw": "B. A. Maxwell and L. A. Meeden, “Integrating robotics research with undergraduate education,” in IEEE Intelligent Systems Magazine, 2000.",
                    "cites": null
                },
                {
                    "id": 18449228,
                    "title": "International workshop: Teaching robotics teaching with robotics.",
                    "authors": [],
                    "date": null,
                    "doi": "10.1109/arso.2005.1511630",
                    "raw": "International workshop: Teaching robotics teaching with robotics.",
                    "cites": null
                },
                {
                    "id": 18449239,
                    "title": "Introduction to AI Robotics,",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "R. R. Murphy, Introduction to AI Robotics, 1st ed. MIT Press, 2000.",
                    "cites": null
                },
                {
                    "id": 18449238,
                    "title": "Introduction to Autonomous Mobile Robots.",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1017/s0263574705221628",
                    "raw": "R. Siegwart and I. R. Nourbakhsh, Introduction to Autonomous Mobile Robots. Bradford Company, 2004.",
                    "cites": null
                },
                {
                    "id": 18449231,
                    "title": "Laˇ sˇ s´ akov´ a, “Robotic activities for visually impaired secondary school children,” in",
                    "authors": [],
                    "date": "2012",
                    "doi": null,
                    "raw": "M. Kab´ atov´ a, L. Jaˇ skov´ a, P. Leck´ y, and V. Laˇ sˇ s´ akov´ a, “Robotic activities for visually impaired secondary school children,” in Proc. Int. Workshop Teaching Robotics Teaching with Robotics: Integrating Robotics in School Curriculum, Riva del Garda, Italy, April 2012.",
                    "cites": null
                },
                {
                    "id": 18449244,
                    "title": "Learning-based computer vision with",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1535/itj.0902.03",
                    "raw": "G. Bradski, A. Kaehler, and V. Pisarevsky, “Learning-based computer vision with OpenCV,” Intel Technology Journal, vol. 9, 2005.",
                    "cites": null
                },
                {
                    "id": 18449233,
                    "title": "Mission on Mars. Interactive robotics session, on video conference, through the use of Interactive White Board (IWB),” in",
                    "authors": [],
                    "date": "2012",
                    "doi": null,
                    "raw": "T. Urschitz and T. Carl, “Mission on Mars. Interactive robotics session, on video conference, through the use of Interactive White Board (IWB),” in Proc. Int. Workshop Teaching Robotics Teaching with Robotics: Integrating Robotics in School Curriculum, Riva del Garda, Italy, 2012.",
                    "cites": null
                },
                {
                    "id": 18449245,
                    "title": "NXT workshops: Constructionist learning experiences in rural areas,” in",
                    "authors": [],
                    "date": "2010",
                    "doi": null,
                    "raw": "K. Pitti, B. Curto, J. Garcia, and V. Moreno, “NXT workshops: Constructionist learning experiences in rural areas,” in Proc. Int. Workshop ”Teaching robotics, teaching with robotics”, Darmstadt, Germany, November 2010.",
                    "cites": null
                },
                {
                    "id": 18449229,
                    "title": "Review of computer vision",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1109/te.2002.808280",
                    "raw": "G. Bebis, D. Egbert, and M. Shah, “Review of computer vision education,” IEEE Transactions on Education, vol. 46, pp. 2–21, 2003.",
                    "cites": null
                },
                {
                    "id": 18449241,
                    "title": "Robotics in edutainment,” in",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1109/robot.2000.844148",
                    "raw": "M. Asada, R. D’Andrea, A. Birk, H. Kitano, and M. Veloso, “Robotics in edutainment,” in Proc. IEEE Int. Conf. on Robotics and Automation, 2000, pp. 795–800.",
                    "cites": null
                },
                {
                    "id": 18449232,
                    "title": "Teaching robotics at the postgraduate level: Assessment and feedback for on site and distance learning,” in",
                    "authors": [],
                    "date": "2011",
                    "doi": null,
                    "raw": "J. Carter, S. Matthews, and S. Coupland, “Teaching robotics at the postgraduate level: Assessment and feedback for on site and distance learning,” in Proc. Int. Conf. on Robotics in Education, 2011.",
                    "cites": null
                },
                {
                    "id": 18449226,
                    "title": "The student as producer: reinventing the student experience in higher education,” in The future of higher education: policy, pedagogy and the student experience. Continuum,",
                    "authors": [],
                    "date": "2009",
                    "doi": null,
                    "raw": "M. Neary and J. Winn, “The student as producer: reinventing the student experience in higher education,” in The future of higher education: policy, pedagogy and the student experience. Continuum, 2009.",
                    "cites": null
                },
                {
                    "id": 18449236,
                    "title": "Using educational robots to enhance the performance of minority students,” in",
                    "authors": [],
                    "date": "2012",
                    "doi": null,
                    "raw": "D. Catlin and S. Robertson, “Using educational robots to enhance the performance of minority students,” in Proc. Int. Workshop Teaching Robotics Teaching with Robotics: Integrating Robotics in School Curriculum, Riva del Garda, Italy, April 2012.",
                    "cites": null
                },
                {
                    "id": 18449227,
                    "title": "What is Logo? And who needs it?” in Logo philosophy and implementation. Logo Computer Systems Inc.,",
                    "authors": [],
                    "date": "1999",
                    "doi": null,
                    "raw": "S. Papert, “What is Logo? And who needs it?” in Logo philosophy and implementation. Logo Computer Systems Inc., 1999.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "http://eprints.lincoln.ac.uk/6031/2/lincoln12transeducation.pdf"
            ],
            "updatedDate": "2021-12-22T04:29:55",
            "yearPublished": 2013,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "0018-9359"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/9052390.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/9052390"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/9052390/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/9052390/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/4212729"
                }
            ]
        },
        {
            "acceptedDate": "2012-08-29T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Abbas Jamalipour"
                },
                {
                    "name": "Alotaibi"
                },
                {
                    "name": "Anagnostopoulos"
                },
                {
                    "name": "Bamis"
                },
                {
                    "name": "Biradar"
                },
                {
                    "name": "Biradar"
                },
                {
                    "name": "Casella"
                },
                {
                    "name": "Farkas"
                },
                {
                    "name": "Floriano"
                },
                {
                    "name": "Giovanidis"
                },
                {
                    "name": "Guo"
                },
                {
                    "name": "Guo"
                },
                {
                    "name": "Guo"
                },
                {
                    "name": "Guo"
                },
                {
                    "name": "Huang"
                },
                {
                    "name": "Hwang"
                },
                {
                    "name": "Jiang"
                },
                {
                    "name": "Kharraz"
                },
                {
                    "name": "Li"
                },
                {
                    "name": "Lopez"
                },
                {
                    "name": "Ning"
                },
                {
                    "name": "Papoulis"
                },
                {
                    "name": "Qingyang Song"
                },
                {
                    "name": "Sarma"
                },
                {
                    "name": "Shiqiang Wang"
                },
                {
                    "name": "Torkestani"
                },
                {
                    "name": "Wang"
                },
                {
                    "name": "Weng"
                },
                {
                    "name": "Yang"
                },
                {
                    "name": "Ye"
                },
                {
                    "name": "Zhang"
                },
                {
                    "name": "Zhaolong Ning"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/192063216",
                "https://api.core.ac.uk/v3/outputs/9550010"
            ],
            "createdDate": "2012-12-12T03:57:27",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 105,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/105",
                    "logo": "https://api.core.ac.uk/data-providers/105/logo"
                }
            ],
            "depositedDate": "2012-11-01T00:00:00",
            "abstract": null,
            "documentType": "research",
            "doi": "10.1016/j.jnca.2012.08.004",
            "downloadUrl": "",
            "fieldOfStudy": "computer science",
            "fullText": " 1 \n \nLink Stability Estimation Based on Link Connectivity Changes in \nMobile Ad-hoc Networks \nQingyang Song1, Zhaolong Ning1, Shiqiang Wang1, 2, and Abbas Jamalipour3 \n1 School of Information Science and Engineering, Northeastern University, Shenyang, 110819, P. R. China \n2 Dept. of Electrical and Electronic Engineering, Imperial College London, SW7 2AZ, United Kingdom \n3 School of Electrical and Information Engineering, University of Sydney, NSW, 2006, Australia \n \nEmails: songqingyang@ise.neu.edu.cn, zhaolongning@gmail.com, shiqiang.wang11@imperial.ac.uk, \na.jamalipour@ieee.org \n \n \n \n \n \n \n \n \n \nA preliminary version of this paper has been presented in the 2010 IEEE International Conference \non Wireless Communications, Networking and Information Security (IEEE WCNIS 2010) [1]. This \npaper provides a more detailed analysis of the proposed link stability estimation algorithm, extends the \nsimulation results, and also proposes an application of the link stability estimation scheme to routing \nprotocols of mobile ad hoc networks. \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 2 \nAbstract - Link stability issue is significant in many aspects, especially for the route selection \nprocess in mobile ad-hoc networks (MANETs). Most previous works focus on the link stability in static \nenvironments, with fixed sampling windows which are only suitable for certain network topologies. In \nthis paper, we propose a scheme to estimate the link stability based on link connectivity changes, \nwhich can be performed on the network layer, without the need of peripheral devices or low layer data. \nWe adopt a variable sized sampling window and propose a method to estimate the link transition rates. \nThe estimation scheme is not restricted to specific network topologies or mobility models. After that, \nwe propose a routing method which adjusts its operating mode based on the estimated link stability. \nSimulation results show that the proposed scheme can provide correct estimation in both stationary and \nnon-stationary scenarios, and the presented routing protocol outperforms conventional routing schemes \nwithout link stability estimation. \n \nKeywords - Link stability; mobile ad-hoc networks (MANETs); probabilistic model; routing \nprotocol \n \n \n \n \n \n \n \n \n \n \n \n \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 3 \n1 Introduction \nMany design issues in mobile ad-hoc networks (MANETs) are challenging due to the dynamic \ncharacteristics of the networks. Links can be unstable due to the random property of the wireless \nchannel (e.g. fading, shadowing or noise) which affect the network performance significantly, \nespecially for the routing process [2-5, 8]. Hence, link stability estimation methods are worth to be \nstudied. \nIn [6] and [7], the notion of associativity, used as a metric to evaluate the link stability, was defined \nas the number of consecutive beacons. This method is simple; however it relies on some experimentally \nchosen parameters that have large influence on its performance, and is unstable in some scenarios. In \n[9], a network utility maximization problem was formulated by decomposing the input rate control and \nthe scheduling problem under stability constraints. This is a rather complicated linear programming (LP) \nproblem by introducing scheduling mechanism, after that it was relaxed to a simpler form under certain \nassumptions. However, fixed scheduling rates per link is not always true especially in MANETs. \nIn [10], a probabilistic model was used to evaluate the link stability by redefining the link \navailability as the conditional probability that a link remains connected throughout a specific time \nperiod, given that it is currently connected. This concept is extended in [11] and [12] for multicast \nscenarios; however, the link stability prediction schemes proposed in [10−12] are based on the random \nwalk or random way point model and require the prior knowledge of some parameters of the mobility \nmodel. In [13], the authors presented a pattern matching based approach to predict link quality \nvariation. This approach does not require the use of any external hardware, and relies simply on Signal \nto Noise Ratio (SNR) measurements. The nodes monitor and store the links’ SNR values to their \nneighbors in order to obtain a time series of SNR measurements. When a prediction on the future state \nof a link is required, the node looks for similar SNR patterns to the current situation in the past (time \nseries) using a cross-correlation function. The matches found are then used as a base for the prediction. \nMost link stability estimation schemes in the literatures [10−14] are based on low layer measurements, \nsuch as RSS; however, collecting and analyzing low layer data is generally more complex than only \nconsidering the link connectivity, which is accomplished on the network layer. Several literatures focus \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 4 \non Global Positioning System (GPS) based mobility prediction schemes [15-16]. These methods predict \nthe concrete geographical locations and velocity of nodes; however, GPS devices are not cost effective \nand restricted in many short range or indoor applications. \nIn MANETs, nodes communicate with each other by forming a multi-hop network in a \ndecentralized manner without the aid of any pre-existing infrastructure. Ad-hoc networks have to face \nseveral challenges, such as dynamic topology, real-time communication, resource constraint, \nbandwidth management and packet broadcast overhead, thus making it complicated to design routing \nprotocols [17-22]. There have been many routing protocols developed for MANET over the past few \nyears, which can be generally classified into position-based and topology-based routing protocols. \nPosition-based routing protocols select paths based on geographical information with geometrical \nalgorithms. Topology-based routing protocols, which can be further divided into proactive, reactive \nand hybrid routing protocols [23, 24], select paths based on topological information. In proactive \nrouting protocols such as optimized link state routing (OLSR) and destination sequenced distance \nvector (DSDV), every node knows a route to every other node all the time. There is no latency, \nhowever permanent maintenance of unused routes increases the control overhead. This type of \nprotocols suits for the situation of low node mobility and high traffic load. Reactive protocols, such as \nad-hoc on demand distance vector (AODV) and dynamic source routing (DSR), compute a route only \nwhen it is needed. This reduces the control overhead but introduces latency for the first packet to be \nsent due to the time needed for the on-demand route setup. This protocol is appropriate for high node \nmobility and low traffic load. Hybrid routing protocols, such as zone routing protocol (ZRP) and \nhybrid wireless mesh protocol (HWMP), try to combine the advantages of both the philosophies: \nproactive is used for near nodes or often used paths, while reactive routing is used for more distant \nnodes or less often used paths [25, 26]. Many of these routing protocols use shortest path algorithm \nwith minimum hop count to reduce transmission latency and error rate. However, studies show that \nthe shortest path is not necessarily the best path. Selecting a route based on the shortest hop-count \nmetric without considering link stability leads to frequent route failures. Since link stability and \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 5 \nrouting selection have some joint effects on the network performance, these two methods are worth to \nbe studied. \nIn recent years, many literatures focus on routing protocols design. In [6], the authors proposed a \nQuality of Service (QoS) routing with throughput and delay constraints, and then the destination can \nselect the link with the highest route stability value to reply to the source each time. In [18], the \nauthors proposed a scheme for multipath multicast routing in MANETs to select neighbors with high \nreliability pair factor based on minimum value of reliability pair factor of a path. However, the link \nstate varies with time, and it is rather complex to select paths for dynamic topology for [6] and [18]. \nIn [22], the authors presented a greedy-based backup routing (GBR) protocol to improve route \nstability in mobile ad hoc networks. Route discovery for the primary path is mainly based on greedy \nforwarding; therefore, a primary path established in GBR approximately achieves the smallest hop \ncount which turns back to the hop based routing strategy.  \nIn this paper, we propose a scheme to estimate the link stability and apply the proposed estimation \nscheme to routing protocols. We first propose a mathematical model for link stability, and then \npropose a method to estimate the parameters of the link connectivity model. A variable sized sampling \nwindow is adopted to make the estimation scheme applicable for both stationary and non-stationary \nscenarios. Afterwards, we propose a routing method with link stability estimation and compare the \npacket transmission effectiveness among different routing protocols in MANETs. Different from \nprevious literatures that focused on routing design, our routing protocol neither adopts the shortest \nhop count method nor greedily selects the links with highest route stability value, it dynamically \nswitches among different routing protocols according to the channel state, which is easy for \nimplementation. The proposed link stability estimation and routing schemes are not restricted to \ncertain network topologies, and they do not require prior information about the mobility model of the \nnetwork. \nThe remainder of this paper is organized as follows. Section 2 describes the link connectivity model \n(LCM) used for link stability estimation. Section 3 gives detailed description of the proposed link \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 6 \nstability estimation scheme and presents a routing method with link stability estimation. Section 4 \nshows the simulation results. Section 5 summarizes the paper. \n2 Link Connectivity Model \nIn this section, we propose a probabilistic model to describe the connectivity of a link. Based on \nthis model, the metrics to evaluate the link stability are outlined. \n2.1 Model Construction \nLet i and j denote arbitrary nodes in the network, a packet sending from Node i to Node j at an \narbitrary time instant t can be successfully received by Node j if the following Signal-to-Interference-\nplus-Noise Ratio (SINR) condition is satisfied: \n j\niktVk\nkjkj\niji\nI\ntgtPN\ntgtP β≥\n+ ∑\n≠∈ ),(\n)()(\n)()(\n  (1) \nwhere Nj denotes the additive background noise at the receiver of Node j, Pi (t) denotes the transmitting \npower of Node i, gij (t) is the channel gain from Node i to Node j, VI (t) is the set of interfering nodes \ntransmitting at the same time instant t, and βj is the SINR requirement for the receiver of Node j to \nsuccessfully decode a packet. When (1) is satisfied, the link from Node i to Node j (denoted by i → j) \nis connected at time t. If the two nodes’ wireless channel characteristics are almost identical, Link i → \nj can be considered as bi-directional link denoted by Link i ↔ j. \nDue to the random properties of the wireless channels, the connectivity of Link i ↔ j can be \ndenoted by the process \n \n⎩⎨\n⎧\n↔\n↔\n=\n       at time connected is Link ,1\n at time connectednot  is Link ,0\n)(\nt ji \nt ji \ntX ij  (2) \nAs in [27, 28], when only considering the radio channel characteristics, Xij (t) has the Markov \nproperty. The link connectivity is mainly affected by the radio channel characteristics when both nodes \nare fixed or move at a fairly low speed. The probability of incorrect reception of a packet is generally \nindicated by the error probability, which is independent of whether the previous packet has been \nreceived successfully.  \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 7 \nWe regard Xij (t) as the output of a two-state continuous-time Markov chain. We call this Markov \nchain the continuous time link connectivity model (CTLCM) for Link i ↔ j. The transition rate from \nState 0 to State 1 is denoted by λij (t), and the transition rate from State 1 to State 0 is denoted by μij (t). \nWhen we sample Xij (t) at a sampling interval Δt, the resulting random series {Xij (kΔt)} can be \nregarded as the output of a two-state discrete-time Markov chain. This Markov chain is called the \ndiscrete time link connectivity model (DTLCM) for Link i ↔ j. The transition probability from State m \nto State n (m, n = 0, 1) is denoted by pmn (t).  \nIn the following context, the subscripts i and j are omitted where no ambiguity occurs and within a \ncertain interval, λ(t), μ (t) and pmn (t) are independent of t, which can be denoted by λ, μ and pmn \nrespectively. Meanwhile, by sampling the output Xij (t) of the CTLCM, we can obtain the transition \nprobability matrix of the DTLCM: \n ⎥⎦\n⎤⎢⎣\n⎡\n+−\n−+\n+\n=Δ Δ+−Δ+−\nΔ+−Δ+−\ntt\ntt\nt )()(\n)()(\nee\nee1)( μλμλ\nμλμλ\nμλμμ\nλλλμ\nμλP  (3) \nUnder the above described model, we can estimate the future link states based on the transition \nprobabilities at present time, as long as the time we estimate lies in the same interval as the present \ntime. \n2.2 Link Stability Metrics \nTo define the link stability metrics, we start with the following definition. \nDefinition 1: A continuous time (or respectively, discrete time) Markov chain of finite states is \npiecewise homogeneous when its transition rates (or probabilities) are constant over partitioned time \nintervals, but not constant over the whole time axis. More precisely, given a Markov chain with K \nstates, let ρm ′n ′ (t) denote the transition rate (or probability) from State m′ to State n′ (m′, n′ = 0, 1, …, \nK−1), and T1, T2, …, Tq, …, TQ be discrete points on the time axis. If for ),[, 121 +∈∀ qq TTtt , ]1,1[ −∈ Qq , \nwe have ρm ′n ′ (t1) = ρm ′n ′ (t2); but for ),[ 11 +∈∀ qq TTt , ),[ 12 qq TTt −∈ , ]1,2[ −∈ Qq , there exists some m′, n′, \nsuch that  ρm ′n ′ (t1) ≠ ρm ′n ′ (t2), then the corresponding Markov chain is piecewise homogeneous, and the \nintervals [Tq, Tq+1) are called the homogeneous intervals of the Markov chain. \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 8 \nSince the CTLCM is a continuous time Markov chain, the waiting time of a state transition has an \nexponential distribution [29]. Let η01 denote the waiting time of State 0 to State 1 transition, and η10 \ndenote the waiting time of State 1 to State 0 transition. Within a specific homogeneous interval, we \nhave \n ⎪⎩\n⎪⎨⎧\n=>\n=>\n−\n−\nμτ\nλτ\nτη\nτη\ne}{\ne}{\n10\n01\nP\nP\n (4) \nwhere τ ≥ 0. \nLet tp denote the present time, we evaluate the link stability probability when the link remains \nconnected for time τ (called the remaining probability): \n τμτητ )(10 e}{)( p\nt\nremain Pp\n−\n=>=  (5) \nand the probability the link recovers within time τ after a link failure (called the recovering \nprobability): \n τλτητ )(01 e1}{)( p\nt\nrecover Pp\n−\n−=≤=  (6) \npremain (τ) is generally more significant than precover (τ), since the probability of the link remains \nconnected for a given time τ can be used to estimate how long the link is connected. However, in many \nscenarios of ad-hoc networks, especially when hidden terminals are present, collisions are likely to \noccur which may result in a temporal loss of packets. Therefore, introducing precover (τ) is necessary as \nit can be adopted to determine whether a link failure is temporal.  \n3 Link Stability Estimation \nThe main task of link stability estimation is to estimate the probabilities denoted in (5) and (6). \nSince the remaining and recovering probabilities for different values of τ have to be estimated, it is \nmore convenient to estimate the transition rates λ(tp) and μ(tp) in the CTLCM and evaluate premain (τ) \nand precover (τ) according to (5) and (6). In this section, we discuss methods to estimate λ(tp) and μ(tp). \nWe first consider the case where all the link state samples are from the homogeneous interval, then \ndiscuss how to deal with samples from different homogeneous intervals, and finally outline the \nprocedure of link stability estimation.  \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 9 \n3.1 Transition Rate Estimation \nTo estimate the transition rates in the CTLCM, we firstly consider that all the link state samples are \nfrom the homogeneous interval tp, and omit the variable tp for simplicity. The time of link connection \nor disconnection has an exponential distribution. Hence, a straightforward idea is to estimate λ and μ \nwith the reciprocal value of the mean continuous connection or disconnection time of a link. However, \nsince we can only obtain the link connectivity at discrete time instants by sending periodically beacons \nin practice, the connection or disconnection time is actually truncated and may increase estimation \nerrors. In this paper, we propose an estimation scheme, and estimate the transition probabilities pmn in \nthe DTLCM. Based on the estimated transition probabilities, the transition rates λ and μ in the CTLCM \ncan be obtained. \n3.1.1 Estimating pmn \nEach node in the network broadcasts beacons periodically to notify its presence to other nodes \nwithin its transmission range. When we set the interval of beacons equal to Δt, we obtain samples of \nthe connectivity of Link i ↔ j based on whether Node i has received a beacon from Node j at each \nsampling instant. And the sampled link connectivity is the output of the DTLCM. Hence, the transition \nprobability pmn can be estimated based on the link state samples at discrete time instants. \nSince the DTLCM within the homogeneous interval containing tp is a two-state discrete-time \nhomogeneous Markov chain, its transition probability matrix can be expressed by: \n ⎥⎦\n⎤⎢⎣\n⎡\n=\n1110\n0100\npp\npp\nP  (7) \nwhere pm0 + pm1 = 1. Let \n \n⎩⎨\n⎧\n=\n≠\n=\n+\n+\nkk\nkk\nm XX\nXX\nY\n1\n1\n,1\n,0\n, for mXk k =∈∀ ,Z  (8) \nwhere Xk = X (kΔt) and Z is the set of integers. The probability function (p.f.) of Ym is \n ymm\ny\nmmmm pppyf\n−\n−=\n1)1();(  (9) \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 10 \nObviously, Ym has a Bernoulli distribution with parameter pmm. Its expectation μ = pmm, and its \nvariance σ2 = pmm (1 − pmm). \nSince pm0 + pm1 = 1, we only have to estimate p00 and p11, then p01 and p10 can be calculated easily. \nLet t0 be the latest sampling instant in the DTLCM prior to the present time tp, and C(L) = { c(t) : t = t0 \n− LΔt, t0 − (L−1)Δt, … , t0} be the set of sample values, where c(t) ∈ {0, 1} denotes the link \nconnectivity at time t, and L is a positive integer which indicates how many samples we take into \naccount in our estimation. In our preliminary discussion, we regard L as the number of all the stored \nsamples which varies when the samples are from different homogeneous intervals, which will be \ndescribed in Section 3.2. \nSince the future state is determined by the current state in the Markov process, we need to consider \nthe transitions between two consecutive samples rather than the individual sample values. We \ncategorize the transitions into two groups: one group contains the transitions starting at State 0, and the \nother group contains the transitions starting at State 1. Let Nm (L) denote the number of transitions \nstarting at State m in the set C(L), then the frequency of State m to State m transitions in the set C(L) \ncan be denoted by )(Lym . More precisely, \n \n∑\n∑\n=\n=\nΔ−−\nΔ−−⋅Δ−−−\n= L\ni\nL\ni\ntitc\ntitctitc\nLy\n1\n0\n1\n00\n0\n)](1[\n)](1[)])1((1[\n)(  (10) \nand \n∑\n∑\n=\n=\nΔ−\nΔ−⋅Δ−−\n= L\ni\nL\ni\ntitc\ntitctitc\nLy\n1\n0\n1\n00\n1\n)(\n)())1((\n)(  (11) \nThe remaining problem is to perform an estimation of the parameter in the p.f. of a Bernoulli \ndistribution.  \nAlthough the maximum likelihood estimator (MLE) of pmm is )(Lym [30], some problems will arise \nwhen the sample size Nm (L) is small. This problem is more likely to occur shortly after a node \ninitialization, since insufficient link state samples have been collected in that case. A solution to this \nproblem is using the Bayes estimator which intends to minimize the expected mean squared error of the \nestimated parameter. The Bayes estimator of pmm [30] is \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 11 \n \n2)(\n1)()(ˆ\n+\n+\n=\nLN\nLyLNp\nm\nmm\nmm  (12) \nWhen ∞→)(LNm , )(ˆ Lyp mmm = , which is equal to the MLE value of pmm and is unbiased.  \n3.1.2 Estimating λ and μ \nAfter we estimated pmn in the DTLCM, we propose a method to estimate the transition rates λ and μ \nin the CTLCM. The estimation of the transition rates is based on the condition that the link break and \nreconnect (or vice versa) with a negligible probability α1 within one sampling interval Δt. We make \nsome restrictions on the value of the sampling interval Δt to satisfy the condition above, so that \n 1101 }1)0(|1)(,{ αη ≤==ΔΔ≤= XtXtPpe  (13) \nand 1010 }0)0(|0)(,{ αη ≤==ΔΔ≤= XtXtPpe  (14) \nFrom (3), we can obtain \nt\ne\ne\ne\np\ntXtXtPp\nXtXtPp\nXtXPtp\nΔ−+=\n=Δ⊆Δ>=Δ>+=\n==ΔΔ>+=\n==Δ=Δ\nμ\nηη\nη\ne\n})1)({}({   }1)0(|{\n}1)0(|1)(,{\n}1)0(|1)({)(\n1\n10101\n101\n11\n             (15) \nHence, 1\n)(\n111 e\nee)( α\nμλ\nμλ μμλμ ≤−\n+\n+\n=−Δ= Δ−\nΔ+−\nΔ− t\nt\nt\ne tpp  (16) \nSimilarly, 1\n)(\n000 e\nee)( α\nμλ\nλμ λμλλ ≤−\n+\n+\n=−Δ= Δ−\nΔ+−\nΔ− t\nt\nt\ne tpp  (17) \nEqs. (16) and (17) have no analytical solution. For applications with strict requirements, they can be \nsolved numerically and even in real time. While in ordinary cases, Δt can be set to a fixed value that is \nselected experimentally. Under the above condition, the transition probability matrix of the DTLCM \ncan be simplified as \n ⎥⎦\n⎤⎢⎣\n⎡\n−\n−\n=⎥⎦\n⎤⎢⎣\n⎡\nΔ>Δ≤\nΔ≤Δ>\n=Δ Δ−Δ−\nΔ−Δ−\ntt\ntt\ntPtP\ntPtP\nt μμ\nλλ\nηη\nηη\nee1\ne1e\n}{}{\n}{}{\n)(\n1010\n0101P  (18) \nThen the transition rates in the CTLCM can be easily solved by \n \n⎩⎨\n⎧\nΔ−=\nΔ−=\ntp\ntp\n/)(ln\n/)(ln\n11\n00\nμ\nλ\n (19) \nEq. (19) indicates that λ is a function of p00 and μ is a function of p11. The MLE estimators of λ and \nμ under the condition ∞→)(LNm can be obtained by \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 12 \n ⎪⎩\n⎪⎨⎧\nΔ−=\nΔ−=\ntp\ntp\n/)ˆ(lnˆ\n/)ˆ(lnˆ\n11\n00\nμ\nλ  (20) \nIf 00pˆ and 11pˆ maximize the likelihood function, λˆ and μˆ  maximize the likelihood function. With \nordinary values of Nm (L), λˆ and μˆ are also likely equal to their actual values since they are evaluated \nfrom 00pˆ and 11pˆ that have minimal expected mean squared estimation errors. \nBy substituting (20) into (5) and (6), premain (Δt) and precover (Δt) can be directly evaluated from the \nestimated transition probabilities 00pˆ and 11pˆ , getting rid of the assumption that waiting time of link \ntransitions are exponentially distributed in prior literatures. From the simulation results in Section 5, \nwe will see that even with a non-Markov process, our proposed method can still make relatively correct \nestimation of premain (τ) and precover (τ) when τ is small. \n3.2 Sampling window size selection \nNow we consider the case where the collected samples are from different homogeneous intervals. \nThis may happen when the node changes its mobility within the time of its operation, which causes a \nchange in the transition rates λ and μ and the transition probabilities pmn respectively.  \nSince the exact boundary points between homogeneous intervals are difficult to find, and in some \ncases the speed of a node is a continuous function of time or the variation of the actual transition \nprobabilities are so slight that they are not observable from the samples. We can regard those samples \nthat are dramatically different from the recent samples from the previous homogeneous interval. \nTherefore, the boundary point can be seen as the time instant when λ, μ and pmn significantly vary from \ntheir previous values or the time instant when slight variations add up to a significant variation.  \nThe problem of finding the latest boundary point can be regard as choosing an appropriate sampling \nsize L. The time interval in which the link state is sampled called sampling window which is equal to \nLΔt, should be set appropriately to balance between the number of samples and the transient response \nof a dramatic change in link stability. In other words, when the channel state is relative stability, L \nshould increase and the variance of the estimated parameters can be reduced. When the channel state \nchanges dramatically, L should decrease to avoid the influence of past state.  \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 13 \nWe adopt the hypothesis testing method to find the optimal L. Its main idea is to expand the \nsampling window in the direction of past time, i.e. increase L by one at each step. After each expansion, \nthe transition probabilities pmn(tp) are estimated. Then we test the hypothesis with H0: {the transition \nprobabilities throughout the sampling window are equal to the estimated values}, and continue the \nexpansion until the hypothesis is rejected. The hypothesis value of L before the rejection is returned as \nthe optimal sample size, and the transition probabilities pmn (tp) estimated with that value of L are used \nfor further processing. \n \nFig. 1. Example of homogeneous intervals and sampling window. \nLet Ym1, Ym2, … , YmN be the N samples from the sample space of Ym. Since Ym has a Bernoulli \ndistribution as described in Section 3.1, from the Central Limit Theorem, we know that when N is \nsufficiently large, the distribution of \n \nNpp\npY\nmmmm\nmmm\n/)1( −\n−  (21) \nis approximately a standard normal distribution. Hence, considering the samples from C(L), the \nrejection region for H0: {pmm = pmm0} with significant level α2 is \n \n⎪⎭\n⎪⎬\n⎫\n⎪⎩\n⎪⎨\n⎧\n>\n−\n−\n− 2/1\n00\n0\n2)(/)1(\n)(: αuLNpp\npLy\nmmmmm\nmmmy  (22) \nwhere uα is the α-quantile of the standard normal distribution, and pmm0 is the hypothesis testing \nprobability for H0. \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 14 \nTo test whether the estimated values 00pˆ and 11pˆ are correct within the entire sampling window, we \nconsider a union set of hypothesis. The rejection region for the hypothesis H0: { 0000 pˆp = and \n1111 pˆp = within C(L)} is \n UU\nL\nl m mmmmm\nmmm u\nlNpp\nply\n0\n1\n0\n2/1 2)(/)ˆ1(ˆ\nˆ)(:\n= =\n− ⎪⎭\n⎪⎬\n⎫\n⎪⎩\n⎪⎨\n⎧\n>\n−\n−\nαy  (23) \nEq. (23) indicates that we test whether the hypothesis is accepted for all possible intervals from the \npresent time tp on to the past direction, rather than only the entire sampling window. The reason is \nwhen the sampling window contains different homogeneous intervals, the estimation may be correct for \nthe entire sampling window, however it is less likely to be correct for a subinterval containing tp. \nIn order to make the Central Limit Theorem be applicable, we should assure a sufficiently large \nNm(l), and the rejection region for H0: { 0000 pˆp = and 1111 pˆp = within C(L)} can be modified as \n UU\nL\nl m\nminm\nmmmmm\nmmm NlNu\nlNpp\nply\n0\n1\n0\n2/1 )(,\n)(/)ˆ1(ˆ\nˆ)(:\n2\n= =\n− ⎪⎭\n⎪⎬\n⎫\n⎪⎩\n⎪⎨\n⎧\n>>\n−\n−\nαy  (24) \nwhere Nmin is the minimum number of transitions (which is obtained from the samples as described in \nSection 3.1) starting from State 0 or State 1. When the sample size is too small, (22) and (23) make no \nsense since they are derived based on the Central Limit Theorem. Hence, the hypothesis can only be \nrejected when the amount of samples is sufficient.  \n3.3 The Estimation Process \nDefine that Node i is responsible for the stability of Link i ↔ j, according to the above discussions, \nthe pseudo code of the estimation process can be formulated by Algorithm 1. \nLmax is the total number of stored samples. In addition, the samples outside the sampling window are \nfreed from memory when they have never been included in the sampling window for a pre-defined time \nperiod. The hypothesis testing process contains one loop itself and the complexity of the above \nestimation process is O(L2). \n \n \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 15 \nAlgorithm 1: Pseudo code of estimation process \nfor L = 1, 2, …, Lmax do \n{Estimate 00pˆ and 11pˆ  according to (12)}; \nif (H0: { 0000 pˆp = and 1111 pˆp = within C(L)}  \nis rejected according to (24)) break; \n0000 pˆp st ← , 1111 pˆp st ← ; \nstpp 0000ˆ ← , stpp 1111ˆ ← ; \n{Estimate λˆ and μˆ  according to (20)}; \n           {Evaluate the stability according to (5) and (6)}; \nThe estimation process can be simplified by solving an acceptance interval [pmm1, pmm2] for a \nspecific set of samples, so that if the estimated transition probability ],[ˆ 21 mmmmmm ppp ∈ , H0: {pmm = mmpˆ } \nis accepted. This interval can be easily solved from (22): \n \n⎪⎩\n⎪⎨\n⎧\n−+−=\n−−−=\n)2/()4(\n)2/()4(\n2\n2\n2\n1\naacbbp\naacbbp\nmm\nmm  (25) \nwhere 2 2/1 2)( α−+= uLNa m , ])()(2[\n2\n2/1 2α−+−= uLyLNb mm  and )()(\n2 LyLNc mm= . \nTo simplify the estimation process further, we regard samples with the same link state as samples \nfrom the same homogeneous interval, which is a general case since we have to sample at a much higher \nrate than the mobility variation in order to estimate the transition probabilities. With this consideration, \nthe probabilities only need to be estimated when a change in link connectivity occurs at the left \nboundary point of the sampling window. Otherwise, the sampling window continues to expand. By this \nmethod, the computational time of the estimation process can be saved. The resulting simplified \nestimation process is listed in Algorithm 2. The complexity of the simplified estimation process is O(L). \nThe estimation results of these two schemes are the same. \n \n \n \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 16 \nAlgorithm 2: Pseudo code of simplified estimation \nprocess \n[p001st, p002st] ← [0,1], [p111st, p112st] ← [0,1]; \nfor L = 1, 2, …, Lmax do \n if (c(t0 − LΔt) = c(t0 − (L − 1) Δt)) continue; \n {Estimate 00pˆ and 11pˆ  according to (12)}; \n {Calculate p001, p002, p111 and p112 from (24)}; \n ],[],[],[ 002001002001002001 pppppp stststst ∩← , \n ],[],[],[ 112111112111112111 pppppp stststst ∩← ; \n if (( ],[ˆ 00200100 stst ppp ∉ and N0(L) > Nmin) \n  or ( ],[ˆ 11211111 stst ppp ∉  and N1(L) > Nmin))  \n  break; \n 0000 pˆp st ← , 1111 pˆp st ← ; \nstpp 0000ˆ ← , stpp 1111ˆ ← ; \n{Estimate λˆ and μˆ  according to (20)}; \n{Evaluate the stability according to (5) and (6)}; \n3.4 Routing Selection Strategy \nIn this section, we first propose a method to classify the nodes based on the estimated link stability. \nThen we present a routing selection strategy with link stability estimation. A link can be considered as \nstable if the link stability estimation value is higher than a upper bound value 3α , and the link can be \nconsidered as unstable if the link stability estimation value is below a lower bound 4α . Otherwise, we \nconsider the link is relatively stable. \nIn order to make full use of the advantages of different routing protocols under different link \nstability states, we propose a routing combination protocol containing different protocols, i.e. OLSR, \nAODV and ZRP. Our Routing Combination Protocol under Link Estimation (RCPLE) routing algorithm \nis carried out as follows: (1) we firstly conduct link stability estimation according to Section 3.1, and \nclassified the nodes into fixed, relatively fixed and mobility nodes; (2) we adopt different sampling \nwindows based on the estimated link stability according to Section 3.2; (3) we switch over different \nrouting protocols dynamically according to the estimated link states; for example, if the link is \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 17 \nconsidered stable, we adopt the OLSR protocol; we choose the AODV protocol when the link is \nconsidered unstable, and we employ the ZRP protocol in other situations. \n4 Simulation Results \nThe performance of the proposed estimation scheme is evaluated using OPNET simulations. A 1000 \n× 1000 m2 rectangular network area is considered. The transmission range of the nodes in the network \nis 250 m, and the data rate is 11 Mbps. In the simulations, we take α2 = 0.02, Δt = 3 s and Nmin = 100. \nThe values of α2 and Nmin determine the length of homogeneous interval, if α2 is too large or Nmin is too \nsmall, the homogeneous interval will be very small, which results in inaccuracy of the estimation. On \nthe contrary, if the homogeneous interval is too large, it is hard to track the real-time variations of the \nlink. The value of Δt should be set small enough to guarantee the link break and reconnect (or vice \nversa) with a negligible probability, however Δt should not be too small, because the estimated value is \nthe most accurate when τ = Δt. \n4.1 Performance of the Link Stability Estimation Scheme \nWe first evaluate the performance of the link stability estimation scheme by placing only two nodes \nin the network area, where one node is placed at the center of the network area and the other node \nmoves according to a specific mobility model. This is a simplification of multiple-node networks. Each \nsimulation was run with 20 different random seeds to evaluate the overall performance. The first \nmobility model we use is the random walk model with the epochs (i.e. the time the velocity remains \nunchanged) being exponentially distributed with mean 20 s, as suggested in [10]. We consider the case \nin non-stationary networks where the moving node moves at a fixed speed within a specific time \ninterval, but changes its speed among different intervals. The simulation length is set to 50000 s. In \neach interval of 10000 s, the moving node moves at a fixed speed. The set of speed in different time \nintervals is {10, 30, 5, 20, 40} m/s. The average values of the estimated transition rates are compared \nwith their actual values which are computed from the reciprocal mean connected and unconnected time \nof the link. It can be observed from Fig. 2 that the estimated values follow the actual values despite of \nsome slight deviation. One reason for this deviation is due to the assumption that pe0 and pe1 are small. \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 18 \nAnother reason is that the random process describing the link connectivity has the Markov property, is \nnot exactly true in real scenarios. Hence, the transition rates computed are not exactly equal to the \nvalues estimated by (20). \n0\n0.05\n0.1\n0 10000 20000 30000 40000 50000\nTr\nan\nsit\nio\nn \nR\nat\ne \n(s\n−\n1 )\nSimulation Time (s)\nPredicted λ Predicted μ\nActual λ Actual μ\nλ μ\nμ\n \nFig. 2. Transition rates. \nTo further study the selection of the sampling window, we examine the sampling window length LΔt \nat different simulation time. Fig. 3 shows that after a change in speed, the sampling window length \ngreatly decreases, while it generally increases with time when the speed remains constant. This \ntendency is in our expectation as discussed in Section 3.2. The reason is that the link connectivity may \nchange frequently when the distance between the two nodes fluctuates around the transmission range; \nand when the distance is significantly smaller or larger than the transmission range, the link may \nremain connected or unconnected for a long time since the speed is low. This may lead the link \nstability estimation scheme to decrease the sampling window length in order to cover the most recent \ntrend. \n0\n2000\n4000\n6000\n8000\n10000\n0 10000 20000 30000 40000 50000\nSa\nm\npl\nin\ng \nW\nin\ndo\nw\nLe\nng\nth\n (s\n)\nSimulation Time (s)  \nFig. 3. Sampling window length. \nAnother important issue we have to study is whether the estimated remaining probability premain (τ) \nand recovering probability precover (τ) are equal to their actual values. We consider a stationary network \nand set the simulation length to 10000 s. The estimated probabilities are compared with the actual \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 19 \nfrequency the link remains connected for time τ or recovers within time τ during simulation. Fig. 4 (a) \nshows the results when the moving node moves at a fixed speed v = 10 m/s, Fig. 4 (b) shows the results \nwhen v is uniformly distributed between 10 m/s and 19 m/s, and in Fig. 4 (c) v = 20 m/s. We also \nconsider the scenario where two nodes move together in the network area, one at speed v1 = 10 m/s and \nthe other at v2 = 20 m/s as shown in Fig. 5. Then we use the random waypoint model as the mobility \nmodel for the moving node and examine the performance with different pause time, as shown in Fig. 6. \nIt can be observed that all the estimation results are approximately equal to the actual situations. \nDeviations are also due to pe0 and pe1 as discussed above. \n0\n0.2\n0.4\n0.6\n0.8\n1\n0 20 40 60 80 100\nPr\nob\nab\nili\nty\nTime τ (s)\nPredicted recovery Predicted remaining\nActual recovery Actual remaining\n \n(a) \n0\n0.2\n0.4\n0.6\n0.8\n1\n0 20 40 60 80 100\nPr\nob\nab\nili\nty\nTime τ (s)\nPredicted recovery Predicted remaining\nActual recovery Actual remaining\n \n(b) \n0\n0.2\n0.4\n0.6\n0.8\n1\n0 20 40 60 80 100\nPr\nob\nab\nili\nty\nTime τ (s)\nPredicted recovery Predicted remaining\nActual recovery Actual remaining\n \n(c) \nFig. 4. Remaining and recovering probabilities with random walk model, one node moving: (a) v = 10 m/s, (b) v ∈ [10, 19] \nm/s, (c) v = 20 m/s. \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 20 \nThe above results are obtained with the random walk or random waypoint model. The next question \nis whether the proposed scheme can be generalized to other mobility patterns, especially when the \nnodal mobility is not random. We still place one node fixed at the center. But we set the moving node \nto move in a “ ” formed trajectory. In this scenario, the actual link connectivity is a periodic function \nof time and is no longer random. Fig. 6 shows the results for this scenario when the moving node \nmoves at speed v = 10 m/s and v = 20 m/s respectively. The deviations are large at some values of τ, \nsince the Markov process does not hold in this scenario. However, the estimated values are relatively \ncorrect when τ is small. This makes sense in real applications since, on one hand, the flow duration of \napplications is generally short; on the other hand, the exact remaining probability is not quite important \nwhen the probability is lower than a certain threshold, because we consider the link as unstable anyway. \nAnd remaining probabilities are more significant in link stability evaluation. \n0\n0.2\n0.4\n0.6\n0.8\n1\n0 20 40 60 80 100\nPr\nob\nab\nili\nty\nTime τ (s)\nPredicted recovery Predicted remaining\nActual recovery Actual remaining\n \nFig. 5. Remaining and recovering probabilities with random walk model, two nodes moving,  v1 = 10 m/s, v2 = 20 m/s. \n0\n0.2\n0.4\n0.6\n0.8\n1\n0 20 40 60 80 100\nPr\nob\nab\nili\nty\nTime τ (s)\nPredicted recovery Predicted remaining\nActual recovery Actual remaining\n \n(a) \n \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 21 \n0\n0.2\n0.4\n0.6\n0.8\n1\n0 20 40 60 80 100\nPr\nob\nab\nili\nty\nTime τ (s)\nPredicted recovery Predicted remaining\nActual recovery Actual remaining\n \n(b) \nFig. 6. Remaining and recovering probabilities with non-random movement, one node moving: (a) v = 10 m/s, (b) v = 20 m/s. \n4.2 Performance of the Routing Selection Scheme \nTo evaluate the performance of the routing selection scheme, we place 40 nodes in the network area. \nWe divide the nodes into two groups. Group I contains 20 nodes. These 20 nodes are selected as fixed \nnodes, and formed a 4 × 5 grid with 175 m separation between neighboring nodes. The nodes from \nGroup I represent a fixed backbone network and remain in their locations throughout the whole \nsimulation. Group II contains the other 20 nodes. These are initially placed randomly in the network \narea. They move according to the random walk model at an average speed which is set to {0.5, 1, 2, 5, \n10, 20, 40} m/s respectively in these routing protocols. Each simulation was run with 100 different \nrandom seeds to obtain the overall performance. Two nodes (one from Group I and one from Group II) \nare selected to generate packets and send them to random destinations, at a packet rate 0.5 pk/s. The \nlength of each packet is 1024 bit.  \nWe first compare the packet delivery rate among three different routing protocols, namely, OLSR, \nAODV and ZRP protocols, which are representations of proactive, reactive and hybrid routing \nprotocols respectively. Packet delivery rate (or success rate) is defined as the ratio of the number of \npackets received to the number of packets sent [7]. It can be observed from Fig. 7 that performances of \nthe OLSR protocol are the best when nodes are static, however, as the node speed increases, the packet \ntransmission rate decreases. This is because OLSR is a proactive routing protocol, and each node \nconstructs and maintains routing information of all the other nodes no matter whether there exists \ncommunication demands or not. For the AODV protocol, when the nodes are static, the performance is \nnot better than the OLSR protocol, when the speed increases to 15m/sec, the packet transmission rate \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 22 \nreaches the maximum value. That is because AODV transmits data according to whether there are \ndemands or not, and the routing table is construct and maintained on demand. For the ZRP protocol, we \ncan draw the conclusion that when the nodes are slight mobile, the packet transmission rate achieves \nthe max value. This is because it combines proactive and reactive protocols in an organic way and \nadopt different routing protocols in different scenarios. As the speed extends to some extent, the \ndelivery rates of these three single protocols descend quickly due to the highly varying topology of the \n40-node network.  \n0 2 5 15 25 40\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nSpeed (m/s)\nPa\nck\net\n d\nel\niv\ner\ny \nra\nte\n \n \nRCPLE\nAODV\nZRP\nOLSR\n \nFig. 7. Packet delivery rate for different random walk speeds. \nSince our proposed routing strategy RCPLE is a combination of these three different routing \nprotocols and one of them is selected according to the estimated link stability, the choice of the upper \nand lower bounds affects the performance of the network. We have run simulations to compare the \nperformance of the network for different upper and lower bounds, and found that for the network we \nsimulated, good packet delivery performance is provided when the upper bound 3α  equals to 0.7 and \nthe lower bound 4α  equals to 0.4 . Although ZRP is a hybrid routing protocol combining proactive and \nreactive routing protocols, it only considers regional radius (measured by the hop number) and neglects \nthe link connection or stability within each hop. Our proposed routing algorithm selects paths \naccording to the estimated link stability and changes the sampling window size according to the packet \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 23 \ndelivery rate, which is more dynamic than the ZRP protocol. Therefore, the proposed protocol \noutperforms ZRP as well. \n5 Conclusions \nThe link stability estimation scheme we proposed in this paper is based on link connectivity changes \nand is easy to implement. Compared with link associativity based estimation schemes, the proposed \nscheme focus on a probabilistic model and the estimation results have explicit meanings both in theory \nand practice. Meanwhile, the proposed scheme is simpler than the methods using GPS or low layer \nmeasurements, and it is not restricted to a specific network topology. We adopted a variable sized \nsampling window which is more flexible for the dynamic link state, and is also a major contribution in \nour work.  \nSince link state has a pervasive impact on routing process, we proposed a routing method which \nadjusts its operating mode based on the estimated link stability. Simulation results show that the \nproposed stability estimation scheme is able to estimate the link stability in both stationary and non-\nstationary scenarios and the proposed routing method enhances packet delivery rate effectively in ad-\nhoc networks. Future work can be focused on reducing the fluctuation of the sampling window length. \nReferences \n[1] S. Wang, Q. Song, J. Feng, and X. Wang, “Predicting the link stability based on link connectivity \nchanges in mobile ad hoc networks,” in Proc. IEEE International Conference on Wireless \nCommunications, Networking and Information Security (WCNIS), pp.409-414, 2010. \n[2] E. Alotaibi and B. Mukherjee, “A survey on routing algorithms for wireless ad-Hoc and mesh \nnetworks”, Computer Networks, vol. 56, no.2, pp. 940–965, 2012. \n[3] L. Guo, J. Cao, H Yu, L. Li, “Path-based routing provisioning with mixed shared protection in \nWDM mesh networks”, Journal of Lightwave Technology, vol. 24, no. 3, pp. 1129-1141, 2006. \n[4] L. Guo. “LSSP: A novel local segment-shared protection for multi-domain optical mesh networks”, \nComputer Communications, vol. 30, no. 8, pp. 1794-1801, 2007. \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 24 \n[5] P. Lopez, R. Tinedo, and J. Alsina, “Moving routing protocols to the user space in MANET \nmiddleware”, Journal of Network and Computer Applications, vol. 33, no. 5, pp. 588-602, 2010. \n[6]. N. Sarma and S. Nandi, “Route stability based QoS routing in mobile ad-hoc networks”, Wireless \nPersonal Communications, vol. 54, no.1, pp. 203–224, 2010. \n[7] A. Bamis, A. Boukerche, I. Chatzigiannakis and S. Nikoletseas, “A mobility aware protocol \nsynthesis for efficient routing in ad-hoc mobile networks”, Computer Networks, vol. 52, no.1, pp. \n130–154, 2008. \n[8] D. Floriano, F. Guerriero, and P. Fazio, “Link-stability and energy aware routing protocol in \ndistributed wireless networks”, IEEE Transcations on Parallel and Distributed Systems, vol. 23, no. \n4, pp. 713-726, 2012. \n[9] A. Giovanidis and S. Stanczak, “Stability and Distributed Power Control in MANETs with Per \nHop Retransmissions”, IEEE Transcations on Communications, vol. 59, no. 6, pp. 1632-1643, \n2011. \n[10] S. Jiang, D. He and J. Rao, “A prediction-based link availability estimation for routing metrics in \nMANETs”, IEEE/ACM Trans. on Networking, vol. 13, no. 6, pp. 1302–1312, 2005. \n[11] J. Torkestani and M. Meybodi, “A link stability-based multicast routing protocol for wireless \nmobile ad-hoc networks”, Journal of Network and Computer Applications, vol. 34, no. 4, pp. \n1429-1440, 2011. \n[12]. R. Biradar, S. Manvi and M. Reddy, “Link stability based multicast routing scheme in MANET”, \nComputer Networks,  vol. 54, no. 7, pp. 1183-1196, 2010. \n[13] K. Farkas, T. Hossmann, F. Legendre, B. Plattner and S. Das, “Link quality prediction in mesh \nnetworks”, Computer Communications, vol. 31, no. 8, pp. 1497–1512, 2008. \n[14] R.Vadivel and V. Bhaskaran, “Adaptive reliable routing protocol using combined link stability \nestimation for mobile ad-hoc networks”, in Proc. AIP Conference, pp. 625-632, 2010. \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 25 \n[15] T. Anagnostopoulos, C. Anagnostopoulos and S. Hadjiefthymiades, “An adaptive location \nprediction model based on fuzzy control”, Computer Communications, vol. 34, no. 7, pp. 816–834, \n2011. \n[16] R. Huang and G. Záruba, “Location tracking in mobile ad-hoc networks using particle filters”, \nJournal of Discrete Algorithms, vol. 5, no. 3, pp. 455-470, 2007. \n[17] N. Weng and J. Yang, “A cross-layer stability-based routing mechanism for ultra wideband \nnetworks”, Computer Communication, vol. 33, no. 18, pp. 2185-2194, 2010. \n[18] R. Biradar and S. Manvi, “Neighbor supported reliable multipath multicast routing in MANETs”, \nJournal of Network and Computer Applications, vol. 35, no. 3, pp. 1074-1085, 2012. \n[19]. Z. Ning, L. Guo, Y. Peng, and X. Wang, “Joint scheduling and routing algorithm with load \nbalancing in wireless mesh networks”, Computers and Electrical Engineering, vol. 38, no. 3, pp. \n533-550, 2012. \n[20] L. Guo, L. Zhang, Y. Peng, J. Wu, X. Zhang, W. Hou and J. Zhao, “Multi-path routing in spatial \nwireless ad hoc networks”, Computers and Electrical Engineering, vol. 38, no. 3, pp. 473–491, \n2012. \n[21] J. Wang, Y. Liu and J. Yu, “Building a trusted route in a mobile ad hoc network considering \ncommunication reliability and path length”, Journal of Network and Computer Applications, vol. \n34, no. 4, pp. 1138-1149, 2011. \n[22] W. Yang, X. Yang, S. Yang, and D. Yang, “A greedy-based stable multi-path routing protocol in \nmobile ad hoc networks”,  vol. 9, no. 4, pp. 662-674, 2011. \n[23] Y. Zhang, J. Luo and H. Hu, “Wireless mesh networking: architectures, protocols and standards”, \nAuerbach Publications, 2007. \n[24]  M. Kharraz, S. Hamid, and Z. Albert, “On-demand multicast routing protocol with efficient route \ndiscovery”,  Journal of Network and Computer Applications, vol. 35, no. 3, pp. 942-950, 2012. \n[25] F. Li and Y. Wang, “Routing in vehicular ad hoc networks: A survey”, IEEE Vehicular \nTechnology Magazine, vol. 2, no. 2, pp. 12-22, 2007. \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n 26 \n[26] L. Guo, Y. Peng, X. Wang, D. Jiang and Y. Yu, “Performance evaluation for on-demand routing \nprotocols based on OPNET modules in wireless mesh networks”, Computers and Electrical \nEngineering, vol. 37, no. 1, pp. 106-114, 2011. \n[27] S. Hwang and D. Kim, “Markov model of link connectivity in mobile ad-hoc networks”, \nTelecommunication Systems, vol. 34, no. 1, pp. 51–58, 2007. \n[28] Z. Ye and A. Abouzeid, “Optimal stochastic location updates in mobile ad hoc networks”, IEEE \nTransactions on Mobile Computing, vol. 10, no. 5, pp. 638-652, 2011. \n[29] A. Papoulis and S. Pillai, “Probability, Random Variables and Stochastic Processes”, New York: \nMcGraw-Hill Companies, Inc., 2002. \n[30] G. Casella and R. L. Berger, “Statistical Interference, 2nd Edition”, Singapore: Thomson \nLearning Asia, 2002. \n \nNOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. \nChanges resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms  \nmay not be reflected in this document.  \nChanges may have been made to this work since it was submitted for publication.  \nA definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004\n",
            "id": 4349177,
            "identifiers": [
                {
                    "identifier": "9550010",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:spiral.imperial.ac.uk:10044/1/10264",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "1990538053",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "10.1016/j.jnca.2012.08.004",
                    "type": "DOI"
                },
                {
                    "identifier": "192063216",
                    "type": "CORE_ID"
                }
            ],
            "title": "Link stability estimation based on link connectivity changes in mobile ad-hoc networks",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "1990538053",
            "oaiIds": [
                "oai:spiral.imperial.ac.uk:10044/1/10264"
            ],
            "publishedDate": "2012-11-30T00:00:00",
            "publisher": "'Elsevier BV'",
            "pubmedId": null,
            "references": [
                {
                    "id": 17777405,
                    "title": "A cross-layer stability-based routing mechanism for ultra wideband networks”,",
                    "authors": [],
                    "date": "2010",
                    "doi": "10.1016/j.comcom.2010.09.006",
                    "raw": "N. Weng and J. Yang, “A cross-layer stability-based routing mechanism for ultra wideband networks”, Computer Communication, vol. 33, no. 18, pp. 2185-2194, 2010.",
                    "cites": null
                },
                {
                    "id": 17777415,
                    "title": "A greedy-based stable multi-path routing protocol in mobile ad hoc networks”,",
                    "authors": [],
                    "date": "2011",
                    "doi": "10.1016/j.adhoc.2010.09.004",
                    "raw": "W. Yang, X. Yang, S. Yang, and D. Yang, “A greedy-based stable multi-path routing protocol in mobile ad hoc networks”,  vol. 9, no. 4, pp. 662-674, 2011.",
                    "cites": null
                },
                {
                    "id": 17777399,
                    "title": "A link stability-based multicast routing protocol for wireless mobile ad-hoc networks”,",
                    "authors": [],
                    "date": "2011",
                    "doi": "10.1016/j.jnca.2011.03.026",
                    "raw": "J. Torkestani and M. Meybodi, “A link stability-based multicast routing protocol for wireless mobile ad-hoc networks”, Journal of Network and Computer Applications, vol. 34, no. 4, pp. 1429-1440, 2011.",
                    "cites": null
                },
                {
                    "id": 17777394,
                    "title": "A mobility aware protocol synthesis for efficient routing in ad-hoc mobile networks”,",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1016/j.comnet.2007.09.023",
                    "raw": "A. Bamis, A. Boukerche, I. Chatzigiannakis and S. Nikoletseas, “A mobility aware protocol synthesis for efficient routing in ad-hoc mobile networks”, Computer Networks, vol. 52, no.1, pp. 130–154, 2008.",
                    "cites": null
                },
                {
                    "id": 17777398,
                    "title": "A prediction-based link availability estimation for routing metrics in MANETs”,",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1109/tnet.2005.860094",
                    "raw": "S. Jiang, D. He and J. Rao, “A prediction-based link availability estimation for routing metrics in MANETs”, IEEE/ACM Trans. on Networking, vol. 13, no. 6, pp. 1302–1312, 2005.",
                    "cites": null
                },
                {
                    "id": 17777388,
                    "title": "A survey on routing algorithms for wireless ad-Hoc and mesh networks”,",
                    "authors": [],
                    "date": "2012",
                    "doi": "10.1016/j.comnet.2011.10.011",
                    "raw": "E. Alotaibi and B. Mukherjee, “A survey on routing algorithms for wireless ad-Hoc and mesh networks”, Computer Networks, vol. 56, no.2, pp. 940–965, 2012.",
                    "cites": null
                },
                {
                    "id": 17777402,
                    "title": "Adaptive reliable routing protocol using combined link stability estimation for mobile ad-hoc networks”, in",
                    "authors": [],
                    "date": "2010",
                    "doi": "10.1063/1.3516385",
                    "raw": "R.Vadivel and V. Bhaskaran, “Adaptive reliable routing protocol using combined link stability estimation for mobile ad-hoc networks”, in Proc. AIP Conference, pp. 625-632, 2010. NOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. Changes resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms may not be reflected in this document. Changes may have been made to this work since it was submitted for publication. A definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004  25",
                    "cites": null
                },
                {
                    "id": 17777403,
                    "title": "An adaptive location prediction model based on fuzzy control”,",
                    "authors": [],
                    "date": "2011",
                    "doi": "10.1016/j.comcom.2010.09.001",
                    "raw": "T. Anagnostopoulos, C. Anagnostopoulos and S. Hadjiefthymiades, “An adaptive location prediction model based on fuzzy control”, Computer Communications, vol. 34, no. 7, pp. 816–834, 2011.",
                    "cites": null
                },
                {
                    "id": 17777413,
                    "title": "Building a trusted route in a mobile ad hoc network considering communication reliability and path length”,",
                    "authors": [],
                    "date": "2011",
                    "doi": "10.1016/j.jnca.2010.11.007",
                    "raw": "J. Wang, Y. Liu and J. Yu, “Building a trusted route in a mobile ad hoc network considering communication reliability and path length”, Journal of Network and Computer Applications, vol. 34, no. 4, pp. 1138-1149, 2011.",
                    "cites": null
                },
                {
                    "id": 17777409,
                    "title": "Joint scheduling and routing algorithm with load balancing",
                    "authors": [],
                    "date": "2012",
                    "doi": "10.1016/j.compeleceng.2011.12.001",
                    "raw": ". Z. Ning, L. Guo, Y. Peng, and X. Wang, “Joint scheduling and routing algorithm with load balancing in wireless mesh networks”, Computers and Electrical Engineering, vol. 38, no. 3, pp. 533-550, 2012.",
                    "cites": null
                },
                {
                    "id": 17777401,
                    "title": "Link quality prediction in mesh networks”,",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1016/j.comcom.2008.01.047",
                    "raw": "K. Farkas, T. Hossmann, F. Legendre, B. Plattner and S. Das, “Link quality prediction in mesh networks”, Computer Communications, vol. 31, no. 8, pp. 1497–1512, 2008.",
                    "cites": null
                },
                {
                    "id": 17777400,
                    "title": "Link stability based multicast routing scheme in MANET”,",
                    "authors": [],
                    "date": "2010",
                    "doi": "10.1016/j.comnet.2009.11.005",
                    "raw": ". R. Biradar, S. Manvi and M. Reddy, “Link stability based multicast routing scheme in MANET”, Computer Networks,  vol. 54, no. 7, pp. 1183-1196, 2010.",
                    "cites": null
                },
                {
                    "id": 17777395,
                    "title": "Link-stability and energy aware routing protocol in distributed wireless networks”,",
                    "authors": [],
                    "date": "2012",
                    "doi": "10.1109/tpds.2010.160",
                    "raw": "D. Floriano, F. Guerriero, and P. Fazio, “Link-stability and energy aware routing protocol in distributed wireless networks”, IEEE Transcations on Parallel and Distributed Systems, vol. 23, no. 4, pp. 713-726, 2012.",
                    "cites": null
                },
                {
                    "id": 17777404,
                    "title": "Location tracking in mobile ad-hoc networks using particle filters”,",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1007/11561354_9",
                    "raw": "R. Huang and G. Záruba, “Location tracking in mobile ad-hoc networks using particle filters”, Journal of Discrete Algorithms, vol. 5, no. 3, pp. 455-470, 2007.",
                    "cites": null
                },
                {
                    "id": 17777390,
                    "title": "LSSP: A novel local segment-shared protection for multi-domain optical mesh networks”,",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1016/j.comcom.2007.02.010",
                    "raw": "L. Guo. “LSSP: A novel local segment-shared protection for multi-domain optical mesh networks”, Computer Communications, vol. 30, no. 8, pp. 1794-1801, 2007. NOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. Changes resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms may not be reflected in this document. Changes may have been made to this work since it was submitted for publication. A definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004  24",
                    "cites": null
                },
                {
                    "id": 17777423,
                    "title": "Markov model of link connectivity in mobile ad-hoc networks”,",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1007/s11235-006-9025-x",
                    "raw": "S. Hwang and D. Kim, “Markov model of link connectivity in mobile ad-hoc networks”, Telecommunication Systems, vol. 34, no. 1, pp. 51–58, 2007.",
                    "cites": null
                },
                {
                    "id": 17777391,
                    "title": "Moving routing protocols to the user space in MANET middleware”,",
                    "authors": [],
                    "date": "2010",
                    "doi": "10.1016/j.jnca.2010.03.018",
                    "raw": "P. Lopez, R. Tinedo, and J. Alsina, “Moving routing protocols to the user space in MANET middleware”, Journal of Network and Computer Applications, vol. 33, no. 5, pp. 588-602, 2010.",
                    "cites": null
                },
                {
                    "id": 17777411,
                    "title": "Multi-path routing in spatial wireless ad hoc networks”,",
                    "authors": [],
                    "date": "2012",
                    "doi": "10.1016/j.compeleceng.2011.11.013",
                    "raw": "L. Guo, L. Zhang, Y. Peng, J. Wu, X. Zhang, W. Hou and J. Zhao, “Multi-path routing in spatial wireless ad hoc networks”, Computers and Electrical Engineering, vol. 38, no. 3, pp. 473–491, 2012.",
                    "cites": null
                },
                {
                    "id": 17777407,
                    "title": "Neighbor supported reliable multipath multicast routing in MANETs”,",
                    "authors": [],
                    "date": "2012",
                    "doi": "10.1016/j.jnca.2011.12.008",
                    "raw": "R. Biradar and S. Manvi, “Neighbor supported reliable multipath multicast routing in MANETs”, Journal of Network and Computer Applications, vol. 35, no. 3, pp. 1074-1085, 2012.",
                    "cites": null
                },
                {
                    "id": 17777419,
                    "title": "On-demand multicast routing protocol with efﬁcient route discovery”,",
                    "authors": [],
                    "date": "2012",
                    "doi": "10.1016/j.jnca.2011.03.012",
                    "raw": "M. Kharraz, S. Hamid, and Z. Albert, “On-demand multicast routing protocol with efﬁcient route discovery”,  Journal of Network and Computer Applications, vol. 35, no. 3, pp. 942-950, 2012.",
                    "cites": null
                },
                {
                    "id": 17777424,
                    "title": "Optimal stochastic location updates in mobile ad hoc networks”,",
                    "authors": [],
                    "date": "2011",
                    "doi": "10.1109/tmc.2010.201",
                    "raw": "Z. Ye and A. Abouzeid, “Optimal stochastic location updates in mobile ad hoc networks”, IEEE Transactions on Mobile Computing, vol. 10, no. 5, pp. 638-652, 2011.",
                    "cites": null
                },
                {
                    "id": 17777389,
                    "title": "Path-based routing provisioning with mixed shared protection in WDM mesh networks”,",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/jlt.2005.863249",
                    "raw": "L. Guo, J. Cao, H Yu, L. Li, “Path-based routing provisioning with mixed shared protection in WDM mesh networks”, Journal of Lightwave Technology, vol. 24, no. 3, pp. 1129-1141, 2006.",
                    "cites": null
                },
                {
                    "id": 17777422,
                    "title": "Performance evaluation for on-demand routing protocols based on",
                    "authors": [],
                    "date": "2011",
                    "doi": "10.1016/j.compeleceng.2010.10.002",
                    "raw": "L. Guo, Y. Peng, X. Wang, D. Jiang and Y. Yu, “Performance evaluation for on-demand routing protocols based on OPNET modules in wireless mesh networks”, Computers and Electrical Engineering, vol. 37, no. 1, pp. 106-114, 2011.",
                    "cites": null
                },
                {
                    "id": 17777387,
                    "title": "Predicting the link stability based on link connectivity changes in mobile ad hoc networks,” in",
                    "authors": [],
                    "date": "2010",
                    "doi": "10.1109/wcins.2010.5544120",
                    "raw": "S. Wang, Q. Song, J. Feng, and X. Wang, “Predicting the link stability based on link connectivity changes in mobile ad hoc networks,” in Proc. IEEE International Conference on Wireless Communications, Networking and Information Security (WCNIS), pp.409-414, 2010.",
                    "cites": null
                },
                {
                    "id": 17777392,
                    "title": "Route stability based QoS routing in mobile ad-hoc networks”,",
                    "authors": [],
                    "date": "2010",
                    "doi": "10.1007/s11277-009-9718-z",
                    "raw": ". N. Sarma and S. Nandi, “Route stability based QoS routing in mobile ad-hoc networks”, Wireless Personal Communications, vol. 54, no.1, pp. 203–224, 2010.",
                    "cites": null
                },
                {
                    "id": 17777421,
                    "title": "Routing in vehicular ad hoc networks: A survey”,",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1109/mvt.2007.912927",
                    "raw": "F. Li and Y. Wang, “Routing in vehicular ad hoc networks: A survey”, IEEE Vehicular Technology Magazine, vol. 2, no. 2, pp. 12-22, 2007. NOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. Changes resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms may not be reflected in this document. Changes may have been made to this work since it was submitted for publication. A definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004  26",
                    "cites": null
                },
                {
                    "id": 17777396,
                    "title": "Stability and Distributed Power Control in MANETs with Per Hop Retransmissions”,",
                    "authors": [],
                    "date": "2011",
                    "doi": "10.1109/tcomm.2011.042111.090486",
                    "raw": "A. Giovanidis and S. Stanczak, “Stability and Distributed Power Control in MANETs with Per Hop Retransmissions”, IEEE Transcations on Communications, vol. 59, no. 6, pp. 1632-1643, 2011.",
                    "cites": null
                },
                {
                    "id": 17777429,
                    "title": "Statistical Interference, 2nd Edition”, Singapore: Thomson Learning Asia,",
                    "authors": [],
                    "date": "2002",
                    "doi": null,
                    "raw": "G. Casella and R. L. Berger, “Statistical Interference, 2nd Edition”, Singapore: Thomson Learning Asia, 2002. NOTICE: this is the author's version of a work that was accepted for publication in Journal of Network and Computer Applications. Changes resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms may not be reflected in this document. Changes may have been made to this work since it was submitted for publication. A definitive version was subsequently published in Journal of Network and Computer Applications, [35, 6, Nov. 2012] http://dx.doi.org/10.1016/j.jnca.2012.08.004",
                    "cites": null
                },
                {
                    "id": 17777417,
                    "title": "Wireless mesh networking: architectures, protocols and standards”,",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1201/9781420013542",
                    "raw": "Y. Zhang, J. Luo and H. Hu, “Wireless mesh networking: architectures, protocols and standards”, Auerbach Publications, 2007.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "http://www.commsp.ee.ic.ac.uk/%7Esw4410/papers/LinkStab.pdf"
            ],
            "updatedDate": "2022-02-23T18:09:53",
            "yearPublished": 2012,
            "journals": [
                {
                    "title": "Journal of Network and Computer Applications",
                    "identifiers": [
                        "1084-8045",
                        "issn:1084-8045"
                    ]
                }
            ],
            "links": [
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/4349177"
                }
            ]
        },
        {
            "acceptedDate": "2009-06-10T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "A.O. Durahim"
                },
                {
                    "name": "Aoki"
                },
                {
                    "name": "B. Sunar"
                },
                {
                    "name": "Bellare"
                },
                {
                    "name": "Black"
                },
                {
                    "name": "Boneh"
                },
                {
                    "name": "Brassard"
                },
                {
                    "name": "Carter"
                },
                {
                    "name": "Chevallier-Mames"
                },
                {
                    "name": "Choukri"
                },
                {
                    "name": "Clarke"
                },
                {
                    "name": "Clarke"
                },
                {
                    "name": "E. Savaş"
                },
                {
                    "name": "Gaj"
                },
                {
                    "name": "Gassend"
                },
                {
                    "name": "Gaubatz"
                },
                {
                    "name": "Hodjat"
                },
                {
                    "name": "Hopkins"
                },
                {
                    "name": "Joye"
                },
                {
                    "name": "Kaps"
                },
                {
                    "name": "Krawczyk"
                },
                {
                    "name": "Lee"
                },
                {
                    "name": "Lim"
                },
                {
                    "name": "McCune"
                },
                {
                    "name": "Ö. Kocabaş"
                },
                {
                    "name": "Reyhani-Masoleh"
                },
                {
                    "name": "Satoh"
                },
                {
                    "name": "Suh"
                },
                {
                    "name": "Sunar"
                },
                {
                    "name": "T.B. Pedersen"
                },
                {
                    "name": "Yan"
                },
                {
                    "name": "Yang"
                },
                {
                    "name": "Zhang"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/186516004",
                "https://api.core.ac.uk/v3/outputs/11741227"
            ],
            "createdDate": "2013-07-12T17:46:56",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 393,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/393",
                    "logo": "https://api.core.ac.uk/data-providers/393/logo"
                }
            ],
            "depositedDate": "2009-01-01T00:00:00",
            "abstract": "The authors present a lightweight authentication mechanism that verifies the authenticity of code and thereby addresses the virus and malicious code problems at the hardware level eliminating the need for trusted extensions in the operating system. The technique proposed tightly integrates the authentication mechanism into the processor core. The authentication latency is hidden behind the memory access latency, thereby allowing seamless on-the-fly authentication of instructions. In addition, the proposed authentication method supports seamless encryption of code (and static data). Consequently, while providing the software users with assurance for authenticity of programs executing on their hardware, the proposed technique also protects the software manufacturers’ intellectual property through encryption. The performance analysis shows that, under mild assumptions, the presented technique introduces negligible overhead for even moderate cache sizes",
            "documentType": "research",
            "doi": "10.1049/iet-cdt.2007.0122",
            "downloadUrl": "https://core.ac.uk/download/pdf/11741227.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "Transparent Code Authentication at the Processor Level\nAhmet O. Durahim1, Erkay Savas¸1, Berk Sunar2,\nThomas B. Pedersen1, O¨vu¨nc¸ Kocabas¸1\n1 Faculty of Engineering and Natural Sciences\nSabanci University\nIstanbul, Turkey TR-34956\n2Department of Electrical and Computer Engineering\nWorcester Polytechnic Institute,\nWorcester MA, 01609, USA\nFebruary 9, 2009\nAbstract\nWe present a lightweight authentication mechanism which verifies the authenticity\nof code and thereby addresses the virus and malicious code problems at the hardware\nlevel eliminating the need for trusted extensions in the operating system. The tech-\nnique we propose tightly integrates the authentication mechanism into the processor\ncore. The authentication latency is hidden behind the memory access latency, thereby\nallowing seamless on-the-fly authentication of instructions. In addition, the proposed\nauthentication method supports seamless encryption of code (and static data). Conse-\nquently, while providing the software users with assurance for authenticity of programs\nexecuting on their hardware, the proposed technique also protects the software man-\nufacturers’ intellectual property through encryption. The performance analysis shows\nthat, under mild assumptions, the presented technique introduces negligible overhead\nfor even moderate cache sizes.\nKeywords: Code authentication, stream cipher encryption, message authentication\ncodes, universal hashing.\n1 Introduction\nThe protection of computer systems from tampering and malicious code has become a major\ngoal in recent years due to new business models that require a strong trust base in open\npersonal computer systems. To cite a few, electronic commerce, electronic government, and\nonline Banking require the handling of highly sensitive information. The lack of a root for\ntrust in computing systems and the shear complexity of software systems, e.g. operating\n1\nsystem (OS), prevents solid security schemes from being developed and deployed. Therefore,\nthe establishment of trust in computing systems remains the major obstacle preventing\nwidespread adoption of such key technologies.\nTo bring trust into computing systems is a sophisticated paradigm. The Trusted Plat-\nform Module (TPM) [41] was developed by the Trusted Computing Group as a standard for\ntrust in computing. Despite the rapid advance in the standard achieved by strong backing of\nmajor device manufacturers and software providers, the techniques proposed in the standard\nprovide only very general high level descriptions of the security and at this point do not con-\nsider performance related issues such as operational efficiency and architectural integration.\nFor instance, a popular means for verifying an execution environment (attestation) is to\ncompute the hash of a program and/or data (state of a program). The computed hash value\nwhich is stored in TPM’s Platform Configuration Registers (PCRs), are signed by the private\nkey in the TPM to generate a quote. As shown in [32, 33], the performance of any operation\ninvolving TPM suffers significantly. Although initial or occasional use of the facilities of the\nTPM can be afforded, the heavy computational and communication (between processor and\nthe TPM) burden eliminates the possibility of “on-the-fly” use of the TPM. Clearly, the\nsolution is to integrate vital security mechanisms into the processor architecture; essentially\nembedding some of the TPM functionality (and possibly more that cannot be provided by\nthe TPM) into the processor architecture. Some of the security mechanisms which have been\nproposed to embed in the processor are memory encryption, memory authentication, and\ncode authentication.\nThough memory and code authentication are closely related issues, there are important\ndifferences between the two. A memory authentication technique guarantees that memory,\nwhich may be under the control of an adversary, behaves as valid memory. Memory au-\nthentication, however, cannot detect if programs which run (legitimately) on the processor\nmodifies data and/or code accidentally or maliciously. It is the role of code verification to\nguarantee that only “certified” programs can run on the processor, and that no program\ncan, accidentally or otherwise, modify the code of itself or other programs. In this paper we\npropose a new, efficient method for code authentication.\nBefore and during the Trusted Computing initiative, less comprehensive schemes were\nproposed. In an earlier work, Chevallier-Mames et al. [9] proposed to employ a just-in-\ntime delivery scheme of code which would eliminate the code tampering problem as the\nexecuted code would always be fresh, delivered just-in-time when needed and then verified\nand executed. This idea would address many of the security challenges we face today in\nembedded systems security. However, this new model brings new problems with it. The\nmost critical one is performance. Most importantly, the latency introduced right before\nthe execution will be unacceptable for most interactive users unless performed under the\n“one second” threshold. To overcome this obstacle, the authors advocate the use of fast\nsecret-key algorithms over public-key schemes, and propose thread-level batch verification of\ninstructions. Nevertheless, the reference does not give performance measures or a detailed\narchitectural level description on the feasibility of the proposed scheme.\nSimilar security features have been studied in the computer architecture community as\nwell. For instance, the Aegis architecture [39] aims to provide memory encryption and\ntamper resilience by instruction verification. The model proposes two different architectures\ndifferentiated by whether the OS is trusted or the trust is confined within the boundaries of\n2\nthe chip.\nIn [19] Gassend et al. propose a memory authentication scheme based on Merkle trees\n[34] to authenticate an arbitrarily large untrusted RAM memory. In [11] Clarke et al. pro-\npose a scheme which keeps the hash values of (logs of) all read and write operations. The\nmemory can then be authenticated off-line by comparing the logs with the actual content\nof the memory. Furthermore, in [12] Clarke et al. propose a hybrid protocol with the aim\nof balancing the drawbacks and benefits of the previously mentioned two protocols. The\noverhead of this scheme tends to a constant as the number of instructions between critical\ninstruction grows. We will later on discuss the connection of these works to the proposed\nauthentication scheme.\nIn another work Yang et al. [43] focus on the related problem of achieving encrypted\nmemory. The work provides more detail at the architectural level and achieves to hide\nmuch of the encryption latency behind the memory access cycles by utilizing a stream cipher\nlike scheme. Despite the novelty of the design, the downside of this work is that there is\nno mechanism to prevent or detect tampering with the memory. In fact, if no additional\nsteps are taken for integrity verification the proposed scheme is vulnerable to straightforward\nmanipulations by an attacker. That is, the attacker may freely flip bits in encrypted memory\nwhich are translated to bit-flips in the plaintext (due to the stream cipher approach) with\ndisastrous results. The paper proposes to use the hash tree based verification technique\nintroduced earlier by Clarke et al. [11] to bring cryptographic integrity checks into their\nencryption technique. However, no treatment is given on whether this would be achievable\nwithout re-introducing the latency which the paper wanted to shift off the critical path in\nthe first place. Another inconvenience of this approach is that it makes use of so-called\nsequence number codes which are used to eliminate the information leakage that occurs\nwhen the same memory location is modified and re-encrypted. Information leakage occurs\ndue to re-encryption of the same memory block with essentially the same seed (or key)\nwhich means that the XOR-difference in the ciphertexts will reveal the XOR-difference of\nthe plaintexts. Despite these shortcomings the work provides an innovative idea, i.e. hiding\nthe encryption/decryption latency behind the access latency. The same idea was successfully\napplied to memory authentication and encryption in [42], that combines Galois Counter\nMode of operation with Merkle’s tree approach.\nThe main argument for moving memory and code verification away from the TPM is\nperformance. The general purpose nature of the TPM makes it difficult to give efficient\nimplementations. We claim that the best performance is obtained by treating memory and\ncode verification separately, thus being able to optimize the methods for the different issues\nthat arise in the two problems. Memory authentication needs to handle changes in data, but\ndoes not need to verify the origin of the data (it is written by the processor anyway). On\nthe other hand, code authentication can use the fact that code is static, but has to ensure\nthat it is the intented code.\nIn this paper we propose an efficient method for run-time authentication of code and static\ndata. The proposed technique achieves to hide the bulk of the verification complexity behind\nthe memory access latency. Our scheme relies on stream ciphers and universal families of hash\nfunctions. It is modular, so that the cipher and/or hash function can be easily substituted.\nWe discuss how our code authentication scheme can be used in a number of scenarios with\ndifferent security requirements. The security of the scheme is thoroughly analyzed. Finally,\n3\nwe present a performance analysis which shows that the presented technique introduces\nnegligible overhead when caches of moderate sizes are employed.\n2 Authentication in the Stored-Program Model\nIn the stored-program paradigm, the executable part of a program (i.e. instructions) along\nwith data are stored in a single structure. This abstraction is realized physically in the\nmain memory by modern day computers. The processor requests the instruction block to\nbe executed by supplying its starting address to the memory. The memory provides the\ninstructions as requested. The memory also stores and supplies data, which can either be\ngenerated statically by the software publisher or dynamically by the program itself while\nin execution. Both instructions and data need to be authenticated for safe execution of\nprograms in the processor.\nThe main memory is a place where many different programs and their associated data\nco-habit and therefore it is a particularly insecure place where instructions and data are\nsubject to modifications by unauthorized parties and malicious programs. It is true that\nmost computers employ some protection mechanisms that prevent programs from intervening\naddress spaces of each other. However, the protection mechanism is enforced by the OS,\nwhich is overly complex and not necessarily a trusted piece of code. Hence, it is reasonable\nto assume that instructions and data can be modified during program execution.\nThe processor on the other hand, is commonly assumed to be a safe place once the\ninstructions are brought from memory. The instructions and data are first put in an on-chip\nmemory known as cache memory, whence the instructions and data are fetched for execution.\nThe rationale behind believing that processor is a safe place for instructions and data is\nthat no instruction can contaminate the instructions and data in the cache. Contaminated\ninstructions and data are easily detected in our scheme since any update in the cache requires\nverification of the updated cache block before the execution of the instructions in the block1.\nIn the program authentication model we propose, the instructions arrive in the cache\nalong with their authentication tags, which are verified before they are executed. Instructions\nare transferred between the main memory and processor cache in groups, known as cache\nblocks or cache lines. While the instruction length varies from one byte to 17 bytes in\ncontemporary processors, a typical length of cache block ranges from 8 to 512 bytes. In\nthe proposed scheme, all the instructions in a cache block rather than a single instruction\nas suggested in [9] are used to generate an authentication tag. This approach has two\nadvantages:\n1. verification is faster, and\n2. less overhead is incurred during the transfer of the tag from memory to the processor.\nIt is true that the proposed technique implies a small and modular modification to mi-\ncroprocessor core and that any modification to hardware is much harder than any modifi-\ncation to the software (more precisely to the OS where the most of the software protection\n1Other non-invasive attacks (e.g. fault induction) to contaminate the instructions in the cache can be\nthwarted using other means such as adversarial fault tolerant computing techniques (e.g. error detection); a\nvast topic which is beyond the scope of this paper.\n4\nmechanism is implemented). It is, all the same, certainly relevant and we strongly believe\nunavoidable, to explore these modifications since a software-only approach is proved to be\ninadequate for providing a proper security and trust. Furthermore, there is a strong per-\nception that security, trust (and perhaps privacy) must be taken into account as design\nparameters in early stages of microprocessor development. Major microprocessor companies\nalready introduced new technologies [15, 13, 3] that provides hardware support for security\nand trust. Similarly, many research groups have long been working on hardware modifica-\ntions for providing security, trust, and privacy as an integral part of the microprocessors\n[44, 30, 12, 19, 11, 43, 9, 39].\nIn summary, the processor and main memory in the computer system are tied to each\nother through the instructions and data of a program and the former does not trust the\nlatter. Restricting code verification within the boundaries of the CPU minimizes the trust\nbase. This has significant consequences, as the OS no longer needs to be trusted. In fact,\nthe operating system itself as well as the device drivers and the BIOS software, and virtually\nany piece of code can be authenticated on-the-fly.\nThe integrity of the instructions should be verified transparently with unnoticeable delay\nto the end user. For ease of use, convenience, and better scalability, signature schemes based\non public key cryptography (PKC) seem to offer certain advantages. The key distribution\nproblem is less of a concern, since the verification is realized using public key of the software\nowner or trusted software repository. Public keys, either stored in software or embedded in\nhardware, do not cause a security breach when they are compromised. But their authenticity\nmust be validated using certificates, prior to verification. Nevertheless, there are two impor-\ntant factors that prohibit us from using PKC in code authentication: public-key signatures\nare too long, and public-key signature verification is slow. While long public keys can be\ntolerated up to a certain extent, high verification time is definitely unacceptable for on-the\nfly code authentication. Only symmetric cipher based authentication can provide both short\nauthentication tags and fast (on-the-fly) verification of instructions.\nIn our scheme we utilize efficient Message Authentication Codes (MACs) [38, page 140]\nbuilt using universal families of hash functions due to their high encryption throughput and\nshort authentication tags. In addition to the performance benefits, MACs constructed using\nuniversal families of hash functions allow the designer to quantify the security level precisely,\nthus eliminating needs for performance degrading margins.\n3 Alternative Protocols for Generation of Authentica-\ntion Tags\nNone of the techniques for memory encryption/authentication proposed in [30, 43, 39, 11,\n19, 12] addresses the issue of initial generation of the authentication tags and of cipherexts\nfor instructions and static data. This concern is closely related to trust in computing since\nthe user should be able to verify the authenticity of data and instructions before she uses\nthem for the first time. The issue can be reduced to key distribution as the aforementioned\ntechniques use a secret key for encryption and integrity check. Here we discuss several\nalternative scenarios along with the classical approach for key distribution (or more precisely\n5\ntag generation) in the proposed scheme for software authentication. Unfortunately, there is\nnot one single technique that can be used in all application scenarios where their requirements\nand constraints differ. Application developers and architects should choose a particular\ntechnique depending on these requirements and constraints.\nSince the proposed code authentication technique utilizes a symmetric encryption to\ngenerate and verify the authentication tags, those parties which generate and verify the tags\nshould be in possession of a secret key. However, distribution of the secret keys to involved\nparties is a major problem. In the classical setting, there are two parties in the authentication\narchitecture: the software producer who owns the intellectual rights of the software and the\nsoftware user who wants to execute the software on his/her hardware in an authenticated\nway. The two parties share a secret key, K, that the software owner uses to generate the\nauthentication tag, T (P,K) for the program P . The secret key, K, may also be implemented\nin the user’s processor by the hardware manufacturer to prevent the user from sharing the\nauthenticated software with other users. The secret key can be unique either to a single\nuser or to a group of users. Clearly, this will be less desirable to the user who has to trust\nthe software owner. This technique can majorly be used in special-purpose processors such\nas those in game consoles and embedded processors deployed in automobiles where software\nproducer is in complete knowledge of the hardware.\nSoftware\nOwner\nSoftware\nUser\nShare\nsecret key\nK\nP \nTrusted\nSoftware\nRepository\nRegister\nSoftware  \nT(P, K)\nFigure 1: Authenticated Program Distribution Protocol based on a Trusted Software Repos-\nitory\nAnother scenario illustrated in Figure 1 can help remedy the aforementioned concerns\nby involving the use of so-called trusted software repository (TSR). The TSR is responsible\nfor generating authentication tags for the programs. The software owners register their soft-\nware products with the trusted software repository, which inspects and stores the registered\nprograms in its local database. Note that the repository does not have to store the entire\nprogram, but a representative of it, e.g. its hash in case the storage is of a concern. The\nrepository shares secret keys with users and generates authentication tags for a specific pro-\ngram registered in the local database upon request by a user. The user first has to prove to\nthe repository that it is indeed a legitimate holder of the program. The repository checks\nwhether the program in question is identical to the one in the repository by comparing the\nhashes of the programs. If the first two steps are in order, the repository generates an\n6\nauthentication tag T (P,K) for the program P and sends it to the user. If the bandwidth\nbetween the user and the TSR is a concern, the TSR can only send, for instance, the hash of\nthe all tags concatenated. The user generates all the tags for the program blocks herself and\nchecks their integrity using the hash value she received from the TSR. This method can also\nbe used in the other schemes described in this section and can help reduce the bandwidth\nrequirements in case of software updates and fixes.\nOne concern with the centralized approach is that the TSR may quickly become communi-\ncation and computation bottlenecks. This concern can easily be addressed using replication.\nThe proposed protocol requires an infrastructure but otherwise does not require any changes\nto the OS loader and no changes other than the authentication mechanism itself to the pro-\ncessor core. Although the technique provides a sound alternative for code authentication in\ngeneral-purpose hardware it entails a trusted party whom the users must have complete con-\nfidence in; a situation some users may find discomforting due to privacy and trust concerns.\nOn the other hand, the trusted party model and its variations [21] play a fundamental role\nin many cryptographic protocols. Thus, the TSR-based solution can be employed in code\nauthentication whenever such a party exists.\nThe two techniques outlined above have one more shortcoming in common besides the\naforementioned concerns: privacy. Zhang et al. in [44] pointed out that user privacy is an\nimportant issue even in the otherwise secure and trusted systems. Their solution is based on\nan architectural modification similar to our approach in this respect. One possible solution\nto privacy problem that will protect users against third parties tracking their activities is to\nutilize public key cryptography. The software owner signs its software using its private key\nand sends the software and the signature to the legitimate user who authenticates herself\nwithout revealing her true identity2. Upon downloading the software, the user, knowing\nthe public key of the owner, verifies the software with the available trusted base (previously\nverified trusted — and authenticated — signature verification program or a TPM). During\nthe verification process, the processor fetches the blocks of the software one by one starting\nfrom the first block. As it verifies the code using public key cryptography, the processor can\ngenerate authentication tags for blocks using the secret key; this is possible since tag gen-\neration and verification processes are identical. The proposed authentication infrastructure\ninside the processor which can verify authentication tags is also capable of generating tags\nwithout any modification. Consequently, neither the user reveals her identity nor the secret\nkey leaves the trusted hardware. The techniques in [30] and [31] can be used for generating,\nprotecting, and managing the secret key inside the processor. Although the tag generation\nby the user may incur high latency, it needs to be performed only once during the installation\nof the program and therefore the latency can be tolerated.\n4 Details of the Code Authentication Procedure\nIn our scheme we make use of efficient MAC’s to ensure the integrity of individual blocks\nof a program. The MAC is obtained by the application of a hash function picked randomly\n2The user can employ group or ring signatures or group keys as suggested in [44] to prove that she is\none of the legitimate users. Any further discussion of anonymous authentication is beyond the scope of this\nwork.\n7\nfrom a universal family of hash functions on the message, followed by the generation of the\ntag by encryption via a stream cipher.\nAj+1\nAj\nLj+1\nLj\nprogram\nblocks\nVirtual\nAddress Program ID \n    (I)\nMj Universal\nHash\nFunction\nKU = (k1, ..., kn)\nStream\nCipher\nAj KTI\nT\nCryptographic\nTag for Lj\nLj\nAj\nFigure 2: Cryptographic (authentication) tag generation\nThe steps taken in the generation of authentication tags, illustrated in Figure 2, are\nsummarized as follows:\n• Key Distribution One of the critical phases of the authentication mechanism is\nthe distribution of the authentication keys. For authentication, we need two keys:\nKU ∈ {0, 1}\nm which is used in the universal hash computation of the message block,\nand KT ∈ {0, 1}\nk which is used in the encryption mask generation through the stream\ncipher. Here k denotes the length of the key which will be used for the stream cipher.\nIn the TSR-based scheme proposed in Section 3 the infrastructure eliminates the need\nfor the user to obtain a key. The TSR creates the software authentication tags. The\nkey is pre-built into the processor core. Hence, the TSR and the processor core share\nkeys by pre-distribution. Furthermore, for tamper-resilience the key may be tied to\nthe hardware by means of physically one way functions [31].\n• Software Preparation The software (or rather a program since it is executable) P is\npartitioned into program blocks3, Lj ∈ {0, 1}\nl. The program has a unique identification\nnumber, I ∈ {0, 1}i and each block has an address4 specifying the placement of the\nblock in the memory, Aj ∈ {0, 1}\na. The message to be signed is Mj := (Lj||I||Aj) ∈\n{0, 1}m, where m = l + i+ a and || stands for the concatenation operation.\n• Digest Computation Several efficient cryptographic hash functions have been pro-\nposed in the literature [8, 7, 29, 5, 26]. Universal hash functions, were first introduced\nby Carter and Wegman [8]. Roughly speaking, universal families of hash functions are\ncollections of hash functions that map messages into short output strings such that the\ncollision probability of any given pair of messages is the same as for a randomly chosen\n3A program block is mapped into a cache block which is the smallest amount of information transferred\nbetween the processor and memory.\n4Virtual address of the block must be used since the physical address cannot be known at this stage.\n8\nfunction. A universal family of hash functions can be used to build an unconditionally\nsecure MAC. For this, the communicating parties share a secret and randomly chosen\nhash function from the universal family of hash functions, and a secret encryption key.\nA message is authenticated by hashing it with the shared secret hash function and then\nencrypting the resulting hash using the key. Carter and Wegman [8] showed that when\nthe family of hash functions is strongly universal, i.e. a stronger version of universal\nfamilies of hash functions where messages are mapped into their images in a pairwise\nindependent manner, and the encryption is realized by a one-time pad, the adversary\ncannot forge the message with probability better than that obtained by choosing a\nrandom string for the MAC.\nWe use the hash function PR (NH-polynomial with reduction) proposed in [26] to\ngenerate message digests (or representatives) for our software blocks, Mj. The method\nfor hashing, which is proven to be universal on n equal-length strings (indicating that\ncollisions cannot be forced to occur too often), is easy to use, and being based on binary\npolynomial operations in GF (2w) is easy to implement. Galois field GF (2w) also known\nas binary extension field is constructed using a binary irreducible polynomial of degree\nw, p. The elements of of GF (2w) are all the binary polynomials whose degrees are\nsmaller than w. The arithmetic in GF (2w) are regular polynomial arithmetic with an\nadditional reduction step by the irreducible polynomial (noted as (mod p)) whenever\nthe degree of the result is larger than or equal to w. Since the carry propagation is not\nan issue in GF (2w) arithmetic, any constructions utilizing binary extension fields are\npreferred for its speed and small area.\nThe software block Mj is written as a vector Mj = (m1,m2, · · · ,mn), where ml’s are\nw-bit long and are considered as elements of GF (2w) for the hash computation. The\ndigest is computed using a hash function chosen randomly from the family of hash\nfunctions PR as defined below.\nDefinition 1 [26] Given Mj = (m1,m2, · · · ,mn) and K = (k1, k2, · · · , kn), where ml\nand kl ∈ GF (2\nw), for any even n ≥ 2, and a degree-w polynomial p irreducible over\nGF (2w), PR is defined as follows:\nDj = PRK(Mj) =\nn/2∑\nl=1\n(m2l−1 + k2l−1)(m2l + k2l) (mod p) .\nIn summary, as a result of message digest computation, a w bit message digest, Dj, is\nobtained for an m bit program block, which is the concatenation of instruction block,\nblock address, and program identification number in encrypted form and w < m\n• Cryptographic (Authentication) Tag Computation: The tag is computed as\nTj = Dj ⊕ RT where the pseudo-random pad RT is the block output of the utilized\nstream cipher, i.e. RT = SC(KT , A, I). The lengths of Tj, Dj and RT are naturally\nidentical. The generated cryptographic tag is stored in memory as explained in Sec-\ntion 6.\n9\nCryptographic tag generation is performed off-line and therefore does not constitute a\nperformance bottleneck. The software validation process, explained below, is performed on-\nthe-fly while the instructions are fetched from the memory, and thus naturally raises concerns\nof performance degradation. However, we show that almost all computation during software\nvalidation can be done in parallel to the memory access operation.\n4.1 Software Validation\nLj+1\nLj\nMemory\nPhysical\nAddress Program ID\nUniversal\nHash\nFunction\nK\nU\n= (k\n1\n, ..., k\nn\n)\nStream\nCipher\nA\nj KTI\nTj\nAddress(Tj)\nProcessor\nVirtual\nAddress\n    A\nj\nLj\nI\nMj\nMemory\nManager\nTj\n= ?\nPAj\nPAj\nR\nT\nFigure 3: Validation of software blocks\nThe outline of software validation is illustrated in Figure 3. When a memory request for\nan instruction block goes out, the memory access latency is used to generate the pseudo-\nrandom pad RT with the stream cipher SC(KT , A, I). The existence of extremely fast im-\nplementations of stream and block ciphers justifies the assumption that sufficient pseudo-\nrandom bits can be generated for an instruction block [36, 27, 18, 2] in the time it takes\nto perform a memory access, i.e. the memory access latency. The resulting pad RT is\naccumulated in a register within the processor.\nAs instructions are retrieved from the memory into the CPU, they are incrementally\nhashed using the fast universal family of hash functions. Each incremental hash operation\nconsists of only two additions and one multiplication in GF (2w) which can be performed\nextremely fast. After the last block is hashed, the accumulated tag value is computed as\nTj = PRKU (Mj)⊕RT . The tag Tj computed in the authentication unit must be identical to\nthe tag that are fetched from the memory, in which case the verification is accomplished. As\nlong as the latency of an incremental hash computation operation can be hidden behind the\nmemory access latency, the verification operation will run concurrently. The result of the\nverification as well as the instructions will be ready right after the last block is brought in\nfrom memory and incrementally hashed. Hence, the overhead introduced by this step, is in\nthe retrieval of the authentication tag from memory and in the latency of the last incremental\n10\nhash and XOR computations. The latter incurs insignificant overhead, i.e. 1 clock cycle,\ncompared to the overhead due to the former5. The overhead introduced due to both factors\nis studied in Section 7.\nWhen an instruction block along with its authentication tag, (Mj, Tj) is received, the\nauthentication circuit that is built into the processor first computes the digest PRKU (Mj) as\ndefined above, where Mj := (Lj||I||Aj) and Lj, I, and Aj are program block, program ID,\nand block address, respectively. The circuit checks whether the tag verifies and decides to\nexecute (or not) accordingly, as shown in Figure 3.\n4.2 Encryption of Software Blocks\nThe proposed scheme provides only integrity for program blocks in order to prevent unau-\nthorized pieces of software from executing in the processor. However, we can easily enhance\nour scheme with encryption to protect the IP of software manufacturer. The executable\ncode can be kept in encrypted form in memory. Using the stream cipher approach in [43],\nthe computation time for encryption and decryption operations can be hidden behind the\nmemory access latency. The only concern is whether a sufficient amount of pseudo-random\npad can be generated during the memory access. The number of pseudo-random bits re-\nquired for encryption depends on the size of the instruction block. Assuming we partition the\nsoftware using the cache block size, the number of bits required for encryption/decryption\ncan be as many as 512 bytes. The number of bits required for the authentication procedure\nexplained above, on the other hand, are only 16 bytes (128 bits) for an acceptable level of\nsecurity. Fast stream and block cipher implementations [36, 27, 18, 2] justify the assumption\nthat the required number of pseudo-random bits can be generated during the memory access\noperation.\n5 Security Analysis\nWe follow the strategy developed by Krawczyk in [29]. That is, given a message M authen-\nticated by the tag t = h(M) ⊕ r where h is randomly selected from some family of hash\nfunctions and r denotes a random string, the adversary should not be able to findM ′ (6=M)\nand t′ such that t′ = h(M ′)⊕ r with non-negligible probability6. Here it is assumed that the\nadversary knows the description of the family of hash functions but not the chosen h or r\nvalues. In this scenario Krawczyk defines a family of hash functions to be \u000f-AXU (almost-\nXOR-Universal)7 if it resists such an attack with probability larger than \u000f. The following\ntheorem quoted from the same reference establishes the necessary and sufficient condition\nto obtain such a MAC.\n5“Early-start” and “critical-word-first” techniques cannot be applied in the proposed scheme since the\nprocessor waits for the entire block to execute the requested instruction, which may arrive earlier.\n6The symbol ⊕ denotes the parallel bitwise exclusive-or operation.\n7The reference by Krawczyk uses instead the terminology \u000f-otp secure. We prefer to use the equivalent\nbut more common terminology, i.e. \u000f-AXU in the text.\n11\nTheorem 1 A necessary and sufficient condition for a family H of hash functions to be\n\u000f-AXU is that for all M1 6=M2 and c of w-bit constant,\nPrh[h(M1)⊕ h(M2) = c] = \u000f .\nIn the same reference, Krawczyk introduces an LFSR-based universal family of hash functions\nwhich is proved to be \u000f-AXU. In practice, any \u000f-AXU family of hash functions may be\nused in the code authentication scheme proposed in this paper, as long as it is possible to\nbuild a circuit with reasonable footprint which will hash a block of code, in less time than\nthe memory access latency. Here we use the PR universal family of hash functions, since\nreference [26] gives detailed hardware implementation results which gives hard evidence that\nthis performance condition will be met by this family of hash functions.\nAlthough not shown in the original reference, the universal family of hash functions PR\nmay be easily shown to be \u000f-AXU (cf. Appendix).\nTheorem 2 For any even n ≥ 2 and w ≥ 1, PR[n,w] is \u000f-AXU on n equal-length strings,\nfor \u000f = 2−w.\nAs a direct consequence of Theorems 1 and 2 the family of hash functions PR is \u000f-AXU.\nThis means that the MAC obtained by computing T = PRK(C)⊕RT is \u000f-AXU secure with\n\u000f = 2−w when RT is randomly chosen and used only once. Alternatively, if RT is generated\nby a pseudo-random number generator (or stream cipher), the security of the MAC rests on\nthe security of the pseudo-random number generator.\nFinally, we would like to note that there is an implicit benefit of using universal hash\nfunctions in this application. Since the security level can be quantified through the \u000f value,\nthe length of the authentication tag can be precisely optimized to be minimal for a required\nsecurity level. For instance, for practical applications \u000f = 2−80, or in other words a 80-bit\nauthentication tag per cache line should suffice to provide an acceptable level of security.\nHowever, we use 128-bit authentication tag per cache line in order to demonstrate the effi-\nciency and feasibility of our scheme for even a higher level of security.\n6 Processor and Memory Organization\nThe processor and memory organizations need to incorporate small changes in order to\nsupport the proposed model for authenticated code execution. Figure 4, depicts a generic\nmemory organization adopted from common MIPS processors, where GP and PC stands\nfor global pointer — for easy access to static and global data — and program counter,\nrespectively. For easy access, the authentication tags are placed next to the data segment.\nThe order of the tags is the same as the instruction order in the memory. The address of the\ntag of the first instruction is kept in a special purpose register, named here as tag pointer,\nin short TP . Inside the processor architecture, as depicted in Figure 5, in addition to TP ,\na small memory (named as authentication cache) is kept for storing authentication tags,\nand an authentication unit is included for validation of instructions. The authentication\ncache contains authentication tags of instruction blocks, and is assumed to be of the same\nsize as one cache line. The authentication cache uses the same working principle as the\n12\nHidden\r\nStack\r\nHeap\r\nTag Segment\r\nData Segment\r\nText Segment\r\nReserved\r\nPC\r\nGP\r\nTP\r\nFigure 4: Memory Organization\ninstruction/data cache. When an instruction is to be authenticated, its tag is searched in\nthe authentication cache first; in case it is not found in the authentication cache, the current\ncontent is evicted and the required authentication tag block is fetched from the memory.\nWe assume that a memory block (which resides in one cache line) is the smallest amount\nof information transferred between the processor and the main memory, independent of the\ntype of information (i.e. instruction, data, or authentication tag). Therefore, a program\nblock is identical to a block of instruction cache.\nThe authentication unit inside the processor, which incorporate a stream cipher SC and\nother necessary circuit, computes the authentication tag, as the instructions arrive in the\ninstruction cache in a cumulative fashion. The computed tag for the instruction block is then\ncompared against the tag in the authentication cache. If they match, all the instructions in\nthe cache line are considered as authentic and ready for safe execution.\nIf the authentication fails, the action taken by hardware or software (e.g. the OS) de-\npends on the application. The safest method is to terminate the program execution entirely\nin security-related applications. If the program has the privilege of accessing sensitive in-\nformation (e.g. secret keys, private information), it is best to prevent the execution of any\nuntrusted instruction since the damage can be irreversible otherwise. On the other hand,\nthere may be some other applications where we can just discard the results in case the pro-\ngram that generates the results fails to authenticate in a certain instruction. In this case, the\nprogram does not have to be terminated; but a flag may be set to indicate an authentication\nfailure.\nWhile requested instructions are accessed through PC (program counter), the correspond-\n13\n Authentication\n       Cache Authentication\n         Unit\n  Instruction\n      Cache\n Datapath\n MMU\n \nPC\n \nTP\nMemory\nMemory\nCombinational\n        Logic\nSecret Key\n        K\nFigure 5: Processor Architecture\ning authentication tags are accessed using both PC and TP. TP is initially set to the address\nof tag for the first instruction block. As the PC is updated (either incrementing it by 4 or\nbranch/jump target address), TP is updated using the formula\nTP := TP +\n⌊\nPC\nnI × nT\n⌋\n,\nwhere nI and nT are the number of instructions and number of authentication tags in a\ncache line, respectively.\n7 Performance Analysis\nIn this section we demonstrate the efficiency and practicality of our scheme employing the\nstandard clock-cycle per instruction (CPI) metric as a basis for our performance analysis\nwith 128-bit authentication tags per cache line for a good security level. In this section,\nwe compare the performance of the authenticated architecture w.r.t. a base model which\nsimply stands for a regular processor architecture without the proposed code authentication\nfeature. As described earlier, although the proposed design applies on virtually any processor\narchitecture, we base our analysis on a standard RISC architecture with five pipeline stages:\nIF (inst. fetch), ID (inst. decoding), EX (execute), MEM (memory access), WB (write\nback).\nThe performance parameters related to the software profile are given in Table 1.\nSimilarly, architectural parameters determined by the organization of the processor and\nmemory system are enumerated in Table 2. cAR stands for the elapsed time between when\nthe first word of data appears at the memory output and the address is sent to memory.\n14\nTable 1: Performance parameters related to software profile\nParameter Definition\nfL Frequency of load instructions\nfD Frequency of data dependencies caused by load instructions\nfR Frequency of data dependencies caused by load instructions\nand resolved by reordering\nfB Frequency of branch instructions\nfJ Frequency of jump (unconditional branch) instructions\nfTaken Percentage of branch instructions that are taken\nThis parameter primarily depends on the memory technology and the size of the memory.\ncT represents the time required to transfer an entire block of instructions from the memory\noutput through the bus into the instruction cache after address resolution. This parameter\ndepends on the size of the instruction block and the bus width.\nTable 2: Architectural parameters\nParameter Definition\ncAR Time spent (in number of clock cycles) for address resolution\ncT Time required to transfer an instruction block from the memory to cache\nnI Number of instructions in a cache line\nnT Number of tags in a cache line\nfMP Percentage of branch mispredictions\nThe Base Model\nThe equation modeling the CPI in the base model is given as\nCPI = 1 + CL + CB +MRI ×MPI +MRD ×MPD .\nThe term CL represents the overhead contributed to the CPI due to data dependency asso-\nciated with load instructions, i.e., when a data load instruction is followed by an instruction\nthat uses the result of the load instruction causes a delay in many common processor archi-\ntectures. Here we assume that in the five-stage pipelined RISC architecture the delay is one\nclock cycle. This assumption is based on the fact that, in the pipeline that employs data\nforwarding from the MEM stage to the EX stage, it is sufficient that the next instruction\nstalls for one clock cycle in the ID stage. The majority of such dependencies are resolved\nby reordering of the instruction by the compiler. Still we can formulate the effect of data\ndependency related delays in the analysis as\nCL = fL × fD × (1− fR) .\n15\nThe term CB represents the overhead contributed to the CPI from branch mispredictions.\nWe assume the RISC architecture includes a simple branch prediction scheme of assume-\nnot-taken, where the branch outcome is resolved in the ID stage. Therefore, one clock cycle\nis lost on misprediction.\nCB = fB × fMP\nThe product MRI ×MPI stands for the latency incurred in instruction cache misses. The\nterm MRI stands for the instruction cache miss rate and depends on the size of the in-\nstruction cache as well as on the statistics of the branching distance of the software profile.\nThe miss penalty, MPI , represents the number of clock cycles needed to bring a block of\ninstruction from the main memory to the instruction cache. Similarly, MRD ×MPD stands\nfor the latency incurred in data cache misses. MPI (or MPD) has two components: ad-\ndress resolution and the transfer of the instructions through the bus into the cache. Hence,\nMPI = cAR + cT . The most explicit form for the CPI is given as follows\nCPI = 1 + fL × fD × (1− fR) + fB × fMP + (MRI +MRD)× (cAR + cT ) .\nwhere we assume that instruction and data caches have the same configurations.\nThe Authenticated Processor Model\nThe CPI performance of the processor architecture that also includes an authentication unit\nis modeled as\nCPIAuth = CPI + CAuth .\nThe overhead caused by the authentication is broken down as follow\nCAuth = 1×MRI +MRI ×MPI × factor\nwhere factor represents the rate of authentication (cache) misses while the first term accounts\nfor the overhead due to one extra clock cycle to compute the final step in the cryptographic\ntag calculation. Authentication misses are closely related to instruction flow during program\nexecution and minimized for sequential execution. Control flow instructions (e.g. branch\nand jump) occasionally change instruction flow which results in higher miss rates for both\ninstruction and authentication caches. Since authentication cache can contain more than\none tag, it sometimes does not miss even if there is an instruction cache miss8. The term\nfactor represents the fraction of instruction misses that also result in an authentication cache\nmiss.9\nThe range of instructions, whose tags are in the same authentication block, is hereafter\ncalled window of instructions. The number of instructions in a window can be calculated\nusing the simple formula nT × nI . We can categorize the authentications misses depending\non their cause, therefore obtain a more refined formula for factor as follows:\nfactor = factorSE + factorB + factorJ .\n8We assume that the smallest cache block is 128 bit.\n9The case that instruction cash hits while authentication cache misses does not entail fetching the au-\nthentication tag from memory. Instructions found in the cache must have been already authenticated and\nhence trusted.\n16\nThe term factorSE represents the number of authentication misses during sequential exe-\ncution of the instructions. Namely, the first instruction in a window always result in an\nauthentication miss even if there are no branch or jump instructions. The formula to calcu-\nlate factorSE is derived by discarding the jump and taken branch instructions:\nfactorSE =\n1− fB × fTaken − fJ\nnT × nI\n.\nA branch instruction, when taken, causes an authentication cache miss when the target\ninstruction is beyond the current window of instructions. Therefore, not all taken branches\nresult in an authentication miss since the target instruction can be in the current window of\ninstructions. We use the term branch distance (BD) to denote the difference in the address of\nthe current instruction and the address of the target instruction. Similarly, branch offset b is\nthe number of bits that encode the distance in the branch instruction, i.e. b = dlog2(BD)e.\nThe statistics given in [22] shows that the majority of the branches are to relatively close\nlocations, where the branch offset can be captured with only 7-8 bits. Consequently, whether\na branch instruction leads to an authentication cache miss depends on both the branch offset\nand the position of the branch instruction in the instruction window. For instance, a forward-\ntaken branch, which is placed near the end of the window, will cause an authentication cache\nmiss with a very high probability.\nThe probability that a branch instruction causes an authentication miss increases in\nproportion to the increase of the branch offset. When the branch offset is for example b = 2,\nthe branch displacement will be at most 4, and therefore the last four instructions10 may\ncause an authentication cache miss if they are forward-taken branches. Using all branch\noffset values in PC-relative addressing common in RISC processors, i.e. n = 1 . . . 15, we\ncan derive a formula that gives the probability that an instruction causes an authentication\ncache miss as follows:\nfactorB =\n15∑\nb=1\nfB × fTaken ×\nmin(2b, nI × nT )\nnI × nT\n× fb(b),\nwhere fb(b) represents the percentage of all branches whose branch offset is b-bits\n11.\nUnconditional branch instructions, including function calls, are usually referred to as\njump instructions and therefore are always assumed to cause authentication cache misses.\nTherefore, we have factorJ = fJ .\nThe MIPS dynamic instruction mix for five SPECint2000 benchmark programs (i.e. gap,\ngcc, gzip, mcf, perl) reported in [22, page 148] shows that branch and jump instructions\nmakeup 12% and 1% of instructions, respectively. In other words, fB = 12% and fJ = 1%\n12.\nFor the same benchmarks, the ratio of conditional branches, fb(b), whose branch offset is b-bit\nis also reported in [22, page 173]. The frequency of taken branches fTaken for the SPEC2000\nprograms is reported as 66% in [22, page 232]. The instruction miss rate varies with cache\n10For a conservative estimate we take the maximum amount of displacement for a given number of offset\nbits.\n11This unified formula incorporates the effects of both forward and backward branches\n12For a conservative estimate we do not use SPECfp2000 benchmarks whose statistics features much lower\nfrequencies of branch instructions\n17\n8 16 32 64 128 256\n0\n0.5\n1\n1.5\n2\nCache Size (KB)\nCP\nI O\nve\nrh\nea\nd\na. CPI Overheads\n \n \nDCacheMiss\nICacheMiss\nAuth\n8 16 32 64 128 256\n0\n0.5\n1\n1.5\n2\n2.5\nb. Relative CPI Overheads\nCP\nI O\nve\nrh\nea\nd\nCache Size (KB)\n8 16 32 64 128 256\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nc. Relative CPI Overheads\nCP\nI O\nve\nrh\nea\nd\nCache Size (KB)\nFigure 6: CPI Overheads due to instruction authentication, instruction and data cache\nmisses for different cache sizes\nand block sizes as well as the workload. The miss penalty depends on block size and width\nof the bus between the processor and memory. Using the miss rates in [22, page 390] for five\nSPECint2000 programs and miss penalty values in [22, page 414] for a fixed block size of\n64 bytes, we can calculate the CPI overhead due to instruction authentication. In Figure 6,\nwe illustrate overheads to CPI due to instruction authentication, instruction cache and data\ncache misses for different cache sizes (and therefore different instruction and data miss rates).\nFigure 6.a distinguishes CPI overheads due to these three factors using different line styles.\nFigure 6.b illustrates total CPI overhead due to instruction authentication, instruction and\ndata cache misses, with different shades representing the overheads due to different factors.\nNote that, the CPI overhead due to instruction authentication is hardly noticeable (darkly\nshaded area on top) when compared the CPI overheads due to instruction and data cache\nmisses (lightly shaded areas at the bottom). Thus, we compared the CPI overheads due\nto instruction authentication and instruction cache misses in Figure 6.c, where the effect\nof instruction authentication is now slightly more noticeable. The proposed model benefits\nfrom large cache blocks, and hence incurs higher overheads when the block size is small.\nHowever, the overhead due to instruction authentication is still acceptable for even the\nsmallest block sizes common in embedded processors. The results in Figure 7 demonstrate\nthe relative overhead due to instruction authentication to overhead due to instruction cache\nmisses, which is taken as 1 for all block sizes. As observed from Figure 7 CPI overhead of\n18\n16 32 64 128 256\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\nAuthentication Overhead vs. Instruction Cache Miss Overhead\nR\nel\nat\niv\ne \nCP\nI O\nve\nrh\nea\nd\nCache Block Size (B)\nFigure 7: CPI Overheads due to instruction authentication normalized to CPI overhead of\ninstruction cache misses for different block sizes\ninstruction authentication is at most 30% of CPI overhead due to instruction cache misses.\nConsidering CPI overheads due to other factors (such as data cache miss, data dependency,\nbranch missprediction), the overhead due to instructions authentication is still negligible for\neven the smallest cache blocks commonly used in embedded processors.\nSome processors employ a technique in which the requested instruction starts executing\nwithout waiting for the other instructions in the same cache block to arrive from the memory.\nThe caches that employ this technique are known as early-start caches. Furthermore, there\nare critical-word-first caches that fetch the requested instruction first from the memory.\nThe proposed code authentication scheme, unfortunately, does not support these techniques\nsince the processor has to wait for the entire instruction block to arrive in the cache and to\nauthenticate before the execution of the requested instruction. We, therefore, explore the\noverhead incurred due to the fact the “early start” is not supported. In Figure 8, the area in\nblue shade represents the CPI overhead of lost clock cycles due to the “late start.” The late\nstart results in only a limited contribution to the overhead and this contribution increases\nwith the block size.\n19\n8 16 32 64 128 256\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\nCache Size (KB)\nCP\nI O\nve\nrh\nea\nd\na. Relative CPI Overheads\nAutentication vs. Late Start Losses\n \n \n16 32 64 128 256\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\nb. Late Start + Authentication vs.\nInstruction Misses\nCP\nI O\nve\nrh\nea\nd\nBlock Size (B)\nEarly Start Loss\nAuth\nFigure 8: CPI Overheads due to instruction authentication (including losses due to late-start)\nnormalized to CPI overhead of instruction cache misses for different block sizes\n8 Simulation Results and Implementation Issues\n8.1 Simulation Results\nWe used SimpleScalar [1] simulation tool to evaluate the overhead of the proposed scheme\non the overall CPI (clock cycle per instruction). SimpleScalar is a cycle-accurate simulator\nfor MIPS32 which is a RISC architecture used in both high-end and low-end processors. We\nused the out-of-order execution configuration for a conservative estimate for the overhead\nsince out-of-order execution method usually achieves lower CPI values. In our experiments,\nfive commonly employed benchmark programs are used: AES RIJNDAEL, COMPRESS,\nDIJKSTRA, GO, and SHA-113.\nIn order to evaluate the effects of the proposed authentication scheme on a wide range of\nprocessors, we simulated the five benchmarks for several cache configurations where different\ncache and block sizes are used. We keep the configurations as modest as possible. For\ninstance, we use only single-level cache organization where the largest cache size is 64 KB\nthat is becoming common in embedded processors. Since the cryptographic computations\nare overlapped with memory access cycles, we only considered memory access latency for\n13Benchmark programs are available at http://www.seas.gwu.edu/~bhagiweb/cs211/SimpleScalar/\nSimpleScalarInstructions.html\n20\nauthentication tags as an overhead. We assumed a small, 16-entry authentication cache\nimplemented as a FIFO buffer for holding recently accessed tags that takes approximately\n320 B on-chip memory space.\nAuthentication Cache Cache Size (KB)\nwith 16 entry 1 4 8 32 64\n16 67.92 53.77 49.01 28.03 13.74\nBlock 32 26.121 26.07 23.91 7.67 5.86\n(Size) 64 11.87 13.49 12.87 3.81 2.57\n(Bytes) 128 1.42 2.28 2.66 1.55 1.27\n256 0.17 0.39 0.35 0.55 0.56\nTable 3: CPI overhead as percentage in simulation results\nThe simulation results are summarized in Table 3 where one easily observes that the\noverhead percentage of CPI even in modest cache configurations becomes negligible. For\nhigh-end performance processors, the overhead will virtually disappear.\n8.2 Implementation Issues and Hardware Overhead\nThe most important design issue is whether the authentication mechanism can easily and\ninexpensively be integrated into a wide range of processor cores. Prohibitively high increase\nin hardware area is not acceptable while the latency of key generation mechanism (i.e. stream\ncipher cf. Figure 3) must be lower than the access latency of a memory block. As can be\nobserved in Figure 3, the stream cipher is the most latency-sensitive and area demanding\nunit in the proposed authentication mechanism. The other units are merely simple modulo-2\nadders, comparators, and relatively small amount of memory that we can use to store keys\nand tag values. In order to explore the feasibility of the proposed authentication and its\noverhead, we investigate hardware cost and latencies of state-of-the-art implementations of\ntwo ciphers; namely Trivium, a stream cipher proposed for eSTREAM (ECRYPT Stream\ncipher project)14, and AES that is originally a block cipher, but can be used to generate key\nbits. Four state-of-the art ASIC realizations of these ciphers are presented in Table 4, where\nthe stream cipher implementation in [28] is both light-weight in terms of area and sufficiently\nfast to easily match the latency requirements of our application. Similarly, even the ASIC\nrealizations of the stronger AES cipher15 can be profitably used in a wide range of processor\ncores (from embedded to high end performance processors) for code authentication.\nThe performance gap between the processor and memory technologies is one of the major\nissues in processor design since it tends to increase as the processor technologies improve\nmuch faster than memory technologies. Therefore, transferring a block of data or instruction\nbetween the memory and the cache is always one of the most expensive operations. The\naccess latency of a memory block depends on the size of the memory block used for memory-\nto-cache transfers besides other factors. Depending on the bus width and the block size,\n14see http://www.ecrypt.eu.org/stream/ and http://www.ecrypt.eu.org/stream/triviumpf.html\n15AES offers 128-bit security versus Trivium’s 80-bit.\n21\nTable 4: State-of-the art designs suitable for stream cipher\nDesign Technology Max. Frequency Area Throughput bits/cycle\nTrivium by Gaj et al. [28] 90nm 800 MHz ≈ 5645 51.2 Gpbs 64\nAES by Satoh [37] 0.11µm 145 MHz 12454 1.595 Gpbs 11\nAES by Hodjat et al. [23] 0.18µm 606 MHz 473000 77.6 Gpbs 128\nAES by Northpole Eng. [17] 0.25µm 323 MHz 26000 41.3 Gpbs 127.86\nbringing a block from memory to cache may typically take tens to hundreds of clock cycles.\nThe access latency of memory blocks in terms of the number of clock cycles used in the\nSimpleScalar [1] simulator is given in the first row of Table 5 for different block sizes. The\nproposed authentication scheme requires that the same number of bits as the block size be\ngenerated by the ciphers. Table 5 shows that all designs except for the one in [37] generate\nthe number of key bits sufficient for both instruction authentication and encryption. The\ndesign in [37] can be profitably used for embedded systems where typical memory block sizes\ndo not exceed 64 B. The design in [28], which is very fast and extremely area efficient, easily\nmatches the requirements of both embedded and of high-end performance processors.\nTable 5: Number of clock cycles required by each design to generate key bits when operated\nat frequencies up to maximum\nDesign 1 bit 1 B 16 B 32 B 64 B 128 B 256 B\nMemory Latency\nin SimpleScalar - - 26 34 50 82 146\n[28] 21 21.125 23 25 29 37 53\n[37] 0.086 0.678 11 22 44 88 176\n[23] 0.00781 0.0625 1 2 4 8 16\n[17] 0.00782 0.0626 1 2 4 8 16\nThe high-end performance processors work at higher frequencies and therefore may put\nhigher demands on our key generation mechanism. Working at their highest possible clock\nfrequencies, three designs in [28, 23, 17] can still provide a sufficient number of key bits for\na processor working at frequencies as high as 2 GHz (cf. Table 6). Note that for processors\nworking at very high frequencies, memory latencies can be much higher than our modest\nestimate derived from the SimpleScalar simulator. Even though the design in [23] is excessive\nin chip area usage, its overhead is negligible for high-end processors whose transistor counts\nare expressed in hundreds of millions coming close to a billion.\nIn summary, a quick overview of current technology and state-of-the-art for cryptographic\nalgorithm realizations reveals that the proposed code authentication scheme can easily be\nintegrated into a wide range of processor cores with a negligibly low overhead in area and\ntime.\n22\nTable 6: Number of clock cycles required by each design to generate key bits at clock\nfrequencies 1 GHz/2 GHz\nDesign 16 B 32 B 64 B 128 B 256 B\n[28] 28.75/57.5 31.25/62.5 36.25/72.5 46.25/92.5 66.25/132.5\n[23] 1.65/3.3 3.3/6.6 6.6/13.2 13.2/26.4 26.4/52.8\n[17] 3.09/6.2 6.2/12.4 12.4/24.8 24.8/49.6 49.6/99.2\nAnother practical issue is faults occurring in memory, registers, data bus, and control\ncircuitry that corrupt the bits in instructions, data and of special interest to us in authen-\ntication tags. The faults introduced to computation by mother nature or adversaries are\nshown to result in many undesired and dangerous situations where secrets are compromised\n[6, 24, 10] and the computation is corrupted or stalled. Therefore, there is a plethora of pub-\nlished works (e.g. [25, 40, 16, 35, 20]) that propose solutions to protect different aspects of\nthe computation. Only foreseeable result due to faults in authentication tags is that the com-\nputation is interrupted as the hardware refuses to execute the unauthenticated instructions.\nThis can be considered as a security flaw since adversary may mount a denial of service at-\ntack by introducing faults in the authentication tags. Therefore, authentication tags should\nbe protected against faults, adversarial or otherwise. However, the faults in authentication\ntags do not require a special treatment since corrupted bits in data and instructions normally\nresults in similar consequences. For instance, a corrupted bit in an instruction can result in\nan undefined instruction or access to restricted parts of the memory; each case an exception\nis thrown and execution most probably stops. The advantage of the proposed scheme is\nthat any mechanism deployed for the protection of data and instructions naturally protects\nthe authentication tags since they are part of the program state and stored and treated in\nthe same manner. Another advantage of using authentication tags is that any adversarial\nfault attack is immediately detected due to authentication circuitry. The protection against\nadversarial faults and fault attacks must be approached from a holistic perspective so that\nevery aspect of the computation is considered. This is an active research area in its infancy\nand naturally deserves serious treatment that is beyond the scope of this work.\n9 Previous Work and Comparison\nThere is plethora of work on secure and trusted execution of programs where the primary\ngoal is to provide the programs with a secure environment free from the interference of other\nmalicious programs. The common perception is that software-only solutions are inadequate\nand that hence hardware support is necessary. Furthermore, the operating system that\nimplements the core protection mechanism is overly complex and it is not possible write\na bug-free OS that is safeguarded against attacks. Major microprocessor manufacturers,\n(Intel, AMD, ARM) already introduced hardware extensions to their processor cores that\nallow isolated execution of programs [13, 15, 3]. This can be achieved by making the por-\ntions of memory, of cache, of TLB used by a program inaccessible to other programs. The\n23\ntechniques proposed in [32, 33] deals with performance problems in isolated execution of\nsecurity-sensitive codes by minimizing trusted code base and by proposing some hardware\nextensions. They also allow fast and fine-grained attestation of the code executed. However,\nboth the code base that manages the isolated execution mechanism and the code running\nin isolated environment have certain privileges (e.g. accessing sensitive information) and\ntherefore must be trusted. The proposed code authentication scheme provides this trust by\nauthentication every code block before their execution in an efficient manner.\nAnother line of work is concerned with encryption and authentication of frequently and\ndynamically changing data used by programs in execution. Theorems 1 and 2 imply that\nno adversary can generate an alternative value for a block. He can, however, substitute\na data block with an old (and already authenticated) data block. This is not a breach of\nsecurity for authenticated code, since the code is assumed to be static (and each block of\ncode is authenticated together with its address), but for dynamic data this is a security\nbreach. Authenticating data is not in the scope of this work, but since data can have an\nimpact on the execution of a program, we describe how our code authentication scheme can\nwork together with previously proposed memory authentication schemes — in particular the\nschemes presented in the papers [11, 19, 12, 42].\nIn [19] Gassend et al. propose a memory authentication scheme based on Merkle trees\n[34]. To authenticate an arbitrarily large untrusted RAM memory, an m-array tree structure\nis built up. Each node of the tree contains collision resistant hashes of each of them children.\nThe leaves of the tree contain the actual data. Parent nodes contain hash values of data\nor other hash values. The root of the tree is kept on-chip in a trusted register of constant\nsize, all other nodes are kept in main memory or cache. To authenticate a block of data (or\nan inner node) all hash values from the data to the root of the hash-tree are checked. To\nupdate data, each hash value for the data block to the root has to be recalculated. This\nscheme is secure, since all modifications to the memory require a modification of the root of\nthe tree, which is kept in a trusted register, thus no unauthenticated modifications can take\nplace. The worst case time complexity of each read or write operation is O(logm(N)) (for\nbalanced hash-trees), where N is the size of the memory. However, the scheme is improved by\nobserving that nodes that are already in trusted L1 cache do not need re-authentication. Re-\nauthentication of dirty cache-lines is only done when the cache-lines are flushed to memory.\nThe memory overhead of the scheme is 1/(m− 1).\nIn [11] Clarke et al. propose a scheme which keeps (the hash values of) logs of all read\nand write operations. The memory can then be authenticated off-line by comparing the\nlogs with the actual content of the memory. This scheme is inspired by incremental hashing\nby Bellare et al. [4]. Clarke et al. introduce the new concept of multiset hash functions,\nwhich are families of hash functions that map a multiset (a set with possible repetitions) to a\nconstant size hash value. They give three constructions of multiset hash functions which are\nincremental — that is: given two multisets and their hash values, it is efficient to compute the\nhash value of the multiset-union of the two multisets. The idea for memory authentication\nis to keep logs of all read and write operations. Log entries contain address, data, and\ntimestamp triples. Since the logs can become arbitrarily large, only two multiset hashes are\nkept: one for read and one for write operations. When authentication is requested all memory\nlocations have to be compared with the logs. This gives a very inefficient authentication of\nO(N), but read and write operations require only a small constant overhead (adding entries\n24\nto the read and write logs). The only memory requirement for this scheme are the two\nmultiset hash values of the logs. Since authentication is very expensive, it is only done when\ndata is exported out of a program execution environment (e.g. microprocessor).\nIn [12] Clarke et al. propose a hybrid protocol with the aim of balancing the drawbacks\nand benefits of the two protocols described in [11] and [19]. The overhead of this scheme\ntends to a constant as the number of instructions between critical instruction grows.\nThe state-of-the-art in memory encryption/authentication implementations is introduced\nby Yan et al. in [42], where Galois/Counter Mode of operation (GCM) is used to reduce\nauthentication latency and overlap it with memory accesses (similar to our scheme in this\nrespect). Using GCM encryption and authentication together resembles our authentication\nscheme; however our construction is more general and more flexible in the sense that it\nallows different implementation alternatives. They provide implementation results of their\nschemes where only 4% IPC degradation is reported which is a significant improvement over\nclassical hash- or MAC-based schemes. As the work targets high-end performance processors,\nthe implementation results are given for a system with a rich configuration; e.g. 1 MB L2\ncache with 64-byte blocks, 128-bit system bus that connects memory and processor etc.\nThe size of the cache and cache blocks have a decisive effect on the miss rates and a cache\nof 1 MB as used in the experiments of [42] results in a very low miss rate and hence the\ncryptographic operation for tag computation becomes extremely infrequent. A system bus of\n128-bit perfectly matches 128-bit Galois field multiplication used in the GCM authentication.\nAnother issue is that the length of the authentication tag is a mere 64-bit which does not\nprovide a sufficient security since finding collusions is not difficult for 64-bit authentication\ntags16. Although 64-bit authentication tags can be used for frequently changing data blocks,\nits use for static instruction blocks is definitely not advisable. The scheme uses 32 KB\non-chip cache for counter values, which may not be accommodated in embedded systems.\nFurthermore, the authentication scheme must be implemented with the encryption scheme,\nwhich is an additional burden for cases where the encryption is not needed. And finally, the\nscheme relies on block ciphers which are considered to be slower than stream ciphers that\ncan be profitably used for their speed and low resource requirements.\nIt is natural to ask how the schemes proposed in [11], [19], and [12] relate to our scheme.\nThese schemes solve the problem of “checking if the untrusted RAM behaves like valid RAM\n[12].” The aim of these papers is to ensure that only the main processor can modify the\nmemory. However, the processor will authenticate any modification that is made to the\nmemory by code which is running on the processor. There is no mechanism to prevent a\nvirus, for instance, to modify data (or code) in the memory. To avoid this problem two\nthings are needed: 1) code must be authenticated before it is executed (in particular when\nit is loaded into the memory), and 2) modification of code must be prevented. The schemes\nin [11, 19, 12, 42] can be used in conjunction with a TPM to authenticate code at the\ntime it is loaded, by, e.g. letting the TPM verify a digital signature of the code before the\ncode is executed. This, however, will increase the load time of a program considerably. To\nprevent modification of code, write protection mechanisms should be added to the schemes in\n[11, 19, 12, 42]. However, such a scheme will always inherit the non-constant authentication\n16The GCM schemes allows up to 128-bit MACs. However, a longer tag than 64-bit will negatively affect\nthe IPC degradation in [42].\n25\noverhead from these techniques.\nIn the schemes of [11] and [19] the validation of any block of memory involves the trusted\non-chip hash value(s). In particular, after loading a new program into memory the scheme\nin [19] has to compute the entire sub-hash-tree covering the code of the program in order to\nvalidate the first instruction. For large programs this will give relatively long loading times.\nIn [11] all verifications involves verifying the entire memory.\nThe scheme in [11] is specially tuned for dynamic data, since it keeps a log of read and\nwrite operations. However, verification of any block of memory in [11] involves hashing of\nthe entire memory. While keeping read and write logs is a good solution for dynamic data,\nit clearly introduces unnecessary overhead if the data is static.\nSince our scheme does not face the difficulties of dynamically changing memory, we can\navoid the associated overhead. Each authentication operation in our scheme only introduces\na small constant overhead which can mostly be hidden in the memory access operation.\nFurthermore, as discussed in Section 3, our scheme can be used directly to verify that the\ncode which is loaded and executed is authentic, as well as to protect the intellectual property\nof software manufacturers.\nThe above discussion shows how our scheme and the schemes in [11, 19, 12, 42] comple-\nment each other. Our scheme can authenticate a program and the static data that is loaded\ninto the memory, while the schemes in [11, 19, 12, 42] can guarantee that dynamic data is\ncreated by the application and not the adversary. While our scheme can be used for code\nand static data authentication for efficiency reasons, the previous schemes in [11, 19, 12, 42]\ncan be used for dynamic data authentication once the program execution starts.\nLee et al. in [30] propose a technique for code authentication based on AES-MAC com-\nputation along with encryption. They report that the latency due to authentication is 100\nclock cycles and that overall performance degradation is around 1%. One major issue with\nthis technique is that the high latency of authentication tag computation is not overlapped\nwith memory access operation. Another, probably more important issue, is that perfor-\nmance figures are obtained for a very large L2 cache of 2MB for which instruction misses are\nextremely infrequent and therefore instruction authentication occurs extremely infrequently\nas well. However, for smaller caches used in embedded processors, the performance penalty\ncan be prohibitively high. The last issue with the technique in [30] is that 16 bytes of an\ninstruction block is used to store the authentication tag for the rest of the instruction block.\nSince the effective size of the instruction block is actually reduced, the miss rate will increase.\nFurthermore, the technique may not be applicable for cache lines of smaller sizes; namely\n16 B and 32 B common in embedded processors. In contrast to the technique in [30], our\nmethod does not affect the instruction miss rate, it can be applied in embedded systems\nwith much smaller cache and block sizes, most of the cryptographic computations can be\noverlapped with memory access cycles, and our authentication cache can take advantage of\nspatial locality.\nYang et al. in [43] propose an efficient memory encryption technique where cryptographic\ncomputation is overlapped with memory access cycles. This technique deals with the en-\ncryption/decryption of dynamically changing data and data/instruction authentication is\nnot addressed. They report 1.28% performance penalty for a system with 256 KB L2 unified\ncache of 128 B cache lines where an additional 64 KB on-chip storage is required.\nAny comparison of our scheme with the previous work is not fair due to the following\n26\nreasons: i) different problems are addressed (trusted code execution through instruction\nauthentication vs. memory encryption and authentication), ii) different classes of processors\nare targeted (a wide range of processors versus high-end performance processors), and iii)\ndifferent simulation environments (simulation tool and benchmarks) are used. Our two basic\nclaims are that the code authentication is a different problem than memory encryption and\nauthentication and that we do not have to suffer high hardware and timing complexities of\nthe latter since the former can be solved in a much more efficient way. Having said that\nthe comparison is not fair, it may still be useful to display different aspects of our scheme\nvis-a`-vis with those of the previous schemes as we do in Table 7.\nTable 7: An unfair comparison with the previous schemes\nAspects [30] [43] [42] proposed\nCache size 2 MB 256 KB 1 MB 64 KB (max.)\nBlock size 64 B 128 B 64 B 16 B and up\nCache levels 2 2 2 1\nOn-chip memory for tags NA 64 KB 32 KB 320 B (max.)\nReported overhead (best) 1% 1.28% 4% 0.56%\nDynamic data authentication No No Yes No\nSeparate Authentication and\nEncryption Yes NA No Yes\nCryptographic Primitive Block Cipher Stream Cipher Block Cipher Stream Cipher\nAuthentication Tag Size 128 NA 64 128\nAs observed in Table 7, the proposed scheme can provide code and static data authenti-\ncation of sufficient security level with a very low overhead both in time complexity and area\nusage. Its low cost nature renders itself to be used even in low-cost resource constrained\nembedded processors for which the estimated cost of the previous schemes are not afford-\nable. Conversely, the latency overhead of the proposed scheme will virtually disappear for\nhigh-end performance computers.\n10 Conclusion\nIn this paper we presented a code authentication scheme with low overhead that is tightly\nintegrated into a processor architecture to facilitate on-the-fly code and static data au-\nthentication. The presented scheme builds on the previously proposed idea of hiding the\ncomputational latency of encryption behind memory access latencies. The security of the\nscheme is established using message authentication codes, based on efficient universal fam-\nilies of hash functions, which provide security when used with one-time pad encryption.\nFurthermore, since the proposed authentication technique manages to hide the latency, it\nis suited to be used to ensure the integrity of code and static data blocks encrypted with\nefficient stream ciphers which otherwise are open to many forms of attacks. Hence, the\nproposed authentication technique enables off-the-critical-path code encryption. The perfor-\nmance analysis shows that the presented architecture bears little overhead for even modest\n27\ncache sizes. Our simulation results confirms our claims on the reduced overhead in time and\nspace complexities.\n11 Acknowledgement\nThe authors would like to thank the anonymous referees for their helpful comments. The\nworks of Erkay Savas¸ and Ahmet O. Durahim are supported by the Scientific and Techno-\nlogical Research Council of Turkey (TUBITAK) under project number 105E089 (TUBITAK\nCareer Award). The work of Berk Sunar is supported by the National Science Foundation\nunder Grants No. ANI-0133297 (NSF CAREER Award) and CNS-0831416.\nReferences\n[1] The simplescalar tool set. Available at http://wwww.simplescalar.com/.\n[2] Kazumaro Aoki and Helger Lipmaa. Fast implementations of AES candidates. In AES\nCandidate Conference, pages 106–120, New York City, USA, 13–14 April 2000.\n[3] ARM. TrustZone Technology Overview. http://www.arm.com/products/security/\ntrustzone/.\n[4] Mihir Bellare, Oded Goldreich, and Shafi Goldwasser. Incremental cryptography: The\ncase of hashing and signing. In Desmedt [14], pages 216–233.\n[5] John Black, Shai Halevi, Hugo Krawczyk, Ted Krovetz, and Phillip Rogaway. UMAC:\nFast and secure message authentication. In Michael J. Wiener, editor, CRYPTO’99,\nvolume 1666 of Lecture Notes in Computer Science, pages 216–233. Springer, 1999.\n[6] Dan Boneh, Richard A. DeMillo, and Richard J. Lipton. On the importance of checking\ncryptographic protocols for faults (extended abstract). In EUROCRYPT, pages 37–51,\n1997.\n[7] Gilles Brassard. On computationally secure authentication tags requiring short secret\nshared keys. In CRYPTO’82, Lecture Notes in Computer Science, pages 79–86. Springer-\nVerlag, 1982.\n[8] Larry Carter and Mark N. Wegman. Universal classes of hash functions. J. Comput.\nSyst. Sci., 18(2):143–154, 1979.\n[9] Benoˆıt Chevallier-Mames, David Naccache, Pascal Paillier, and David Pointcheval. How\nto disembed a program? In Marc Joye and Jean-Jacques Quisquater, editors, CHES\n2004, volume 3156 of Lecture Notes in Computer Science, pages 441–454. Springer-\nVerlag, 2004.\n[10] H. Choukri and M. Tunstall. Round reduction using faults. In L. Breveglieri and\nI. Koren, editors, 2nd International Workshop on Fault Diagnosis and Tolerance in\nCryptography (FDTC’05), pages 13–24, 2005.\n28\n[11] Dwaine E. Clarke, Srinivas Devadas, Marten van Dijk, Blaise Gassend, and G. Edward\nSuh. Incremental multiset hash functions and their application to memory integrity\nchecking. In Chi-Sung Laih, editor, ASIACRYPT 2003, volume 2894 of Lecture Notes\nin Computer Science, pages 188–207. Springer-Verlag, 2003.\n[12] Dwaine E. Clarke, G. Edward Suh, Blaise Gassend, Ajay Sudan, Marten van Dijk, and\nSrinivas Devadas. Towards constant bandwidth overhead integrity checking of untrusted\ndata. In IEEE Symposium on Security and Privacy, pages 139–153. IEEE Computer\nSociety, 2005.\n[13] Intel Corporation. LeGrande technology preliminary architecture specification. Intel\nPublication no. D52212, May 2006.\n[14] Yvo Desmedt, editor. Advances in Cryptology - CRYPTO ’94, 14th Annual Interna-\ntional Cryptology Conference, Santa Barbara, California, USA, August 21-25, 1994,\nProceedings, volume 839 of Lecture Notes in Computer Science. Springer-Verlag, 1994.\n[15] Advanced Micro Devices. AMD64 virtualization: Secure virtual machine architecture\nmanual. AMD Publication no. 33047 rev. 3.01, May 2005.\n[16] D. Pradhan ed. Fault Tolerant Computing – Theory and Techniques, volume 1. New\nJersey: Prentice-Hall, 1st edition, 1986.\n[17] North Pole Engineering. AES core. http://www.hardware-ciphers.com/en/aes/\nasic-unrolled.html.\n[18] Kris Gaj and Pawel Chodowiec. Fast implementation and fair comparison of the final\ncandidates for advanced encryption standard using field programmable gate arrays. In\nDavid Naccache, editor, CT-RSA, volume 2020 of Lecture Notes in Computer Science,\npages 84–99. Springer, 2001.\n[19] Blaise Gassend, G. Edward Suh, Dwaine E. Clarke, Marten van Dijk, and Srinivas\nDevadas. Caches and hash trees for efficient memory integrity. In Proceedings of Ninth\nInternational Symposium of High Performance Computer Architecture (HPCA 2003),\npages 295–306, February 2003.\n[20] Gunnar Gaubatz and Berk Sunar. Robust finite field arithmetic for fault-tolerant public-\nkey cryptography. In Luca Breveglieri, Israel Koren, David Naccache, and Jean-Pierre\nSeifert, editors, FDTC, volume 4236 of Lecture Notes in Computer Science, pages 196–\n210. Springer, 2006.\n[21] Oded Goldreich. Secure multi-party computation. Working Draft, Verison 1.1, 1998.\nciteseer.ist.psu.edu/article/goldreich98secure.html.\n[22] J. Hennesy and D. Patterson. Computer Architecture: A Quantitative Approach. Mor-\ngan Kaufmann Publishers Inc. (Elsevier), 3rd edition, 2002.\n29\n[23] Alireza Hodjat and Ingrid Verbauwhede. Speed-area trade-off for 10 to 100 Gbits/s\nthroughput AES processor. In 2003 IEEE Asilomar Conference on Signals, Systems,\nand Computers, November 2003.\n[24] Marc Joye, Arjen K. Lenstra, and Jean-Jacques Quisquater. Chinese remaindering\nbased cryptosystems in the presence of faults. J. Cryptology, 12(4):241–245, 1999.\n[25] Albert L. Hopkins Jr. and T. Basil Smith III. The architectural elements of a symmetric\nfault-tolerant multiprocessor. IEEE Trans. Computers, 24(5):498–505, 1975.\n[26] Jens-Peter Kaps, Kaan Yu¨ksel, and Berk Sunar. Energy scalable universal hashing.\nIEEE Trans. Computers, 54(12):1484–1495, 2005.\n[27] K. Kaukonen and R. Thayer. A stream cipher encryption algorithm “ARCFOUR”, inter-\nnet engineering task force (IETF) internet draft, July 14 1999. http://www.mozilla.\norg/projects/security/pki/nss/draft-kaukonen-cipher-arcfour-03.txt.\n[28] K.Gaj, G. Southern, and R. Bachimanchi. Comparison of hardware performance of\nselected phase ii eSTREAM candidates. State of the Art of Stream Ciphers Work-\nshop (SASC 2007), February 1 2007. http://www.ecrypt.eu.org/stream/papersdir/\n2007/026.pdf.\n[29] Hugo Krawczyk. Lfsr-based hashing and authentication. In Desmedt [14], pages 129–\n139.\n[30] Ruby B. Lee, Peter C. S. Kwan, John Patrick McGregor, Jeffrey S. Dwoskin, and\nZhenghong Wang. Architecture for protecting critical secrets in microprocessors. In\nISCA, pages 2–13. IEEE Computer Society, 2005.\n[31] Daihyun Lim, Jae W. Lee, Blaise Gassend, G. Edward Suh, Marten van Dijk, and\nSrinivas Devadas. Extracting secret keys from integrated circuits. IEEE Trans. VLSI\nSyst., 13(10):1200–1205, 2005.\n[32] Jonathan M. McCune, Bryan Parno, Adrian Perrig, Michael K. Reiter, and Hiroshi\nIsozaki. Flicker: an execution infrastructure for tcb minimization. In Joseph S. Sventek\nand Steven Hand, editors, EuroSys, pages 315–328. ACM, 2008.\n[33] Jonathan M. McCune, Bryan Parno, Adrian Perrig, Michael K. Reiter, and Arvind\nSeshadri. How low can you go?: recommendations for hardware-supported minimal tcb\ncode execution. In Susan J. Eggers and James R. Larus, editors, ASPLOS, pages 14–25.\nACM, 2008.\n[34] Ralph C. Merkle. Secrecy, authentication, and public key systems. PhD thesis, Electrical\nEngineering, Stanford, 1979.\n[35] Arash Reyhani-Masoleh and M. Anwar Hasan. Towards fault-tolerant cryptographic\ncomputations over finite fields. ACM Trans. Embedded Comput. Syst., 3(3):593–613,\n2004.\n30\n[36] Marcin Rogawski. Hardware evaluation of estream candidates: Grain, lex, mickey128,\nsalsa20 and trivium. State of the Art of Stream Ciphers Workshop (SASC 2007), Febru-\nary 1 2007. http://www.ecrypt.eu.org/stream/papersdir/2007/025.pdf.\n[37] Akashi Satoh, Sumio Morioka, Kohji Takano, and Seiji Munetoh. A compact rijndael\nhardware architecture with s-box optimization. In Colin Boyd, editor, ASIACRYPT,\nvolume 2248 of Lecture Notes in Computer Science, pages 239–254. Springer, 2001.\n[38] D. R. Stinson. Cryptography: Theory and Practice. Chapman & Hall/CRC (Taylor\nFrancis Group), 3rd edition, 2006.\n[39] G. Edward Suh, Dwaine E. Clarke, Blaise Gassend, Marten van Dijk, and Srinivas\nDevadas. AEGIS: architecture for tamper-evident and tamper-resistant processing. In\nUtpal Banerjee, Kyle Gallivan, and Antonio Gonza´lez, editors, ICS 2003, pages 160–171.\nACM, 2003.\n[40] Berk Sunar, Gunnar Gaubatz, and Erkay Savas. Sequential circuit design for embed-\nded cryptographic applications resilient to adversarial faults. IEEE Trans. Computers,\n57(1):126–138, 2008.\n[41] Trusted Computing Group, Incorporated. TCG Software Stack (TSS), Specification\nVersion 1.2, Level 1. Part1: Commands and Structures, January 6 2006. https://www.\ntrustedcomputinggroup.org/specs/TSS/TSS_Version_1.2_Level_1_FINAL.pdf.\n[42] Chenyu Yan, Daniel Englender, Milos Prvulovic, Brian Rogers, and Yan Solihin. Im-\nproving cost, performance, and security of memory encryption and authentication. In\nISCA, pages 179–190. IEEE Computer Society, 2006.\n[43] Jun Yang, Lan Gao, and Youtao Zhang. Improving memory encryption performance in\nsecure processors. IEEE Trans. Computers, 54(5):630–640, 2005.\n[44] Youtao Zhang, Jun Yang, Yongjing Lin, and Lan Gao. Architectural support for pro-\ntecting user privacy on trusted processors. SIGARCH Computer Architecture News,\n33(1):118–123, 2005.\nProof of Theorem 2\nProof 1 LetM , M ′ be distinct members of the domain A with equal lengths. We are required\nto show that\nPr [PRK(M)⊕ PRK(M\n′) = c] = 2−w .\nNote that since we are working over a finite field GF (2w) the exclusive-or operation is iden-\ntical to an addition operation and we may use the symbols ‘⊕’ and ‘+’ interchangibly. Ex-\npanding the terms inside the probability expression, we obtain\nn/2∑\ni=1\n(m2i−1 + k2i−1)(m2i + k2i) +\nn/2∑\ni=1\n(m′2i−1 + k2i−1)(m\n′\n2i + k2i) = c (mod p) .\n31\nThe probability is taken over uniform choices of (k1, k2, . . . , kn) with each ki ∈ GF (2\nw). Since\nM and M ′ are distinct, mi 6= m\n′\ni for some 1 ≤ i ≤ n. Addition and multiplication in GF (2\nw)\nare commutative, hence there is no loss of generality in assuming m2 6= m\n′\n2. Hence we need\nto prove that for any choice of k2, k3, . . . , kn that\nPrk1∈GF (2w)\n\n(m1 + k1)(m2 + k2) +\nn/2∑\ni=2\n(m2i−1 + k2i−1)(m2i + k2i)+\n(m′1 + k1)(m\n′\n2 + k2) +\nn/2∑\ni=2\n(m′2i−1 + k2i−1)(m\n′\n2i + k2i) = c (mod p)\n\n ≤ 2−w\nLet\ny =\nn/2∑\ni=2\n(m′2i−1 + k2i−1)(m\n′\n2i + k2i) +\nn/2∑\ni=2\n(m2i−1 + k2i−1)(m2i + k2i) .\nRewriting the identity inside the probability yields\nk1(m2 +m\n′\n2) = y + c+m1(m2 + k2) +m\n′\n1(m\n′\n2 + k2) (mod p) .\nSince m2 6= m\n′\n2, the term (m2+m\n′\n2) cannot be zero and its inverse in GF (2\nw) exists. Hence\nthere is exactly one k1 ∈ GF (2\nw) satisfying the equation, which is\nk1 = (m2 +m\n′\n2)\n−1 (y + c+m1(m2 + k2) +m\n′\n1(m\n′\n2 + k2)) (mod p) .\nTherefore,\nPr [PRK(M)⊕ PRK(M\n′) = c] = 2−w .\n32\n",
            "id": 5013678,
            "identifiers": [
                {
                    "identifier": "1979121530",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "11741227",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1049/iet-cdt.2007.0122",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:research.sabanciuniv.edu:12979",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "186516004",
                    "type": "CORE_ID"
                }
            ],
            "title": "Transparent code authentication at the processor level",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "1979121530",
            "oaiIds": [
                "oai:research.sabanciuniv.edu:12979"
            ],
            "publishedDate": "2009-01-01T00:00:00",
            "publisher": "'Institution of Engineering and Technology (IET)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "https://research.sabanciuniv.edu/12979/4/authcode_final.pdf",
                "http://research.sabanciuniv.edu/12979/3/code_auth.pdf"
            ],
            "updatedDate": "2021-05-01T05:25:32",
            "yearPublished": 2009,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "1751-8601"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/pdf/11741227.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/11741227"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/11741227/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/11741227/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/5013678"
                }
            ]
        },
        {
            "acceptedDate": "2010-11-23T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Germen, Murat"
                }
            ],
            "contributors": [
                "Murat"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/11742534",
                "https://api.core.ac.uk/v3/outputs/431520016"
            ],
            "createdDate": "2013-07-12T17:46:59",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 393,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/393",
                    "logo": "https://api.core.ac.uk/data-providers/393/logo"
                }
            ],
            "depositedDate": "2010-11-01T00:00:00",
            "abstract": "Photography is a powerful two dimensional representation tool to document three dimensional volumes like architecture. It is possible to manipulate photos with two dimensional tools like Photoshop in order to suggest new three dimensional re/formations and re/interpret architecture. One can alternatively use two dimensional textures as mappings to create realistic three dimensional model renderings. This project is a combination of these two approaches: photographing architecture, turning the resulting photos into transparent image files, and then mapping these photos onto three dimensional volumes in order to create a ‘new’ architecture from an ‘existing’ architecture. \n\n\n\nOne of the advantages of using photographs to create architecture is that the photo pool can easily be composed of visuals from various cultures and you may end up using an amalgam of visuals from, say, so-called opposite cultures. This possibility reminds the peaceful collaboration of musicians from different cultures to create a unique music. In addition, this act can also be taken as a migration of media through appropriation of photography for three dimensional volume creation and re/presentation. At this point, we are talking about a double representation, since photography is a representation tool already and it gains another representational dimension when it is remapped onto three dimensional volumes for the construction of an alternative reality.\n\n\n\nThis article concentrates on using a representation tool (photography) to construct a three dimensional space (architecture) within a virtual three dimensional environment (Second Life®). During the process the concepts of perception, reality, cultural context, re/presentation and appropriation will be examined",
            "documentType": "research",
            "doi": "10.1386/mvcr.1.1.35_1",
            "downloadUrl": "https://core.ac.uk/download/pdf/11742534.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "33\nMVCR 1 (1) pp. 33–48  Intellect Limited 2010\nMetaverse Creativity \nVolume 1 Number 1 \n© 2010 Intellect Ltd Article. English language. doi: 10.1386/mvcr.1.1.33_1\nKEYWORDS\nphotography\nconstruct\nperception\nvirtual reality\nrepresentation\nmetaverse\nvirtual architecture\nperspectivism\nMURAT GERMEN\nSabanci University\nUsing 2D photography as a \n3D constructional tool within \nthe metaverse\nABSTRACT\nPhotography is a powerful 2D representation tool to document 3D volumes like \narchitecture. It is possible to manipulate photos with 2D tools like Photoshop in \norder to suggest new 3D re/formations and re/interpret architecture. One can \nalternatively use 2D textures as mappings to create realistic 3D model renderings. \nThis project is a combination of these two approaches: photographing architecture, \nturning the resulting photos into transparent image files, and then mapping these \nphotos onto 3D volumes in order to create a ‘new’ architecture from an ‘existing’ \narchitecture. \nOne of the advantages of using photographs to create architecture is that the \nphoto pool can easily be composed of visuals from various cultures and you may \nend up using an amalgam of visuals from, say, so-called opposite cultures. This \npossibility reminds the peaceful collaboration of musicians from different cultures \nto create a unique music. In addition, this act can also be taken as a migration \nof media through appropriation of photography for 3D volume creation and re/\npresentation. At this point, we are talking about a double representation, since \nphotography is a representation tool already and it gains another representa-\ntional dimension when it is remapped onto 3D volumes for the construction of an \n alternative reality.\nThis article concentrates on using a representation tool (photography) to con-\nstruct a 3D space (architecture) within a virtual 3D environment (Second Life®). \n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n11. \n12. \n13. \n14. \n15. \n16. \n17. \n18. \n19. \n20. \nMVCR_1.1_art_Germen_33-48.indd   33 10/12/10   1:45:42 PM\nMurat Germen\n34\nDuring the process the concepts of perception, reality, cultural context, re/presenta-\ntion and appropriation will be examined.\nARCHITECTURE, PHOTOGRAPHY AND TRUTH\nPhotography is the only medium that enables architectural works to be shared \nwith people who do not have access to these works. It is, in this respect, the \nultimate representation of architecture that is built. There are various tech-\nniques, lenses, rules of thumb that are used in architectural photography \nin order to make the process as ‘appropriate’ as possible. But these special \ntechniques usually provide us with unique visual recording possibilities that \nare practically and physically impossible to the naked eye. The so-called ‘per-\nspective correction’ process, much used in architectural photography, carries \nthe potential of producing some steeply converging lines, especially when \nthe photographer is close to the building being photographed. Consequently, \nthe shifting motion in photography causes another shift in our perception: \nphotography does not reflect the truth.\nFigures 1 and 2: Professional architectural photography samples where two \nhorizontal photos taken with a wide-angle, tilt-shift lens were combined in order \nto widen the angle even further and augment the perception. This coverage is not \npossible with a single shot and offers a unique aesthetic. Photos by Murat Germen, \n9 August 2007.\n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n11. \n12. \n13. \n14. \n15. \n16. \n17. \n18. \n19. \n20. \n21. \n22. \n23. \n24. \n25. \n26. \n27. \n28. \n29. \n30. \n31. \n32. \n33. \n34. \n35. \n36. \n37. \n38. \n39. \n40. \n41. \n42. \n43. \n44. \n45. \n46. \n47. \n48. \n49. \n50. \n51. \n52. \nMVCR_1.1_art_Germen_33-48.indd   34 10/12/10   1:45:42 PM\nUsing 2D photography as a 3D constructional tool …\n35\nConsidering the fact that there are different lenses ranging from wide to \ntele-angle, different films for different purposes yielding different contrast his-\ntograms, different speed values that lead to various levels of graininess, the \nfact that we do not see in black and white, etc.; it is possible to assert that \ncameras do not see in the manner we see and therefore the photographs that \ncameras take have no possibility of reflecting the truth as we see with our \neyes. Piotrowski and Robinson approach the problem from another angle: \nPhotography, on the other hand, filters reality in a different way. A \nphotograph seems to be an ‘objective’ record of the field of vision that \nis trustworthy because the photochemical process provides a  reliable \nmethod of recording an image. […] All that makes photography appear \nbelievable or objective conceals how much a photograph is a constructed \nrepresentation.\n(Piotrowski and Robinson 2001: 54)\nUnlike a person’s experience in architectural space, a photographer’s picture \nsingles out a particular view and freezes it in time. The image is composed \nso that it is seen in a certain manner, making particular relationships visible \nand hiding others. Photographers frequently manipulate light, either artifi-\ncial or natural, to enhance selected attributes of architecture. Promotional \nphotographs of architecture, rather than supporting a symbolic dialogue \nbetween the viewer and a depicted building, encourage the viewer’s desire \nto own a similar kind of architectural commodity. ‘This constructed desire \nfor the represented object shapes the commercial subject-object relationship’ \n(Piotrowski 2001: 54).\n With reference to the notion of an ideal truth Mark Kingwell puts it clearly: \nThe image is made, not found, and the making is inherently personal, \nrooted in prejudice. The important truth is to recognize and acknowl-\nedge bias openly, not least in the essential decisions around framing \nthe image. […] Our investigation must entail a special kind of refusal: \na refusal to take the taken-for-granted for granted. It follows that \nthe responsible image is the one that makes that refusal necessary, \n unavoidable, insistent. That is the truth in the image though perhaps \nnot the truth we thought to find.\n(Kingwell 2006: 16) \nThe concepts of objectivity and the presence of a single dogmatic reality are \nalso criticized by Vilém Flusser, who states that ‘the apparent non-symbolic, \n‘objective’ character of technical images has the observer looking at them as \nif they were not really images, but a kind of window on the world’. He goes \non to say that the viewer trusts what he/she sees in the way in which they \ntrust their own eyes. If there is any criticism involved, it is not as a critique \nof image, but as a critique of vision; the critique is not concerned with their \nproduction, but with the world ‘as seen through’ them. \nSuch a lack of critical attitude towards technical images is dangerous in \na situation where these images are about to displace texts. The uncritical \nattitude is dangerous because the ‘objectivity’ of the technical image is a \ndelusion. They are, in truth, images, and as such, they are symbolical.\n(Flusser 2000: 4)\n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n11. \n12. \n13. \n14. \n15. \n16. \n17. \n18. \n19. \n20. \n21. \n22. \n23. \n24. \n25. \n26. \n27. \n28. \n29. \n30. \n31. \n32. \n33. \n34. \n35. \n36. \n37. \n38. \n39. \n40. \n41. \n42. \n43. \n44. \n45. \n46. \n47. \n48. \n49. \n50. \n51. \n52. \nMVCR_1.1_art_Germen_33-48.indd   35 10/12/10   1:45:46 PM\nMurat Germen\n36\nSince we deal with symbols at this point, the notion of representation comes \nin. As Fritjof Capra states in his Tao of Physics, ‘representation of reality is \nso much easier to grasp than reality itself, we tend to confuse the two and \nto take our concepts and symbols for reality’ (Capra 1975: 28). This is also \nvery much in parallel with Jean Baudrillard’s statements in his philosophi-\ncal treatise Simulacres et Simulation, where he asserts that simulated copy \nFigure 3 and 4: Staging a succession of planes devoid of the typical depth of field, \nwith all its planes kept clearly and no shadows cast (due to online rendering \nlimitations in Second Life®), leads to an idiosyncratic perception mode that \nfurther fosters the concept of constructed reality and creation of a personal world. \nThis personal world exists in the virtual world and the particular experience of \nthe constructed reality takes place with the help of a concept that we can call \n‘tele-presence’, which focuses on the relationship between an individual and his/\nher personally mediated environment. Three-dimensional modelling artworks \nconstructed in Second Life® by Murat Germen, 2008–2009.\n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n11. \n12. \n13. \n14. \n15. \n16. \n17. \n18. \n19. \n20. \n21. \n22. \n23. \n24. \n25. \n26. \n27. \n28. \n29. \n30. \n31. \n32. \n33. \n34. \n35. \n36. \n37. \n38. \n39. \n40. \n41. \n42. \n43. \n44. \n45. \n46. \n47. \n48. \n49. \n50. \n51. \n52. \nMVCR_1.1_art_Germen_33-48.indd   36 10/12/10   1:45:46 PM\nUsing 2D photography as a 3D constructional tool …\n37\nhas superseded the original object, therefore representation has replaced the \n reality it illustrated. Since representations are personal definitions of particular \npersonal experiences and perceptions, it becomes rather problematical to talk \nabout objectivity where reality is concerned.\nARCHITECTURE AND CONSTRUCT\nThe concept of construction in the architectural design process is a tempo-\nrary process which finally transforms itself into an end ‘product’: a building, \na culture, a society, an idea, a freedom, a dogma, etc. Construction sites \ncan be conceived as stages where this process is being ‘performed’ over \nand over again. The inherent incompleteness within the constructing act \npushes us to dream; on the other hand, a completed product loses its nar-\nrative potential as it as it gives us all the necessary pieces that constitute \nthe whole: there is no puzzle to solve and no story to write. Construction \nsites, in this sense, are like historical ruins; Paul Zucker asserts that ‘devas-\ntated by time or wilful destruction, incomplete as they are, ruins represent \na combination of man-made forms and of organic nature’ (Zucker 1961: \n119). As a tribute to and resting on this statement, the more incomplete the \nconstruct is the more organic life gets, and the more surprises and the less \nboundaries we have.\nArchitectural photography has the potential to recreate the previously \nmentioned puzzle in order to bring an alternative representation to archi-\ntecture. The architectural photographer is sometimes offered the freedom of \nreinterpreting and reconstructing architecture in order to be able to present a \nnovel virtual perception to the audience. The idea here is to get a set of spatial \nclues that may even be used later in other architectural projects. The artist/ \nauthor was personally invited to two different concept exhibits in which he \nFigure 5: ‘Metagen’ Series #12 by Murat Germen., 2008–2009.\n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n11. \n12. \n13. \n14. \n15. \n16. \n17. \n18. \n19. \n20. \n21. \n22. \n23. \n24. \n25. \n26. \n27. \n28. \n29. \n30. \n31. \n32. \n33. \n34. \n35. \n36. \n37. \n38. \n39. \n40. \n41. \n42. \n43. \n44. \n45. \n46. \n47. \n48. \n49. \n50. \n51. \n52. \nMVCR_1.1_art_Germen_33-48.indd   37 10/12/10   1:45:50 PM\nMurat Germen\n38\nwas given the freedom to invent a virtual architecture through photography. \nThe concept text written for one of these exhibits reads as follows \nI went, saw, stopped, attempted to grasp and enter it, looked at \n construction process and workers with respect, tried to internalize, \nwanted to claim it for a while, dreamed of creating a microcosm out of \nthe macrocosm I was in, shot and shot and shot and finally selected: \nThe created world, though intended for all, was probably quite a \n personal illusion … \n(Germen 2006)\nThe following two quotes from William Mitchell will help the author in clari-\nfying the notion of ‘reconstruction of space’: \nThe city – as understood by urban theorists from Plato to Aristotle to \nLewis Mumford and Jane Jacobs – can no longer hang together and \nfunction as it could in earlier times. It’s due to bits; they’ve done it in. \nTraditional urban patterns cannot coexist with cyberspace. But long live \nthe new, network-mediated metropolis of the digital electronic era.\n(Mitchell 2000: 3) \nThe buildings, neighbourhoods, towns, and cities that emerge from the \nunfolding digital revolution will retain much of what is familiar to us \ntoday. But superimposed on the residues and remnants of the past, like \nthe newer neural structures over that old lizard brain of ours, will be \nglobal constructions on high-speed telecommunications links, smart \nplaces, and increasingly indispensable software. This latest layer will \nshift the functions and values of existing urban elements, and radically \nremake their relationships. The resulting new urban tissues will be char-\nacterized by live/work dwellings, twenty-four-hour neighbourhoods, \nloose-knit, far-flung configurations of electronically mediated meeting \nplaces, flexible, decentralized production, marketing and distribution \nsystems, and electronically summoned and delivered services. This will \nredefine the intellectual and professional agenda of architects, urban \ndesigners, and others who care about the space and places in which we \nspend our daily lives. \n(Mitchell 2000: 7)\nThe above mentioned redefinition process can also be associated with the \nconception of simulacra as offered by Jean Baudrillard. During the 1980s, \nBaudrillard became influenced by Marshall McLuhan and began develop-\ning ideas about what determines the nature of social relations, with special \nemphasis on modes and forms of communication. His most famous formula-\ntion on what he calls ‘simulacra’ and ‘simulation’ fits here. He argues that the \nwestern societies have undergone a ‘procession of simulacra’, a chain of four \n‘orders of simulacra’:\n1. The era of the original.\n2. The counterfeit.\n3. The mechanically produced copy.\n4. The simulated ‘third order simulacra’ where the copy has replaced the \noriginal.\n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n11. \n12. \n13. \n14. \n15. \n16. \n17. \n18. \n19. \n20. \n21. \n22. \n23. \n24. \n25. \n26. \n27. \n28. \n29. \n30. \n31. \n32. \n33. \n34. \n35. \n36. \n37. \n38. \n39. \n40. \n41. \n42. \n43. \n44. \n45. \n46. \n47. \n48. \n49. \n50. \n51. \n52. \nMVCR_1.1_art_Germen_33-48.indd   38 10/12/10   1:45:52 PM\nUsing 2D photography as a 3D constructional tool …\n39\nBaudrillard further argues that in modern society the simulated copy has \nsuperseded the original object or the original experience and ‘the map has \nbecome the territory’ (Baudrillard, 1998: 166). Art theoreticians and philoso-\nphers have already discussed the extent to which reality is represented in pho-\ntographs. The general acceptance today is the idea that photographic images \nonly imply reality or truth and photographs in daily life do replace the reality \ncopied or represented in them; examples of this are people kissing loved ones’ \nportraits or the huge industry built around pornography, or mouth-watering \nfood photographs (Cetin 2007). Following this argument, one can justify the \nmotivation of practicing architectural design within the realm of digital pho-\ntography since the image created within the photograph carries the potential \nto replace the ‘truth’. This argument can additionally be supported by the fol-\nlowing quote from Lynda H. Schneekloth: \nArchitecture, landscape architecture, planning, and other environmental \ndesign fields are practices whose primary aim is to make the world, to \nmake something new. We give material form to some vision of human \nsociety and place. The shadow side of this creation, this making, is that \nthese fields are also about ‘unmaking’ the world. The world already \nexists, and every time we plan, design, and/or construct some aspect \nof worldness, we are replacing and therefore unmaking something else. \n(Schneekloth 1998: 1)\nARCHITECTURE WITHOUT ARCHITECTS\nIn Architecture Without Architects, originally published in 1964, Bernard \nRudofsky provides ‘a demonstration of the artistic, functional, and cultural \nrichness of vernacular architecture’. Rudofsky discusses spaces and buildings \nmade without the involvement of architects. He is interested in buildings pro-\nduced through ‘communal enterprise’ before architecture ‘became an expert’s \nart’. Some of his examples are buildings made by builders without the direct \ninvolvement of users; others are a collaborative effort between builders and \nusers (Hill 2003: 58). The participation of the dweller in the design and con-\nstruction processes requires leeway and the\nflexibility by technical means suggests two further types of user creativ-\nity: constructional, a fabrication of a new space or a physical modifi-\ncation of an existing form, space or object, such as removing the lock \nfrom a door; conceptual, a use, form, space or object intended to be \nconstructed, such as a door. \n(Hill 2003: 88)\nConceptual creativity encourages the user to be creative mentally and pro-\nvide practical data to be used in more responsive architecture. Concerning this \nRudofsky says, ‘vernacular architecture does not go through fashion cycles. It \nis nearly immutable, indeed unimprovable [sic], since it serves its purpose to \nperfection’ (Rudofsky 1964: 2).\nBernard Rudofsky was neither an architect nor a theorist in the usual \nsense. At the start of his career he completed a number of houses in Italy and \nBrazil, where he employed the formal language of the modernists (although \nhis writings appear to indicate that Rudofsky was primarily engaged as a \ncritic and  culture theorist from the 1940s onwards). He did not just write \n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n11. \n12. \n13. \n14. \n15. \n16. \n17. \n18. \n19. \n20. \n21. \n22. \n23. \n24. \n25. \n26. \n27. \n28. \n29. \n30. \n31. \n32. \n33. \n34. \n35. \n36. \n37. \n38. \n39. \n40. \n41. \n42. \n43. \n44. \n45. \n46. \n47. \n48. \n49. \n50. \n51. \n52. \nMVCR_1.1_art_Germen_33-48.indd   39 10/12/10   1:45:52 PM\nMurat Germen\n40\nabout  architecture and design, but also on topics such as clothing, shoes, eat-\ning and bathing. The common element behind all of these activities, though, \nwas the human body, and his lamentation of the loss of sensual awareness. \nNo lifestyle should be preformed, preordained or preconceived. The interac-\ntion of the human being with the environment he has shaped has to be char-\nacterized by an individual attitude towards the life of a responsible citizen \n(Platzer and Wit 2007).\nIt is obvious that not everybody has the ability to build and design; not \neverybody can become an architect. Yet this fact should not lead to the con-\nception that the architect should be in full control of the entire process. There \nis more potential for a truer localization of architectural design if users are \ninvolved in the design process. If the architect takes control of everything, \nlocal design trends to be introduced by him/her face the danger of becom-\ning overly globalized, due to the inevitable presence of governing fashionable \nstyles dictated by ‘high architecture’ or hegemonic macro trends that directly/\nindirectly force architects to follow them: \nHistorically, in professional practice, many architects retained their posi-\ntion by servicing powerful clients and accepting their values. When the \npowerful ignored, misunderstood, or repressed the needs of others in \nthe society, the views of the less powerful did not play a role in the defi-\nnition of architectural knowledge or practice. Insofar as the traditional \nperspective is followed, it excludes the powerless, or the ‘other’, ‘ and \nhas proved unable to effectively encompass social justice, the politics \nof diversity, or the politics of empowerment. […] Involving the user, \nthe ordinary citizen, the public, not only would require more time and \nFigure 6: ‘Metagen’ Series #18 by Murat Germen., 2008–2009.\n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n11. \n12. \n13. \n14. \n15. \n16. \n17. \n18. \n19. \n20. \n21. \n22. \n23. \n24. \n25. \n26. \n27. \n28. \n29. \n30. \n31. \n32. \n33. \n34. \n35. \n36. \n37. \n38. \n39. \n40. \n41. \n42. \n43. \n44. \n45. \n46. \n47. \n48. \n49. \n50. \n51. \n52. \nMVCR_1.1_art_Germen_33-48.indd   40 10/12/10   1:45:52 PM\nUsing 2D photography as a 3D constructional tool …\n41\nenergy but would demand substantial changes to existing practices. […] \nClearly a culturally critical position is needed’.  \n(Piotrowski and Robinson 2001: 76)\nAs a contrast, in vernacular architecture from the primitive age, or even in \nseveral parts of the world nowadays, there is no segregation between the \narchitect and the community because normally the architect is indeed a mem-\nber of the community. Thus there is no differentiation between both cultures \nand there are no conflicts of interests since they have the same way of life, \nuse the same symbols and codes, and apply the same strategies. The result is \nusually that every part of vernacular architecture – be it its technology, con-\nnections with nature or with the social system – is culturally related. Although \nthe typology of the building is merely simple and less dramatic, its immense \nlevel of ingenuity is beyond belief (Paramita 2009: 3).\nEXPERIMENTATION IN SECOND LIFE®\nAs digital photography became more accepted, influential and widespread, art-\nists/designers started to take advantage of photos to create novel 2D/3D enti-\nties. Panoramic photography, photo-mosaics, stop-motion studies are examples \nof 2D creations using numerous photographs. Software packages whereby one \ncan employ photographs to create 3D scenes and environments have also infil-\ntrated the marketplace in recent years. In such cases photographs mostly act as \nplanar surface information to be used as mappings onto volumetric faces and \nthey provide valuable knowledge/detail on the identity of a particular entity.\nVirtual architecture is a term used for architecture specifically created in the \ncomputer environment and never used within the realm of architectural pho-\ntography. This article concentrates on the prospect of constructing architecture \nvirtually through photography within the metaverse. Artists from widely dis-\nparate periods, ranging from Piranesi to Lebbeus Woods, previously dreamed \nabout architectures that could exist virtually, on paper. Nowadays the compu-\nter screen and particularly 3D environments, which can be accessed via that \nscreen, appear to fulfil this dream of many millennia. While space is usually \ndefined/experienced as a physical entity, we have recently begun to observe \nthat the notion of ‘space’ can exist/be perceived/used as a non-physical organ-\nism by means of interactive media and virtual environment applications in the \ncomputer platform. Such creations bring new definitions of ‘space’ and can be \nnamed as ‘informational space’ or ‘cognitive space’.\nThe artist/author has been pursuing his own line of enquiry into creat-\ning such novel identities through the usage of photography mapped onto \nnew media. Since past studies mostly revolved around setting up panorama \nstitches, investigating 3D objects and environments was the next logical step. \nThere were various offline and online 3D environment alternatives in which \none could carry this experimentation out. Second Life® was selected because it \nhas a powerful 3D construction interface. \nOf equal importance to the artist/author is the fact that Second Life® is a \nglobal(ized) milieu where  participants worldwide pursue interactive 3D crea-\ntivity. This global platform upon which participants from many diverse back-\ngrounds can interact, and even build collaboratively, is of added interest to the \nartist/author who believes that personal experience is closely associated with \nlocal culture and consequently influences the particular representations that \nan individual will create. No matter how hard one tries to keep away from \n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n11. \n12. \n13. \n14. \n15. \n16. \n17. \n18. \n19. \n20. \n21. \n22. \n23. \n24. \n25. \n26. \n27. \n28. \n29. \n30. \n31. \n32. \n33. \n34. \n35. \n36. \n37. \n38. \n39. \n40. \n41. \n42. \n43. \n44. \n45. \n46. \n47. \n48. \n49. \n50. \n51. \n52. \nMVCR_1.1_art_Germen_33-48.indd   41 10/12/10   1:45:53 PM\nMurat Germen\n42\nFigure 8: ‘Metagen’ Series #29 by Murat Germen., 2008–2009.\nFigure 7: ‘Metagen’ Series #23 by Murat Germen., 2008–2009.\n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n11. \n12. \n13. \n14. \n15. \n16. \n17. \n18. \n19. \n20. \n21. \n22. \n23. \n24. \n25. \n26. \n27. \n28. \n29. \n30. \n31. \n32. \n33. \n34. \n35. \n36. \n37. \n38. \n39. \n40. \n41. \n42. \n43. \n44. \n45. \n46. \n47. \n48. \n49. \n50. \n51. \n52. \nMVCR_1.1_art_Germen_33-48.indd   42 10/12/10   1:45:53 PM\nUsing 2D photography as a 3D constructional tool …\n43\ncultural constraints in order to stay free, there is a collective memory which \nis embedded in our genes and that intuitively/unconsciously guides individu-\nals when making decisions. Thus, instead of escaping from tradition, a more \nbalanced conduct can emerge as reinterpreting local customs, rituals, prac-\ntices, institutions, beliefs, etc. in the presence of new ways of communication. \nAdaptation, reinterpretation, revision, variation, reconsideration, adjustment, \nimprovement are not necessarily notions forcing one to give his/her principles \nup; on the contrary, they ensure that individuals stay alert, fresh, ready, crea-\ntive and open-minded. The more one culture’s representations are updated \nthe more progressive this culture gets and the more it has to share. \nAs Andréa Zhouri states, instantaneous global communication and mass \ntransportation have made distances ‘shorter’, time and space have become \ncompressed, and contact with different cultures now shapes personal experi-\nence of the world in a global way. Of course, such ‘global’ experiences require \nsome preconditions in the form of financial means, access to new technol-\nogy and linguistic skills. Certainly environmental and human rights agents \nshare this ‘global’ experience. Thus remote areas have become closer and \ninterlinked just as ‘the exotic’ has become familiar. However, this is not to \nsay that environmentalists and advocates for human rights all hold the same \nhomogeneous image or understanding of the world. Neither is it to say that \nthe intensification of contact implies a better understanding of and commu-\nnication with ‘the other’ (Hussey and Thompson 2000: 178). If a culture and/\nor individual has a comprehensive assessment of personal experience(s) and \na resulting definition of priorities leading to conscious representations, it/she/\nhe will have more chance to generate self-esteem, self-confidence and conse-\nquently understand ‘the other’ in order to coexist in peace.\nFigure 9: ‘Metagen’ Series #37 by Murat Germen., 2008–2009.\n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n11. \n12. \n13. \n14. \n15. \n16. \n17. \n18. \n19. \n20. \n21. \n22. \n23. \n24. \n25. \n26. \n27. \n28. \n29. \n30. \n31. \n32. \n33. \n34. \n35. \n36. \n37. \n38. \n39. \n40. \n41. \n42. \n43. \n44. \n45. \n46. \n47. \n48. \n49. \n50. \n51. \n52. \nMVCR_1.1_art_Germen_33-48.indd   43 10/12/10   1:45:59 PM\nMurat Germen\n44\nOne consideration throughout this process was that the constitution of \nspace involving multiple incompatible perspectives to be present in photos \nshould be used. This can be likened to Ottoman miniatures where various \nconflicting perspectives can coexist. This diversity of perspectives takes us to \nthe idea of ‘perspectivism’, originally proposed by Friedrich Nietzsche, where \nall ideations take place from particular perspectives. This means that there are \nmany possible conceptual schemes, or perspectives which determine any pos-\nsible judgement of truth or value that we may make; this implies that no way \nof seeing the world can be taken as definitively ‘true’. If we take this a little bit \nfurther, there is no strictly objective ‘reality’ to be re/presented, but instead a \ndetailed depiction of our personal perception, which is closer to reality since it \ndescribes a particular experience (which is different for every individual). This \nexperience is a symbolic association as representation includes everything \npeople construct as a visual record or figurative manifestation of a reality.\nWithin this approach, architects usually reduce the definition of rep-\nresentation to the creation of such visual forms as drawings or mod-\nels that selectively double or imitate the physical reality of a building. I \nwould like to move beyond this traditional view to define representation \nas a culture-specific and dynamic process of establishing the relation-\nships between reality and the signs created to symbolize this reality. In \nthis process, reality becomes thinkable, and its meanings are symboli-\ncally assigned.\n(Piotrowski and Robinson 2001: 42)\nThroughout the building activity undertaken in Second Life®, the artist/author \nwas highly aware of the fact that, in general, buildings do not communicate \nbut represent – a distinction essential to the study of architectural specificity \nof thought. This representational process is far more complex and dynamic \nthan the process of sending, preserving, retrieving, and decoding well-formed \nmessages. According to Piotrowski, buildings and cities represent when they \nserve as repositories of materialized concepts that manifest how people have \ndefined themselves in their lived reality. In this way, a building becomes a \nrepository of cultural memory and helps to expand the sense of reality beyond \nthe here and now. Any piece of architecture functions in this manner when its \nvalue is found in the interconnections it establishes with other buildings, prac-\ntices of everyday life, social structures, attributes of the natural environment, \nor metaphysical concepts, although many aspects of these relationships may \nbe perceivable only to people identifying with the local culture(s). This process \nof establishing a symbolic network of relationships can be viewed as analo-\ngous to what Jean-François Lyotard calls the emergence of representational \nconsciousness. He observes that the viewer’s accumulation of experiences, \nand the delay to the immediacy of reaction of what is being perceived at a \nparticular moment, show ‘how perception stops being “pure”, i.e., instanta-\nneous, and how representational consciousness can be born of this reflection \n(in the optical sense), of this “echo”, of the influx on the set of other possi-\nble – but currently ignored – paths which form memory’ (Lyotard 1991: 42). \nThrough this process, according to Lyotard, human thoughts establish net-\nworks of relationships within functioning concepts of reality.\nAs a space of representation, a building only foregrounds concepts of real-\nity and implies modes of thought and perception. For example, it invites a tacit \ndialogue between old and new, or between a culturally shared and a personal \n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n11. \n12. \n13. \n14. \n15. \n16. \n17. \n18. \n19. \n20. \n21. \n22. \n23. \n24. \n25. \n26. \n27. \n28. \n29. \n30. \n31. \n32. \n33. \n34. \n35. \n36. \n37. \n38. \n39. \n40. \n41. \n42. \n43. \n44. \n45. \n46. \n47. \n48. \n49. \n50. \n51. \n52. \nMVCR_1.1_art_Germen_33-48.indd   44 10/12/10   1:46:03 PM\nUsing 2D photography as a 3D constructional tool …\n45\nsense of reality. Whatever exists or happens in a building, we interact with \nit symbolically. Any building admits various and even conflicting concepts of \nreality. Such hybridity of meanings is possible because concepts of reality and \nphysical forms of buildings, although symbolically related, are never fully code-\npendent; they are differently constructed. Because buildings do not impose \nconcepts of reality but make them thinkable, many concepts may coexist and \nbe in symbolic dialogue with one another within a physical space. Similarly, \nit does matter how a person interacting with a building finds personal rele-\nvance in this interaction. ‘To reveal these kinds of meanings, the building must \nsomehow engage, like Lacan’s mirror, a personal sense of reality’ (Piotrowski \n2001: 45). This personal sense of reality makes us question the inherent nature \nof the concept of ‘representation’ and helps us to extend it into a more flexible \n(and maybe more correct) notion/formulation of ‘re-presentation’.\nThe movie industry would appear to be yet another platform in which \n representation has a significant part, especially when it comes to adapting/\naltering/converting cities for particular needs, such as creating futuristic sci-fi \ncities or architecture which never existed. A particular type of illustration called \n‘matte painting’ created by illustrators (and not architects) usually serves as \ndeparting points for such architecture. The fact that illustrators can create vir-\ntual architecture can also lead to the assumption that photographers who can \nread space properly can use photography as a tool to reinvent, reinterpret and \nreform architecture. The urban space created in Luc Besson’s renowned movie \nThe Fifth Element (1997) is one of the best examples where an ‘almost impos-\nsible’ artificial architecture is envisioned and implemented as a simulation. \nThe complicated upwards and sideways stretch of the built environment takes \nthe limited one-axis 3D volume structure to a richer multiple-axes structure, \nFigure 10: ‘Metagen’ Series #42 by Murat Germen., 2008–2009.\n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n11. \n12. \n13. \n14. \n15. \n16. \n17. \n18. \n19. \n20. \n21. \n22. \n23. \n24. \n25. \n26. \n27. \n28. \n29. \n30. \n31. \n32. \n33. \n34. \n35. \n36. \n37. \n38. \n39. \n40. \n41. \n42. \n43. \n44. \n45. \n46. \n47. \n48. \n49. \n50. \n51. \n52. \nMVCR_1.1_art_Germen_33-48.indd   45 10/12/10   1:46:03 PM\nMurat Germen\n46\nwhich allows circulation in all directions (i.e., not just in the horizontal direc-\ntion as was usual then).\nIt would seem to be apparent that fictional processes, like movie making \nand novel writing, can be used to expose unseen studies of architecture. By \nthe same token, the most faithful representational tool of architecture, i.e., \nphotography, can also be employed to exercise ‘fictional’ architecture; this can \nlater be taken advantage of to enable ‘real’ architecture to be built.\nThe artist/author wanted to take advantage of the Second Life® environ-\nment in order to test his proposal of performing architectural design with the \naid of photography. Thus the fundamental concern was to create architecture \nthrough the usage of architectural representation: a layering whereby 2D real-\nlife photographs, taken by the author on previous occasions, were converted \ninto highly contrasted black and white images with a transparent background. \nThese images were mapped onto a complex 3D quasi-architectural construct \nbuilt in the metaverse and which consisted of transparent object planes. Once \nthe construct was textured with the real-life architectural photographs, a new \ngeneration of photographs (an ‘architecture built upon architecture’) was cre-\nated using the Second Life® software’s snapshot feature. \nOne further observation as the project was implemented was that virtual \narchitecture seems to be a very potent platform when it comes to the proper \nplanning of architecture through a multiplicity of sections. Even though the \nconstructs assembled by the artist/author were highly complex through the \nimagery mapped onto their surfaces, as well as through their volumetric com-\nponents, the care which was invested into proceeding with a sectional logic \nduring the early phases paid off: when the plan of the construct was applied \ninto the Z axis, the underlying strategy became evident. During the second \nphase of the building activity the prototype constructs were then extended \ninto the X and Y-axis of the 3D realm through a process of equal interval \n repetitions. Through these repetitions sections/planes become volume, and \nthe flow amongst volumes constitutes architecture.\nWhat emerged was a layering of two realities, ‘real’ and ‘virtual’, visible \nsimultaneously and which a user of the metaverse could also experience inter-\nactively in a 3D manner. Indeed the superimposed ‘building’ is one that a vir-\ntual resident of a metaverse could conceivably use as a dwelling or a meeting \narea. As such, a form of virtual architecture, which exists as a non-physically \nphysical entity, enables the sense that the notion of ‘space’ can be perceived, \nindeed exist and be used, by means of interactive media and particularly 3D \nvirtual environment applications.\nCONCLUSION\nArchitecture today need no longer be considered as a monument that smoth-\ners social life. The notion that architecture is a means of controlling and \nincarcerating people in solitary and inflexible permanent structures should be \nchallenged in today’s networked and fluid societies. Tendencies for oppres-\nsion through architecture must be challenged, and, to be effective, resistance \nmust remain alive and regenerative through collaboration (Cowan 2002: 20). \nHill supports this proclamation by stating that the architectural profession has \ncome to employ a restrictive visual and verbal language that empties architec-\nture of its inhabitants. The text suggests that the traditional language of archi-\ntectural production and discourse can be dismantled and recast to include, \nand respond to, the signs of inhabitation. Conversely, the ‘illegal’ architect, \n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n11. \n12. \n13. \n14. \n15. \n16. \n17. \n18. \n19. \n20. \n21. \n22. \n23. \n24. \n25. \n26. \n27. \n28. \n29. \n30. \n31. \n32. \n33. \n34. \n35. \n36. \n37. \n38. \n39. \n40. \n41. \n42. \n43. \n44. \n45. \n46. \n47. \n48. \n49. \n50. \n51. \n52. \nMVCR_1.1_art_Germen_33-48.indd   46 10/12/10   1:46:07 PM\nUsing 2D photography as a 3D constructional tool …\n47\nwho questions and subverts these conventions, codes and laws of architec-\nture, is most likely to value the user and transform architectural practice (Hill \n1998: 10). A very fresh example of this suggestion is the architecture designed \nby non-architect individuals within the Second Life® environment.\nThis series of artworks and processes focus on the possibility of (re)design-\ning architecture virtually with the help of one of the most important repre-\nsentation tools: photography. Photography can be utilized in the process of \n‘constructing’ a new space  – that we can call ‘narrative space’– from an exist-\ning spatial body. This narrative space can also be defined as a ‘manufactured \nmetaspace’ which is a space beyond reality and representation: a constructed \nreality that exists solely in digital realms like Second Life® where boundaries \nare unnoticeable. Despite the fact that this constructed reality is not a physi-\ncally built entity, it can reveal some spatial clues that can later be used in \ntangible architectural projects of the real world. While the idea of juxtaposing \na series of disparate photos sounds questionable, the new aesthetic challenge \nof formulating the visual continuity of photos in sequence offers new ways of \nconstructing space and conveying narrative information as a result of a new \nspatial flow among contiguous planar spaces.\nA final quote from Mark Kingwell reinforces this endeavour of making \npersonal worlds of architecture using photography: \nPhotographs are not multiple depictions of some single reality, waiting \nout there to be cornered and cropped, and somehow regulating, even \nin the cornering and cropping, how/what the image means. Rather, \nphotographs offer multiple meanings. The presented image is not a \nreflection, or even an interpretation, of singular reality. It is, instead, \nthe creation of a world. […] The truth of the image is the truth of time: \nnot its metaphysical essence, whatever that might be, but its presence; \nits inescapability. A photograph, I want to say, is a machine for mak-\ning worlds. \n(Kingwell 2006: 16)\nREFERENCES\nBaudrillard, J. (1998), Simulacra and Simulations, USA: Stanford University \nPress, p. 166.\nCapra, F. ([1975] 2000), The Tao of Physics, fourth edition, Boston, USA: \nShambhala Publications Inc. (original edition: Wildwood House), p. 28.\nCetin, O. C. (2007), ‘Thomas Demand as a Baudrillard Practitioner: The pho-\ntographic works of Thomas Demand as a Proof of or as Inspired by Jean \nBaudrillard’s Simulation Theory’, VCD508 lecture notes, Istanbul, Turkey: \nBilgi University.\nCowan, G. (2002), ‘Nomadology in Architecture Ephemerality, Movement and \nCollaboration’, dissertation, Australia: School of Architecture and Urban \nDesign, University of Adelaide, p. 20.\nFlusser, Vilém (2000), Towards A Philosophy of Photography, London, UK: \nReaktion Books, p. 4.\nGermen, M. (2006), Under Construction, Exhibition Catalog, Istanbul: Kanyon.\nHill, J. (ed.) (1998), Occupying Architecture: Between the Architect and the User, \nLondon: Routledge, p. 10.\nHill, J. (2003), Actions of Architecture: Architects and Creative Users, New York: \nRoutledge, pp. 58, 88.\n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n11. \n12. \n13. \n14. \n15. \n16. \n17. \n18. \n19. \n20. \n21. \n22. \n23. \n24. \n25. \n26. \n27. \n28. \n29. \n30. \n31. \n32. \n33. \n34. \n35. \n36. \n37. \n38. \n39. \n40. \n41. \n42. \n43. \n44. \n45. \n46. \n47. \n48. \n49. \n50. \n51. \n52. \nMVCR_1.1_art_Germen_33-48.indd   47 10/12/10   1:46:07 PM\nMurat Germen\n48\nHussey, S. and Thompson, P. (eds) (2000), The Roots of Environmental \nConsciousness: Popular Tradition and Personal Experience, London: Routledge, \np. 174.\nKingwell, Mark (2006), The Truth in Photographs: Edward Burtynsky’s Revelations \nof Excess, Germany: Steidl, p. 16.\nLyotard, J. F. (1991), Matter and Time: The Inhuman, Reflections on Time, \nCalifornia: Stanford University Press, p. 42.\nMitchell, W. J. (2000), E-topia: Urban Life, Jim – But Not as We Know It, \nCambridge: MIT Press, pp. 3, 7.\nParamita, K. D. (2009), ‘Culture Based Architecture: Recognising the \nDifference’, ARC 6988 Design Methodologies, MAAD lecture notes, \nSheffield: The University of Sheffield, p. 3.\nPiotrowski, A. and Robinson, J. W. (eds) (2001), The Discipline of Architecture, \nMinneapolis: University of Minnesota Press, pp. 54, 76, 42, 45.\nPlatzer, M. and Wit, W. de (2007), Lessons from Bernard Rudofsky, Architekturzentrum \nWien, http://www.azw.at/event.php?event_id=639&lang_id=en. Accessed \n3 June 2010.\nRudofsky, B. (1964), Architecture Without Architects – A Short Introduction \nto Non-Pedigreed Architecture, Garden City, New York: Doubleday & \nCompany, Inc., p. 2.\nSchneekloth, L. H. (1998), ‘Unredeemably Utopian: Architecture and Making/\nUnmaking the World, Architecture, Design and Utopia’, Utopian Studies, \n9: 1, p. 1.\nZucker, P. (1961), ‘Ruins: An Aesthetic Hybrid’, The Journal of Aesthetics and \nArt Criticism, 20: 2, p. 119.\nSUGGESTED CITATION\nGermen, M. (2010), ‘Using 2D photography as a 3D constructional tool within the \nmetaverse’, Metaverse Creativity 1: 1, pp. 33–48, doi: 10.1386/mvcr.1.1.33_1\nCONTRIBUTOR DETAILS\nMurat Germen received his BSc in City Planning from the Technical University \nof Istanbul, in 1987, and his MA in Architecture with Henry Adams Gold \nMedal for academic excellence from the Massachusetts Institute of Technology \nin 1992, where he studied as a Fulbright Scholar. Germen has published, \n exhibited widely and internationally. He has awards and honourable  mentions \nfrom international competitions like IPA, PX3 - Prix de la Photographie, \nEPSON International Panorama Awards. Germen has published research \noutput at various international conferences such as SIGGRAPH (2006, 2007), \nISEA (2009), MutaMorphosis: Challenging Arts and Sciences (2007), Towards \na Science of Consciousness (2009), Cyberworlds, CAe (2007, 2008, 2009), EVA \nLondon (2008, 2010), Computer Art Congress/CAC.2 (2008), Creativity and \nCognition (2007), eCAADe (2006, 2010) and ASCAAD (2007).\nContact: Sabanci University, FASS, Orhanli, Tuzla, Istanbul 34956, Turkey.\nE-mail: muratgermen@sabanciuniv.edu\n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. \n10. \n11. \n12. \n13. \n14. \n15. \n16. \n17. \n18. \n19. \n20. \n21. \n22. \n23. \n24. \n25. \n26. \n27. \n28. \n29. \n30. \n31. \n32. \n33. \n34. \n35. \n36. \n37. \n38. \n39. \n40. \n41. \n42. \n43. \n44. \n45. \n46. \n47. \n48. \n49. \n50. \n51. \n52. \nMVCR_1.1_art_Germen_33-48.indd   48 10/12/10   1:46:07 PM\n",
            "id": 5014458,
            "identifiers": [
                {
                    "identifier": "oai:research.sabanciuniv.edu:16792",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "11742534",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "431520016",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2149365112",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "10.1386/mvcr.1.1.35_1",
                    "type": "DOI"
                }
            ],
            "title": "Using 2D photography as a 3D constructional tool within the metaverse",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2149365112",
            "oaiIds": [
                "oai:research.sabanciuniv.edu:16792"
            ],
            "publishedDate": "2010-11-01T00:00:00",
            "publisher": "'Intellect'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://research.sabanciuniv.edu/16792/1/Using_2D_photography_as_a_3D_constructional_tool_within_the_metaverse_elif_SL_makale_ISBN_page_info.pdf"
            ],
            "updatedDate": "2021-06-19T02:52:40",
            "yearPublished": 2010,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "2040-3550"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/pdf/11742534.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/11742534"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/11742534/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/11742534/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/5014458"
                }
            ]
        },
        {
            "acceptedDate": "2008-01-04T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Dunne, Aubrey K."
                },
                {
                    "name": "Mallon, John"
                },
                {
                    "name": "Whelan, Paul F."
                }
            ],
            "contributors": [
                "The Pennsylvania State University CiteSeerX Archives"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/147598140",
                "https://api.core.ac.uk/v3/outputs/143907042",
                "https://api.core.ac.uk/v3/outputs/11309026",
                "https://api.core.ac.uk/v3/outputs/285038714"
            ],
            "createdDate": "2013-07-10T11:53:32",
            "dataProviders": [
                {
                    "id": 145,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/145",
                    "logo": "https://api.core.ac.uk/data-providers/145/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 3365,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/3365",
                    "logo": "https://api.core.ac.uk/data-providers/3365/logo"
                },
                {
                    "id": 2921,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/2921",
                    "logo": "https://api.core.ac.uk/data-providers/2921/logo"
                },
                {
                    "id": 346,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/346",
                    "logo": "https://api.core.ac.uk/data-providers/346/logo"
                }
            ],
            "depositedDate": "2007-01-01T00:00:00",
            "abstract": "Generic camera calibration is a non-parametric calibration technique that is applicable to any type of vision sensor. However, the standard generic calibration method was developed with the goal of generality and it is therefore sub-optimal for the common case of cameras with a single centre of projection (e.g. pinhole, fisheye, hyperboloidal catadioptric). This paper proposes novel improvements to the standard generic calibration method for central cameras that reduce its complexity, and improve its accuracy and robustness. Improvements are achieved by taking advantage of the geometric constraints resulting from a single centre of projection. Input data for the algorithm is acquired using active grids, the performance of which is characterised. A new linear estimation stage to the generic algorithm is proposed incorporating classical pinhole calibration techniques, and it is shown to be significantly more accurate than the linear estimation stage of the standard method. A linear method for pose estimation is also proposed and evaluated against the existing polynomial method. Distortion correction and motion reconstruction experiments are conducted with real data for a hyperboloidal catadioptric sensor for both the standard and proposed methods. Results show the accuracy and robustness of the proposed method to be superior to those of the standard method",
            "documentType": "research",
            "doi": "10.1109/iccv.2007.4408990",
            "downloadUrl": "https://core.ac.uk/download/11309026.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "Efficient Generic Calibration Method for General Cameras with Single Centre of\nProjection\nAubrey K. Dunne John Mallon\nVision Systems Group\nDublin City University\naubrey.dunne@eeng.dcu.ie\nPaul F. Whelan\nAbstract\nGeneric camera calibration is a non-parametric calibra-\ntion technique that is applicable to any type of vision sensor.\nHowever, the standard generic calibration method was de-\nveloped with the goal of generality, and it is therefore sub-\noptimal for the common case of cameras with a single cen-\ntre of projection (e.g. pinhole, fisheye, hyperboloidal cata-\ndioptric). This paper proposes novel improvements to the\nstandard generic calibration method for central cameras\nthat reduce its complexity, and improve its accuracy and ro-\nbustness. Improvements are achieved by taking advantage\nof the geometric constraints resulting from a single centre\nof projection. Input data for the algorithm is acquired us-\ning active grids, the performance of which is characterised.\nA new linear estimation stage to the generic algorithm is\nproposed incorporating classical pinhole calibration tech-\nniques, and it is shown to be significantly more accurate\nthan the linear estimation stage of the standard method.\nA linear method for pose estimation is also proposed and\nevaluated against the existing polynomial method. Distor-\ntion correction and motion reconstruction experiments are\nconducted with real data for a hyperboloidal catadioptric\nsensor for both the standard and proposed methods. Results\nshow the accuracy and robustness of the proposed method\nto be superior to those of the standard method.\n1. Introduction\nThere is currently a trend towards increased use of wide-\nangle dioptric and catadioptric cameras within the vision\ncommunity due to the richer feature set and a greater per-\nsistence of vision that these camera types provide. As a con-\nsequence of this trend, a number of models and calibration\nalgorithms have recently been proposed for such cameras.\nThe most basic models extend the pinhole camera model\nwith one or two radial distortion terms [1, 2, 3, 4]. These\nmethods are less accurate for wide-angle and catadioptric\nlenses as the camera incorporates more distortion. Many of\nthe common distortion models (polynomial, divisional, ra-\ntional) can be augmented with and increasing number of pa-\nrameters [5, 6] to allow wider angle lenses to be calibrated.\nHowever, they are not suitable for fisheye or catadioptric\nlenses for which the field of view exceeds 180o.\nSeveral methods have been proposed that model wide-\nangle cameras as radially symmetric imagers [7, 8], thus\nsimplifying the unknown parameter set. In [7], distortion\nis modelled using a varying focal length instead of an im-\nage displacement approach, allowing cameras with fields of\nview greater than 180o to be modelled. The complete class\nof single viewpoint catadioptric camera configurations was\nderived in [9], and this has been the basis for the develop-\nment of parametric calibration models that are specific to a\nparticular camera/lens configuration, most notably types of\ncentral catadioptric [10] and non-central catadioptric [11].\nThe equivalence between catadioptric projections and map-\npings of the sphere was demonstrated in [12], resulting in\na unifying model for catadioptric cameras. Nevertheless,\nonly a fewmethods have been proposed that can model both\ndioptric (with FOV greater than 180o) and catadioptric cam-\neras, i.e. a unifying model for all central cameras [7][13].\nAll the above calibration techniques assume a parametric\ncamera model of some form, where the task is to estimate\nthe (usually small) set of model parameters. In contrast, a\nnon-parametric approach was proposed by Grossberg [14].\nThis general camera model consists of a mapping in which\neach pixel is mapped to the direction of a half-ray in space,\ntogether with an anchor point. In principle, the ray direction\nfor each pixel is completely independent of the ray direc-\ntions of the surrounding pixels, thus allowing application to\nany type of central or non-central camera. The calibration\ntechnique described in [14] uses two images of a grid in dif-\nferent, known, positions. By determining the location seen\nby each pixel on each grid, the set of all camera ray direc-\ntions can be determined. A generalisation to this calibra-\ntion method, termed generic calibration, was proposed by\nSturm and Ramalingam [15], wherein the world transforma-\n1\ntion between grid positions is not known a priori. Here, the\ncalibration consists of determining the points seen by a pixel\non each of three grids in unknown orientations. Effectively\nthis becomes the estimation of the positions and orientations\nof each of the three grids, since knowledge of these allows\nthe world ray-plane intersections to be determined. The cal-\nibration process as proposed in [15, 16] can be summarised\nas follows: Three images of a calibration grid in different\norientations are acquired, and for each camera pixel the lo-\ncation it sees on each grid is determined. The ray anchor\npoints (camera centre for central cameras) and grid orien-\ntations are then linearly estimated using this location data,\nand refined via bundle adjustment. Finally, the calibration\narea is extended by imaging the grid in new orientations\nthat intersect with the previously calibrated region, and us-\ning geometric constraints and bundle adjustment to estimate\ntheir pose. All the ray directions are stored in a lookup table\nas Plu¨cker matrices. This calibration method will hereafter\nbe referred to as the standard generic method.\nThe standard generic method is applicable to any camera\ngeometry, and thus the calibration process is very general in\norder to cope with both central and non-central cameras.\nConsequently, many of the inherent geometric constraints\nof central cameras are not taken advantage of when calibrat-\ning these cameras. This paper proposes a new generic cal-\nibration method for cameras with a single centre of projec-\ntion (hereafter referred to as the proposed generic method)\nthat is optimum given the constraints of central cameras.\nThe three key contributions in this paper are as follows:\nFirstly, the issue of specifying accurate input data is ad-\ndressed. The standard generic method [16] uses homo-\ngraphic interpolation with chessboard grid patterns. In the-\nory, generic calibration can achieve pixel level calibration,\nand thus it seems appropriate to use pixel level data as input\nto the algorithm. Such data can be obtained by the use of\nspatio-temporally varying grids displayed on a flat screen\nmonitor. We have termed these grids ’active grids’, and\nwhile this method has been used before [14][17][18], no\ndiscussion has been preferred on their performance for cali-\nbration purposes. An explanation of active grids is given in\n§2, along with an evaluation of their performance relative to\nstandard localisation techniques. Secondly, a novel method\nfor the linear estimation of the camera centre is proposed.\nThe estimation of the camera centre in the standard generic\nmethod is ”rather complicated” according to Sturm himself\n[15] and is given without any geometrical interpretation. In\n§3 active grids are shown to allow other, more simple and\nmore accurate methods of determining the camera centre\nfor central cameras. Thirdly, an alternative pose estimation\nstage is proposed. The pose estimation stage proposed by\nSturm is a 3-point technique that does not lend itself well to\nlarge scale single shot pose estimation. The new pose esti-\nmation algorithm is derived and evaluated in §4. Together,\n0 50 100 150 200 250 300\n0\n50\n100\n150\n200\n250\n300\nDistortion model: 1/(1+k1r\n2+k2r\n4), k1=−0.8, k2=0.4 \nf=680mm, px=150mm, py=150mm \nFigure 1. Vector plot of error residuals for homographic interpo-\nlation (20mm grid pitch) showing bias (left). Vectors are scaled\n×20. Binary and sinusoidal active grid patterns for encoding ver-\ntical location (right).\nthe above modifications serve to make the proposed generic\nmethod for central cameras both more robust and more ac-\ncurate than the standard method. Simulations and experi-\nments with real data are presented in §5 that demonstrate\nthe improved performance. The effects of the modifications\nand the accuracy of the complete calibration are shown and\ndiscussed.\n2. Active Grids\nBinary chessboard grids are typically used in camera cal-\nibration, since the corners of the chessboard grid squares\ncan be easily extracted and accurately localised in images\nof the grids. This results in many grid to image correspon-\ndences. For the standard generic method, these correspon-\ndences must be used to determine the intersection points\nof camera rays with the grid (i.e. the location seen on the\ngrid by each camera pixel). In most cases, the intersection\npoints will not lie exactly on a grid corner. Therefore, ho-\nmographic interpolation is employed in [15] to determine\nthe intersection points based on the extracted image coordi-\nnates of the four closest grid corner points. However, this\napproach is unsuitable for high fidelity calibration, since\nany distortion present in the images of the calibration grids\nintroduces a bias in the results. Fig 1 (left) shows a vector\nplot of the error residuals after homographic interpolation\nis applied to 500 random points on a 300mm × 300mm\ngrid (simulated camera with radial distortion). The system-\natic bias in the plot increases with distance from the image\ncentre, suggesting it is primarily due to radial distortion.\nOur use of active grids overcomes this problem with ho-\nmographic interpolation by providing a direct localisation\nof the point seen by every pixel viewing the active grid,\nthus enabling pixel-level calibration. An active grid is a\nflat-screen TFT monitor that is used to display a temporal\nsequence of spatially varying grayscale patterns. The loca-\ntion of any point on the active grid can be decoded from the\nintensity displayed at that point across the sequence of pat-\nterns. We have used patterns from the domain of structured\nlight to encode location. This approach is similar in spirit\nto the approach used by Sagawa in [17] for distortion cor-\nrection. However, in that case the displayed patterns vary\nonly temporally, resulting in difficulty resolving the bound-\naries of narrow stripes (the authors fall back on linear in-\nterpolation in these situations). In our method, each active\ngrid requires 22 patterns to be consecutively displayed in\norder to fully encode the location data. Two patterns are\nused to determine thresholds, 12 patterns encode location\non an 8× 8 grid using Gray coded binary patterns, and 8 si-\nnusoidal grayscale patterns encode location spatially within\neach square in this grid. The set of binary and sinusoidal\npatterns that encode vertical location are shown in Fig. 1\n(right). Both the binary patterns and their inverses are dis-\nplayed to make the decoding near white/black boundaries\nmore robust. The phase of the sinusoidal patterns is shifted\nby 90o between consecutive patterns, and location is de-\ncoded from them as in [19].\nActive grids overcome the distortion bias associated with\nhomographic interpolation and consequently are ideal for\nuse in the calibration process. The proposed generic method\nrelies substantially on active grids for its accuracy, and\ntherefore, since they have not previously been benchmarked\nagainst standard techniques for feature localisation, a per-\nformance evaluation of them is provided. Corner detection\nin chessboard patterns was recently shown to be invariant to\nboth perspective bias and distortion bias, and so to outper-\nform non-corner based patterns [20]. Consequently we used\nderivative corner localisation and saddle-point corner local-\nisation techniques with a chessboard grid for the bench-\nmarking process. The comparison between the active grids\nmethod and these two standard methods is shown in Fig 2.\nThe experiments were conducted with real data by subpix-\nally localising corners in the image of a chessboard grid,\ndisplayed on a TFT monitor, using the two standard meth-\nods. An active grid was then displayed on the monitor and\ndecoded. The subpixel corner locations for the active grid\nmethod were determined by searching this decoded loca-\ntion data with the known metric grid dimensions. A second\nactive grid was then placed in front of the camera, and the\nlocations on this grid seen by the corner subpixels of each\nmethod were decoded directly. By mapping these locations\nto the knownmetric chessboard structure via homographies,\nthe RMS error residuals for each method were determined.\nThe robustness of active grids to variations in camera-grid\ndisplacement, orientation, image blur and additive Gaussian\nnoise is seen in Fig. 2 to be superior to that of the standard\nmethods under almost all conditions. Note that, where not\notherwise specified, the camera-grid distance is 200mm, the\norientation is 0o, and there is no blurring or additive noise.\nThe excellent performance of the active grids in the com-\nparison is partly due to their robustness to image sensor\nblooming in regions of high contrast: for active grids the\nhighest resolution data is extracted from the sinusoidal pat-\nFigure 2. Performance plots for saddle point localisation, deriva-\ntive localisation and active grids localisation. Orientation is mea-\nsured between the grid normal and the camera axis in the horizon-\ntal plane.\nterns, which contain only low contrast.\n3. Linear Estimation\nThe purpose of the linear estimation stage in central\ngeneric calibration is to determine the position of the cam-\nera centre in the camera coordinate system attached to the\nbase (usually first) grid. The camera centre is the single\npoint through which all camera rays would pass if no reflec-\ntion or refraction occurred. The linear estimation stage of\nthe standard generic method is based on a collinearity con-\nstraint: for each ray, the camera centre and the world coordi-\nnates of the intersection point of that ray with each grid are\ncollinear. This can be expressed mathematically by stack-\ning the local homogeneous coordinate for each intersection\npoint in a 4 × 4 matrix. Collinearity is enforced by ensur-\ning the determinant of this matrix is zero. The algorithm\nfor determining the camera centre and plane positions and\norientations from this starting point for the standard generic\nmethod is ”rather complicated” as stated by Sturm in [15].\nSee [15, 16] for a detailed explanation.\nBy taking a novel interpretation of existing methods for\nthe calibration of pinhole cameras, an alternative, less com-\nplex, linear estimation stage is proposed. As known, pin-\nhole calibration techniques are not suitable for wide field-\nof-view cameras due to the existence of severe non-linear\nimage distortion that invalidates the linear projectionmodel.\nThe key idea of the proposed method is that an additional\nactive grid is used as a synthetic image plane in the cali-\nbration, thus forming a synthetic pinhole camera. By plac-\ning the synthetic plane in front of the general camera so as\nto intersect the camera rays on the object side of the cam-\nera optics, as shown in Fig. 3, a distortion free image is\nFigure 3. Linear estimation of camera centre for proposed generic\ncalibration. Synthetic image plane allows use of pinhole calibra-\ntion techniques for determining centre.\nformed on the synthetic image plane. The synthetic camera\ncan then be calibrated using any standard pinhole calibra-\ntion method, with the desired estimate of the camera cen-\ntre being [px py f ]T . The pose of grids two and three can\nalso be extracted from the synthetic pinhole calibration us-\ning well known techniques [1][3]. In this way the non-linear\ncalibration problem is converted to a linear calibration prob-\nlem. This new approach provides a key link between the\nestablished theory of pinhole calibration and the generic\ncalibration of central cameras, allowing the generic cali-\nbration of non-pinhole central cameras using pinhole cal-\nibration techniques. The number of grids required for the\nproposed generic method is three - two for the pinhole cali-\nbration plus one for the synthetic image plane - which is the\nsame number as required for the standard generic method.\nNote that active grids are ideal for use as synthetic image\nplanes in this method as they can directly provide the re-\nquired ray-grid intersection points.\nA question arises as to which pinhole calibration tech-\nnique should be used for the proposed linear estimation\nstage? To answer this, two well known pinhole calibration\ntechniques, those of Sturm [1] and Wang [21], were incor-\nporated into separate implementations of the proposed lin-\near estimation stage. Both of these techniques are based on\nthe same underlying constraints on the image of the abso-\nlute conic, but they take different approaches to determining\nthe solutions. A comparison of the robustness to Gaussian\nnoise of these two implementations of the proposed linear\nestimation stage, and of the standard generic method linear\nestimation stage, is shown in Fig. 4. Errors in the estima-\ntion of the camera centre, and in the translation and rotation\nof the second and third grids involved in the calibration,\nare shown (averaged over 50 trials). The ray-point error is\nthe perpendicular distance between each estimated ray and\nits known points of intersection with each calibration grid.\nThese results are for a simulated camera with camera centre\n[0 0 600]T (in coordinate frame of first grid), and with focal\nlength and distortion parameters chosen to simulate a wide\nFigure 4. Centre estimation performance plots for standard generic\nmethod, standard generic method with bundle adjustment, and\nproposed generic method using both Sturm’s and Wang’s planar\npinhole calibration techniques.\nangle camera with FOV of 100o. Results are shown for the\nstandard generic method both with and without bundle ad-\njustment of the grid transformations, as described in [16].\nBundle adjustment is not applied to the other two methods.\nThe results clearly indicate that both of the implementations\nof the proposed linear estimation stage outperform that of\nthe original generic method across all levels of noise tested.\nFor the proposed generic method, the implementation using\nSturm’s calibration outperforms that using Wang’s calibra-\ntion in all cases, and therefore Sturm’s pinhole technique is\nused in the proposed generic method. It is interesting to note\nthat bundle adjustment does not appear to significantly im-\nprove the calibration result for the standard generic method.\nThis is likely due to the error in the linear estimation of the\ncamera centre (centre estimate is not bundle adjusted).\n4. Pose Estimation\nPose estimation is required during generic calibration\nin order to increase the number of calibrated camera rays.\nOnce the pose of an additional grid is estimated, the cam-\nera ray associated with each pixel that sees this additional\ngrid can be included in the calibration. Exact solutions to\nFigure 5. Proposed linear pose estimation method using synthetic\nimage plane.\nthe general pose estimation problem can be found for either\nthree or four non-collinear point-image pairs by solving a\nfourth or higher degree polynomial [22]. However, closed\nform solutions to the pose estimation problem for more than\nfour points do not exist [23]. The most common approach\nto pose estimation in these cases is to minimise either the\nimage space error or the object space error using standard\nnonlinear minimisation techniques. An iterative technique\n[23] has also been proposed.\nIn the standard generic method [15] a geometric three\npoint algorithm for estimating the pose is described. Given\ncalibrated rays with directions Ri and Rj , and the dis-\ntance dij between their intersection points with the grid\nof unknown pose, the depths λi and λj of the intersec-\ntion points can be computed by simultaneously solving\n|λiRi − λjRj |2 = d2ij for i, j = (l,m, n), i 6= j. Addi-\ntional points are used to determine the correct pose from the\neight possible solutions. A significant disadvantage of the\nalgorithm is that, when included in a RANSAC framework,\na linear re-estimation of the pose based on all inliers is not\npossible (typically the final step in RANSAC). The method\nis also very sensitive to additive noise (although a guided\nselection of sufficiently separated points can alleviate this\nproblem), and is computationally expensive.\nTo overcome these difficulties a linear least-squares so-\nlution to the pose estimation problem is proposed. Although\nthe method does not minimise geometric error, it is linear,\nfast, always gives a solution, and can conveniently be in-\ncorporated within a RANSAC framework. The method is\nderived based on standard camera geometry and the pinhole\ncamera model, similarly to [24]. With reference to Fig. 5,\ngiven a grid in the base position (canonical grid) with world\ncoordinate points Xi ∈ 3, and a grid with an unknown\npose T ∈ 3 relative to the canonical grid containing un-\nknown world points X0i ∈ 3, we wish to determine the\nunknown pose T . This is achieved via the insertion of a\nsynthetic image plane placed in a known orientation (the\norientation selection is discussed later) between the camera\ncentre and the canonical grid, as shown in Fig. 5. Note\nthat this synthetic image plane is a mathematical construct\nonly and is not physically realised. The key to the proposed\napproach is that the synthetic image plane allows the gen-\neral pose estimation problem to be converted to a pinhole\npose estimation problem. Points Xi can be projected onto\nthe synthetic image plane by intersecting the previously cal-\nibrated rays with this plane. The projection is according to\nthe pinhole model\nx0i = PX\n0\ni (1)\nwhere xi ∈ 2 are the imaged points on the synthetic image\nplane and P is the known 3 × 4 camera projection matrix\nassociated with the synthetic image plane. Since the canon-\nical grid is on the world Z = 0 plane, we also have\nx0i = HXi[1 2 4]] (2)\nand\nX 0i = TXi (3)\nwhereH is a homography in 2. Therefore\nHXi[1 2 4]] = PT(:,[1 2 4])Xi[1 2 4]]\n= KR[I(3×3)|− C˜]T(:,[1 2 4])Xi[1 2 4] (4)\nwhere C˜ ∈ 3 is the inhomogeneous coordinate of the cam-\nera centre,K is the 3× 3 camera calibration matrix for the\nsynthetic image plane, andR ∈ SO(3) is the world rotation\nof the synthetic image plane. Therefore\n(KR)−1H =\n⎛\n⎝\nT11 T12 T14 − C˜1\nT21 T22 T24 − C˜2\nT31 T32 T34 − C˜3\n⎞\n⎠ (5)\nLetting\nG = (KR)−1H (6)\ngives\nsR0 =\n¡\nG1 G2 G1 ×G2\n¢\n(7)\nwhere Gi is the ith column of G. An orthonormal R0 is\nobtained via the SVD. The scale factor, s, can be obtained\nas\ns = mean\n¡\nG1 G2\n¢¡\nRˆ1 Rˆ2\n¢ (8)\nThe translation is\ntˆ =\nG4 + C˜\ns\n(9)\nThe desired pose estimate is then\nTˆ =\nµ\nRˆ tˆ\n0(1×3) 1\n¶\n(10)\nThe orientation of the synthetic image plane should be\nchosen so as to be as perpendicular as possible to the known\nrays involved in the estimation process. This orientation is\ndetermined in a least-squares sense by minimising the sum\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8\n0\n0.2\n0.4\n0.6\n0.8\nRotation error\nE\nrr\nor\n (d\neg\nre\nes\n)\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8\n0\n0.5\n1\n1.5\n2\n2.5\n3\nTranslation error\nσ (pixels)\nE\nrr\nor\n (%\n)\nProposed generic\nProposed generic + RANSAC\nStandard generic\nFigure 6. Performance comparison of pose estimation stage of pro-\nposed generic method, proposed generic method with RANSAC,\nand standard generic method.\nof the angles between the calibrated rays and the synthetic\nimage plane, as in [25].\nThe robustness to Gaussian noise of the standard generic\nmethod’s pose estimation stage (embedded in RANSAC,\nfollowed by non-linear optimisation), and the pose esti-\nmation stage of the proposed generic method, both on it’s\nown and embedded in a RANSAC framework, is evaluated\nfor simulated data. The simulated camera centre is fixed\nat [0 0 600]T , and the translations and Euler rotations of\nthe grid whose pose is to be estimated are randomly cho-\nsen from [−150mm 150mm] and [−30o 30o] respectively.\nThe mean rotational and mean percentage translational er-\nrors are shown in Fig. 6. It is seen that the proposed\ngeneric method’s pose estimation consistently outperforms\nthat of the standard generic method for all simulated levels\nof noise. Also, embedding the linear method in a RANSAC\nphase actually reduces the accuracy of the estimates. This is\npossibly due to the non-isotropic nature of the noise that re-\nsults from projecting Gaussian noise from the image plane\nonto a non-parallel plane.\n5. Experimental Results\nBoth the standard and proposed generic methods are\nanalysed for real data with respect to a ray-point error met-\nric, distortion correction, and separate motion reconstruc-\ntion tasks. All images for these experiments were taken\nusing a 360 OneVR hyperboloidal omnidirectional mirror\n1 in conjunction with a Nikon D70 SLR digital camera.\nThis catadioptric configuration has a single centre of pro-\njection. For each calibration method approximately 207o of\nthe horizontal FOV and approximately 82o of the vertical\nFOV of the camera was calibrated, using three grids for the\nlinear estimation of the camera centre, and a further three\ngrids to extend the calibrated region to include additional\npixels. Active grids were used for all grids during calibra-\n1Kaidan Inc., Feasterville, PA\nTable 1. Camera centre and grid transformation estimates for cata-\ndioptric sensor calibration. Centre and translations are measured\nin mm, rotations are measured in degrees.\nMethod Centre R1 T1 R2 T2\nOriginal 169.29 34.65 130.58 35.27 182.84\nmethod 152.06 -141.74 -13.35\n-106.48\nProposed 167.90 36.52 139.66 35.61 192.13\nmethod 159.82 -142.08 -13.70\n-116.21\nTable 2. Ray-point errors (mm) for all rays involved in the linear\nestimation stage for each calibration method (BA = bundle adjust-\nment).\nMethod Error type Error Error after BA\nOriginal Mean 3.2219 1.1634\nmethod SD 1.4923 0.6583\nProposed Mean 0.1924 0.1314\nmethod SD 0.0906 0.0727\ntion, with the same images used as input to both methods so\nthat direct comparisons between the standard generic and\nproposed generic methods are not influenced by the type of\ninput data. A RANSAC stage is applied to the locations\ndecoded from the active grids in order to remove any incor-\nrectly decoded location data (RANSAC thresholds empiri-\ncally chosen with reference to Fig. 2).\nThe first three grids used in the calibration were found\nto have 41467 common intersecting rays, of which 31398\nwere determined to be inliers. Table 1 shows, for each cali-\nbration method, the estimates of the camera centre, and the\nestimated Y Z yaw-roll rotation angles and translation mag-\nnitudes for the second and third grids. Note the significant\ndifference between the estimated values of the camera cen-\ntre z coordinate for each method.\nThe ray-point error metric, described in §3, can be ap-\nplied to each calibration dataset to give an indication of\nthe relative errors in each calibration (actual positions of\nthe camera centre and the second and third grids are not\nknown). Table 2 shows the mean and standard deviation\nof the ray-point errors for each method, both before and\nafter bundle adjustment. Bundle adjustment is applied to\nthe proposed method here for comparative purposes only.\nThe non-bundle-adjusted parameters are used in the remain-\nder of the calibration with the proposed generic method,\nwhereas the bundle adjusted results are used for calibration\nwith the standard generic method (as per [26]). The ray-\npoint error results clearly show that the configuration of the\ncamera centre and the calibration grids is in greater geomet-\nric agreement for the proposed generic method than for the\nstandard generic method. These results also agree with the\nsimulated results in §3, specifically they show that the lin-\near estimates of the proposed generic method are capable of\nFigure 7. Cylindrically unwarped catadioptric image after standard\ngeneric calibration (top) and proposed generic calibration (bottom)\noutperforming the bundle adjusted estimates of the standard\ngeneric method.\n5.1. Distortion Correction\nTwo distortion correction experiments were carried out\nin order to both qualitatively and quantitatively evaluate\neach of the calibration methods.\nIn the first experiment the calibration data is used to re-\nmove the inherent non-linear distortion from the calibrated\narea of an omnidirectional image of a real scene. A portion\nof a cylindrical image is formed by intersecting the cali-\nbrated rays with a unit cylinder, the axis of which is coin-\ncident with the camera centre, and the cylinder is then un-\nwrapped to form a planar image. Fig 7 shows the cylindri-\ncally unwarped images calculated using the calibration data\nfrom the standard generic and proposed generic methods.\nAs expected, real world straight lines that are parallel to the\nmirror axis (vertical) are mapped to straight lines in both of\nthe corrected images. However, some abberation is visible\nin the image corrected using the standard generic method\ncalibration data (highlighted by ellipse). In contrast, the\ncorrected image formed using the proposed generic method\nhas significantly less aberration. Note that the field of view\nof the cylindrical unwarped image for the standard generic\nmethod is less than that of the proposed generic method due\nto the smaller estimate for the z coordinate of the camera\ncentre using the standard generic method.\nQuantitative evaluation of the calibrations was carried\nout by generating a perspectively corrected image of an\n18×12 chessboard calibration grid, with square side length\n53mm. The plane onto which the corrected images are pro-\njected is selected as described in §4. Distortion residuals are\nmeasured after applying a homography between the distor-\ntion corrected image of the grid and the known metric grid\nstructure. Fig. 8 shows the distortion residuals for both the\nstandard and proposed methods. No radial distortion bias is\nvisible in either vector plot, but the plot for the standard\ngeneric method displays large divergences along roughly\nvertical lines at the left and right of the image. These\n0 200 400 600 800 1000\n0\n100\n200\n300\n400\n500\n600\n700\n800\nmm\nm\nm\n0 200 400 600 800 1000\n0\n100\n200\n300\n400\n500\n600\n700\n800\nmm\nm\nm\nFigure 8. Vector plots of residuals after distortion correction of a\nchessboard grid for standard generic method (left) and proposed\ngeneric method (right). Vectors are scaled ×15\ncorrespond to areas where two active grids with misesti-\nmated poses meet, and correspond to the aberrations seen\nin Fig. 7 (top). The divergences and residuals are smaller\nfor the vector plot using the proposed generic method (mean\nRMS error = 2.23mm, STD = 1.06mm) than for the vector\nplot using the standard generic method (mean RMS error =\n4.54mm, STD = 1.96mm) indicating a better calibration.\n5.2. Motion Reconstruction\nMotion reconstruction experiments were conducted for\nthe cases of pure translation and pure rotation. The ex-\nperimental setup consisted of a 3D calibration object (two\northogonal planar chessboard grids) rigidly mounted on a\nstage capable of controlled rotation and translation. For the\ntranslation experiment, the object was translated 100mm in\nsteps of 20mm, and for the rotation stage it was rotated by\n90o in steps of 22.5o. Point matches were manually ex-\ntracted across both image sequences, and used to index the\nPlu¨cker matrix lookup tables for each calibration method to\nget the corresponding ray direction information. The essen-\ntial matrix, E, between each image pair was linearly esti-\nmated using the ray-based epipolar constraint L0EL = 0,\nwhere L,L0 are the first 3 components of the Plu¨cker vec-\ntors derived from the Plu¨cker matrices [25]. Rotations\nand translations are extracted from the essential matrices\naccording to [27]. The motion reconstruction results are\nshown in Fig.9. It can be seen that the motion estimated\nwith the proposed generic method is closer to linear in the\ncase of translation, and closer to 90o in the case of rotation,\nthan for the standard generic method. For visualisation pur-\nposes the differences between the average translation vector\nand the estimated translation vectors are scaled ×4.\n6. Summary and Conclusions\nThis paper proposes a novel method of generic camera\ncalibration for cameras with a single centre of projection.\nThe main contributions of the paper are a performance eval-\nuation of active grids for use in calibration, an improved lin-\near estimation stage based on a new interpretation of an ex-\nisting technique that allows pinhole calibration techniques\nto be applied to the calibration of non-pinhole cameras, and\n050\n100\n−50−40−30−20−100\n0\n10\n20\n30\n40\n50\n60\nmm\nmm\nm\nm\naverage translation\nstandard generic method estimation\nproposed generic method estimation\n0 2 4 6 8 10\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nFigure 9. Translation (left) and rotation (right) reconstruction us-\ning calibration data from standard generic method and proposed\ngeneric method.\na new linear pose estimation stage. The individual compo-\nnents of the proposed method are separately evaluated using\nsimulated data, with the results indicating that the proposed\ngeneric method outperforms the standard generic calibra-\ntion method in terms of accuracy and robustness to noise.\nThe proposed generic method is also evaluated against the\nstandard generic method for real data using a hyperboloidal\nomnidirectional camera, with the results for distortion cor-\nrection and motion reconstruction tasks showing the im-\nproved performance of the proposed generic method.\nAcknowledgements\nThis research is funded by the Irish Research Council\nfor Science, Engineering and Technology: funded by the\nNational Development Plan.\nReferences\n[1] P. Sturm and S. J. Maybank, “On plane-based camera cali-\nbration: A general algorithm, singularities, applications,” in\nCVPR, vol. 1, 1999, pp. 432–437. 1, 4\n[2] A. W. Fitzgibbon, “Simultaneous linear estimation of mul-\ntiple view geometry and lens distortion,” in CVPR, vol. 1,\n2001, pp. 125–132. 1\n[3] Z. Zhang, “A flexible new technique for camera calibration,”\nMicrosoft research, Tech. Rep., 1998. 1, 4\n[4] J. Heikkila, “Geometric camera calibration using circular\ncontrol points,” PAMI, vol. 22, no. 10, pp. 1066–1077, 2000.\n1\n[5] J. Kannala and S. S. Brandt, “A generic camera model and\ncalibration method for conventional, wide-angle, and fish-\neye lenses,” PAMI, vol. 28, no. 8, pp. 1335–1340, 2006. 1\n[6] G. Q. Wei and S. D. Ma, “Implicit and explicit camera cal-\nibration: theory and experiments,” PAMI, vol. 16, no. 5, pp.\n469–480, 1994. 1\n[7] J. Tardif, P. Sturm, and S. Roy, “Self-calibration of a general\nradially symmetric distortion model,” in ECCV, vol. 4, May\n2006, pp. 186–199. 1\n[8] S. Thirthala and M. Pollefeys, “Multi-view geometry of 1d\nradial cameras and its application to omnidirectional camera\ncalibration,” in ICCV, 2005, pp. 1539–1546. 1\n[9] S. Baker and S. Nayar, “A theory of catadioptric image for-\nmation,” in ICCV, 1998, pp. 35–42. 1\n[10] B. Micusik and T. Pajdla, “Estimation of omnidirectional\ncamera model from epipolar geometry,” in CVPR, vol. 1,\n2003, pp. 485–490. 1\n[11] ——, “Structure from motion with wide circular field of\nview cameras,” PAMI, vol. 28, no. 7, pp. 1135–1149, 2006.\n1\n[12] C. Geyer and K. Daniilidis, “An unifying theory for cen-\ntral panoramic systems and pratical implications,” in ECCV,\n2000, pp. 445–461. 1\n[13] J. P. Barreto, “A unifying geometric representation for cen-\ntral projection systems,” CVIU, vol. 103, no. 3, pp. 208–217,\n2006. 1\n[14] M. D. Grossberg and S. K. Nayar, “A general imaging model\nand a method for finding its parameters,” in ICCV, vol. 2,\n2001, pp. 108–115. 1, 2\n[15] P. Sturm and S. Ramalingam, “A generic calibration concept:\nTheory and algorithms,” Rapport de Recherche 5058, 2003.\n1, 2, 3, 5\n[16] ——, “A generic concept for camera calibration,” in ECCV,\nvol. 2, 2004, pp. 1–13. 2, 3, 4\n[17] R. Sagawa, M. Takatsuji, T. Echigo, and Y. Yagi, “Cali-\nbration of lens distortion by structured-light scanning,” in\nICIRS, 2005, pp. 1349–1354. 2, 3\n[18] A. K. Dunne, J. Mallon, and P. F. Whelan, “A comparison of\nnew generic camera calibration with the standard parametric\napproach,” inMVA, vol. 1, 2007, pp. 114–117. 2\n[19] J. Salvi, J. Pages, and J. Batlle, “Pattern codification\nstrategies in structured light systems,” Pattern Recognition,\nvol. 37, no. 4, pp. 827–849, April 2004. 3\n[20] J. Mallon and P. F. Whelan, “Which pattern? biasing aspects\nof planar calibration patterns and detection methods,” PRL,\nvol. 28, no. 8, pp. 921–930, 2007. 3\n[21] J. Wang and Y. Liu, “Characteristic line of planar homog-\nraphy matrix and its applications in camera calibration,” in\nICPR, 2006, pp. 147–150. 4\n[22] R. M. Haralick, C. Lee, K. Ottenberg, and M. Nolle, “Anal-\nysis and solutions of the three point perspective pose estima-\ntion problem,” Hamburg, Tech. Rep., 1991. 5\n[23] C. Lu, G. D. Hager, and E. Mjolsness, “Fast and glob-\nally convergent pose estimation from video images,” PAMI,\nvol. 22, no. 6, pp. 610–622, 2000. 5\n[24] P. Sturm, “Algorithms for plane-based pose estimation,” in\nCVPR, 2000, pp. 1010–1017. 5\n[25] S. Ramalingam, S. K. Lodha, and P. Sturm, “A generic\nstructure-from-motion framework,” CVIU, vol. 103, no. 3,\npp. 218–228, 2006. 6, 7\n[26] S. Ramalingam, P. Sturm, and S. Lodha, “Theory and ex-\nperiments towards complete generic calibration,” Rapport de\nRecherche 5562, 2005. 6\n[27] D. Niste´r, “An efficient solution to the five-point relative pose\nproblem,” PAMI, vol. 26, no. 6, pp. 756–777, 2004. 7\n",
            "id": 4766269,
            "identifiers": [
                {
                    "identifier": "10.1109/iccv.2007.4408990",
                    "type": "DOI"
                },
                {
                    "identifier": "147598140",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "285038714",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:citeseerx.psu:10.1.1.385.5478",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "oai:doras.dcu.ie:4636",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "23533636",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "143907042",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2066056947",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "11309026",
                    "type": "CORE_ID"
                }
            ],
            "title": "Efficient generic calibration method for general cameras with single centre of projection",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2066056947",
            "oaiIds": [
                "oai:doras.dcu.ie:4636",
                "oai:citeseerx.psu:10.1.1.385.5478"
            ],
            "publishedDate": "2007-01-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 16484394,
                    "title": "A comparison of new generic camera calibration with the standard parametric approach,”",
                    "authors": [],
                    "date": "2007",
                    "doi": null,
                    "raw": "A. K. Dunne, J. Mallon, and P. F. Whelan, “A comparison of new generic camera calibration with the standard parametric approach,” in MVA, vol. 1, 2007, pp. 114–117. 2",
                    "cites": null
                },
                {
                    "id": 16484348,
                    "title": "A ﬂexible new technique for camera calibration,”",
                    "authors": [],
                    "date": "1998",
                    "doi": "10.1109/34.888718",
                    "raw": "Z. Zhang, “A ﬂexible new technique for camera calibration,” Microsoft research, Tech. Rep., 1998. 1, 4",
                    "cites": null
                },
                {
                    "id": 16484382,
                    "title": "A general imaging model and a method for ﬁnding its parameters,”",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1109/iccv.2001.937611",
                    "raw": "M. D. Grossberg and S. K. Nayar, “A general imaging model and a method for ﬁnding its parameters,” in ICCV,v o l .2 , 2001, pp. 108–115. 1, 2",
                    "cites": null
                },
                {
                    "id": 16484385,
                    "title": "A generic calibration concept: Theory and algorithms,” Rapport de Recherche 5058,",
                    "authors": [],
                    "date": "2003",
                    "doi": null,
                    "raw": "P. Sturm and S. Ramalingam, “A generic calibration concept: Theory and algorithms,” Rapport de Recherche 5058, 2003. 1, 2, 3, 5",
                    "cites": null
                },
                {
                    "id": 16484357,
                    "title": "A generic camera model and calibration method for conventional, wide-angle, and ﬁsheye lenses,”",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/tpami.2006.153",
                    "raw": "J. Kannala and S. S. Brandt, “A generic camera model and calibration method for conventional, wide-angle, and ﬁsheye lenses,” PAMI, vol. 28, no. 8, pp. 1335–1340, 2006. 1",
                    "cites": null
                },
                {
                    "id": 16484528,
                    "title": "A generic structure-from-motion framework,”",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1016/j.cviu.2006.06.006",
                    "raw": "S. Ramalingam, S. K. Lodha, and P. Sturm, “A generic structure-from-motion framework,” CVIU, vol. 103, no. 3, pp. 218–228, 2006. 6, 7",
                    "cites": null
                },
                {
                    "id": 16484370,
                    "title": "A theory of catadioptric image formation,”",
                    "authors": [],
                    "date": "1998",
                    "doi": "10.1109/iccv.1998.710698",
                    "raw": "S. Baker and S. Nayar, “A theory of catadioptric image formation,” in ICCV, 1998, pp. 35–42. 1",
                    "cites": null
                },
                {
                    "id": 16484380,
                    "title": "A unifying geometric representation for central projection systems,”",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1016/j.cviu.2006.06.003",
                    "raw": "J. P. Barreto, “A unifying geometric representation for central projection systems,” CVIU, vol. 103, no. 3, pp. 208–217, 2006. 1",
                    "cites": null
                },
                {
                    "id": 16484521,
                    "title": "Algorithms for plane-based pose estimation,” in CVPR,",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1109/cvpr.2000.855889",
                    "raw": "P. Sturm, “Algorithms for plane-based pose estimation,” in CVPR, 2000, pp. 1010–1017. 5",
                    "cites": null
                },
                {
                    "id": 16484541,
                    "title": "An efﬁcientsolutiontothe ﬁve-point relative pose problem,”",
                    "authors": [],
                    "date": "2004",
                    "doi": null,
                    "raw": "D.Nist´ er, “An efﬁcientsolutiontothe ﬁve-point relative pose problem,” PAMI, vol. 26, no. 6, pp. 756–777, 2004. 7",
                    "cites": null
                },
                {
                    "id": 16484377,
                    "title": "An unifying theory for central panoramic systems and pratical implications,” in ECCV,",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1007/3-540-45053-x_29",
                    "raw": "C. Geyer and K. Daniilidis, “An unifying theory for central panoramic systems and pratical implications,” in ECCV, 2000, pp. 445–461. 1",
                    "cites": null
                },
                {
                    "id": 16484403,
                    "title": "Analysis and solutions of the three point perspective pose estimation problem,”",
                    "authors": [],
                    "date": "1991",
                    "doi": "10.1007/bf02028352",
                    "raw": "R. M. Haralick, C. Lee, K. Ottenberg, and M. Nolle, “Analysis and solutions of the three point perspective pose estimation problem,” Hamburg, Tech. Rep., 1991. 5",
                    "cites": null
                },
                {
                    "id": 16484392,
                    "title": "Calibration of lens distortion by structured-light scanning,”",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1109/iros.2005.1545167",
                    "raw": "R. Sagawa, M. Takatsuji, T. Echigo, and Y. Yagi, “Calibration of lens distortion by structured-light scanning,” in ICIRS, 2005, pp. 1349–1354. 2, 3",
                    "cites": null
                },
                {
                    "id": 16484401,
                    "title": "Characteristic line of planar homography matrix and its applications in camera calibration,”",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/icpr.2006.363",
                    "raw": "J. Wang and Y. Liu, “Characteristic line of planar homography matrix and its applications in camera calibration,” in ICPR, 2006, pp. 147–150. 4",
                    "cites": null
                },
                {
                    "id": 16484373,
                    "title": "Estimation of omnidirectional camera model from epipolar geometry,”",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1109/cvpr.2003.1211393",
                    "raw": "B. Micusik and T. Pajdla, “Estimation of omnidirectional camera model from epipolar geometry,” in CVPR,v o l .1 , 2003, pp. 485–490. 1",
                    "cites": null
                },
                {
                    "id": 16484416,
                    "title": "Fast and globally convergent pose estimation from video images,”",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1109/34.862199",
                    "raw": "C. Lu, G. D. Hager, and E. Mjolsness, “Fast and globally convergent pose estimation from video images,” PAMI, vol. 22, no. 6, pp. 610–622, 2000. 5",
                    "cites": null
                },
                {
                    "id": 16484388,
                    "title": "generic concept for camera calibration,”",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1007/978-3-540-24671-8_1",
                    "raw": "——, “A generic concept for camera calibration,” in ECCV, vol. 2, 2004, pp. 1–13. 2, 3, 4",
                    "cites": null
                },
                {
                    "id": 16484353,
                    "title": "Geometric camera calibration using circular control points,”",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1109/34.879788",
                    "raw": "J. Heikkila, “Geometric camera calibration using circular control points,” PAMI, vol. 22, no. 10, pp. 1066–1077, 2000.",
                    "cites": null
                },
                {
                    "id": 16484361,
                    "title": "Implicit and explicit camera calibration: theory and experiments,”",
                    "authors": [],
                    "date": "1994",
                    "doi": "10.1109/34.291450",
                    "raw": "G. Q. Wei and S. D. Ma, “Implicit and explicit camera calibration: theory and experiments,” PAMI,v o l .1 6 ,n o .5 ,p p . 469–480, 1994. 1",
                    "cites": null
                },
                {
                    "id": 16484367,
                    "title": "Multi-view geometry of 1d radial cameras and its application to omnidirectional camera calibration,” in ICCV,",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1109/iccv.2005.158",
                    "raw": "S. Thirthala and M. Pollefeys, “Multi-view geometry of 1d radial cameras and its application to omnidirectional camera calibration,” in ICCV, 2005, pp. 1539–1546. 1",
                    "cites": null
                },
                {
                    "id": 16484343,
                    "title": "On plane-based camera calibration: A general algorithm, singularities,",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1109/cvpr.1999.786974",
                    "raw": "P. Sturm and S. J. Maybank, “On plane-based camera calibration: A general algorithm, singularities, applications,” in CVPR, vol. 1, 1999, pp. 432–437. 1, 4",
                    "cites": null
                },
                {
                    "id": 16484396,
                    "title": "Pattern codiﬁcation strategies in structured light systems,”",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1016/j.patcog.2003.10.002",
                    "raw": "J. Salvi, J. Pages, and J. Batlle, “Pattern codiﬁcation strategies in structured light systems,” Pattern Recognition, vol. 37, no. 4, pp. 827–849, April 2004. 3",
                    "cites": null
                },
                {
                    "id": 16484364,
                    "title": "Self-calibration of a general radially symmetric distortion model,” in ECCV,v o l .4 ,M a y",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1007/11744085_15",
                    "raw": "J. Tardif, P. Sturm, and S. Roy, “Self-calibration of a general radially symmetric distortion model,” in ECCV,v o l .4 ,M a y 2006, pp. 186–199. 1",
                    "cites": null
                },
                {
                    "id": 16484346,
                    "title": "Simultaneous linear estimation of multiple view geometry and lens distortion,”",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1109/cvpr.2001.990465",
                    "raw": "A. W. Fitzgibbon, “Simultaneous linear estimation of multiple view geometry and lens distortion,” in CVPR,v o l .1 , 2001, pp. 125–132. 1",
                    "cites": null
                },
                {
                    "id": 16484374,
                    "title": "Structure from motion with wide circular ﬁeld of view cameras,”",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/tpami.2006.151",
                    "raw": "——, “Structure from motion with wide circular ﬁeld of view cameras,” PAMI, vol. 28, no. 7, pp. 1135–1149, 2006.",
                    "cites": null
                },
                {
                    "id": 16484537,
                    "title": "Theory and experiments towards complete generic calibration,” Rapport de Recherche 5562,",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1109/cvpr.2005.347",
                    "raw": "S. Ramalingam, P. Sturm, and S. Lodha, “Theory and experiments towards complete generic calibration,” Rapport de Recherche 5562, 2005. 6",
                    "cites": null
                },
                {
                    "id": 16484399,
                    "title": "Which pattern? biasing aspects of planar calibration patterns and detection methods,”",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1016/j.patrec.2006.12.008",
                    "raw": "J. Mallon and P. F. Whelan, “Which pattern? biasing aspects of planar calibration patterns and detection methods,” PRL, vol. 28, no. 8, pp. 921–930, 2007. 3",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "http://doras.dcu.ie/4636/1/ICCV_07.pdf",
                "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.385.5478"
            ],
            "updatedDate": "2021-12-13T06:11:14",
            "yearPublished": 2007,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/11309026.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/11309026"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/11309026/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/11309026/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/4766269"
                }
            ]
        },
        {
            "acceptedDate": "2007-06-02T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Boyd, Colin"
                },
                {
                    "name": "Foo, Ernest"
                },
                {
                    "name": "Gonzalez Nieto, Juan"
                },
                {
                    "name": "Smith, Jason"
                },
                {
                    "name": "Tritilanunt, Suratose"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/188904104",
                "https://api.core.ac.uk/v3/outputs/10883856",
                "https://api.core.ac.uk/v3/outputs/146913912"
            ],
            "createdDate": "2013-07-02T14:29:55",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 310,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/310",
                    "logo": "https://api.core.ac.uk/data-providers/310/logo"
                },
                {
                    "id": 3110,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/3110",
                    "logo": "https://api.core.ac.uk/data-providers/3110/logo"
                }
            ],
            "depositedDate": "2007-01-01T00:00:00",
            "abstract": "Denial of Service (DoS) attacks are an increasing problem for network connected systems. Key establishment protocols are applications that are particularly vulnerable to DoS attack as they are typically required to perform computationally expensive cryptographic operations in order to authenticate the protocol initiator and to generate the cryptographic keying material that will subsequently be used to secure the communications between initiator and responder. The goal of DoS resistance in key establishment protocols is to ensure that attackers cannot prevent a legitimate initiator and responder deriving cryptographic keys without expending resources beyond a responder-determined threshold. In this work we review the strategies and techniques used to improve resistance to DoS attacks. Three key establishment protocols implementing DoS resistance techniques are critically reviewed and the impact of misapplication of the techniques on DoS resistance is discussed. Recommendations on effectively applying resistance techniques to key establishment protocols are made",
            "documentType": "research",
            "doi": "10.1504/ijwmc.2007.013796",
            "downloadUrl": "https://core.ac.uk/download/10883856.pdf",
            "fieldOfStudy": "computer science",
            "fullText": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nQUT Digital Repository:  \nhttp://eprints.qut.edu.au/ \nSmith, Jason and Tritilanunt, Suratose and Boyd, Colin A. and Gonzalez Nieto, \nJuan M. and Foo, Ernest (2007) Denial-of-service resistance in key establishment. \nInternational Journal of Wireless and Mobile Computing 2(1):pp. 59-71. \n \n          © Copyright 2007 Inderscience \nDenial of Service Resistance\nin Key Establishment\nJ. Smith, S. Tritilanunt, C. Boyd, J. M. Gonza´lez Nieto,\nand E. Foo\nInformation Security Institute\nQueensland University of Technology\nGPO Box 2434, Brisbane, QLD 4001\nAustralia\nAbstract:\nDenial of service (DoS) attacks are an increasing problem for network connected sys-\ntems. Key establishment protocols are applications that are particularly vulnerable to\nDoS attack as they are typically required to perform computationally expensive cryp-\ntographic operations in order to authenticate the protocol initiator, and to generate the\ncryptographic keying material that will subsequently be used to secure the communi-\ncations between initiator and responder. The goal of denial of service resistance in key\nestablishment protocols is to ensure that attackers cannot prevent a legitimate initia-\ntor and responder deriving cryptographic keys without expending resources beyond a\nresponder determined threshold. In this work we review the strategies and techniques\nused to improve resistance to denial of service attacks. Three key establishment proto-\ncols implementing denial of service resistance techniques are critically reviewed and the\nimpact of misapplication of the techniques on denial of service resistance is discussed.\nRecommendations on effectively applying resistance techniques to key establishment\nprotocols are made.\nKeywords: Denial of service, cryptographic protocols, key establishment, key agree-\nment, proofs of work, client puzzles.\nReference to this paper should be made as follows: J. Smith, S. Tritilanunt, C. Boyd,\nJ. M. Gonza´lez Nieto, and E. Foo (2006) “Denial of Service Resistance in Key Estab-\nlishment”, International Journal of Wireless and Mobile Computing\nBiographical notes: Jason Smith is currently a research associate with the Informa-\ntion Security Institute at Queensland University of Technology (QUT) and is working\ntowards his PhD. His research interests include mobile and wireless network security\nand intrusion detection, cryptographic protocols, and denial of service resistance.\nSuratose Tritilanunt received a Master Degree in Computer Engineering from King\nMongkut’s University of Technology Thonburi (KMUTT), Thailand. He is currently a\nPhD student in the Information Security Institute, QUT. His research interests include\nCryptographic protocol, Denial-of-Service attack, Key Establishment protocol, Formal\nMethods, and Network Security.\nColin Boyd is a Professor in the School of Software Engineering and Data Communica-\ntions and Deputy Director of the Information Security Insitute at QUT. His research\ninterests are in the theory and applications of cryptography, particularly cryptographic\nprotocols. He is the author of over 100 fully refereed publications including a promi-\nnent recent book on protocols for authentication and key establishment.\nJuan Gonza´lez Nieto is a Research Fellow at the Information Security Institute, QUT.\nHe obtained his PhD in the field of cryptographic protocols. His main research inter-\nests lie in the area key establishment protocols and public key cryptography, with an\nemphasis on provable security. He has authored over 19 journal and conference papers.\nErnest Foo is a lecturer in the School of Software Engineering and Data Communi-\ncations at QUT and also a member of the Information Security Institute. The main\ntopics of his research include secure protocols and network security with particular\nemphasis on electronic commerce applications.\n1\n1 INTRODUCTION\nKey establishment protocols enable two parties to authen-\nticate each other and establish cryptographic key mate-\nrial using an adversarially controlled network. The cryp-\ntographic keys derived from the key establishment proto-\ncol are then typically used to secure the communications\nchannel by providing confidentiality and integrity services\nusing IPsec [1] for example. The party that initiates the\nkey establishment protocol by transmitting the first mes-\nsage is known as the initiator. The party that receives and\nresponds to the first message is known as the responder.\nIn addition to providing a mechanism for establishing\ncryptographic keys, key establishment protocols may also\nprovide additional services including: identity protection\nfor the initiator or responder; plausible deniability, a prop-\nerty that ensures that evidence of participation by a par-\nticular party in a protocol run is not generated by the pro-\ntocol; and perfect forward secrecy, a property that ensures\ncompromise of long term keys cannot reveal previously ne-\ngotiated session keys.\nThe need to secure the confidentiality and integrity of\ncommunications highlights the threat from malicious enti-\nties active in the network. Increasingly, malicious network\nentities are seeking not only to violate the confidentiality\nand integrity of information, but also to deny or degrade\naccess to network services via denial of service (DoS) at-\ntacks.\nDenial of service attacks can be classified as flooding, or\nlogical (non-flooding). In flooding-based denial of service\nattacks, the attacker generates spurious network traffic or\nrequests for service in order to exhaust available server\nresources, thereby preventing access to the service by le-\ngitimate clients. Directing a continuous stream of traffic\nthat exceeds the bandwidth available to a target service\nis an example of a flooding attack. To successfully mount\na flooding-based attack the attacker must command re-\nsources (processing, memory or network bandwidth) in ex-\ncess of those provisioned at the target system. While the\nresources required for an attacker to successfully mount a\nflooding-based denial of service may be considerable, this\ntype of attack requires limited knowledge of the specific\nservice protocols or their implementation to be successful.\nIn logical attacks, rather than blindly trying to exhaust\nserver resources, the attacker exploits characteristics or\nvulnerabilities of network protocols or applications in such\na way as to exhaust available resources. Exploiting vulner-\nabilities in the target system1 or protocols employed by the\ntarget system, such as with the TCP based SYN flooding\nattacks [3] are examples of logical attacks. Logical attacks\nrequire the attacker to have a greater understanding of the\nprotocols or applications to be targeted, but can dramat-\nically reduce the resources required to successfully attack\na service.\nNeedham [4] identified that attacks may be directed\n1Errors in operating system handling of oversized ICMP echo re-\nquests led to some systems to crash, freeze or reboot on receipt of a\n“ping of death” [2].\nat: (1) servers, henceforth referred to as responders; (2)\nthe network infrastructure, such as links, routers, domain\nname servers; or (3) specific client systems, henceforth re-\nferred to as initiators. Having determined where an attack\nis to be directed, an attacker can attempt either a flooding\nor logical attack.\nKey establishment protocols are applications that are\nparticularly vulnerable to logical denial of service attacks\nas they are typically required to perform computationally\nexpensive cryptographic operations in order to authenti-\ncate the protocol initiator, and to generate the crypto-\ngraphic keying material that will subsequently be used to\nsecure the communications between initiator and respon-\nder. Additionally, key establishment protocols are openly\nspecified, providing attackers with the opportunity to eas-\nily search for denial of service vulnerabilities in the protocol\nspecifications.\nAssuming the presence of malicious network entities, key\nestablishment protocols must adopt strategies and tech-\nniques to ensure that they are not susceptible to denial of\nservice attacks. The ability of a key establishment proto-\ncol to withstand attempts to exhaust responder resources\nvia denial of service attack is termed the denial of service\nresistance of the protocol. The goal of denial of service re-\nsistance in key establishment protocols is to ensure that at-\ntackers cannot prevent a legitimate initiator and responder\nderiving cryptographic keys without expending resources\nbeyond a responder determined threshold.\nWhile we recognize that to be effectively managed denial\nof service must be addressed at the link, network, operat-\ning system, and application layers, we focus our attention\nin this work specifically on the denial of service resistance\ntechniques employed at the application layer by key estab-\nlishment protocols. While either the initiator or responder\nmay be the target of a denial of service attack, in this paper\nwe only consider malicious initiators and the techniques\nused to defend responders against attack. The reason for\nfocusing on responders is that responders typically handle\nrequests for service from many initiators, so the impact of\ndenial of service attacks against a responder is significant.\nThe effect of a denial of service attack targeted at an initia-\ntor is far more localised, only impacting the specific target\nof the attack.\nEven though numerous strategies and techniques are\navailable to improve the resistance a protocol has to denial\nof service attack, they are not always applied correctly\nresulting in protocols that do not effectively improve their\nresistance to denial of service attack. The contributions\nof this work include:\n• a review of the strategies and techniques used to im-\nprove resistance to denial of service attack;\n• the critical analysis of protocols implementing denial\nof service resistance techniques with discussion of the\nimpact of misapplication of the techniques on denial\nof service resistance; and\n2\n• recommendations on effectively applying denial of\nservice resistance techniques to key establishment\nprotocols.\nOwing to space constraints, we only present a subset of\nprotocols that can serve as exemplars for the techniques\nidentified.\nThe remainder of the paper is structured as follows: Sec-\ntion 2 identifies the strategies used to increase resistance to\ndenial of service attack; Section 3 presents the techniques\nused to implement denial of service resistance strategies;\nSection 4 details three protocols that implement denial of\nservice resistance strategies, critically discussing the appli-\ncation of the techniques; and Section 5 presents conclusions\nand directions for future work.\n2 DOS-RESISTANCE STRATEGIES\nA responder in a key establishment protocol will have ac-\ncess to finite processing, storage, and network resources in\norder to complete its functions. Unless these resources are\ncommitted diligently, they may be exhausted by malicious\ninitiators and the responder will have insufficient resources\nremaining to process legitimate incoming requests.\nThe requirement for key establishment protocols to\nexhibit denial of service resistance is well recognized\nby the protocol engineering community and a number\nof design strategies have emerged that promote the\njudicious allocation of resources when processing initiator\nrequests [5, 6]. The proposed strategies can be broadly\nclassified into three types.\n1. Counterbalancing memory expenditure By en-\nsuring that initiators must commit their memory re-\nsources to maintaining protocol state until the respon-\nder has some assurance that a denial of service attack\nis not underway reduces the vulnerability of the re-\nsponder to state or memory-based denial service at-\ntacks and increases the memory resources an attacker\nwill need to attack the responder.\n2. Counterbalancing computational expenditure\nBy counterbalancing computational expenditure at\nthe responder, the protocol designer can ensure\nthat the computational resources of an initiator\nwill be exhausted before those of the responder.\nAchieving this goal may require artificially increasing\nthe computational expenditure of the initiator to\nensure the survivability of the responder [5], or\nhaving the initiator perform computations on behalf\nof the responder, thereby reducing the relative cost\nof computation to the responder.\n3. Gradual authentication While initiators must be\nauthenticated at some point during the protocol ex-\necution, immediate and strong authentication of re-\nquests merely aggravates the denial of service prob-\nlem. The suggested strategy for balancing the need\nfor authentication and computational expenditure is\nto use weak and computationally cheap authentica-\ntion when the protocol is initiated and gradually in-\ncrease the strength of authentication as the protocol\nproceeds [6]. The strategy of gradual authentication\ncan be used to detect attacks (based on IP spoofing\nfor example), verify the computational and memory\ncommitments of the initiator, and link messages from\nthe same source (even though the exact identity of\nthat source may be unknown).\nThe protocols discussed in Section 4 provide examples\nof how gradual authentication can be implemented in\nkey establishment protocols.\nCombining the strategies of counterbalancing computa-\ntional expenditure, counterbalancing memory expenditure,\nand gradually authenticating requests ensures that mali-\ncious initiators are unable to prevent the establishment of\ncryptographic keys between legitimate initiators and re-\nsponders, unless they are prepared to expend significant\nresources of their own.\n3 DOS-RESISTANCE TECHNIQUES\nHaving identified the strategies employed to make respon-\nders in a key establishment protocol more resistant to de-\nnial of service attacks in Section 2, we now describe the\nspecific techniques used. The techniques described may be\nconsidered primitives, some of which are capable of imple-\nmenting more than one strategy and some of which can\nbe combined to meet more complex goals such as grad-\nual authentication. For each technique identified, we dis-\ncuss its construction, the DoS resistance strategies it is\ncapable of supporting, and how it might be combined with\nother techniques. Protocols implementing the techniques\nare critically discussed in Section 4.\n3.1 Cookies\nCookies are time variant, unpredictable data issued by the\nresponder on receipt of a request for service that allow the\nresponder to remain stateless and initiate gradual authen-\ntication of the initiator. First introduced in Photuris [7]\nand subsequently extended for resisting SYN flooding DoS\nattacks [8], cookies are now widely used.\nTypically a cookie is constructed by taking some con-\nnection specific parameters and transforming them with a\ntime variant local secret; a keyed hash of the initiator IP\naddress and nonce for example. It is vitally important that\nthe responder store no state when constructing cookies. In\n3\norder to remain stateless and thereby prevent memory ex-\nhaustion, any relevant state required by the responder can\nalso be encoded in the cookie and returned with the next\nmessage from the initiator. An approach for making pro-\ntocols stateless is presented by Aura and Nikander [9].\nOn receipt of a valid cookie, the responder is able to re-\nconstruct and validate any state encoded in the cookie and\nhas weak assurance that it is in round trip communication\nwith the initiator. Round trip communication implies that\nthe initiator is not using a spoofed address. This assurance\ncan only be considered weak, as an adversary with control\nof an intermediary link, between a claimed address and the\nresponder, would be able to receive cookies for any address\nthey wished to claim.\nUnless cookies are carefully constructed the responder\nmay remain vulnerable to attack even if cookies are used.\nSimpson [10] identified a state exhaustion attack, called a\n“cookie crumb” attack, in the ISAKMP implementation\nof cookies. In contrast to remaining stateless when con-\nstructing cookies, ISAKMP cookies required the storage\nof a small amount of state on each connection request.\nEven though the state information stored per request is\nvery small (a “crumb”) it is easy for an attacker to initiate\na large number of requests, exhausting available memory\nresources.\nIn addition to ensuring that no state is stored on the\nconstruction of a cookie, Karn and Simpson [7] identified\nthat the technique used for generating cookies must also\nsatisfy the following three requirements.\n• The cookie must depend on the participating entities.\n• It must not be possible for anyone other than the is-\nsuing entity to generate a cookie that will be accepted\nby that entity.\n• The cookie generation and verification methods must\nbe computationally efficient.\nThe first requirement prevents an attacker from obtain-\ning valid cookies, intended for other initiators, and using\nthose cookies to generate a large number of requests with\nspoofed IP addresses. The second requirement secures the\ncookie generating process. The use of a secret value in gen-\nerating the cookie prevents others from forging cookies and\nmaking this value time variant ensures that cookies must\nbe used within a predetermined time frame, preventing the\nhoarding of valid cookies. Finally, the third requirement\nprevents DoS attacks directed at the cookie mechanism\nitself.\n3.2 Proofs of Work\nProofs of work, or puzzles, are hard but tractable problems\nthat allow an initiator to prove to a responder that a verifi-\nable level of computational effort has been expended. They\npermit the responder to gain some assurance of the initia-\ntor’s willingness to commit resources to the protocol and\nprovide a mechanism for counterbalancing computational\nexpenditure in the event that the responder is exposed to\na denial of service attack.\nThe concept was first proposed by Dwork and Naor [11]\nto control junk email by having recipients only accept\nemails if they were accompanied by a correct puzzle so-\nlution. It has since been extended to protect authentica-\ntion protocols [12,13] and permit clients to bid for limited\nservice resources [14] using the difficulty of the puzzle as\ncurrency. Jakobsson and Juels [15] formalised the notion\nof reusable proofs of work, where the computational effort\nexpended by the prover in generating the puzzle solution\ncan be reused for some useful function, and provided a\nworking example of a reusable proof of work.\nPuzzles serving as proofs of work can be constructed\nfrom a number of underlying problems, which introduce\na minimal and configurable overhead for legitimate initia-\ntors but result in a significant computational burden for\nattackers who wish to send large numbers of requests to a\nresponder.\nProperties of a good puzzle include [12,13]:\n• generation and verification is inexpensive for\nthe responder;\n• level of difficulty can easily be adjusted from\ntrivial to impossible;\n• solutions should not require specialised\nclient hardware;\n• solutions cannot be precomputed;\n• issuing a puzzle does not require the respon-\nder to store any state;\n• knowledge of the solution to one client’s puz-\nzle is of no benefit in solving other puzzles,\nso that the same puzzle may be provided to\nnumerous clients; and\n• initiators can reuse a puzzle by creating\nnew instances of it.\n3.2.1 Hash-based puzzles\nJuels and Brainard [12] describe the construction of client\npuzzles to protect TCP and SSL against connection de-\npletion (SYN flooding) attacks. In their proposal, when a\nserver becomes heavily loaded (inferred from buffer occu-\npancy), connections are only accepted if they are accompa-\nnied by a proof of work. The puzzle (shown in Figure 1(a))\nis constructed by hashing session parameters (M ), the cur-\nrent time (t), and a responder secret (s). The n-bit out-\nput of this hash operation (X) becomes the preimage to\nanother application of a hash function, whose output (Y )\nforms part of the puzzle. The initiator is provided the par-\ntial preimage X ′ (X with k-bits masked out), and the hash\ndigest Y . In order to solve the puzzle, the initiator must\ntest all k possible preimages until the correct output is\nachieved. On average this will take 2k−1 hash operations.\n4\nhash(t||M ||s) = X\nhash(X) = Y\nX ′ = X & 00, 01, ..., 0k−11k1k+11n−1\npuzzle = (X ′, Y )\nsolution = p\nverification : hash(p||X ′) ?= hash(t||M ||s)\n(a) Juels and Brainard Construction\nhash(NR||X) = 000, ..., 000︸ ︷︷ ︸\nk−bits\n||, ...\n(b) Aura et al. Construction\nFigure 1: Hash-Based Puzzle Constructions\nTo verify the solution (p), the responder checks the time\nvalue t is recent, and then confirms, with a single hash\noperation, that the proposed solution is correct:\nhash(p||X ′) ?= hash(t ,M , s)\nAn alternative construction is proposed by Aura et\nal. [13] and is shown in Figure 1(b). In this proposal the\npuzzle consists of a time variant responder nonce (NR) and\na difficulty parameter (k). To solve the puzzle the initiator\nmust find the value X, that when hashed with the respon-\nder nonce (NR) produces a digest output whose first k-bits\nare zeros.\nThe responder verifies the puzzle solution by checking\nthat NR is recent and that the solution (X) when hashed\nwith the nonce produces an output with the first k-bits as\nzero.\nSigned puzzles The construction of Aura et al. [13] was\ndesigned specifically for use in authentication protocols.\nThe construction does not include any initiator specific\nparameters, allowing the responder to sign a single puz-\nzle and issue it to multiple initiators. Conversely, the in-\nclusion of initiator specific parameters in the Juels and\nBrainard [12] construction would make the signing of puz-\nzles prohibitively expensive.\nPuzzles, reachability and statelessness To operate\neffectively as a replacement for cookies (providing weak\nauthentication of initiator reachability and stateless\nconnections) proofs of work must be constructed so that:\n• the IP address of the initiator must be part of the\npuzzle construction, and\n• required state information is also encoded in the\npuzzle.\nIf the puzzle construction is unable to meet these re-\nquirements, then the puzzle cannot replace the functions\nof a cookie in the protocol. If we reconsider the puzzle\nconstruction of Aura et al. [13], which does not include\nthe IP address of the initiator in the puzzle construction,\nit would be possible for a malicious initiator to receive a\npuzzle challenge on one IP address and construct puzzle\nsolutions for any number of spoofed IP addresses. Ad-\nmittedly, the initiator would have to generate a unique\nsolution for each address it wished to claim, but in spite of\nreceiving a valid puzzle solution, the responder would have\nno assurance that the initiator was actually reachable at\nthe claimed IP address. In order to gain assurance that\nan initiator is able to send and receive messages from a\nclaimed IP address, protocols using the Aura et al. puzzle\nconstruction as a proof of work should also use a cookie.\nThe Juels and Brainard [12] puzzle includes the IP address\nof the initiator in its construction, so protocols using this\nconstruction are able to use the puzzle as a replacement\nfor a cookie.\n3.2.2 Other constructions\nWhile the hash-based construction is prevalent in in-\nteractive protocols owing to its simple construction and\ncheap verification, other puzzle constructions have been\nproposed to accommodate: non-interactive protocols\nsuch as email; puzzles that are intended to act as time\ncapsules with solutions taking years, not milliseconds;\nand techniques for supporting the outsourcing of puzzle\nconstructions.\n• Signature-based puzzles: The puzzle proposed\nDwork and Naor [11] could be constructed from a\nweakened Fiat-Shamir signature, or from the broken\nOng-Schnorr-Shamir signature scheme. The initiator\nis required to forge a verifiable signature as a proof\nof work to the responder. Matsuura and Imai [16]\npresent an alternative form of signature-based proof\nof work in which verification of the responder signa-\nture by the initiator requires the calculation of an\nintermediate value. This value is presented by the\ninitiator to the responder as proof that the signature\nverification, and the associated modular exponentia-\ntions, have been performed (See Section 4.1 for the\ndetails).\n• Time-Lock puzzles: Proposed by Rivest et al. [17],\ntime-lock puzzles are based on the notion that a\nclient has to spend a predetermined amount of\ncomputation time performing repeated squaring to\nfind a solution. The server calculates the number\nof squaring operations which a client can perform,\nand determines the amount of time it wants a client\nto spend solving the puzzle. Time-lock puzzles are\ninherently sequential and non-parallelisable.\n• Diffie-Hellman puzzles: Waters et al. [18] in-\nvestigate numerous techniques for outsourcing the\n5\ngeneration of puzzles in order to remove the com-\nputational burden of puzzle generation from the\nresponder. One of their proposals is based on a Diffie-\nHellman construction, in which given a generator g,\nand a random value a in the range r to r + k, the\npuzzle issued to the initiator contains the values ga\nand r. The initiator searches for a solution by trying\neach candidate value in the range r to r + k until it\nfinds c, such that gc = ga. To bind the solution to\na specific server, the initiator calculates ya, where\ny is the responder’s Diffie-Hellman public key. The\nresponder verifies the solution with a single modular\nexponentiation, raising the challenge value to the\nexponent (x) of its private key i.e. gax. The system\ndescribed by Waters et al. [18] limits puzzles to a\ngiven timeslot. The solutions for a given timeslot\nare all precomputed by the responder, so puzzle\nverification is performed by a table lookup - not an\nonline modular exponentiation.\nProofs of work can be viewed as a way for an initiator\nto make a payment to a responder for the services it will\nprovide. The computational effort expended in generating\nthe proof of work can be: (1) wasted; (2) reused by the\ninitiator in completing the protocol; (3) reused by the ini-\ntiator for some other purpose; (4) reused by the responder\nin completing the protocol; or (5) reused by the responder\nfor some other purpose.\nThe client puzzles employed by the protocols identified\nin the following section, are typically non-reusable proofs of\nwork, so the computational effort expended in generating\nthe proof of work is wasted. We will see however, that the\nmodified version of Internet Key Exchange (IKE) proposed\nby Matsuura and Imai [16] adopts a reusable proof of work\nbased on signature verification of the responder, an action\nthat a legitimate initiator will have to perform in order to\ncomplete the protocol.\nAs identified earlier, Jakobsson and Juels [15] describe a\nproof of work in which the computational effort expended\nin generating the proof of work is reused by the respon-\nder for another application. There are currently no ex-\namples of key establishment protocols implementing this\ntype of reusable proof of work and unless an initiator im-\nplicitly trusts the responder to delegate its computational\nresources, initiators must be aware that the computational\neffort expended in generating a proof of work may be\nreused for malicious purposes.\nEven though proofs of work are increasingly being\nadopted by protocols to aid in the counterbalancing\ncomputational expenditure and as a way of authenticating\nthe willingness of an initiator to commit resources to\nhaving the protocol proceed, their use still faces numerous\nissues.\n• Hash-based constructions meet many of the de-\nsirable properties of proofs of work (puzzles), but\nthey also have the property that exhaustive search-\ning of a preimage search space is a parallelisable\ntask. Using such a technique in the presence of\nan adversary with access to distributed computing\nresources may leave the protocol exposed to denial\nof service. Adopting alternate puzzle construc-\ntions, such as time lock puzzles, that are inherently\nsequential and non-parallelisable may need to be\nconsidered for protocols that are to be used in an\nenvironment where the adversarial model assumes\nthat significant resources are available to the attacker.\n• Proofs of work based on processor bound functions\nresult in puzzle constructions that can be solved in\nnegligible time on a modern desktop computer, but\nmay take an inordinately long time on light weight\ndevices such as mobile phones or personal digital\nassistants. Given an effective strategy for authenti-\ncating the platform that a puzzle was being issued\nto, could allow the responder to tune the difficulty\nof the puzzle. As there is no way to authenticate\nthe platform a puzzle is being issued to, alternative\nconstructions based on memory bound functions are\nbeing investigated [19, 20]. Memory bound functions\nshould exhibit a more uniform response time across\na range of devices, as the difference in memory\naccess speeds between light weight and more powerful\ndevices is far less significant than the difference\nbetween processor speeds.\n3.3 Client-Aided Computation\nAn alternative approach to artificially increasing compu-\ntational expenditure at the initiator with puzzle construc-\ntions is to have the initiator perform computations that\nease the computational burden of the responder. Client\naided computations will most likely be performed before\nthe initiator is fully authenticated, and as the initiator can-\nnot be trusted at this time the type of computations that\ncan be offloaded to the initiator are restricted to those that\ncan be verified as correct.\nCurrently, the only protocol implementing client aided\ncomputation is the client aided RSA implementation of the\nSSL protocol proposed by Castellucia et al. [21] which is\ndescribed in Section 4.3.\n3.4 Gradual Authentication\nWhile the expense of strongly authenticating initiators us-\ning digital signatures will be dependent on many param-\neters, the computational expense of a signature verifica-\ntion will not always be prohibitively expensive. Rabin\nsignatures [22] with a public exponent of 2 or RSA sig-\nnatures [23] with a public exponent of 3 can be verified\nwith only one or two modular multiplications respectively.\nWhile the cost of signature verification with these param-\neters is low, signature generation is somewhat more ex-\npensive, which may not be suitable for all deployment sce-\n6\nnarios. Other signature schemes, RSA with larger pub-\nlic exponents for example, increase the cost of signature\nverification, requiring the responder to perform expensive\nmodular exponentiations. While newly proposed key es-\ntablishment protocols can be specified to accommodate\ncheap signature verification for responders the requirement\nto improve resistance to denial of service attack remains for\nalready deployed protocols and protocols, that for other\nreasons are restricted in the choice of signature schemes\nthey must implement. Gradual authentication provides a\nmechanism for weakly authenticating an initiator, prior to\nperforming stronger and more expensive cryptographic au-\nthentication.\nThe idea of combining weak and strong authentication\nwas first introduced by Meadows [6] and is proposed as\na technique to increase resistance to denial of service at-\ntacks by combining weak authentication when the protocol\nis initiated and moving to strong authentication as it com-\npletes.\nCookies and client puzzles can be considered forms of\nweak authenticators. Cookies provide some assurance that\nthe initiator is able to send and receive packets from the\nclaimed address–implying that the request is not part of a\nconnection depletion attack, which typically relies on using\nrandom spoofed addresses. Receipt of a correct solution to\na client puzzle provides some assurance to the responder\nthat the initiator is willing to expend her own resources in\norder to get the protocol to proceed.\nOther cryptographic techniques, such as the use of mes-\nsage authentication codes and release of hash digest preim-\nages, that allow the responder to cheaply verify messages\nare being adopted by recently proposed protocols such as\nJust Fast Keying (JFK) as discussed in Section 4.\nWhile the use of techniques such as cookies, client puz-\nzles, and releasing hash preimages do not meet strong no-\ntions of authentication, when generated using cryptograph-\nically sound primitives they can be combined in ways which\nenable a responder to discount a range of denial of service\nattacks and present a number of hurdles that must be over-\ncome by an attacker intent on disrupting the protocol exe-\ncution. A key characteristic of the techniques used in grad-\nual authentication is that they are all cheap for the respon-\nder to verify, while their fabrication is relatively expensive\nfor an attacker. Even when signature schemes that min-\nimise verification costs to a responder are adopted, the cost\nof verifying gradual authenticators such as client-puzzles is\nstill cheaper, costing only a single hash operation.\nKey establishment protocols must complete with strong\nauthentication of the initiator. Having weakly authenti-\ncated the initiator, the responder is able to commit to the\ncomputational expenditure associated with strong authen-\ntication with increased assurance that a denial of service\nattack is not underway. The gradual authentication tech-\nnique is employed by the modified Internet Key Exchange\n(IKE) and the Just Fast Keying (JFK) protocols, discussed\nin more detail in Section 4.\nTable 1: Protocol Notation\nMessages Notation\nI The principal who initiates the request message\nknown as Initiator or client\nR The principal who responds to the request message\nknown as Responder or server\nID Identity of the principal\nIP Network address of principal\nH(M) Unkeyed cryptographic hash of the message M\nHK(M)\nKeyed cryptographic hash of the message M , with\nkey K\nEKs{M} Symmetric encryption of message M\nwith the secret key Ks\n{M}KeKa Encryption of M using symmetric key Ke,\nfollowed by MAC generation with symmetric keyKa.\nP [M ] Asymmetric encryption of the message M\nby the public key P belonging to the principal\nS[M ] Digital signature of message M\nwith the private key S belonging to the principal\ng group generator of order q\nN Nonce of principal; a random bit string\nKR,s The responder/server secrets\nsaI Cryptographic and service properties of the security\nassociation (SA) that the initiator wants to establish\nsaR SA information that the responder may need to give\nto the initiator\nKs Session key generated by key establishment protocol\nwhich is used to secure ongoing communications\ngrpinfoR All groups supported by the responder\nx ∈R A Assigns to x an element of the setA chosen uniformlyat random\nN = PQ An RSA modulus\n4 DOS RESISTANCE IN KEY ESTABLISHMENT\nThe process of authenticating initiators and generating\ncryptographic keys to secure ongoing communications re-\nquires responders to commit significant resources in order\nto have the protocol execute to completion. Unless key\nestablishment protocols manage the commitment of their\nresources they will be susceptible to denial of service at-\ntacks.\nIn this section, we identify and discuss three protocols\nthat implement strategies and techniques to improve their\nresistance to denial of service attacks. There are relatively\nfew examples of key establishment protocols implement-\ning denial of service resistance techniques, with our survey\nof the literature only identifying seven protocols (see Ta-\nble 2 in Section 5 for a summary list). The three protocols\npresented in this section were selected as they provide con-\ncrete examples of the range of denial of service resistance\ntechniques being applied to key establishment. The modi-\nfied internet key exchange protocol proposed by Matsuura\nand Imai [16] adopts all three strategies for improving de-\nnial of service resistance and includes an elegant proof of\nwork that is reused by the client to complete the proto-\ncol execution. The Just Fast Keying (JFK) protocol [24]\ndemonstrates new techniques for gradually authenticating\ninitiators, and the client-aided RSA proposal by Castel-\nluccia et al. [21] is the first key establishment protocol to\nadopt client aided computation in order to counterbalance\ncomputational expenditure.\n7\nFor the protocols presented in this section, we focus on\nthose elements of the protocol that implement denial of\nservice resistance techniques. Our representation of the\nprotocols is simplified, with references to header informa-\ntion and certificate requests, that are not relevant to the\ndiscussion of denial of service resistance, deliberately omit-\nted. For complete descriptions of the protocols, the reader\nis referred to the full protocol specifications.\nWe present the notation used for the remainder of this\nsection in Table 1.\n4.1 Modification of Internet Key Exchange Resis-\ntant against DoS\nThe Internet Key Exchange (IKE) protocol [25] was de-\nsigned to perform mutual authentication and establish a\nshared secret key for use in an IPsec security association.\nAs originally specified, the aggressive, signature based au-\nthentication mode of IKE was vulnerable to CPU and\nmemory exhaustion denial of service attacks. In order to\naddress these vulnerabilities Matsuura and Imai [16] pro-\nposed modifications to improve the protocol’s resistance to\nboth computational and memory based denial of service at-\ntacks. The modified protocol is presented in Figure 2 and\nadopts techniques that counterbalance computational and\nmemory expenditure, and implement gradual authentica-\ntion.\n4.1.1 Counterbalancing memory expenditure\nTo address memory based denial of service attacks, this\nmodified version of IKE stores no state after the first mes-\nsage. Unlike the original cookie construction that was vul-\nnerable to a cookie crumb attack, the cookie in the modi-\nfied proposal is constructed as a hash over request specific\nparameters, responder secret s, random fresh material a\nand Diffie-Hellman exponent r. The session specific se-\ncret parameters a and r are not stored by the responder,\ninstead the protocol remains stateless by sending an en-\ncrypted copy of these parameters to the initiator.2 In ad-\ndition to allowing the responder to remain stateless, the\ncookie acts as a reachability test for the initiator, provid-\ning assurance that a spoofed address is not being used.\n4.1.2 Counterbalancing computational expendi-\nture\nComputational denial of service attacks against the origi-\nnal aggressive mode of IKE with signature authentication\nresulted from the responder generating an expensive sig-\nnature on the receipt of an unauthenticated message 1 and\nthe expensive verification of a signature on message 3. To\naddress these vulnerabilities Matsuura and Imai [16] spec-\nify the use of a signature scheme that permits expensive\ncomponents of the signature generation to be precomputed\n2While not specifically indicated in the protocol specification, we\nsuspect that not only the exponent, but the actual Diffie-Hellman\nvalue gr must be securely sent to the initiator, otherwise the respon-\nder would have to recalculate the value on receipt of message 3.\nand has a signature verification procedure that permits the\nrecovery of random fresh material (a) used in the genera-\ntion of the signature. The initiator then uses the recovery\nof the random fresh material a to provide proof to the re-\nsponder that the signature in message 2 has been verified.\nThe expense of generating the signature sigR is reduced\nbut not eliminated in message 2 of the proposed IKE mod-\nification. The verification of the initiator signature (sigI)\nis only performed after verifying that the initiator has in-\ncurred the computational expenses associated with verify-\ning sigR (as explained next).\nIn order to construct a message 3 that is accepted as\ngenuine by the responder, the initiator must verify sigR.\nVerification of sigR, reveals the value a′, which the initiator\nthen uses to construct the proof of work HASH ∗I .\nOn receipt of message 3, the responder first validates the\ncookie and secondly verifies the proof of work by recovering\na and verifying the modified initiator hash. Finally the re-\nsponder can verify the signature of the initiator, with con-\nfidence that the initiator has already committed resources\nto the protocol execution.\nWhile Matsuura and Imai’s modified version of IKE [16]\nintroduces an elegant proof of work based on verification\nof sigR, it suffers from being an untunable parameter that\nprovides no mechanism for the responder to increase the\ncomputational effort that must be expended by the initia-\ntor in order to provide the proof of work. Additionally, the\ntechnique is restricted to specific signature schemes with\nthe Shortened DSS and the Schnorr signature schemes sug-\ngested by Matsuura and Imai [16] as capable of supporting\nprecomputation and providing recovery of a.\n4.1.3 Gradual authentication\nIn addition to counterbalancing computational and mem-\nory expenditure, this modified IKE allows the responder to\ngradually authenticate the initiator. Before verifying sigI\nthe responder has assurance that the initiator is not using\na spoofed address, is willing to commit memory to having\nthe protocol proceed, and has committed the computa-\ntional resources required to verify the responder signature\nsigR.\nWhile the proposal represents an improvement over the\noriginal protocol, the responder is still required to commit\ncomputational resources to generate a signature on receipt\nof the first unauthenticated message leaving the responder\nsusceptible to a flood of message ones.\n4.2 Just Fast Keying\nThe Just Fast Keying (JFK) protocol was developed by\nAiello et al. [24] as a key agreement protocol providing\nidentity protection and capable of operating in a hostile en-\nvironment such as the Internet. The protocol implements\nseveral techniques to counterbalance computational and\nmemory expenditure, and to gradually authenticate initia-\ntor requests. The protocol variant implementing identity\nprotection for the initiator is depicted in Figure 3.\n8\nI R\nr, xR ∈R [1, 2, . . . , q − 2]\na = gxR\n1) i ∈R [1, 2, . . . , q − 2] saI , g\ni ,NI , IDI−−−−−−−−−−−−−−−−−−−−−−→ Cookie = H (s, saR, IPI ,NI ,NR, a, r)\nHASHR = H (NI ||NR||gi||gr\nCookie||IDR)\ns2 = H (HASHR||a)\ns1 = SR · s2 + xR mod q\nsaR, gr ,NR, IDR, sigR = (s1 , s2 )\n2) HASHR = H (NI ||NR||gi||gr Cookie,EKR{a||r ||g\nr}, sigR←−−−−−−−−−−−−−−−−−−−−−−\nCookie||IDR)\na ′ = gs1P−s2R\ns2\n?= H(HASHR||a′)\nHASH ∗I = H (H (NI ,NR),\ng i , gr ,Cookie, a ′, saR, IDI )\nKs = H (NI ,NR, g ir )\nsigI = SI [HASH ∗I ] IDI ,NI ,HASH\n∗\nI ,\nCookie,EKR{a||r ||gr},\n3) saR, g\nr ,NR, sigI−−−−−−−−−−−−−−−−−−−−−−→ decrypt EKR{a||r ||gr}\nCookie ?= H (s, saR,\nIPI ,NI ,NR, a, r)\nHASH ∗I\n?= H (H (NI ,NR),\ng i , gr ,Cookie, a, saR, IDI )\nverify sigI\nKs = H (NI ,NR, g ir )\nFigure 2: Modified aggressive mode of IKE Protocol [16]\nThe responder periodically selects a Diffie-Hellman ex-\nponential (gr) and generates a signature over this value and\ninformation on the groups it supports. The designers of the\nprotocol allow the responder to reduce computational ex-\npenditure, at the expense of perfect forward secrecy, by\nreusing this Diffie-Hellman value with multiple initiators.\n4.2.1 Counterbalancing memory expenditure\nOn receipt of the first message from an initiator, the re-\nsponder remains stateless and weakly authenticates the\nreachability of the initiator by generating a cryptographic\ncookie (Cookie = HHKR(g\nr, NR, N\n′\nI , IPI)), that is re-\nturned by the initiator in message 3. The secret key HKR\nis a time variant local secret that limits the period of time\na cookie will be accepted.\n4.2.2 Counterbalancing computational expendi-\nture\nWhile the protocol permits reuse of the responder exponen-\ntial to reduce computational expenditure at the responder,\nno mechanism to increase computational expenditure at\nthe initiator is provided. The absence of a proof of work\nfrom this recently proposed protocol is conspicuous and\nexposes the responder to a computational denial of service\nin the presence of an initiator willing to reveal their IP\naddress. The initiator could engage the responder with a\nlegitimate message 1, then fabricate a bogus message 3 at\na minimal computational cost. The responder would have\nto perform a modular exponentiation before being able to\ndetermine that the received message was bogus. The ad-\ndition of a proof of work, in the form of a client puzzle,\nwould allow the responder to increase the computational\n9\nI R\ni ∈R [1, 2, . . . , q − 2] r ∈R [1, 2, . . . , q − 2]\nsigR1 = SR[gr , grpinfoR]\n1) N ′I = H (NI )\nN ′I , g\ni , ID ′R−−−−−−−−−−−−−−−−−−−−−−→ Cookie = H (gr ,NR,N ′I , IPI )\nN ′I ,NR, g\nr , grpinfoR,\n2) verify sigR1\nIDR,Cookie, sigR1←−−−−−−−−−−−−−−−−−−−−−−\nKe = Hg ir (N ′I ,NR,\n′1 ′)\nKa = Hg ir (N ′I ,NR,\n′2 ′)\nKs = Hg ir (N ′I ,NR,\n′0 ′)\nsigI = SI [N ′I ,NR, g\ni , gr ,\nIDR, saI ]\nE1 = {IDI , saI , sigI }KeKa\nNI ,NR, g i , gr ,\n3) Cookie,E1−−−−−−−−−−−−−−−−−−−−−−→ N ′I = H (NI )\nCookie ?= H (gr ,NR,\nN ′I , IPI )\nKe = Hg ir (N ′I ,NR,\n′1 ′)\nKa = Hg ir (N ′I ,NR,\n′2 ′)\nKs = Hg ir (N ′I ,NR,\n′0 ′)\nverify and decrypt E1\nverify sigI\nsigR2 = SR[N ′I ,NR, g\ni , gr ,\nIDI , saI , saR]\nE2 = {sigR2 , saR}KeKa\n4) verify and decrypt E2 E2←−−−−−−−−−−−−−−−−−−−−−−\nverify sigR2\nFigure 3: JFKi Protocol [24]\nexpenditure of an initiator attempting to mount such an\nattack - with verification of the puzzle solution only costing\na single hash operation.\n4.2.3 Gradual authentication\nIn message 3, the initiator releases the preimage (NI) to\nthe nonce (N ′I) provided in message 1. This binds message\n1 and 3 to the same initiator. The initiator must also\nderive the Diffie-Hellman key (gir) and the encryption and\nauthentication keys (Ke,Ka) used to protect the contents\nof message 3.\nOn receipt of message 3, the responder conducts a range\nof checks that gradually authenticate the message, prior to\nconducting an expensive signature verification to strongly\nauthenticate the initiator. First, the responder validates\nthe nonce NI . Then the cookie is verified to weakly assure\nthe responder that the initiator is reachable at the claimed\naddress. Once these checks complete successfully, the re-\nsponder then decrypts and verifies the contents of message\n3. Finally, the initiator’s signature is verified.\nThe encryption and authentication (via a message au-\nthentication code) of message 3 provides the responder\nassurance that the initiator is willing to commit compu-\ntational resources to having the protocol proceed. Unlike\na proof of work however, the decryption and MAC verifi-\ncation require the responder to incur an equivalent compu-\ntational cost to the initiator (that of a modular exponen-\ntiation) so there is no counterbalancing of computational\neffort. This technique does provide gradual authentica-\ntion however, as a failure to correctly decrypt or verify the\nMAC of a received message 3 allows the responder to de-\ntect a possible attack before committing the resources for\nan expensive signature verification.\n10\n4.3 Client-Aided RSA SSL / TLS\nCastelluccia et al. [21] observed that the computational ex-\npenditure of a responder in the SSL / TLS protocol could\nbe reduced through the adoption of client aided compu-\ntation. In the SSL / TLS protocol a responder receives a\npublic key encrypted copy of an initiator selected session\nsecret and has to perform an expensive RSA decryption\noperation in order to generate session keys.\nFortunately, work had already been done on implement-\ning server aided signature generation for resource con-\nstrained smart cards [26, 27] and Castelluccia et al. were\nable to apply the same techniques to have the initiator\naid the responder in decrypting the initiator selected ses-\nsion secret. They termed this approach client-aided RSA\n(CA-RSA). The details of the CA-RSA protocol, which is\ndesigned to be compatible with existing SSL/TLS deploy-\nments, are presented in Figure 4.\nIn the SSL three way handshake protocol adopting CA-\nRSA, the first client hello message remains unchanged.\nThe server hello message includes the server’s certificate\nand the vector D = (d1,d2,. . .,dk). The initiator aids\nresponder computation by performing calculations with\nthese values. The client randomly chooses the secret value\nx, which is used to compute the SSL session key. The\nsecret value x is encrypted with the server’s public key\ncomponent e: y = xe (mod N). Next, the client uses D\nto create a new vector Z by computing zi = ydi (mod\nN), for 1 ≤ i ≤ k. The client then returns the vector Z\nand the encrypted session key seed y to the responder in\nthe client key exchange message. Once the server receives\nthis message it uses the elements of vector Z to recover\nx, by computing the values Mp =\n∏k\ni=1 z\nfi\ni (mod P ) and\nMq =\n∏k\ni=1 z\ngi\ni (mod Q). Finally, the responder recovers\nx by calculating Mpnp +Mqnq (mod N), which is much\nless computationally expensive than performing the mod-\nular exponentiation yd (mod N). The responder can now\nderive the session key Ks.\n4.3.1 Counterbalancing memory expenditure\nAs with the unmodified version of TLS, CA-RSA does not\nmake use of cookies to counterbalance memory expendi-\nture. The puzzle construction adopted by the protocol\nis inadequate to replace the function of a cookie for al-\nlowing the protocol responder to remain stateless or serve\nas a reachability test. Two consequences of this are that\nthe responder must store state on each connection request,\nmaking it vulnerable to a memory-based denial of service\nattack, and secondly, the responder has no way of assessing\nwhether the initiator is using a spoofed IP address.\n4.3.2 Counterbalancing computational expendi-\nture\nRecognising that while CA-RSA eases responder compu-\ntational burden it cannot be used as a proof of work, the\nprotocol is supplemented by the addition of a Juels and\nBrainard [12] style client puzzle to ensure that the respon-\nder only attempts to decrypt values from initiators who\ncan provide a proof of work. Unfortunately the puzzle\nconstruction specified is in violation of the guidelines spec-\nified by Juels and Brainard and is constructed by hashing\na random value. A puzzle construction that does not in-\nclude time, or rely on a time variant responder secret or\nany connection specific parameters, introduces numerous\nproblems. Firstly, the responder will be unable to know\nwhether the puzzle solution it is verifying is to a puzzle that\nit issued3 or if it is a puzzle that has been solved previously\nunless it stores state. As mentioned earlier, storing state\nleaves the responder vulnerable to memory-based denial of\nservice attacks. Secondly, the failure to make puzzles time\nvariant provides no mechanism for a responder to defend\nitself against an initiator that hoards puzzles, generating\nsolutions at its convenience, and then flooding the respon-\nder with legitimate puzzle solutions. Finally, the failure\nto encode connection specific parameters into the puzzle,\nor to make use of cookies, prevents the responder remain-\ning stateless after message 1 and results in the responder\nhaving no assurance that the initiator is reachable at the\nIP address claimed. Storing state without confirmation of\ninitiator reachability exposes the responder to anonymous\nmemory-based flooding denial of service attacks.\n4.3.3 Gradual authentication\nAssuming that the responder maintains the significant\namount of state required to keep track of issued puzzles,\nthe receipt of a valid puzzle solution could provide some\nassurance that the initiator has committed computational\nresources to having the protocol proceed.\nWhile this protocol attempts to counterbalance com-\nputational expenditure, the combination of a poorly con-\nstructed proof of work, a failure to counterbalance memory\nexpenditure leaves the protocol vulnerable to denial of ser-\nvice attacks. To counterbalance memory expenditure, the\nresponder should adopt the use of cookies, or use a puzzle\nconstruction that is consistent with meeting the functional\nrequirements of a cookie.\n5 CONCLUSIONS AND FUTURE WORK\nKey establishment protocols are particularly vulnerable to\ndenial of service attacks owing to the significant resources\nthey must expend in authenticating initiators and generat-\ning the cryptographic keys used for securing ongoing com-\nmunications. The goal of denial of service resistance in\nkey establishment protocols is to ensure that attackers can-\nnot prevent a legitimate initiator and responder deriving\ncryptographic keys without expending resources beyond a\nresponder determined threshold.\n3If an initiator can choose its own puzzles to solve independently\nof the responder, there would be a great risk of the initiator precom-\nputing a large number of puzzle solutions to use in a denial of service\nattack.\n11\nI R\ne, d,N : RSA parameters\nfi, gi ∈R {0, 1}c\nD = (d1 , d2 , . . . , dk ) s.t.\nd ≡\nk∑\ni=1\nfidi mod P − 1 and\nd ≡\nk∑\ni=1\ngidi mod Q− 1\nnp = Q(Q−1 mod P )\nnq = P (P−1 mod Q)\n1) IDI ,NI , saI−−−−−−−−−−−−−−−−−−−−−−→ s ∈R {0, 1}a\nt = H (s)\npuzzle = (t , s(b)) ;\ns(b) = last b bits of s\nIDI , IDR,NI ,NR, saR,\n2) J s.t. H(J ||s(b)) = t puzzle, (e,N ),D←−−−−−−−−−−−−−−−−−−−−−−\nx ∈R {0, 1}48\nKs = H(x,NI , NR)\ny = xe mod N\nzi = ydi mod N\nZ = (z1 , z2 , . . . , zk )\nIDI , IDR,NI ,NR,\n3) saR, J , y ,Z−−−−−−−−−−−−−−−−−−−−−−→ H(J ||s(b)) ?= H(s)\nMp =\n∏k\ni=1 z\nfi\ni mod P\nMq =\n∏k\ni=1 z\ngi\ni mod Q\nx = yd =\nMpnp +Mqnq mod N\nKs = H(x,NI , NR)\n4) ServerF inish←−−−−−−−−−−−−−−−−−−−−−−\nFigure 4: CA-RSA Protocol [21]\nIn this paper we have explored the strategies and tech-\nniques that permit responders to counterbalance memory\nexpenditure, counterbalance computational expenditure,\nand to gradually authenticate initiators, thereby deter-\nmining the level of resources an attacker must commit to\ndisrupting the key establishment protocol and improving\nthe responders resistance to denial of service attacks. The\nadoption of denial of service resistance techniques in three\nkey establishment protocols was critically analysed with\nmisapplication of techniques identified and recommenda-\ntions for more effectively applying the techniques made.\nCookies were identified as a technique that can counter-\nbalance memory expenditure and initiate gradual authen-\ntication. Correctly constructed cookies allow the protocol\nresponder to remain stateless and serve as a reachability\ntest, providing the responder with assurance that an ini-\ntiator is able to send and receive messages from a claimed\naddress. Cookie generation must not lead to any state cre-\nation, as this will expose the responder to a “cookie crumb”\nattack.\nProofs of work are hard but tractable problems that can\nbe used by an initiator to prove to a responder that a\nverifiable level of computational effort has been expended.\nProofs of work can be used to counterbalance computa-\ntional expenditure at the responder and authenticate the\ncommitment of initiator to expending resources to hav-\ning the protocol proceed. While proofs of work can be\nconstructed from a range of underlying problem, proofs\nof work based on hash-based constructions are the most\nprevalent as they are simple to construct and can be veri-\nfied cheaply. The requirements for overloading cookie func-\ntionality into a hash-based client puzzle were presented\nwith the observation that puzzles based on the Aura et\nal. [13] construction cannot implement the required func-\n12\nTable 2: DoS Resistance Techniques in Protocols\nKey Exchange Mechanisms Strategies\nProtocol\nModified IKE Cookie Counterbalancing Memory\nPoW Counterbalancing CPU\nCookie → HASH∗I → Signature Gradual Auth.\nJFK Cookie Counterbalancing Memory\nNonce → Cookie → MAC → Signature Gradual Auth.\nCA-RSA Client-Aided Comp. Counterbalancing CPU\nPoW Counterbalancing CPU\nPoW → Decrypt secret seed Gradual Auth.\nPhoturis [7] Cookie Counterbalancing Memory\nCookie → EK{message} → Signature Gradual Auth.\nHIP [28] Cookie Counterbalancing Memory\nPoW Counterbalancing CPU\nCookie → PoW → Signature Gradual Auth.\nIKEv2 [29] Cookie Counterbalancing Memory\nCookie → MAC → Signature Gradual Auth.\nLee & Fung [30] PoW Counterbalancing Memory\nPoW Counterbalancing CPU\nPoW → Signature Gradual Auth.\ntionality, so must be supplemented by a cookie.\nWe note that the number of key establishment proto-\ncols implementing denial of service resistance techniques\nis limited, with our review of the literature revealing only\nseven protocols (Table 2 presents a summary). Of those\nprotocols implementing denial of service resistance tech-\nniques only two, Host Identity Protocol (HIP) and Mod-\nified IKE, use techniques supporting all three strategies:\ncounterbalancing computational expenditure; counterbal-\nancing memory expenditure; and gradual authentication.\nThe protocols implementing all three strategies do not ap-\npear to be significantly more complex than the protocols\nimplementing only a subset of the strategies.\nThe notion of gradual authentication was introduced as\na strategy for allowing responders to gain assurance that\nan attack is not underway and that an initiator is willing\nto commit computational and memory resources to hav-\ning the protocol proceed. Specific techniques for gradually\nauthenticating initiators were presented and discussed. A\ncommon characteristic of each of the techniques is that\nthey all afford the responder the ability to cheaply ver-\nify some aspect of a received message, while fabrication\nof a message that can pass the responders check is expen-\nsive for an attacker. The adversarial models under which\neach technique is secure, however, are yet to be rigorously\nanalysed. Formalising the adversarial models under which\ntechniques used for gradual authentication may be consid-\nered secure is an area for future work.\nTo exhibit strong denial of service resistance charac-\nteristics, we recommend that protocols must adopt tech-\nniques to implement all available strategies, including:\ncounterbalancing computational expenditure; counterbal-\nancing memory expenditure; and gradually authenticating\nrequests. Addressing only a subset of these, counterbalanc-\ning computational expenditure while storing state prema-\nturely for example, will result in protocols that potentially\nhave residual denial of service vulnerabilities.\nResponders should always gain assurance that an initia-\ntor is reachable at a claimed address, either via cookies or\ncorrectly constructed puzzles. Failure to test reachability\nleaves the responder vulnerable to attacks from spoofed\nsource addresses.\nFinally, there would appear to be a pressing need for de-\nveloping techniques for quantifying the denial of service re-\nsistance a protocol exhibits and how this level of resistance\nchanges with the addition or substitution of techniques.\nMeadows’s cost-based framework [6] for analysing denial\nof service resistance appears to be a promising approach.\nAdoption of the framework and analysis of its suitability\nfor informing protocol design choices with respect to the\nselection of denial of service resistance techniques is also\nan area of future work.\nREFERENCES\n[1] S. Kent and R. Atkinson, “Security Architecture for\nthe Internet Protocol,” Standards Track RFC 2401,\nIETF, Nov 1998, http://www.ietf.org/rfc/rfc2401.txt.\n[2] C. E. R. T. (CERT), “Denial-of-Service Attack via\nping,” 1996, [Online]. Available: http://www.cert.\norg/advisories/CA-1996-26.html [Accessed: August\n2004].\n[3] ——, “SYN Flooding Attack,” 1996, [Online]. Avail-\nable: http://www.cert.org/advisories/CA-1996-21.\nhtml [Accessed: August 2004].\n[4] R. M. Needham, “Denial of Service,” in the 1st ACM\nconference on Computer and communications secu-\nrity, Fairfax, Virginia, USA, Dec 1993, pp. 151–153.\n[5] J. Leiwo, P. Nikander, and T. Aura, “Towards net-\nwork denial of service resistant protocols,” in the 15th\nAnnual Working Conference on Information Security\n(SEC2000), vol. 175, Beijing, China, Aug 2000.\n[6] C. Meadows, “A Cost-Based Framework for Analysis\nof DoS in Networks,” Journal of Computer Security,\nvol. 9(1/2), pp. 143–164, Jan 2001.\n[7] P. Karn and W. A. Simpson, “Photuris: Session-\nKey Management Protocol,” Experimental RFC 2522,\nIETF, Mar 1999, http://www.ietf.org/rfc/rfc2522.\ntxt.\n[8] J. Lemon, “Resisting SYN flood DoS attacks with a\nSYN cache,” in the BSDCon 2002, Berkley, CA, USA,\n11-14 Feb 2002, pp. 89–97.\n13\n[9] T. Aura and P. Nikander, “Stateless Connections,” in\nInternational Conference on Information and Com-\nmunications Security. Beijing, China: Springer-\nVerlag, Nov 1997, pp. 87–97.\n[10] W. A. Simpson, “IKE/ISAKMP Considered Harm-\nful,” USENIX ;login, vol. 24, no. 6, dec 1999.\n[11] C. Dwork and M. Naor, “Pricing via Processing or\nCombatting Junk Mail,” in the 12th Annual Interna-\ntional Cryptology Conference on Advances in Cryp-\ntology. Springer-Verlag, 1992, pp. 139 – 147, lecture\nNotes In Computer Science; Vol. 740.\n[12] A. Juels and J. Brainard, “Client Puzzles: A Cryp-\ntographic Defense Against Connection Depletion At-\ntacks,” in the 1999 Network and Distributed System\nSecurity Symposium (NDSS ’99). San Diego, Cali-\nfornia, USA: Internet Society Press, Reston, Feb 1999,\npp. 151–165.\n[13] T. Aura, P. Nikander, and J. Leiwo, “DoS-resistant\nauthentication with client puzzles,” in Security Pro-\ntocols Workshop 2000. Cambridge, Apr 2000, pp.\n170–181.\n[14] X. Wang and M. K. Reiter, “Defending Against\nDenial-of-Service Attacks with Puzzle Auctions (Ex-\ntended Abstract),” in the 2003 IEEE Symposium on\nSecurity and Privacy (SP’03), Berkeley, CA, USA, 11-\n14 May 2003, pp. 78–92.\n[15] M. Jakobsson and A. Juels, “Proofs of work and bread\npudding protocols,” in the IFIP TC6 and TC11 Joint\nWorking Conference on Communications and Multi-\nmedia Security (CMS 99), Sep 1999, also available as\nhttp://citeseer.nj.nec.com/238810.html.\n[16] K. Matsuura and H. Imai, “Modification of In-\nternet Key Exchange Resistant against Denial-of-\nService,” in Pre-Proceeding of Internet Workshop\n2000 (IWS2000), Feb 2000, pp. 167–174.\n[17] R. L. Rivest, A. Shamir, and D. A. Wagner,\n“Time-lock Puzzles and Timed-release Crypto,” Mas-\nsachusetts Institute of Technology, Cambridge, MA,\nUSA, Technical Report TR-684, 10 Mar 1996.\n[18] B. Waters, A. Juels, J. A. Halderman, and E. W.\nFelten, “New Client Puzzle Outsourcing Techniques\nfor DoS Resistance,” in the 11th ACM Conference on\nComputer and Communications Security (CCS 2004).\nWashington DC, USA: ACM Press, 2004, pp. 246–\n256.\n[19] M. Abadi, M. Burrows, M. Manasse, and T. Wobber,\n“Moderately Hard, Memory-bound Functions,” in the\n10th Annual Network and Distributed System Secu-\nrity Symposium, San Diego, California, USA, 6–7 Feb\n2003.\n[20] C. Dwork, A. Goldberg, and M. Naor, “On Memory-\nBound Functions for Fighting Spam,” in the 23rd An-\nnual International Cryptology Conference (CRYPTO\n2003), Aug 2003, pp. 426–444.\n[21] C. Castelluccia, E. Mykletun, and G. Tsudik, “Im-\nproving Secure Server Performance by Re-balancing\nSSL/TLS Handshakes,” Cryptology ePrint Archive,\nReport 2005/037, 2005, http://eprint.iacr.org/.\n[22] M. O. Rabin, “Digitalized signatures and public-key\nfunctions as intractable as factorization,” Mas-\nsachusetts Institute of Technology, Technical Re-\nport MIT/LCS/TR-212, January 1979. [Online].\nAvailable: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/\ntr.outbox/MIT-LCS-TR-212.ps.gz\n[23] R. L. Rivest, A. Shamir, and L. Adleman, “A method\nfor obtaining digital signatures and public-key cryp-\ntosystems,” Commun. ACM, vol. 21, no. 2, pp. 120–\n126, 1978.\n[24] W. Aiello, S. M. Bellovin, M. Blaze, J. Ioannidis,\nO. Reingold, R. Canetti, and A. D. Keromytis, “Ef-\nficient, DoS-resistant, secure key exchange for Inter-\nnet protocols,” in the 9th ACM conference on Com-\nputer and communications security. Washington,\nDC, USA: ACM Press, 2002, pp. 48–58.\n[25] D. Harkins and D. Carrel, “The Internet Key Ex-\nchange (IKE),” Standards Track RFC 2409, IETF,\nNov 1998, http://www.ietf.org/rfc/rfc2409.txt.\n[26] T. Matsumoto, K. Kato, and H. Imai, “Speeding\nUp Secret Computations with Insecure Auxiliary De-\nvices,” in Proceedings of the 8th Annual International\nCryptology Conference on Advances in Cryptology,\n1988, pp. 497 – 506.\n[27] G. Horng, “A Secure Server-Aided RSA Signature\nComputation Protocol for Smart Cards,” Journal of\nInformation Science and Engineering, vol. 16, pp.\n847–855, 2000.\n[28] R. Moskowitz, “The Host Identity Protocol (HIP),”\nInternet Draft, Internet Engineering Task Force,\nFebruary 2005, http://www.ietf.org/internet-drafts/\ndraft-ietf-hip-base-02.txt.\n[29] C. Kaufman, “Internet Key Exchange (IKEv2) Pro-\ntocol,” Internet Draft, IETF, Sep 2004, http://www.\nietf.org/internet-drafts/draft-ietf-ipsec-ikev2-17.txt.\n[30] M. C. Lee and C. K. Fung, “A Public-Key Based\nAuthentication and Key Establishment Protocol Cou-\npled with a Client Puzzle,” Journal of the American\nSociety for Information Science and Technology, vol.\n54(9), pp. 810–823, Jun 2003.\n14\n",
            "id": 4577514,
            "identifiers": [
                {
                    "identifier": "2092446343",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "146913912",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:eprints.qut.edu.au:15002",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "188904104",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10883856",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1504/ijwmc.2007.013796",
                    "type": "DOI"
                }
            ],
            "title": "Denial-of-Service Resistance in Key Establishment",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2092446343",
            "oaiIds": [
                "oai:eprints.qut.edu.au:15002"
            ],
            "publishedDate": "2007-01-01T00:00:00",
            "publisher": "'Inderscience Publishers'",
            "pubmedId": null,
            "references": [
                {
                    "id": 7149052,
                    "title": "A Cost-Based Framework for Analysis of DoS in",
                    "authors": [],
                    "date": "2001",
                    "doi": null,
                    "raw": "C. Meadows, “A Cost-Based Framework for Analysis of DoS in Networks,” Journal of Computer Security, vol. 9(1/2), pp. 143–164, Jan 2001.",
                    "cites": null
                },
                {
                    "id": 7149049,
                    "title": "Denial-of-Service Attack via ping,”",
                    "authors": [],
                    "date": "1996",
                    "doi": null,
                    "raw": "C. E. R. T. (CERT), “Denial-of-Service Attack via ping,” 1996, [Online]. Available: http://www.cert. org/advisories/CA-1996-26.html [Accessed: August 2004].",
                    "cites": null
                },
                {
                    "id": 7149050,
                    "title": "Flooding Attack,”",
                    "authors": [],
                    "date": "1996",
                    "doi": null,
                    "raw": "——, “SYN Flooding Attack,” 1996, [Online]. Available: http://www.cert.org/advisories/CA-1996-21. html [Accessed: August 2004].",
                    "cites": null
                },
                {
                    "id": 7149053,
                    "title": "Photuris:",
                    "authors": [],
                    "date": "1999",
                    "doi": null,
                    "raw": "P. Karn and W. A. Simpson, “Photuris: SessionKey Management Protocol,” Experimental RFC 2522, IETF, Mar 1999, http://www.ietf.org/rfc/rfc2522. txt.",
                    "cites": null
                },
                {
                    "id": 7149055,
                    "title": "Pricing via Processing or Combatting Junk Mail,”",
                    "authors": [],
                    "date": "1992",
                    "doi": null,
                    "raw": "C. Dwork and M. Naor, “Pricing via Processing or Combatting Junk Mail,” in the 12th Annual International Cryptology Conference on Advances in Cryptology. Springer-Verlag, 1992, pp. 139 – 147, lecture",
                    "cites": null
                },
                {
                    "id": 7149054,
                    "title": "Resisting SYN ﬂood DoS attacks with a SYN cache,” in the BSDCon 2002,",
                    "authors": [],
                    "date": "2002",
                    "doi": null,
                    "raw": "J. Lemon, “Resisting SYN ﬂood DoS attacks with a SYN cache,” in the BSDCon 2002, Berkley, CA, USA, 11-14 Feb 2002, pp. 89–97. 13[9] T. Aura and P. Nikander, “Stateless Connections,” in International Conference on Information and Communications Security. Beijing, China: SpringerVerlag, Nov 1997, pp. 87–97.",
                    "cites": null
                },
                {
                    "id": 7149051,
                    "title": "Towards network denial of service resistant protocols,”",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "J. Leiwo, P. Nikander, and T. Aura, “Towards network denial of service resistant protocols,” in the 15th Annual Working Conference on Information Security (SEC2000), vol. 175, Beijing, China, Aug 2000.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "http://eprints.qut.edu.au/15002/",
                "https://eprints.qut.edu.au/15002/",
                "http://eprints.qut.edu.au/15002/1/15002.pdf"
            ],
            "updatedDate": "2021-10-13T05:47:43",
            "yearPublished": 2007,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "1741-1084"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/10883856.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/10883856"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/10883856/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/10883856/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/4577514"
                }
            ]
        },
        {
            "acceptedDate": "2008-01-12T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Bruza, Peter"
                },
                {
                    "name": "Lau, Raymond"
                },
                {
                    "name": "Li, Yuefeng"
                },
                {
                    "name": "Wu, Shengtang"
                },
                {
                    "name": "Xu, Yue"
                },
                {
                    "name": "Zhou, Xujuan"
                }
            ],
            "contributors": [
                "Lin, T Y",
                "Motwani, R",
                "Ho, H",
                "Kacprzyk, J",
                "Broder, A",
                "Hass, L"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/210053419",
                "https://api.core.ac.uk/v3/outputs/45559809",
                "https://api.core.ac.uk/v3/outputs/211499888",
                "https://api.core.ac.uk/v3/outputs/146913386",
                "https://api.core.ac.uk/v3/outputs/10883330"
            ],
            "createdDate": "2013-07-02T14:29:54",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 310,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/310",
                    "logo": "https://api.core.ac.uk/data-providers/310/logo"
                },
                {
                    "id": 3110,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/3110",
                    "logo": "https://api.core.ac.uk/data-providers/3110/logo"
                },
                {
                    "id": 924,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/924",
                    "logo": "https://api.core.ac.uk/data-providers/924/logo"
                },
                {
                    "id": 333,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/333",
                    "logo": "https://api.core.ac.uk/data-providers/333/logo"
                }
            ],
            "depositedDate": "2007-11-01T00:00:00",
            "abstract": "Web service-oriented Grid is becoming a standard for achieving loosely coupled distributed computing. Grid services could easily be specified with web-service based interfaces. In this paper we first envisage a realistic Grid market with players such as end-users, brokers and service providers participating co-operatively with an aim to meet requirements and earn profit. End-users wish to use functionality of Grid services by paying the minimum possible price or price confined within a specified budget, brokers aim to maximise profit whilst establishing a SLA (Service Level Agreement) and satisfying end-user needs and at the same time resisting the volatility of service execution time and availability. Service providers aim to develop price models based on end-user or broker demands that will maximise their profit. In this paper we focus on developing stochastic approaches to end-user workflow scheduling that provides QoS guarantees by establishing a SLA. We also develop a novel 2-stage stochastic programming technique that aims at establishing a SLA with end-users regarding satisfying their workflow QoS requirements. We develop a scheduling (workload allocation) technique based on linear programming that embeds the negotiated workflow QoS into the program and model Grid services as generalised queues. This technique is shown to outperform existing scheduling techniques that don't rely on real-time performance information",
            "documentType": "research",
            "doi": "10.1109/wi.2007.24",
            "downloadUrl": "https://core.ac.uk/download/10883330.pdf",
            "fieldOfStudy": "computer science",
            "fullText": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nQUT Digital Repository:  \nhttp://eprints.qut.edu.au/ \nZhou, Xujuan and Li, Yuefeng and Bruza, Peter and Wu, Sheng-Tang and Xu, \nYue and Lau, Raymond Y.K. (2007) Using Information Filtering in Web Data \nMining Process. In Proceedings IEEE/WIC/ACM International Conference on \nWeb Intelligence, pages pp. 163-169, Silicon Valley, USA. \n \n          © Copyright 2007 IEEE \nPersonal use of this material is permitted. However, permission to \nreprint/republish this material for advertising or promotional purposes or for \ncreating new collective works for resale or redistribution to servers or lists, or to \nreuse any copyrighted component of this work in other works must be obtained \nfrom the IEEE. \nUsing Information Filtering in Web Data Mining Process\nXujuan Zhou, Yuefeng Li, Peter Bruza, Sheng-Tang Wu, Yue Xu\nFaculty of Information Technology\nQueensland University of Technology\nBrisbane, QLD, Australia, 4000\n(x.zhou, y2.li, p.bruza,yue.xu)@qut.edu.au\nst.wu@student.qut.edu.au\nRaymond Y.K. Lau\nDepartment of Information Systems\nCity University of Hong Kong\nTat Chee Avenue, Kowloon, Hong Kong\nraylau@cityu.edu.hk\nAbstract\nThe amount of Web information is growing rapidly, im-\nproving the efficiency and accuracy of Web information re-\ntrieval is uphill battle. There are two fundamental issues\nregarding the effectiveness of Web information gathering:\ninformation mismatch and overload. To tackle these diffi-\ncult issues, an integrated information filtering and sophis-\nticated data processing model has been presented in this\npaper. In the first phase of the proposed scheme, an infor-\nmation filter that based on user search intents was incorpo-\nrated in Web search process to quickly filter out irrelevant\ndata. In the second data processing phase, a pattern taxon-\nomy model (PTM) was carried out using the reduced data.\nPTM rationalizes the data relevance by applying data min-\ning techniques that involves more rigorous computations.\nSeveral experiments have been conducted and the results\nshow that more effective and efficient access Web informa-\ntion has been achieved using the new scheme.\n1. Introduction\nIn recent years, Web Searching is becoming the most\nconvenient way of finding information. Traditionally, Web\nsearch engines only deal with the users direct queries. How-\never, it is quite often that the information seekers are unable\nto specify their information needs precisely and accurately\ndue to the lack of training or unfamiliar with the collection\nmakeup and retrieval environments. The queries submit-\nted to search engines by Web users are generally very short\ncontaining only several keywords. These short queries are\nnot able to reflect the Web users underlying search intents.\nAs the results, it often leads to information overload and in-\nformation mismatch problems. If large amount of returned\nhits must be go through manually, Web users are likely frus-\ntrated and they may not get any useful information - it is like\nfinding a needle in the haystack. Therefore, it is very desir-\nable to develop efficient personalized information gathering\nsystems to meet what users want.\nIn dealing with Web information overload and mismatch\nissues, classical methodologies/techniques from informa-\ntion retrieval/filtering (IR/IF) and data mining have been\napplied separately with various successes. For example, In-\nformation filtering (IF) has been used specifically in deal-\ning with these issues. Information filtering [2] is an infor-\nmation access activity similar to information retrieval. IF\nhas a similar function to IR, but IF systems are commonly\npersonalized to support long-term, relatively stable, or pe-\nriodic goals or desires. In [13], the authors developed a\nsingle agent filtering model which consisted of three main\nmodules: information representation, information classifi-\ncation and user interest profile learning. This work deals\nwith a single complete information filtering agent operating\nin a single user environment. It performs well on relatively\nsmall datasets. In [14], the authors described two multi-\nagent information filtering systems: distributed knowledge\napproach and distributed functionality approach. These are\nextensions of earlier work on single-agent information fil-\ntering in order to overcome the limitations of single-agent\ncentralized information filtering.\nTraditional information filtering used the term-based\n2007 IEEE/WIC/ACM International Conference on Web Intelligence\n0-7695-3026-5/07 $25.00 © 2007 IEEE\nDOI 10.1109/WI.2007.24\n163\nuser profile. Based on simple term-based user profile, the\nthreshold of filtering is difficult to define and not very sen-\nsitive. High percentage of useful information is filter out,\nthus greatly compromise the system effectiveness. There-\nfore, one of the key issues in developing an effective filter-\ning system is to construct accurate and comprehensive user\nprofiles that can describe the user information needs and in-\nformation searching intentions.\nWeb users have different information search intentions.\nTheir search intentions may be dynamic and uncertain as\nwell. Some users have a clearly defined idea of what infor-\nmation they are seeking for and their search intentions are\nvery clear and more focus on specific topics. Whereas oth-\ners have only a loosely formed idea of the information they\nare searching for and their searching intentions are not well\ndeveloped and they have very broad interests. On one hand,\na user may be interested in more focused information and\nhis/her search goal is to find out accurate information which\nrelates to the keyword queries. On the other hand, a user\nmay wish to find more general information and after that\nthey may find their further information search directions.\nHere, Web user search intents will be generalized as speci-\nficity and exhaustivity intent. Specificity (spe) describes\nthe extent of the pattern (or topic) i.e., users interests have\na narrow and focusing goal or the search boundary is bet-\nter defined, whereas exhaustivity (exh) describes a different\nextent of the searching pattern (or topic) i.e., general/wider\nscope of user interests. Due to the dynamic and complex na-\nture of Web users, automatically acquiring worthwhile user\nprofiles was found to be very challenging.\nMany text mining methods were developed to achieve\nthe goal of information filtering including phrase and pat-\ntern based techniques. It was assumed that the phrase (or\npattern) based approaches should perform better than term-\nbased ones because phrase may carry more semantic in-\nformation than terms. However, many experimental re-\nsults [17] showed that performance of phrased based meth-\nods was lower than expectation, often under-perform term-\nbased methods. The phrases carry less ambiguous and more\nsuccinct semantic meaning than individual terms. Why\nphrase (or patten) based method did not perform better? The\nlikely reasons we argue are: (1) Phrases have inferior sta-\ntistical properties to words; (2) They have low frequency of\noccurrence; and (3) There are a large number of redundant\nand noisy phrases (or patterns) among them.\nIn this paper, we propose a new design of web informa-\ntion system that includes two phases: filtering and pattern\ndiscovery. The objective of filtering is to quickly filter out\nthe likely irrelevant data. It is expected that unmatched data\ncan be greatly reduced after filtering. More sophisticated\ndata processing can then be carried out on the “cleaned”\ndata effectively. As a result, Web mining system could be\nmore efficient to deliver the users with more relevant results.\nThe main contribution of the research work presented in\nthis paper is the development of a novel Web information\ngathering method which integrates information filtering and\npattern discovery strategies together to delivery more accu-\nrate results for the Web users. The remainder of the pa-\nper is organized as follows. Section 2 highlight previous\nresearches in the related area and compare these research\nworks with ours. The proposed ontology-based user profile\nfiltering method is presented in Section 3. Data mining pro-\ncessing is then illustrated in Section 4. The empirical testing\nresults are reported in Section 5. The concluding remarks\nand future researches are given in Section 6.\n2 Related work\nInformation mismatch and information overload are two\nfundamental issues regarding the efficiency of using the\nWeb data. Agent technology is one of the first solutions to\ntackle information overload problems. Software agent can\nobserve, receive feedbacks or ask users directly how to as-\nsist them during the searching and browsing activities [11].\nAgent and multi-agent architectures can autonomously co-\nordinate and cooperate in order to reach users’ goal [16, 5].\nUnfortunately, agent-based approaches can only show us\nthe architectures of information gathering systems. They\ncannot provide more contributions for finding useful knowl-\nedge from data to overcome the fundamental issues. Re-\ncently, a few new methods such as [4] have been developed\nthat combine Information retrieval and Information filtering\nwith agent system to address information overload issues.\nThe application of data mining techniques to Web data,\ncalledWeb mining, is an emerging area inWeb Intelligence.\nCurrently, a Web mining system can be viewed as the use\nof data mining techniques to automatically retrieve, extract,\ngeneralize, and analyst information on the Web [7]. Web\nmining can be classified into four categories [7]: Web us-\nage mining, Web structure mining, Web content mining and\nWeb user profile mining. It is believed that Web mining can\nbring in a great deal of intelligence for Web searching. For\nexample, the primitive object of Web usage mining is the\ndiscovery of Web access patterns. With Web usage min-\ning, the user log can be analyzed. Some patterns about user\nbehavior can be obtained from usage logs and then these\npatterns can be turned into a user profile. The user profiles\nare then utilized to filter incoming articles for the individual.\nThe profiles can be constructed using a variety of learning\ntechniques including the vector space model, genetic algo-\nrithm, and the probabilistic model or clustering. Recently,\na number of ontology-based user profiles models have been\ndeveloped, e.g., [6, 20].\nThis proposed method intends to utilize both the speed\nadvantage of IF and the rigorous natural of data mining pro-\ncesses. The integration of IF and data mining process would\n164\nfurther improve the effectiveness of conventional data min-\ning technique by noise reduction.\n3 Ontology-based user profiles\nIn this project, the user profile is constructed from the\ntopics of a user’s interest i.e., search intent. The topic in\na particular document comprises the terms which represent\nthe subjects. By using the ontological approach, the user\nprofile includes the topic’s semantic relationship. Hence,\nthis type of user profile is called topic ontology. Syntacti-\ncally we assume that the topic ontology is constructed from\nprimitive objects (e.g., terms). They consist of primitive\nclasses and compound classes. The primitive classes are\nthe smallest concepts that cannot be assembled from other\nclasses. However they may be inherited by derived concepts\nor their children. The compound classes are constructed\nfrom a set of primitive classes. Before the detail learning\nprocedure is introduced, the following definitions are given\nfirst.\n3.1 Definitions\nLet T = {t1, t2, . . . , tk} be a set of keywords (or terms),\nand D be a training set of documents, which consists of a\nset of positive documents, D+; and a set of negative doc-\numents, D−, where each document is a set of terms (may\ninclude duplicate terms).\nIn the phase of filtering, we let D− = ∅ because we\nattempt to use a smallest training set for quickly filtering\nout the irrelevant information.\nA set of terms is referred to as a termset. Given a doc-\nument d (or a paragraph) and a term t, tf(d, t) is defined as\nthe number of occurrences of t in d. A set of term frequency\npairs, P = {(t, f)|t ∈ T, f = tf(t, d) > 0}, is referred to\nas a pattern in this paper.\nLet termset(P ) = {t|(t, f) ∈ P} be the termset of P .\nIn this paper, pattern P1 equals to pattern P2 if and only if\ntermset(P1) = termset(P2). A pattern is uniquely de-\ntermined by its termset. Two patterns should be composed\nif they have the same termset (or they are in a same cate-\ngory). In this paper, we use the composition operation, ⊕,\nthat defined in [8] to generate new patterns.\nLet P1 and P2 be two patterns. P1 ⊕ P2 is called the\ncomposition of P1 and P2 which satisfies:\np1 ⊕ p2 = {(t, f1 + f2)|(t, f1) ∈ p1, (t, f2) ∈ p2} ∪\n{(t, f)|t ∈ (termset(p1) ∪ termset(p2))−\n(termset(p1) ∩ termset(p2)),\n(t, f) ∈ p1 ∪ p2}\nRough association rules(see [9, 10])are applied in the\nphase of filtering. A rough association rule has the form\nof\n< termset, wd >→ positive,\nwhere termset is set of selected terms, and wd is a weight\ndistribution of these terms in the rule. A rough association\nrules can be obtained by using the composition operation\non a set of patterns and then normalizing the corresponding\nweights (see [9] or [10]).\nFigure 1. Backbone of the Ontology.\n3.2 Ontology extraction\nIn this section, the method which extracts ontology from\ntraining set to represent user profiles is presented. An ex-\nample (see Fig. 1) is used to help describe our ideas.\nExample 1: Θ = {pet, shop, city, accommodation} is\na set of primitive objects. There are three relevant docu-\nments (d1, d2, d3) in the training set, which are represented\nas a set of keyword-frequency pairs:\nd1 = {(dog, 4), (shop, 6)},\nd2 = {(cat, 5), (shop, 15)},\nd3 = {(pet, 3), (shop, 7), (city, 10)}.\nThere are “is-a” and “part-of”relations between these ob-\njects. For instance: dog and cat are pets; hotel is-an accom-\nmodation. Using the inheritance ( is-a or part-of relation),\ntwo compound objects can be obtained: p1 and p2 from d1,\nd2 and d3, where, d1 → p1, d2 → p1 and d3 → p2. The\n“is-a” and “part-of” relation is used to show the relation\nbetween compound and primitive objects. A document is\nirrelevant if its any part-of section does not include any pat-\ntern. Fig. 1 illustrates this case.\nAn identity for each class X is defined to measure the\nrelationship between classes. An identity of class X is,\nid(X) = {Z|Z is a primitive class, and there is a path from\nX to Z}, e.g., id(p1) = {pet, shop}. The class X = class\nY if and only if id(X) = id(Y ). The following is the pro-\ncedure for an ontology extraction:\nStep 1. Lexical entry extraction\n• Use tf ∗ idf [15] to get a set of keywords (e.g., we use\n150 keywords for each topic) from the training set;\n165\n• Select primitive objects (terms) from the set of key-\nwords using the existing background knowledge,\nwhere each term is a group of keywords, e.g., term\n“pet” may include {pet, dog, cat};\nStep 2. Determine patterns\n• Discover patterns from all relevant documents in the\ntraining sets using the method [9] or [17];\nStep 3. Generate a graph representation as one shown in\nFig. 1.\nLet O = {(p1, N1), (p2, N2), . . . , (pn, Nn)} be a set of\ncompound objects which comes from the discovered pat-\nterns, Ω be the set of its classes: primitive or compound,\nwhere pi is a pattern (1 ≤ i ≤ n) and Ni denotes the num-\nber of appearance of the similar objects. For example, in\nFig. 1 there is O = {(p1, 2), (p2, 1)} because of d1 → p1,\nd2 → p1 and d3 → p2. A support function can be attained\nfrom O, which satisfies:\nsupport : P → [0, 1] such that\nsupport(pi) =\nNi∑\n(pj ,Nj)∈ONj\n. (1)\nwhere P = {p|(p,N) ∈ O}\nThen the following set-valued mapping to describe the\nknowledge implied in O can be obtained:\nΓ : P → Ω such that\nΓ(pi) =\n{\nX if pi is related to class X\nΩroot otherwise,\n(2)\nwhere, Ωroot is the root class in Ω. We call Γ a deploying\nmapping of P on Ω. If the users could not get a correspond-\ning class for a pattern, the pattern is indexed by default in\nthe root class in the ontology (Note: in “is-a” taxonomy we\noften use empty sets). This convention makes sense if we\nassume that the root class represents the entire collection we\ndiscuss.\nLet Θ be the set of primitive objects. We can get an id\nmapping from the deploying mapping:\nξ : P → 2Θ − {∅}, such that ξ(pi) = id(Γ(pi)). (3)\nAt last, we can obtain a probability functions prξ to rep-\nresent the discovered knowledge on the ontology, which sat-\nisfies:\nprξ(θ) =\n∑\nX∈Ω,θ∈X\n∑\np∈{p′ |(p′ ,N)∈O,ξ(p′ )=X} support(p)\n|X|\n(4)\nfor all θ ∈ Θ.\nThe main objective here is to cluster available documents\ninto three regions based on user intention as described as the\nabove. Let p be a pattern and o be a new incoming docu-\nment. Our basic assumption is that o should be relevant if\nid(p) ⊆ id(o). The set of all objects o in the set of new in-\ncoming objects such that id(p) ⊆ id(o) is called the cover-\ning set for p and denoted as [p]. The positive region (POS)\nis the union of all covering sets for all p ∈ P .\nThe set of all objects o in the set of new incoming objects\nsuch that ∃p ∈ P ⇒ id(p)∩ id(o) 6= ∅ is called the bound-\nary region (BND). Also, the set of all objects o in the set of\nnew incoming objects such that ∀p ∈ P ⇒ id(p)∩ id(o) =\n∅ is called the negative region (NEG). Given an object o,\nthe decision rules can be determined naturally as follows:\n∃p ∈ P ⇒ id(p) ⊆ id(o) 6= ∅\no ∈ POS\n∃p ∈ P ⇒ id(p) ∩ id(o) 6= ∅\no ∈ BND , and\n∀p ∈ P ⇒ id(p) ∩ id(o) = ∅\no ∈ NEG .\nThe probability function prξ on Θ (see Equation 4) has\nthe following property:∑\nt∈id(o)\nprξ(t) ≥ min\np∈P\n{\n∑\nt∈ξ(p)\nprξ(t)} for all o ∈ pos. (5)\nFrom the above analysis, we can use\nminp∈P {\n∑\nt∈ξ(p) prξ(t)} as a threshold [7] for nor-\nmal information filtering. A very important conclusion\nwe can draw from the above analysis is that our method\ncan guarantee the processing of filtering can retrieve all\npositive documents (i.e., POS). However, this threshold\nis derived without taking into account the documents in\nthe boundary region (i.e., BND). Therefor, it needs to be\nrevised to consider all the documents in both positive and\nboundary regions. If the original one is called threshold1\nthen the new one can be defined as follows:\nthreshold = threshold1 + α (6)\nwhere α is experimental coefficient.\nTerm frequency is a very useful source in information\nfiltering. In order to use term frequency, the id mapping ξ\nin Equation 3 can be extended to the following mapping β,\nwhich satisfies:\nβ : P → 2Θ×[0,1] − {∅} such that\n∑\n(fst,snd)∈β(pi)\nsnd = 1 and\nξ(pi) = {fst|(fst, snd) ∈ β(pi)} (7)\n166\nβ is called a frequency distribution of ξ; or we call <\nξ(p), β(p) >→ positive be a rough association rule.\nUsing the frequency distribution, we can refine probabil-\nity functions prξ by obtaining another probability functions\nprβ on Θ, which satisfies:\nprβ(θ) =\n∑\n(pi,Ni)∈O,(θ,snd)∈β(pi)\nsupport((pi)×snd} (8)\nfor all θ ∈ Θ.\nUsing the example in Fig. 1 again, we have O =\n{(p1, 2), (p2, 1)}, and ξ(p1) = {pet, shop} and ξ(p2)\n= {pet, shop, city}. Assume support(p1) = 2/3 and\nsupport(p2) = 1/3, then we have the corresponding fre-\nquency distribution β, which satisfies (notice: d1 → p1,\nd2 → p1 and d3 → p2):\nβ(p1) = {(pet, (4 + 5)÷ (4 + 5 + 6 + 15)),\n(shop, (6 + 15)÷ (4 + 5 + 6 + 15))}\n= {(pet, 0.3), (shop, 0.7)}, and\nβ(p2) = {(pet, 3÷ (3 + 7 + 10)),\n(shop, 7÷ (3 + 7 + 10)),\n(city, 10÷ (3 + 7 + 10))}\n= {(pet, 0.15), (shop, 0.35), (city, 0.5)}.\nThe new filtering algorithm first updates\nthe ontology in Fig. 1 by replacing p1 and p2\nwith “p1 : {(pet, 0.3), (shop, 0.7)}” and “p2 :\n{(pet, 0.15), (shop, 0.35), (city, 0.5)}”, respectively.\nIt then determines a threshold (see Equation 5 and 6).\nIt also extracts a set of terms from each new incoming\ndocument. At last it calculates the probability of the\ndocument (see Equation 8) and makes a decision according\nto the threshold.\n4 Data mining processing\nAfter filtering task carried out, the most irrelevant docu-\nments have been removed from test set. The second stage is\nto process the remaining documents with more rigourously\ndata mining processes by using pattern taxonomy model.\nIt is not difficult to discover the phrases from documents\nwhen each paragraph is treated as a transaction. The main\nissue is how to represent the relations between phrases. One\nmethod is to use a document index graph (DIG) [3], where\neach node is a unique word, and each edge is a two adjacent\nnodes which appear successive in a document. The draw-\nback of this method is that a DIG may index some nonsense\nphrases. In this research, sequential patterns [1] is used to\nrepresent phrases. A new concept, which is similar to the\nnotation of closed sequential patterns [12, 19], is employed\nto create a pattern taxonomy model. The following is a brief\ndescription about PTM.\nA sequence s =< x1, . . . , xm > (xi ∈ T is a termset)\nis an ordered list. A sequence α =< a1, . . . , am > is a\nsub-sequence of another sequence β =< b1, . . . , bn >, de-\nnoted by α ⊆ β, if and only if ∃i1, . . . , im such that 1 ≤ i1\n< i2 . . . < im ≤ n and α1 ⊆ βi1 , α2 ⊆ βi2 , . . . , αm ⊆\nβim . A sequential pattern s is a very closed sequential\npattern of s′ if s ⊆ s′ and support(s) − support(s′) <\nλ× support(s′), where λ is a small positive decimal.\nThe above definitions can be used to create a pattern tax-\nonomy as showed in Fig. 2, where a, b, c, and d are terms,\nthe arrows are “is-a” relation, e.g., phrase < (a)(b) > is a\nsub-sequence of < (a)(b)(c) >.\nIf the frequency is used to define the support function\nfor all patterns, then support(< (a)(b) >) ≥ support(<\n(a)(b)(c) >). In general, 3 sub-sequence patterns of <\n(a)(b)(c) > can be obtained. They are < (a)(b) >,\n< (a)(c) > and < (b)(c) >. If patterns have supports\nwhich are very closed to their parents supports then these\npatterns are called non-closed patterns. The not very closed\nsequential patterns will be removed. e.g.,< (a)(c) > in\nFig. 2 has been pruned.\nFigure 2. Pattern taxonomy.\nAfter a pattern taxonomy has been extracted from a train-\ning set, it is utilized to calculate pr(d)which is the relevance\ndegree of each new incoming document d for a given topic.\nThe following is the procedure of making decisions to re-\nturn relevant document to the user:\n1. Find all longest patterns in document d;\ne.g., (< (a)(b)(c) >) is a longest pattern\nif (< (a)(b)(c)(d) >) does not appear in d.\n2. Determine pr(d) according to the taxonomy.\ne.g., pr(d) = support(< (a)(b)(c) >) + support(<\n(a)(b) >) + support(< (b)(c) >).\n5 Experimental evaluation\nThe standard TREC test collections RCV1 (Reuters Cor-\npus Volume 1) was used to test the effectiveness of the pro-\nposed model. RCV1 corpus consists of all and only English\nlanguage stories produced by Reuter’s journalists between\nAugust 20, 1996, and August 19, 1997 with total 806,791\n167\ndocuments. TREC has developed and provided 100 topics\nfor the filtering track aiming at building a robust filtering\nsystem. The first fifty of these were constructed by human\nresearchers and the rest by intersecting two Reuters’ topic\ncategories. Each topic is divided into two sets: training and\ntest set. Our experiments used the Split of TREC-10/2000.\nWith this split, the first 6 weeks’ items, 20 August through\n30 September 1996, were taken as the training set with total\n23,307 documents. The remainder of the collection formed\nthe test set which includes 783,484 documents.\nThe document relevant judgments have been given for\neach topic. This means that every document is assigned\nto be either positive or negative.“Positive” means the doc-\nument is relevant to the assigned topic; otherwise “nega-\ntive” will be given to the document. The set of 100 TREC\ntopics is used to represent the diverse Web user’s informa-\ntion needs. The experiments simulated user feedback by\nassuming that the user would recognize as relevant the cho-\nsen some documents that were officially judged as relevant\nfrom a set of given documents.\nRCV1 collection is marked in XML. To avoid bias in ex-\nperiments, all of the meta-data information in the collection\nhave been ignored. The documents have been preprocessed\nby using the same text processing method before they are\nused in all experiments. The tasks of removing stop-words\naccording to a given stop-words list and stemming term by\napplying the Porter Stemming algorithm are conducted.\nThe F-beta (Fβ) measure and the P/R break-even point\nare employed for our experimental performance measures.\nFβ is a version of the Van Rijsbergen measure of retrieval\nperformance. This measure is a function of Recall (R) and\nPrecision (P), together with a free parameter beta which de-\ntermines the relative weighting of recall and precision. It is\ncalculated by the following function:\nFβ =\n(β2 + 1)PR\nβ2P +R\n;\nThe parameter β = 1 is used in our study, which means\nthat recall and precision is weighed equally.\nThe P/R break-even point indicates the value at which\nprecision equals recall. The larger a P/R break-even point\nor Fβ-measure score is, the better the system performs.\nOur new method was called Filtering-based Web Infor-\nmation Gathering (FWIG). It was compared with the base-\nline method, namely Pattern Taxonomy Model (PTM) de-\nveloped by [18]. PTM is a new approach of informa-\ntion gathering. Many up-to-date Web mining techniques\n(e.g., sequential association rules, closed-pattern based non-\nredundant association rules and rough association rules)\nhave been intergraded into this method. PTM has been\ntested with RCV1 data collection. The results indicate that:\n(i) Data mining based methods took longer time for train-\ning; (ii) Closed patterns were better than frequent patterns;\nFigure 3. The F-measure for PTM and FWIG.\nFigure 4. The break-even point for PTM and\nFWIG.\nFigure 5. Results of filtering process\n(iii) The effectiveness of the data mining based methods is\nsimilar to IF based methods; It was observed that perfor-\nmance of PTM can be hindered by: (a) too many noise in\nthe inputs data (documents); (b) The measures (e.g., support\n168\nand confidence) used for knowledge discovery (data mining\nphase) can not be simply used in the phase of leveraging of\ndiscovered knowledge to answer what users want.\nThe results of the experiments are illustrated in figures\nFig. 3, Fig. 4. They include the average scores of Fβ-\nmeasure, Break-even point. The scores for the all TREC\ntopics are displayed in three columns, 1 to 50 topics, 51\nto 100 topics and all 100 topics. As shown in the figures,\nthe results of the first 50 topics and the second 50 topics\nare both significant. In the first 50 topics, the performances\nof FWIG are better than PTM. But in the second 50 top-\nics, the improvement of the new method is more significant.\nThe possible explanation is that the first 50 topics is con-\nstructed manually whereas the second 50 topics are deter-\nminate by machine learning. The average scores of all 100\ntopics also demonstrate that FWIG has a considerable im-\nprovement. The improvement of the new method is mainly\ndue to the success of irrelevant information removal by the\nfiltering process. Fig. 5 illustrates this scenario. Hence, the\nsecond stage (Pattern discovery) can work more effectively\nwith less“noise”.\n6 Conclusions\nThis paper illustrates a new model which integrates the\nsearch intent based filtering and pattern based data mining\ntechnology together to alleviate Web information overload\nand mismatch problems. The proposed method has been\nevaluated using standard TREC data collection with very\npositive results.\nCompared with the orthodox data mining method\nPTM [17, 18], the experiments based on the new method\ndemonstrated that the performance of information retrieval\ncan be significantly improved. The improvement was con-\nsistent in all 100 topics experiments. This confirms that the\nproposed Web information gathering method that combines\nfiltering and data mining processes is able to filter out most\nirrelevant objects and reduce the chance of generating noisy\npatterns hence improve the computational accuracy and ef-\nficiency. In the next step, this project will focus on improv-\ning the search intent-based user profiles models and incor-\nporate them into other state-of-the-art IF and data mining\nprocesses for Web information gathering.\nAcknowledgement\nThis paper was partially supported by Grant DP0556455\nfrom Australian Research Council.\nReferences\n[1] R. Agrawal and R. Srikant. Mining sequential patterns. In\nICDE95, pages 3–14, 1995.\n[2] N. Belkin and B. Croft. Information filtering and informa-\ntion retrieval: Two sides of the same coin? Communications\nof the ACM, 35(2), December 1992.\n[3] K. Hammouda and M. Kamel. Sphrase-based document\nsimilarity based on an index graph model. In ICDM02,\npages 203–210, 2002.\n[4] R. Y. K. Lau, P. Bruza, and D. Song. Belief revision for\nadaptive information retrieval. In SIGIR, pages 130–137,\n2004.\n[5] Y. Li, C. Zhang, and S. Zhang. Cooperative strategy for web\ndata mining and cleaning. Applied Artificial Intelligence,\n17(5-6):443–460, 2003.\n[6] Y. Li and N. Zhong. Ontology-based web mining model.\nIn IEEE/WIC International Conference on Web Intelligence,\n2003.\n[7] Y. Li and N. Zhong. Web mining model and its applica-\ntions for information gathering. Knowledge-Based Systems,\n17:207–217, 2004.\n[8] Y. Li and N. Zhong. Mining ontolgoy for automatically ac-\nquiring web user information needs. IEEE Transactions on\nKnowledge and Data Engineering, 18:554–568, 2006.\n[9] Y. Li and N. Zhong. Mining rough association from text\ndocuments. In RSCTC, pages 368–377, 2006.\n[10] Y. Li and N. Zhong. Rough association rule mining in text\ndocuments for acquiring web user information needs. In\nIEEE/WIC/ACM International Conference on Web Intelli-\ngence, WI06, pages 226 – 232, 2006.\n[11] P. Maes. Agents that reduce work and information overload.\nCommun. ACM, 37(7):30–40, 1994.\n[12] B. Mobasher, H. Dai, T. Luo, Y. Sun, and J. Zhu. Combin-\ningweb usage and content mining for more effective person-\nalization. In International Conference on Ecommerce and\nWeb Technologies, 2000.\n[13] J. Mostafa, S. Mukhopadhyay, W. Lam, and M. J. Palakal.\nA multilevel approach to intelligent information filtering:\nModel, system, and evaluation. ACM Trans. Inf. Syst.,\n15(4):368–399, 1997.\n[14] S. Mukhopadhyay, S. Peng, R. R. Raje, J. Mostafa, and M. J.\nPalakal. Distributed multi-agent information filtering - a\ncomparative study. JASIST, 56(8):834–842, 2005.\n[15] G. Salton and M. McGill. Introduction to Modern Informa-\ntion Retrieval. McGraw-Hill, New York, 1983.\n[16] M. Wooldridge and N. R. Jennings. Agent theories, archi-\ntectures, and languages: A survey. In ECAI Workshop on\nAgent Theories, Architectures, and Languages, pages 1–39,\n1994.\n[17] S.-T. Wu, Y. Li, and Y. Xu. Deploying approaches for pat-\ntern refinement in text mining. In ICDM, pages 1157–1161,\n2006.\n[18] S.-T. Wu, Y. Li, Y. Xu, B. Pham, and Y.-P. P. Chen. Auto-\nmatic pattern-taxonomy extraction for web mining. In Web\nIntelligence, pages 242–248, 2004.\n[19] X. Yan, J. Han, and R. Afshar. Clospan: Mining closed\nsequential patterns in large datasets. In SDM2003, pages\n166–177, 2003.\n[20] X. Zhou, Y. Li, Y. Xu, and R. Lau. Relevance assessment of\ntopic ontology. In The Fourth International Conference on\nActive Media Technology, Relevance Assessment of Topic\nOntology, 2006.\n169\n",
            "id": 4577107,
            "identifiers": [
                {
                    "identifier": "210053419",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1109/wi.2007.24",
                    "type": "DOI"
                },
                {
                    "identifier": "10883330",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:eprints.usq.edu.au:29690",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "45559809",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "211499888",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "146913386",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2096991122",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "oai:eprints.qut.edu.au:14310",
                    "type": "OAI_ID"
                }
            ],
            "title": "Using Information Filtering in Web Data Mining Process",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2096991122",
            "oaiIds": [
                "oai:eprints.usq.edu.au:29690",
                "oai:eprints.qut.edu.au:14310"
            ],
            "publishedDate": "2007-01-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 7159717,
                    "title": "A multilevel approach to intelligent information ﬁltering: Model, system, and evaluation.",
                    "authors": [],
                    "date": "1997",
                    "doi": null,
                    "raw": "J. Mostafa, S. Mukhopadhyay, W. Lam, and M. J. Palakal. A multilevel approach to intelligent information ﬁltering: Model, system, and evaluation. ACM Trans. Inf. Syst., 15(4):368–399, 1997.",
                    "cites": null
                },
                {
                    "id": 7159720,
                    "title": "Agent theories, architectures, and languages: A survey.",
                    "authors": [],
                    "date": "1994",
                    "doi": null,
                    "raw": "M. Wooldridge and N. R. Jennings. Agent theories, architectures, and languages: A survey. In ECAI Workshop on Agent Theories, Architectures, and Languages, pages 1–39, 1994.",
                    "cites": null
                },
                {
                    "id": 7159715,
                    "title": "Agents that reduce work and information overload.",
                    "authors": [],
                    "date": "1994",
                    "doi": null,
                    "raw": "P. Maes. Agents that reduce work and information overload. Commun. ACM, 37(7):30–40, 1994.",
                    "cites": null
                },
                {
                    "id": 7159722,
                    "title": "Automatic pattern-taxonomy extraction for web mining. In Web Intelligence,",
                    "authors": [],
                    "date": "2004",
                    "doi": null,
                    "raw": "S.-T. Wu, Y. Li, Y. Xu, B. Pham, and Y.-P. P. Chen. Automatic pattern-taxonomy extraction for web mining. In Web Intelligence, pages 242–248, 2004.",
                    "cites": null
                },
                {
                    "id": 7159708,
                    "title": "Belief revision for adaptive information retrieval.",
                    "authors": [],
                    "date": "2004",
                    "doi": null,
                    "raw": "R. Y. K. Lau, P. Bruza, and D. Song. Belief revision for adaptive information retrieval. In SIGIR, pages 130–137, 2004.",
                    "cites": null
                },
                {
                    "id": 7159723,
                    "title": "Clospan: Mining closed sequential patterns in large datasets.",
                    "authors": [],
                    "date": "2003",
                    "doi": null,
                    "raw": "X. Yan, J. Han, and R. Afshar. Clospan: Mining closed sequential patterns in large datasets. In SDM2003, pages 166–177, 2003.",
                    "cites": null
                },
                {
                    "id": 7159716,
                    "title": "Combiningweb usage and content mining for more effective personalization.",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "B. Mobasher, H. Dai, T. Luo, Y. Sun, and J. Zhu. Combiningweb usage and content mining for more effective personalization. In International Conference on Ecommerce and Web Technologies, 2000.",
                    "cites": null
                },
                {
                    "id": 7159709,
                    "title": "Cooperative strategy for web data mining and cleaning.",
                    "authors": [],
                    "date": "2003",
                    "doi": null,
                    "raw": "Y. Li, C. Zhang, and S. Zhang. Cooperative strategy for web data mining and cleaning. Applied Artiﬁcial Intelligence, 17(5-6):443–460, 2003.",
                    "cites": null
                },
                {
                    "id": 7159721,
                    "title": "Deploying approaches for pattern reﬁnement in text mining.",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "S.-T. Wu, Y. Li, and Y. Xu. Deploying approaches for pattern reﬁnement in text mining. In ICDM, pages 1157–1161, 2006.",
                    "cites": null
                },
                {
                    "id": 7159718,
                    "title": "Distributed multi-agent information ﬁltering - a comparative study.",
                    "authors": [],
                    "date": "2005",
                    "doi": null,
                    "raw": "S. Mukhopadhyay, S. Peng, R. R. Raje, J. Mostafa, and M. J. Palakal. Distributed multi-agent information ﬁltering - a comparative study. JASIST, 56(8):834–842, 2005.",
                    "cites": null
                },
                {
                    "id": 7159706,
                    "title": "Information ﬁltering and information retrieval: Two sides of the same coin?",
                    "authors": [],
                    "date": "1992",
                    "doi": null,
                    "raw": "N. Belkin and B. Croft. Information ﬁltering and information retrieval: Two sides of the same coin? Communications of the ACM, 35(2), December 1992.",
                    "cites": null
                },
                {
                    "id": 7159719,
                    "title": "Introduction to Modern Information Retrieval.",
                    "authors": [],
                    "date": "1983",
                    "doi": null,
                    "raw": "G. Salton and M. McGill. Introduction to Modern Information Retrieval. McGraw-Hill, New York, 1983.",
                    "cites": null
                },
                {
                    "id": 7159712,
                    "title": "Mining ontolgoy for automatically acquiring web user information needs.",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "Y. Li and N. Zhong. Mining ontolgoy for automatically acquiring web user information needs. IEEE Transactions on Knowledge and Data Engineering, 18:554–568, 2006.",
                    "cites": null
                },
                {
                    "id": 7159713,
                    "title": "Mining rough association from text documents.",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "Y. Li and N. Zhong. Mining rough association from text documents. In RSCTC, pages 368–377, 2006.",
                    "cites": null
                },
                {
                    "id": 7159705,
                    "title": "Mining sequential patterns.",
                    "authors": [],
                    "date": "1995",
                    "doi": null,
                    "raw": "R. Agrawal and R. Srikant. Mining sequential patterns. In ICDE95, pages 3–14, 1995.",
                    "cites": null
                },
                {
                    "id": 7159710,
                    "title": "Ontology-based web mining model.",
                    "authors": [],
                    "date": "2003",
                    "doi": null,
                    "raw": "Y. Li and N. Zhong. Ontology-based web mining model. In IEEE/WIC International Conference on Web Intelligence, 2003.",
                    "cites": null
                },
                {
                    "id": 7159724,
                    "title": "Relevance assessment of topic ontology.",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "X. Zhou, Y. Li, Y. Xu, and R. Lau. Relevance assessment of topic ontology. In The Fourth International Conference on Active Media Technology, Relevance Assessment of Topic Ontology, 2006. 169 169 169 169 169",
                    "cites": null
                },
                {
                    "id": 7159714,
                    "title": "Rough association rule mining in text documents for acquiring web user information needs.",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "Y. Li and N. Zhong. Rough association rule mining in text documents for acquiring web user information needs. In IEEE/WIC/ACM International Conference on Web Intelligence, WI06, pages 226 – 232, 2006.",
                    "cites": null
                },
                {
                    "id": 7159707,
                    "title": "Sphrase-based document similarity based on an index graph model.",
                    "authors": [],
                    "date": "2002",
                    "doi": null,
                    "raw": "K. Hammouda and M. Kamel. Sphrase-based document similarity based on an index graph model. In ICDM02, pages 203–210, 2002.",
                    "cites": null
                },
                {
                    "id": 7159711,
                    "title": "Web mining model and its applications for information gathering.",
                    "authors": [],
                    "date": "2004",
                    "doi": null,
                    "raw": "Y. Li and N. Zhong. Web mining model and its applications for information gathering. Knowledge-Based Systems, 17:207–217, 2004.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "https://eprints.qut.edu.au/14310/",
                "http://eprints.qut.edu.au/14310/"
            ],
            "updatedDate": "2022-02-27T09:32:38",
            "yearPublished": 2007,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/10883330.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/10883330"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/10883330/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/10883330/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/4577107"
                }
            ]
        },
        {
            "acceptedDate": "2005-09-27T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Gardner, John"
                },
                {
                    "name": "Johnson, Daniel"
                }
            ],
            "contributors": [
                "Kitamura, Y",
                "Kishino, F",
                "Nagato, N",
                "Kato, H"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/146907502",
                "https://api.core.ac.uk/v3/outputs/10877445",
                "https://api.core.ac.uk/v3/outputs/86632887"
            ],
            "createdDate": "2013-07-02T14:29:40",
            "dataProviders": [
                {
                    "id": 310,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/310",
                    "logo": "https://api.core.ac.uk/data-providers/310/logo"
                },
                {
                    "id": 3110,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/3110",
                    "logo": "https://api.core.ac.uk/data-providers/3110/logo"
                },
                {
                    "id": 559,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/559",
                    "logo": "https://api.core.ac.uk/data-providers/559/logo"
                }
            ],
            "depositedDate": "2005-01-01T00:00:00",
            "abstract": "The current paper applies media equation research to video game de-sign. The paper presents a review of the existing media equation research, de-scribes a specific study conducted by the authors, discusses how the findings of the study can be used to inform future game design, and explores how other media equation findings might be incorporated into game design. The specific study, discussed in detail in the paper, explores the notion of team formation between humans and computer team-mates. The results show that while highly experienced users will accept a computer as a team-mate, they tend to react more negatively towards the computer than to human teammates (a ‘Black Sheep’ Effect",
            "documentType": "research",
            "doi": "10.1007/11558651_45",
            "downloadUrl": "https://core.ac.uk/download/10877445.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "  \n \nCOVER SHEET \n \n \nThis is the author version of article published as: \n \nJohnson, Daniel M. and Gardner, John (2005) Effects of team-based \ncomputer interaction: The media equation and game design \nconsiderations. In Kishino, F and Kitamura, Y and Kato, H and \nNagato, N, Eds. Proceedings Entertainment Computing - ICEC 2005, \nLecture Notes in Computer Science 3711, pages pp. 468-479, \nMakuhari, Japan. \n \nCopyright 2005 Springer Verlag \n \nAccessed from   http://eprints.qut.edu.au \n \n \n \n \n \n \n \n \n \n \nJohnson, D., & Gardner, J. (2005). 'Effects of team-based computer interaction: \nThe media equation and game design considerations', in F Kishino, Y Kita-\nmura, H Kato & N Nagata (eds), Entertainment Computing - ICEC 2005, Lec-\nture Notes in Computer Science, Springer, Laxenburg, Austria, vol. 3711, pp. \n468-79. \nEffects of Team-Based Computer Interaction: The \nMedia Equation and Game Design Considerations  \nDaniel Johnson1 and John Gardner2 \n1 Communication Design, Queensland University of Technology, Kelvin Grove, Queen-\nsland, 4059, Australia \ndm.johnson@qut.edu.au \n2 School of Business, University of Queensland, Australia \nAbstract. The current paper applies media equation research to video game de-\nsign. The paper presents a review of the existing media equation research, de-\nscribes a specific study conducted by the authors, discusses how the findings of \nthe study can be used to inform future game design, and explores how other \nmedia equation findings might be incorporated into game design. The specific \nstudy, discussed in detail in the paper, explores the notion of team formation \nbetween humans and computer team-mates. The results show that while highly \nexperienced users will accept a computer as a team-mate, they tend to react \nmore negatively towards the computer than to human teammates (a ‘Black \nSheep’ Effect). Keywords: Media Equation, Team Formation, Groups, Game \nDesign \nIntroduction \nThe media equation is based on the idea that people respond socially to computers. In \nits simplest form the media equation can be stated as ‘media equals real life’: more \nbroadly it is the concept that people’s interactions with televisions, computers and \nnew media are fundamentally social and natural [1]. In media equation studies, the \nsocial dynamics surrounding human-human interactions are shown to exist in human-\ncomputer interactions. The studies conducted supporting the media equation all fol-\nlow a similar research process. The process is as follows: (a) pick a social science \nfinding (usually within social psychology or sociology) which concerns behaviour or \nattitudes towards humans, (b) substitute ‘computer’ for ‘human’ in the statement of \nthe theory e.g., ‘people like people that flatter them’ becomes ‘people like computers \nthat flatter them’ [2], (c) replicate the methodology of the social science study but re-\nplace one or more humans with computers, (d) determine if the social rule still applies \n[3]. \nA myriad of different media equation effects are described in the literature. The \nvast majority of this research can be considered to fall into four categories, reflecting \nthe kinds of psychological or sociological effects that are being explored. Human re-\nsearch in the areas of traits, social rules and norms, identity, and communication has \nbeen shown to be applicable to human-computer interactions. Media equation re-\nsearch focusing on human traits includes studies on gain-loss theory [4], social facili-\ntation [5], social presence [6] and principles of attraction [6-8]. For example, research \nhas shown that people tend to prefer computers that are similar to themselves [8, 9] \nwhich parallels the tendency to prefer other people who are similar to oneself (the \nsimilarity attraction hypothesis) [10-12]. The media equation research concentrating \non social rules and norms has explored reciprocity [13-16], flattery [2, 17], politeness \n[18], assignment of roles [19] and praise and criticism [20]. For example, there is evi-\ndence that people perceive a computer who criticises others to be smarter than a com-\nputer that praises others, which is the same process that tends to occur between people \n[20]. Media equation research focusing on identity incorporates studies on group for-\nmation and affiliation [21, 22], self-serving bias [23], and stereotyping [24]. For ex-\nample, research has shown that people (both male and female) will apply gender-\nbased stereotypes to a computer as a function of whether the computer communicates \nusing a male or female voice [24]. The media equation research directed towards is-\nsues of communication has included studies exploring party host behaviour [25], bal-\nance theory [26] and emotion theory and active listening [27]. The latter researchers, \nfor example, found that for people experiencing negative affect (e.g., frustration), in-\nteracting with a computer that provided sincere non-judgmental feedback led to a \nmoderation of the negative feelings experienced (as often happens when people talk \nto other people who offer sincere non-judgmental feedback). \nGeneral Applications of the Media Equation \nRecently, the media equation has been applied to the design and implementation of \nsoftware programs, interfaces and electronic devices. Both the general theory, that \npeople tend to treat computers as though they are real people and places, and specific \nmedia equation findings have proven useful to designers. Cooper [28] applied the me-\ndia equation theory (particularly findings regarding politeness; see [18]) when creat-\ning a series of principles for programmers to use when designing software. Diederiks \n[29] analyzed ‘L-icons’ (virtual personal friends that make viewing recommendations \nfor a television system) and ‘Bello’ (a virtual pet dog that facilitates voice control for \na television set) and found evidence that animated characters deploying social behav-\niour and social rules make it easier to interact with consumer electronic products. \nFriedman, Kahn and Hagman [30] found that as a result of the social cues they pro-\nvide, AIBO robots (small robotic dogs produced by SONY) provide their owners with \nsocial companionship and emotional satisfaction. Based on the media equation litera-\nture, Johnson [31] designed and implemented guidebots (or virtual tutors) based on \nthe behaviour of actual human teachers. The results of this work are being used to \ncreate a social intelligence model to be incorporated into a guidebot-enhanced inter-\nface, which will be able to assess when pedagogical interventions are appropriate. \nThese examples represent a sample taken from the population of published applica-\ntions of the media equation, and as such do not represent the full scope of applications \ndrawing on media equation research that are currently in existence. \nExperience as a Moderator of the Media Equation \nRecent research conducted by Johnson, Gardner and Wiles [17] found evidence sug-\ngesting a link between degree of experience with computers and propensity to show a \nmedia equation response to computers. An informal survey of computer users of vary-\ning levels of experience revealed that most people expect that users with high levels \nof experience with computers are less likely to exhibit the tendency to treat computers \nas though they were real people. This belief is based on the argument that more expe-\nrienced users, having spent more time using computers, are more likely to view the \ncomputer as a tool. They are more likely to be aware of the computer’s true status -- \nthat of a machine. However, the research conducted does not support this argument, \nJohnson et al. found that more experienced participants were more likely to exhibit a \nmedia equation response. \nSpecifically, participants of high experience, but not low experience, displayed a \nmedia equation pattern of results, reacting to flattery from a computer in a manner \ncongruent with peoples’ reactions to flattery from other humans. High experience par-\nticipants tended to believe that the computer spoke the truth, experienced more posi-\ntive affect as a result of flattery, and judged the computer’s performance more favora-\nbly (for a detailed discussion of these findings, their relation to the media equation \nand it’s theorized relationship to mindlessness the reader is directed to the original \npaper [17]) \nGroup Identity and the Media Equation \nOne of the strongest findings in intergroup behaviour research is the minimal group \neffect [32]. The minimal group paradigm is an experimental methodology developed \nby Tajfel and colleagues in order to explore the minimal conditions required for inter-\ngroup behaviour (including team formation) [33]. In Tajfel and colleague’s original \nstudy, participants were invited to take part in a study on decision making and as-\nsigned to one of two groups on the basis of their reported preference for paintings by \neither the artist Klandinski or Klee. Participants were then given the opportunity to al-\nlocate money to pairs of fellow participants identified only by group membership. The \nresults indicated that participants tended to strongly favor their own group [33]. The \nstriking feature of this and subsequent minimal group paradigm (MGP) studies is that \nteam affiliation effects resulted even though the categorisation was on the basis of a \nlargely arbitrary criterion, the groups created had no history or future, and self-interest \nwas not a motivating factor for participants [32].  \nMore recently, MGP studies have shown categorisation and associated affiliation \neffects on the basis of coin toss allocations to groups K or W [34], painting prefer-\nences [35, 36], line length estimation (overestimators and underestimators) [37], ran-\ndom categorisation to groups X and Y [38], figural versus grounded perceptual style \n[39], shape dependency and independency [40], and concave and convex attention \nstyles [41].  \nThe Current Study \nThe current study was designed to test whether minimal categorisation could be used \nto create a sense of team affiliation between human participants and computers. It was \nhypothesized that participants would show group affiliation effects as a result of being \nplaced on a team with a computer (H1). Moreover, it was hypothesized (based on \nprevious research) that this effect would be stronger for participants with a greater de-\ngree of experience with computers (H2).  \nMethod \nProcedure \nSixty University of Queensland students participated in the study (40 females and 20 \nmales). Participants ranged in age 17 to 35, with an average age of 19.8 years. \nParticipants were initially told that the study dealt with decision making with comput-\ners. Participants were informed that the computers they were working on had been \ntrained to use neural networks to complete two different tasks: a text rating task \n(TRT), and a desert survival task (DST).  \nFor the TRT, participants read a body of text and rated the extent to which six \nwords (descriptive, emotive, intriguing, factual, stimulating, entertaining) accurately \ndescribed the text. Then the computer presented its own ratings. Ostensibly, the com-\nputer ratings were based on the neural network it employed; in fact, the computer rat-\nings were systematically different to the participants’ ratings. After viewing the com-\nputer’s ratings, participants were allowed to alter their original ratings.  \nFor the DST, participants were asked to imagine they were stranded in the desert \nand to rank 12 items according to their importance for survival. When participants \nhad completed their ranking, the computer gave a suggested ranking and listed a ra-\ntionale for its suggestions. Ostensibly, the computer used a neural network to rank the \nitems; in fact, the computer’s ranking was systematically different from the partici-\npant’s. After viewing the computers’ suggested ranking and rationale, participants \nwere allowed to alter their own rankings.  \nThe experiment had three conditions: control (N = 25), human team (N = 19), and \nhuman-computer team (N = 17). In the control condition, participants were told they \nwould be working on their own during the experiment, and the experimental materials \nand procedures were designed to promote the notion of individual work. In the human \nteam condition, participants told they would be working as part of one of two teams, \nand materials and procedures were set up to promote the distinction between the peo-\nple in the teams. In the human-computer team condition, participants were also told \nthey would be working as part of one of two teams, but materials and procedures were \nset up to include the computers as part of each team. \nMeasures \nAfter all interaction with the computer was complete, participants completed the writ-\nten questionnaire. Participants’ degree of experience with computers was assessed, \nand they were classified as having low or high experience. To assess mood, the ques-\ntionnaire included the Positive and Negative Affect Scale; 20 mood descriptors de-\nsigned to assess mood states [42]. Participants’ attitudes towards the TRT and the \nDST were assessed; participants also rated the quality of the information provided by \nthe computer. To assess their openness to influence from the computer, participants \nrated seven items drawn from prior research [21, 22]. For each task, participants were \nasked to assess the extent to which their ratings/rankings had changed: a subjective \nmeasure of the degree to which respondents were influenced by the computer. Objec-\ntive measures of the extent to which participants were influenced by the computer \nwere also recorded by summing numerical changes in participants’ ratings and rank-\nings. Demographic data were also collected. \nExploratory factor analyses were conducted to identify variables to be combined \ninto scales. Analysis of the PANAS items showed a two-factor solution, with the 10 \npositive mood descriptors loading on one factor (Positive mood; Cronbach’s \u0000 = .78) \nand the 10 negative mood descriptors loading on another factor (Negative mood; \u0000 = \n.74). The three items assessing ratings of the TRT loaded on a single factor (TRT rat-\ning; \u0000 = .91). Similarly, the three items assessing ratings of the DST loaded on a sin-\ngle factor (DST rating; \u0000 = .87). The three items rating the quality of the computer in-\nformation loaded on a single factor (Information quality; \u0000 = .85), as did the seven \nitems assessing respondent’s openness to influence (Openness to influence; \u0000 = .91). \nResults \nA series of one-way ANOVAs were conducted on the dependent measures, with the \nexperimental manipulation (control, human team and human-computer team) as the \nindependent variable. Results from these initial analyses were non-significant. Prior \nfindings suggest that media equation effects are more apparent amongst people with \nmore extensive experience with computers [17], so the sample was split into respon-\ndents with low and high experience with computers. Subsequent one-way ANOVAs \nwere conducted separately for these two groups. The ANOVAs conducted on the low-\nexperience respondents (N = 32) showed no significant effect of experimental ma-\nnipulation for any of the dependent measures. \nThe ANOVAs conducted on the high-experience respondents (N = 28) showed \nsignificant effects across the experimental manipulation for several measures. There \nwere significant differences across conditions for the ratings of both the TRT (F(2,25) \n= 3.87, p < .05) and the DST (F(2,25) = 3.90, p < .05). Examination of the mean \nscores indicated that high experience participants in the human team and human-\ncomputer team conditions rated both tasks more positively than their counterparts in \nthe control condition (see Table 1). No such pattern existed for low experience par-\nticipants. This finding suggests that for high experience (but not low experience) re-\nspondents, simply being placed in a team was sufficient to promote more positive atti-\ntudes to the tasks than working alone (in the control condition). \nTable 1. Mean Scores for Dependent Measures Across Conditions \nLow Experience High Experience \nMeasure Control Human  Human-Comp  Control Human  \nHuman-\nComp  \nTRT Rating 4.0 3.3 3.2 2.5 4.5 4.4 \nDST Rating 5.7 4.6 5.1 4.4 5.7 6.1 \nTRT Subjective Infl. 2.1 1.0 1.6 1.0 1.7 0.6 \nInformation Quality 5.5 5.6 6.0 6.1 5.8 4.6 \nOpenness to Infl. 5.8 5.2 5.2 6.1 6.0 4.5 \nTRT Objective Infl. 118.6 122.1 115.3 121.1 107.2 143.6 \nDST Objective Infl. 27.0 28.2 25.7 20.8 13.7 32.7 \n \nFor high experience respondents’ subjective measures of how much they were in-\nfluenced by the computer, there were significant differences across conditions for the \ntext rating task (F(2,25) = 4.10, p < .05), but not the desert survival task. Mean scores \nfor the TRT suggested that respondents reported less alteration of their responses in \nthe human-computer team condition than in the control and human team conditions \n(see Table 1). High experience respondents, who were working in a team with other \npeople and a computer, subjectively rated themselves as being less influenced by the \ncomputer in the TRT than high experience participants in either the control condition \nor the human team condition.  \nHigh experience respondents showed significant differences across conditions for \nboth their ratings of the quality of information provided by the computer (F(2,25) = \n5.01, p < .05), and their openness to influence from the computer (F(2,25) = 3.77, p < \n.05). Mean scores for these measures indicated that high-experience respondents in \nthe human-computer team condition rated the quality of information from the com-\nputer lower, and reported lower openness to influence from the computer than those \nin the control and human team conditions (see Table 1). When high experience par-\nticipants were placed in a team with a computer as well as other humans, they were \nless positive about the quality of information provided by the computer, and reported \nbeing less open to being influenced by the computer. \nAmongst high experience respondents, there were significant differences across \nconditions for the objective measures of the computer’s influence for both the text rat-\ning task (F(2,25) = 6.07, p < .01) and the desert survival task (F(2,25) = 4.00, p < .05). \nMean scores for these measures indicated that high-experience respondents in the \nhuman-computer team condition had ratings/rankings that were further from the com-\nputer’s suggestions in both the TRT and the DST than their counterparts in the control \nand human team conditions (see Table 1). In both tasks, high experience participants \nin a team with a computer were less influenced by the computer, than were high expe-\nrienced participants in the human team and control conditions. \nDiscussion \nConsistent with previous media equation research [17], a media equation pattern of \nbehaviour was exhibited by participants who had high experience with computers but \nnot by participants with low experience with computers (supporting H2). An unex-\npected but consistent pattern of results arose for high experience participants across \nthe human team and human-computer team conditions. Broadly, high experience par-\nticipants in the human-computer team condition rated the quality of information from \nthe computer lower, felt they were less open to influence from the computer, per-\nceived their own responses to the text rating task to be less influenced by the com-\nputer’s ratings, and were actually less influenced by the computer’s response in both \nthe text rating and desert survival tasks than were high experience participants in the \nhuman team condition (contrary to H1). This finding is in contrast to the findings of \nNass and colleagues [21, 22] in which participants (on a team with a computer team-\nmate) perceived the computer as having more influence, rated the quality of informa-\ntion from the computer more highly, and conformed more to the computers recom-\nmendations when the computer was made a part of the team.  \nThe contrary pattern of results obtained in the current study is intriguing, as it is \nnot possible to conclude that the identity manipulation did not work. Firstly, team af-\nfiliation effects are present for high experience participants in terms of rating of the \ntasks. Secondly, those in the human-computer team condition did not show the same \nattitudes and behaviour as those in the human team condition. Those who had a com-\nputer teammate as well as human teammates generally reacted negatively towards the \ncomputer in comparison to those who were part of a team without the computer as a \nteammate. This finding suggests that there is something unique about being on a team \nthat includes a computer, which evokes a negative reaction towards that computer. \nThis result raises the question of why these high experience people would be in-\nclined to disregard or undervalue the computer’s recommendations. Our initial con-\nsideration of the social identity literature led us to hypothesise the opposite pattern of \nresults. We expected high experience participants to be more likely to treat the com-\nputer as a team member and thus, expected them to be more likely to be influenced by \nthe computer’s ratings and rankings in the two tasks, when the computer was made \npart of the team. The pattern of results obtained contradicted these expectations; high \nexperience participants working with the computer as a team member were less influ-\nenced by the computer than either low experience participants or participants in which \nthe computer was not a team member. \nHowever, further exploration of the social identity literature leads to explanations \nof these results that are in line with the media equation explanation of people’s reac-\ntions to computers. Research has been conducted identifying a phenomenon known as \nthe ‘Black Sheep Effect’, whereby group members may reject another group member \nbased on that group member’s deviation from the group prototype [43-50]. These \nstudies highlight unfavourable evaluations and derogation of ingroup members as a \nform of ingroup bias that marginalises members who threaten positive ingroup iden-\ntity. If the current findings are considered in light of the ‘black sheep effect’, then \nthey can be interpreted as a clear example of media equation behaviour. Specifically, \nhigh experience participants placed on a team with the computer (but not low experi-\nence participants) treat the computer like a person to the extent that they perceive the \ncomputer as a member of the ingroup, albeit a member of the group that does not con-\ntribute positively to their group identification. As a result, high experience partici-\npants derogate this less positively perceived group member (the computer) by placing \nless value on its recommended ratings and rankings and generally perceiving its con-\ntribution to be of less value. This derogation of the computer and the information it \nprovides does not occur among high experience participants unless the computer is \nimplied to be a member of the team. Low experience participants, on the other hand, \ndo not exhibit a media equation pattern of results; even when it is implied that the \ncomputer is a team member, the computer is not perceived as a member of the group, \nand thus no negative reaction towards the computer as a group member occurs. This \nexplanation is strengthened by the fact that the results extend beyond subjective rat-\nings and into objective behaviour. The pattern of results was consistent for subjective \nratings of the quality of information provided by the computer, subjective ratings of \nthe degree to which participants felt they were open to influence from the computer, \nsubjective ratings of the degree to which participants thought they were influenced by \nthe computer in one of the tasks, and objective measures of the degree to which par-\nticipants actually were influenced by the computer in both tasks. \nIt would seem that while high experience computer users tend to treat computers \nlike real people to the extent that they will accept the categorisation of the computer \nas a fellow group member, the effect is not strong enough for them to perceive the \ncomputer as a positive addition to the group. Rather, high experience users seem to be \nprepared to accept the computer as a teammate, but presumably because of deviations \nfrom the assumed group prototype, the computer is reacted to negatively and its input \nis marginalized or disregarded. According to the black sheep theory of group forma-\ntion, this derogation of the marginal ingroup member (in this case the computer) \nserves to strengthen and protect the existing team identity.  \nAn alternative explanation for this pattern of results would be that participants \nfound the concept of having the computer as a teammate ridiculous. In response to be-\ning forced into an absurd situation, participants became belligerent and deliberately \nchanged their responses to be contrary to the recommendations made by the com-\nputer. However, this explanation cannot account for the fact that only high experience \nparticipants rated the information provided by the computer as less useful, perceived \nthemselves as less influenced by the computer, perceived themselves as less similar to \nthe computer, and changed their responses away from the recommendations made by \nthe computer. Nor can it account for the fact that high experience participants re-\nported enjoying the task more when they worked as part of a team regardless of \nwhether the computer was part of the team.  \nApplication of Current Findings to Games \nWithin the arena of video games, the most obvious applicable finding is that people \ntend to enjoy a task more when placed within a team environment (whether or not a \ncomputer is a member of the team). This suggests that enjoyment of tasks in single-\nplayer games can be improved by helping the player feel like they are part of a team. \nThis technique (increasing enjoyment of tasks by creating a sense of being on a team) \nis already utilised in many games (for example, Jak 3 or Half Life 2, in which the \nplayer is led to believe they are acting in concert with, or for the benefit of, other non-\nplayer characters (NPCs)).  \nThe aforementioned application relates specifically to the player’s attitude towards \nthe tasks being undertaken within a game. The current findings are also relevant to the \nplayer’s attitude towards the other characters within the game. Specifically, consid-\neration should be given to games in which the player controls a character in a team \nsetting and the computer controls NPCs on the team (role playing games, real-time \nstrategy games and first-person shooters). The current research suggests that players \n(at least those of high experience) will accept computer players as part of the team, \nbut may react negatively towards them or value them less than other human members \nof the team (as they deviate from the group prototype in the same way as the com-\nputer team-members in the current study – they are not human). At first glance, it \nmight appear that this is something to be avoided by game developers on the basis \nthat a negative reaction on the part of the player towards an NPC is a bad thing. How-\never, the research on the black sheep effect shows that derogation of a marginal in-\ngroup member (in this case a NPC) serves to strengthen and protect the existing team \nidentity. Thus, it is quite possible that NPCs on teams in videogames can and do play \na “scapegoat” role, becoming a target towards which the human members of the team \ncan release their frustration, direct blame and generally release negative emotions. Ul-\ntimately, NPCs placed in such a role would strengthen the sense of identity amongst \nthe human members of the team. \nApplication of General Media Equation Findings to Games \nFrom the myriad of media equation findings there are a wide variety that can be ap-\nplied to the development of video games. What follows is by no means an exhaustive \nlist, rather the aim is to provide a few examples of how media equation research can \nbe applied to video games and thereby highlight the fact that the media equation is a \nready source of design principles and techniques that can be applied in video game \ndevelopment. \nOne of the strongest and most consistent findings in psychological research is that \nperceptions of similarity increase attraction between people [51]. Nass and colleagues \n[4, 8, 9, 52] were able to show that computers exhibiting similar levels of extrover-\nsion, introversion, submissiveness or dominance (displayed via the style of text based \ncommunication or tone of voice) as their human users were perceived far more posi-\ntively than computers which were not doing so. To this end, it seems safe to assume \nthat videogames that contain NPCs with personalities similar to those of the player \nwill be more positively perceived. For example, in an action adventure game, a short \nquestionnaire or observation of the player’s choices in the early stages of the game \ncould allow for the facilitation of the player interacting with NPCs who exhibit simi-\nlar personality traits to the player; this would facilitate greater positivity on the part of \nthe player towards the NPCs. \nResearch has also shown that the gain/loss theory of interpersonal attractiveness \n(which suggests that individuals will be more attracted to others who initially dislike \nthem and then come to like them, than to others who consistently like them) is applic-\nable to interactions between people and computers [4]. This could be a useful tech-\nnique for games in which players continually interact with one particular NPC across \nthe course of a game (for example a game in which a particular NPC is a ‘sidekick’ or \na tutor/helper). Such characters, if they initially treated the player negatively and be-\ncame more positive or friendly over time, should become far more well-liked by the \nplayer than an NPC that is consistently positive. \nIn the context of the social norm that ‘we should treat others the way they treat us’, \nthere is a great deal of research showing that if people receive a favour from others \nthey feel obligated to reciprocate [32]. A series of media equation studies [14-16] \ndemonstrated that people will feel indebted to a computer that provides a benefit, and \nsubsequently reciprocate to that computer. The notion of the player receiving ‘fa-\nvours’ or ‘benefits’ from a game or NPCs within a game is quite common (‘power-\nups’, weapon upgrades, bonus levels etc.). It may be that these benefits can be lever-\naged by developers to encourage players to meet other objectives, such as the comple-\ntion of specific tasks requested by NPCs. This technique may also be useful outside \nthe game environment, for example; providing players with a bonus level or weapon \nin return for the completion of an online survey focussed on future game develop-\nment. \nOne of the areas of media equation research most obviously applicable to video \ngames is the research conducted on flattery. Just as people tend to react positively to \nflattery from others, researchers [2, 17] have shown that when flattered by computers, \npeople tend to believe that the computer spoke the truth, experience more positive af-\nfect as a result of the flattery, and judge the computer’s performance more favourably. \nThis is one design technique identified in media equation research that is being ap-\nplied in existing video games. Many first-person shooters (such as Quake, Unreal \nTournament and Halo 2) use a voice-over or on-screen text to flatter players when \nthey are performing well. This technique may well prove effective in other video \ngames where little or no feedback is provided to the player while playing the game. \nIt is hoped that the suggested applications of the current study in combination with \nthe aforementioned general examples of design principles derived from media equa-\ntion research will inspire academics and game developers to further explore the poten-\ntial synergies between the media equation and video game design. \nReferences  \n \n1. Reeves, B. and C. Nass, The Media Equation: How People Treat Computers, Television and \nNew Media Like Real People and Places. 1996, Cambridge: Cambridge University Press. \n2. Fogg, B.J. and C. Nass, Silicon sycophants: the effects of computers that flatter. International \nJournal of Human Computer Studies, 1997. 46: p. 551-561. \n3. Nass, C., J. Steuer, and E. Tauber. Computers are social actors. in CHI 97. 1994. Boston. \n4. Moon, Y. and C. Nass, How \"real\" are computer personalities?: Psychological responses to \npersonality types in human-computer interaction. Communication Research, 1996. 23(6): p. \n651-674. \n5. Rickenberg, R. and B. Reeves. The effects of animated characters on anxiety, task perform-\nance, and evaluations of user interfaces. in CHI 2000. 2000. Amsterdam. \n6. Lee, K. and C. Nass, Designing social presence of social actors in human computer interac-\ntion, in CHI Letters. 2003, ACM: Ft. Lauderdale. p. 289-296. \n7. Gong, L. and J. Lai, Shall we mix synthetic speech and human speech? Impact on users' per-\nformance, perception and attitude. CHI Letters 2001, 2001. 3(1): p. 158-165. \n8. Nass, C. and K.M. Lee, Does computer-synthesized speech manifest personality? Experi-\nmental tests of recognition, similarity-attraction, and consistency-attraction. Journal of Ex-\nperimental Psychology: Applied, 2001. 7(3): p. 171-181. \n9. Nass, C. and K.M. Lee. Does computer-generated speech manifest personality? An experi-\nmental test of similarity-attraction. in CHI 2000. 2000. Amsterdam: ACM. \n10. Duck, S.W., Personality similarity and friendship choice: Similarity of what, when? Journal \nof Personality, 1973. 41(4): p. 543-558. \n11. Byrne, D., G.L. Clore, and G. Smeaton, The attraction hypothesis: Do similar attitudes ef-\nfect anything? Journal of Personality and Social Psychology, 1986. 51: p. 1167-1170. \n12. Neimeyer, R.A. and K.A. Mitchell, Similarity and attraction: A longitudinal study. Journal \nof Social and Personal Relationships, 1988. 5(2): p. 131-148. \n13. Fogg, B.J. and C. Nass. How users reciprocate to computers: An experiment that demon-\nstrates behaviour change. in CHI 97. 1997. Atlanta. \n14. Takeuchi, Y., et al. Social response and cultural dependency in human-computer interac-\ntion. in PRICAI 98. 1998. \n15. Nass, C. and Y. Moon, Machines and mindlessness: Social responses to computers. Journal \nof Social Issues, 2000. 56(1): p. 81-103. \n16. Takeuchi, Y., et al. A cultural perspective in social interface. in CHI 2000. 2000. Amster-\ndam. \n17. Johnson, D., J. Gardner, and J. WIles, Experience as a moderator of the media equation: the \nimpact of flattery and praise. International Journal of Human Computer Studies, 2004. 61: p. \n237-258. \n18. Nass, C., Y. Moon, and P. Carney, Are people polite to computers? Responses to computer-\nbased interviewing systems. Journal of Applied Social Psychology, 1999. 29(5): p. 1093-\n1110. \n19. Nass, C., B. Reeves, and G. Leshner, Technology and roles: A tale of two TVs. Journal of \nCommunication, 1996. 46(2): p. 121-128. \n20. Nass, C., et al., Machines, social attributions, and ethopoeia: performance assessments of \ncomputers subsequent to \"self-\" or \"other-\" evaluations. International Journal of Human-\nComputer Studies, 1994. 40: p. 543-559. \n21. Nass, C., B.J. Fogg, and Y. Moon, How powerful is social identity? Affiliation effects in \nhuman-computer interaction. 1995. \n22. Nass, C., B.J. Fogg, and Y. Moon, Can computers be teammaters? International Journal of \nHuman-Computer Studies, 1996. 45: p. 669-678. \n23. Moon, Y. and C. Nass, Are computers scapegoats? Attributions of responsibility in human-\ncomputer interaction. International Journal of Human Computer Studies, 1998. 49: p. 79-94. \n24. Nass, C., Y. Moon, and N. Green, Are Computers Gender Neutral? Gender Stereotypic Re-\nsponses to Computers. Journal of Applied Social Psychology, 1997. \n25. Isbister, K., et al. Helper agent: designing an assistant for human-human interaction in a vir-\ntual meeting space. in CHI 2000. 2000. Amsterdam. \n26. Nakanishi, H., et al. Can software agents influence human relations? Balance theory in \nagent-mediated communities. in International Conference on Autonomous Agents and Mul-\ntiagent Systems. 2003. Melbourne, Australia: ACM. \n27. Klein, J., Y. Moon, and R. Picard, W., This computer responds to user frustration: Theory, \ndesign, results and implications. submitted to Interacting with computers, 1999. \n28. Cooper, A., 14 Principles of Polite Apps. Visual Basic Programmers Journal, 1999(June): p. \n62-66. \n29. Diederiks, E. Buddies in a box. Animated characters in consumer electronics. in Interna-\ntional Conference on Intelligent User Interfaces. 2003. Miami, Florida: ACM. \n30. Friedman, B., P. Kahn, and J. Hagman, Hardware companions? - What online AIBO dis-\ncussion forums reveal about the human-robotic relationship. CHI Letters 2003, 2003. 5(1): \np. 273-280. \n31. Johnson, W. Interaction tactics for socially intelligent pedagogical agents. in International \nConference on Intelligent User Interfaces. 2003. Miami, Florida: ACM. \n32. Vaughan, G.M. and M.A. Hogg, Introduction to social psychology. 1995, Upper Saddle \nRiver, NJ: Prentice-Hall Inc. xv, 438. \n33. Tajfel, H., et al., Social categorization in intergroup behavior. European Journal of Social \nPsychology, 1971. 1: p. 149-177. \n34. Gagnon, A. and R. Bourhis, Discrimination in the minimal group paradigm: Social identity \nor self interest? Personality and Social Psychology Bulletin, 1996. 22(12): p. 1289-1301. \n35. Gaertner, L. and C. Insko, Intergroup discrimination in the minimal group paradigm: cate-\ngorisation, reciprocation or fear? Journal of Personality and Social Psychology, 2000. 79: p. \n79-94. \n36. Gaertner, L. and C. Insko, On the measurement of social orientations in the minimal group \nparadigm: Norms as moderators of the expression of intergroup bias. European Journal of \nSocial Psychology, 2001. 31: p. 143-154. \n37. Dobbs, M. and W. Crano, Outgroup accountability in the minimal group paradigm: Impli-\ncations for Aversive Discrimination and Social Identity Theory. Personality and Social Psy-\nchology Bulletin, 2001. 27(3): p. 355-364. \n38. Grieve, P. and M. Hogg, Subjective uncertainty and intergroup discrimination in the mini-\nmal group situation. Personality and Social Psychology Bulletin, 1999. 25(8): p. 926-940. \n39. Otten, S. and G. Moskowitz, Evidence for implicit ingroup bias: Affect biased spontaneous \ntrait inference in a minimal group paradigm. Journal of Experimental Social Psychology, \n2000. 36: p. 77-89. \n40. Hertel, G. and N. Kerr, Priming ingroup favouritism: The impact of normative scripts in the \nminimal group paradigm. Journal of Experimental Social Psychology, 2001. 37: p. 316-324. \n41. Otten, S. and D. Wentura, Self-anchoring and ingroup favoritism: An individual-profiles \nanalysis. Journal of Experimental Social Psychology, 2001. 37: p. 525-532. \n42. Watson, D., L.A. Clark, and A. Tellengen, Development and validation of brief measures of \npositive and negative affect: The PANAS scales. Journal of Personality and Social Psychol-\nogy, 1988. 54: p. 1063-1070. \n43. Biernat, M., T. Vescio, and L. Billings, Black sheep and expectancy violation: Integrating \ntwo models of social judgment. European Journal of Social Psychology, 1999. 29: p. 523-\n542. \n44. Branscombe, N.R., et al., Ingroup or outgroup extremity: Importance of the threatened so-\ncial identity. Personality and Social Psychology Bulletin, 1993. 19: p. 381-388. \n45. Hogg, M., E.A. Hardie, and K.J. Reynolds, Prototypical similarity, self-categorization, and \ndeperonalized attraction: A perspective on group cohesiveness. European Journal of Social \nPsychology, 1995. 25: p. 159-175. \n46. Jetten, J., R. Spears, and A.S.R. Manstead, Distinctiveness threat and prototypicality: Com-\nbined effects on intergroup discrimination and collective self-esteem. European Journal of \nSocial Psychology, 1997. 27: p. 635-657. \n47. Marques, J.M., et al., The role of categorization and ingroup norms in judgments of groups \nand their members. Journal of Personality and Social Psychology, 1998. 75: p. 976-988. \n48. Marques, J.M., D. Paez, and D. Abrams, Social identity and intragroup differentiation as \nsubjective social control, in Social identity: International perspectives, S. Worchel, et al., \nEditors. 1998, Sage: London, England. p. 124-141. \n49. Marques, J.M. and V.Y. Yzerbyt, The black sheep effect: Judgmental extremity towards in-\ngroup members in inter- and intra-group situations. European Journal of Social Psychology, \n1988. 18: p. 287-292. \n50. Matheson, K., B. Cole, and K. Majka, Dissidence from within: Examining the effects of in-\ntergroup context on group members' reactions to attitudinal opposition. Journal of Experi-\nmental Social Psychology, 2003. 39: p. 161-169. \n51. Smith, R., E., Psychology. 1993, Minesotta: West Publishing. \n52. Moon, Y. and C. Nass. Adaptive agents and personality change: complementarity versus \nsimilarity as forms of adaptation. in CHI 96. 1996. Vancouver Canada: ACM. \n",
            "id": 4573263,
            "identifiers": [
                {
                    "identifier": "10.1007/11558651_45",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:eprints.qut.edu.au:6696",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "10877445",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "86632887",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "146907502",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:espace.library.uq.edu.au:uq:685704",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "1578058686",
                    "type": "MAG_ID"
                }
            ],
            "title": "Effects of Team-Based Computer Interaction: The Media Equation and Game Design Considerations",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "1578058686",
            "oaiIds": [
                "oai:eprints.qut.edu.au:6696",
                "oai:espace.library.uq.edu.au:uq:685704"
            ],
            "publishedDate": "2005-01-01T00:00:00",
            "publisher": "'Springer Science and Business Media LLC'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://eprints.qut.edu.au/6696/",
                "http://www.springer.de/comp/lncs/"
            ],
            "updatedDate": "2021-05-26T05:56:10",
            "yearPublished": 2005,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "0302-9743"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/10877445.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/10877445"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/10877445/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/10877445/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/4573263"
                }
            ]
        },
        {
            "acceptedDate": "2010-03-08T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Bianco, Andrea"
                },
                {
                    "name": "Hay, David"
                },
                {
                    "name": "Neri, Fabio"
                }
            ],
            "contributors": [],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/234878760",
                "https://api.core.ac.uk/v3/outputs/11415145"
            ],
            "createdDate": "2013-07-10T14:52:02",
            "dataProviders": [
                {
                    "id": 12601,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/12601",
                    "logo": "https://api.core.ac.uk/data-providers/12601/logo"
                },
                {
                    "id": 351,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/351",
                    "logo": "https://api.core.ac.uk/data-providers/351/logo"
                }
            ],
            "depositedDate": "2009-11-01T00:00:00",
            "abstract": "Best paper awar",
            "documentType": "research",
            "doi": "10.1109/glocom.2009.5425775",
            "downloadUrl": "https://core.ac.uk/download/pdf/11415145.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "Crosstalk limiting schedulers\nin AWG-based optical switches\nA. Bianco∗, D. Cuda∗∗, G. Gavilanes Castillo∗, F. Neri∗, M. Rodelgo Lacruz†,\nF. J. Gonza´lez Castan˜o†‡, C. Lo´pez Bravo‡, M. Salvat§\n∗ Dip. di Elettronica, Politecnico di Torino, Italy, Email: {andrea.bianco, guido.gavilanescastillo, fabio.neri}@polito.it\n† Gradiant, Spain, Email: mrodelgo@gradiant.org\n‡ Universidade de Vigo, Spain, Email: javier.clbravo@det.uvigo.es\n§ Universitat Polite´cnica de Catalunya, Spain, Email: maria.salvat@estudiant.upc.edu\n∗∗ IEIIT - National Research Center, Italy, Email: davide.cuda@polito.it\nAbstract—Optical switching fabrics are gaining interest as\nin multi Terabit switching devices; the advantages over their\nelectronic equivalents are mainly their information density and\npower consumption. Arrayed Waveguide Gratings (AWGs) are\npromising optical devices proposed by the academic and indus-\ntrial community to build optical switching fabrics. Because of\ntheir wavelength routing property, AWGs allow wavelength reuse\nover different ports introducing in-band crosstalk which strongly\nlimits scalability of AWG-based backplanes. However, this effect\ncan be mitigated or even completely avoided by means of proper\nscheduling algorithms. In this paper, we present several modified\nscheduling algorithms which limit the effect of coherent crosstalk\nin AWG-based switching fabrics and achieve good performance\nin terms of throughput and delay.\nI. INTRODUCTION\nTo cope with the continuous growth of the Internet traffic,\nincreasing at a pace faster than the Moore’s law, the urgency\nof deploying multi Terabit packet switches becomes evident.\nCurrent electronic technologies may not be able to support the\nrealization of such high-end switching devices because they\nare approaching their physical limits, especially considering\nthe maximum length that electronic signals can span before\nrequiring regeneration and their increasing power requirements\nwith the bitrate.\nOn the other hand, optics exhibit a switching complex-\nity which is almost independent of the bit rate, negligible\nconstraints on the length of internal interconnections, good\nscalability and lower power requirements [1]. One of the most\npromising approaches to optical switching fabrics is to use a\npassive wavelength routing device connecting tunable trans-\nmitters and fixed receivers. AWGs have been successful in the\ncommercial deployment of Wavelength-Division Multiplexing\n(WDM) transmission systems. Their use was proposed in [2]\nin multi Terabit switching devices exploiting the wavelength\ndimension to perform switching operations. In addition, AWGs\nare relatively simple passive devices whose insertion losses\ndepend weakly on the port count [3]. However, the shared op-\ntical media constituting AWGs strongly limit their scalability;\ninternally, all light flows interfere on a concave slab waveguide\nto be coupled to the output waveguides; introducing in-band\ncrosstalk when the same wavelength is reused over several\ninputs simultaneously. As a consequence, additional power is\n(a) Switch architecture\nIn\npu\nti λ0 λ1 λ2 λ3 λ4\nλ4 λ0 λ1 λ2 λ3\nλ3 λ4 λ0 λ1 λ2\nλ2 λ3 λ4 λ0 λ1\nλ1 λ2 λ3 λ4 λ0\nOutput j\n(b) λ-matrix for a\n5× 5 AWG\nFig. 1. Reference architecture.\nrequired to correctly receive signals at outputs. The impact of\ncrosstalk in signal degradation has been analyzed in [4], posing\na limit on the maximum size of a practical AWG device around\n15 ports in the worst case (when all inputs are carrying traffic\nin the same wavelength).\nHowever, the simultaneous use of the same wavelength can\nbe avoided by properly scheduling packets, controlling the\nnumber of ports over which the same wavelength is reused. In\n[5] and [6], different solutions to control the concurrent usage\nof the same wavelength were proposed. Those solutions, even\nthough optimal, are highly complex and require specialized\nhardware implementations.\nIn this paper we modify well-known scheduling algorithms\nto respect the wavelength constraint. These algorithms ensure\ngood throughput performance with low complexity, thus of-\nfering a practical alternative for scheduling packets through\nAWG-based optical backplanes.\nII. REFERENCE OPTICAL ARCHITECTURE\nThe optical switching fabric architecture we are considering\nis shown in Fig. 1(a). It consists of an AWG connecting\nN tunable transmitters (TTx) and N fixed receivers. AWGs\nexploit the wavelength dimension to perform the switching\noperation. A data packet at an input port is forwarded to an\noutput port depending on the input wavelength and port: at\neach input port, different wavelengths can be used to reach\ndifferent output ports. As a consequence, at each output port,\n978-1-4244-5638-3/10/$26.00 ©2010 IEEE\nThis full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE Globecom 2010 proceedings.\ninformation is received from different inputs with different\nwavelengths. Thus, if enough transceivers are available, an\nN × N AWG can be simultaneously traversed by N2 inde-\npendent data flows, one for each input/output pair, leading to a\nfull mesh connectivity. However, the use of a single transceiver\nper port, limit to N the maximum number of data flowing\nthrough the switch at the same time. The specific wavelengths\nused to route information through an AWG depend on the\ndesign of the device, but commercial AWGs typically follow\nthe standard ITU grid (with 100 GHz or 50 GHz spacing).\nAlthough different wavelength assignments are possible, we\nassume, with no loss of generality, that i) the N×N AWG op-\nerates with a set of N wavelengths Λ = {λ0, λ1, . . . , λN−1},\nand ii) at input i, information is delivered to output j (with\ni, j = {0, 1, . . . , N − 1}) using wavelength λw, with\nw = (j − i) mod N (1)\nbeing w the wavelength channel number. As an example, the\nresulting input-output matrix defining the wavelength routing\nfunction of a 5 × 5 AWG, named λ-matrix is described in\nFig. 1(b). This cyclic behavior is typical of the interferometric\nnature of the AWG, whose routing function is replicated\nover the wavelength axis with a period called Free Spectral\nRange (FSR). Our assumptions on the AWG behavior imply\nthat only N wavelengths are necessary for the N2 connec-\ntion permutations over N input and N output ports. This\narchitecture uses a single wavelength per port, derived from\nthe single transceiver architecture, but given that cells can\nbe received on any wavelength, since they can come from\nany port, receivers must be wide band and operate in burst\nmode (WBMR). The optical fabric does not include any active\nswitching element: packet switching is actually controlled by\nthe tunable transmitters (implemented with fast tunable lasers)\nand exploits the wavelength routing property of the AWG-\nbased optical fabric.\nIII. SCHEDULER FRAMEWORK\nThe switch is assumed to be synchronous and line cards\nprocess packets electronically by implementing input queues\nand other packet operations at the line card bitrate; this keeps\nelectronic complexity within the single port-speed limit. The\nscheduler uses Virtual Output Queuing (VOQ) at inputs to\navoid the Head of the Line blocking problem. Indeed, VOQs\nstore cells at each input port in N separate FIFO queues,\naccording to their destination port. At each time slot, a\nscheduler controls the Input Queue (IQ) switch transferring\nfixed size cells from at the most N inputs to N outputs taking\nup to one scheduling decision per time slot, since no speedup\nis available. At each scheduling decision a matching is defined\nsuch that at most one cell is transferred from each input port\nand to each output port. Thus, each scheduling decision is\na input-output permutation π = [π [0] , π [1] , . . . , π [N − 1]],\nwith π [i] denoting the output port which input i is transmitting\nto. If an optical switching fabric based on AWG is used to\nforward packets, the scheduler must provide both the input-\noutput permutation π and its wavelength assignment λ (π)\n(evaluated according to Eq. (1)). In the following, we say that\na certain input-output permutation is k-legal if no wavelength\nis used more that k times according to the given wavelength\nassignment. Hence, k represents the maximum number of\ntimes a single wavelength can be used at different input ports\nduring the same time slot.\nThe problem of constraining wavelength reuse to control\ncrosstalk in AWG-based switching fabric has been addressed\nand solved in [5], [6]. It is shown that uniform traffic patterns\ncan be scheduled using only 1-legal permutations with no\nspeedup for switches with an odd number of ports and with\n1 + 1/N speedup for switches with an even number of\nports. Those solutions are highly complex, making them not\npractical. We focus on modifications of iterative maximal size\nmatching algorithms [7], [8], [9] which usually ensure good\nperformance and are simple enough to be implemented in\nhardware. In order to cope with the wavelength constraint, the\nproposed schedulers introduce a λ-phase that ensures that the\nselected permutations are k-legal, i.e., that each wavelength\ncan be used to transmit packets from inputs to outputs at the\nmost k times during the same time slot. As Fig. 1(b) shows,\nwavelengths on the different anti-diagonals of the λ-matrix are\nall different. Thus, a good scheduler have to select input-output\npairs belonging to the anti-diagonals with high probability. We\nrefer to an anti-diagonal as a set of N elements in an N ×N\nmatrix, such that no two elements are in the same row or\ncolumn and they are all different. For instance, in Fig. 1(b)\nthe vector {(1, 5), (2, 4), (3, 3), (4, 2), (5, 1)} corresponds to\nthe main anti-diagonal and {(1, 1), (2, 4), (3, 2), (4, 5), (5, 3)}\nis a generalized anti-diagonal.\nA. 2DRR, iSLIP and RDSRR\nWe consider variations of the well-known two-dimensional\nround-robin (2DRR) [7], of the iSLIP [8] and of the Rotating\nDouble Static Round-Robin (RDSRR) [9] schedulers. We\nselect these algorithms since they achieve good performance\nand can be easily modified to cope with the wavelength\nconstraint.\nIn the algorithms we propose each input (output) is iden-\ntified by a pointer, or arbiter, pIi (pOj ), with 0 ≤ i ≤ N − 1\n(0 ≤ j ≤ N − 1). In addition, wavelengths are identified by\nλ-pointers (or λ-arbiter) pWw and by λ-counters cWw , with w\nbeing the wavelength identifier (0 ≤ w ≤ N − 1). We briefly\nrecall the original version of these three algorithms.\n1) 2DRR: At each time slot, 2DRR scans the request\nmatrix sweeping through a precomputed set of N generalized\ndiagonals. The state of input VOQs is described by the request\nmatrix, which is a matrix whose number of rows is equal to\nthe number of inputs and the number of columns is equal to\nthe number of outputs, and whose elements (i, j) equal 1 if\ninput i has at least one packet occupying the VOQ associated\nwith output j. A generalized diagonal is a set of N elements\nin an N × N matrix, such that no two elements are in the\nsame row or column. Thus, at each time slot, 2DRR analyzes\nthe whole request matrix in N iterations, each iteration being\nassociated with one of the N precomputed diagonals. At each\n978-1-4244-5638-3/10/$26.00 ©2010 IEEE\nThis full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE Globecom 2010 proceedings.\niteration, 2DRR selects input-output pairs (i, j) which belong\nto the chosen generalized diagonal and are set to 1, provided\nthat, input i and output j are both free, i.e., they have not\nbeen previously selected in any of the former iterations of\nthe current time slot. Note that, to cover entirely the request\nmatrix, 2DRR always iterates N times on each time slot (over\nthe N different generalized diagonals).\n2) iSLIP: iSLIP is a matching algorithm which exploits 2\narrays of N round-robin arbiters (one for each input and each\noutput). It is based on three sequential steps which are per-\nformed in parallel on each input and output. At the beginning\nof each time slot, inputs and outputs are unmatched by default.\nThe first iSLIP step is the Request phase: each unmatched\ninput sends a request to every output for which it has a cell in\nthe VOQ. The second step is the Grant phase (from outputs\nto inputs): if an unmatched output receives requests, it sends\na grant to one of the inputs. Each output chooses the first\nrequesting input found in a fixed round-robin scan of inputs\nstarting from the highest priority element (indicated by the\noutput pointer pOj associated with that output). The output\nnotifies each input whether or not its request was granted. The\nfinal step is the Accept phase: if an input receives a grant,\nit accepts the one that appears first in a round-robin scan\nstarting from the highest priority element (indicated by the\ninput pointer pIi ). These three steps are iterated several times\nto progressively increment the matching size on each iteration.\nBoth input and output pointers indicating the highest priority\nelement of the round-robin schedule are incremented (modulo\nN ) to one location beyond the granted input if, and only if,\nthe grant is accepted during the Accept phase.\n3) RDSRR: RDSRR differs from iSLIP because of its\npointer updating rule. Indeed, iSLIP performance relies on its\nability to desynchronize the arbiters to point to different input-\noutput pairs. The RDSRR scheduler still performs a Request,\nGrant and Accept phase but, pointers are always updated by\none (modulo N ) whether there is a grant or not to force full\npointer desynchronization.\nB. λ-2DRR\nThe λ-2DRR scheduler follows the same logic of the 2DRR\nalgorithm, but, it exploits the N generalized anti-diagonals\nto scan over the request matrix. The selection of the anti-\ndiagonals as a covering set of the request matrix implies\nthat all the input-output pairs belonging to the same anti-\ndiagonal can be transmitted using a different wavelength.\nThus, the λ-2DRR minimizes the probability of contentions\nwhen choosing a wavelength. To ensure fairness, at each\ntime slot, the initial anti-diagonal changes according to a\nfixed round-robin scheme. At the beginning of each time slot\nall the λ-counters are initialized to cWw = 0 ∀w and each\ntime wavelength w is used to match an input to an output\ncWw is incremented by one. The λ-2DRR selects input-output\npermutations (i, j) which belong to the chosen generalized\nanti-diagonal, provided that, the element (i, j) of the request\nmatrix is set to 1, input i and output j are both free and their\nselection does not violate the k-legal constraint, i.e., the usage\nof wavelength w = j − i mod N can be granted if cWw < k.\nC. Centralized-iSLIP\nThe main gear of the centralized-iSLIP (C-iSLIP) is the λ-\nvector, an array containing the N λ-counters cWw . Thus, each\nof its elements is associated with a specific wavelength and\nrecords the number of times that a wavelength has been used\nduring the current time slot. All the elements of the λ-vector\nare initialized to cWw = 0 at the beginning of a time slot.\nThe Request and Accept phases do not change. However, the\nGrant phase changes as follows: sequentially, each unmatched\noutput j selects, among all the requests, the one coming from\nthe highest priority input i (indicated by pOj ) and it sends a\nrequest to the λ-vector for λw, with w = j − i mod N . The\nscheduler checks cWw and it grants λw to the requiring output if\ncWw < k, with k being the maximum number a wavelength can\nbe used at each time slot; if λw is granted, cWw is incremented\nby one. Note that, the use of this wavelength is still subject\nto the Accept phase at the inputs. In the C-iSLIP, the Accept\nmessages are sent from inputs to outputs and propagated to\nthe wavelengths. If λw can not be used because its granting\nviolates the k-legal constraint (i.e., cWw ≥ k) the λ-vector does\nnot grant the requiring output indicating that the wavelength\nrequest is Not ACKnowledged (NACK).\nOnce an output receives the ACK/NACK for the requested\nwavelength, the next output of the round-robin scheme is\nserved. To be fair, at each time slot, the first output sending a\nrequest to the λ-vector is selected according to a round-robin\nscheme. After all the outputs have sent a request to the λ-\nvector, outputs which are still not matched to any wavelength\n(received a NACK) send another request for the wavelength\ncorresponding to the input with the next highest priority. The\nGrant phase terminates either when all the outputs are matched\nto one wavelength, or when there are no requests from inputs\nleft. The algorithm can perform several iterations, repeating\nthe same procedure considering only unmatched inputs and\noutputs during previous iterations. Fig. 2 shows an example\nof the C-iSLIP sequence of operations when several inputs\nrequire output O0 which selects input I0 for transmission.\nThe main issue of C-iSLIP is that it can not be executed\nin parallel (all the outputs need to access sequentially the λ-\nvector). The Distribute-iSLIP and the λ-RDSRR schedulers\nsolve this problem.\nD. Distributed-iSLIP\nIn the Distributed-iSLIP (D-iSLIP) each of the N wave-\nlengths is associated with a λ-pointer (pWw ) and a λ-counter\n(cWw ). Each output j is equipped with an additional pointer qOw\nthat manages priorities among the different wavelengths.\nThe Request and Accept phases are unchanged, whereas the\nGrant phase is now divided in two steps (see Fig. 3). During\nthe first step of the Grant phase (from outputs to wavelengths),\nthe outputs forward the requests received (during the Request\nphase) to the corresponding λ-pointer according to Eq. (1).\nIn the second step (from wavelengths to outputs and from\n978-1-4244-5638-3/10/$26.00 ©2010 IEEE\nThis full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE Globecom 2010 proceedings.\n(a) Request phase (b) λ request\n(c) λ−checking and granting (d) Grant phase\nFig. 2. C-iSLIP phases for K = 1 (Accept phase not shown)\noutputs to inputs), each output collects the grants or NACKs\ncoming from the λ-pointers. If an output receives a grant from\nmore than one wavelength, it selects the grant corresponding\nto the first wavelength with the highest priority in the round-\nrobin scheme (indicated by qOw ). Then, each output sends a\ngrant to the input associated with this wavelength. Pointers pWw\nand qWw are incremented (modulo N ) one location above the\ngranted wavelength if the grant is accepted during the Accept\nphase (Accept messages are forwarded by output pointers\nto wavelength arbiters). The two steps of the Grant phase\nare interleaved with the λ-phase. During the λ-phase, each\npointer pWw receives requests forwarded by outputs during\nthe first step of the grant phase. To respect the k-legal\nconstraint, each pointer pWw can send up to k grants back\nto the outputs starting from the one with the highest priority\naccording to the round-robin scheme. Each counter cWw tracks\nthe number of times each wavelength is used. For all the\nother requests, the λ-pointers do not grant the requesting\noutputs, indicating a NACK. Fig. 3 shows an example of the\nD-iSLIP phase sequence, without the accept phase. Solid lines\nindicate requests (from inputs to outputs and from outputs to\nwavelengths), while dotted-dark (dotted-gray) lines stand for\ngrants (NACKs).\nE. λ-RDSRR\nThe λ-RDSRR algorithm is an adaptation of the Rotating\nDouble Static Round-Robin (RDSRR) scheduler [9] which\ndiffers from iSLIP in the pointer updating rule and because\nunlike the D-iSLIP, λ-RDSRR selects the wavelength at the\ninputs. Again, each wavelength is associated with a λ-pointer\npWw and with λ-counter cWw , but the wavelength selection\nis performed at the inputs. Initially, the RDSRR initializes\nthe input and the output pointers to pIi = (−i) mod N\nand pOj = (−j) mod N , respectively. λ-pointers are set to\npWw = w. Note that, this initialization corresponds to one of the\ngeneralized anti-diagonals and is only one of N possible ways\nto initialize pointers. Note that each input points to an output\nthat reciprocally points to that same input, so that wavelength\nassignment is selected to give priority to those pairs.\nAt each time slot the λ-RDSRR performs five phases.\n(a) Request phase (b) Grant phase (step 1)\n(c) λ phase (d) Grant phase(step 2)\nFig. 3. D-iSLIP phases for K = 1 (Accept phase not shown)\nThe Request and Grant phases are identical to iSLIP (except\nthat pointer updates are independent of assignations) and the\nAccept phase is postponed to the λ-phase, which is composed\nby the following two steps. The first step is the Wavelength-\nRequest step: if an input receives one or more grant, it selects\nthe one which appears next in a fixed round-robin schedule,\nstarting from the input pointer pIi , and requests the associated\ntransmission wavelength λw to the corresponding wavelength\npointer according to Eq. (1). The second step of the λ-\nphase is the Wavelength-Grant step; according to the k value,\neach wavelength pointer pWw grants the first k − cWw requests\nfollowing the fixed round-robin schedule starting from the first\nhighest priority input (as indicated by pWw ). The counter cWw is\nupdated to reflect packet assignations, i.e., it is updated only\nif the input-output permutation has been definitively selected\nby the scheduler during the Accept phase. The scheduler can\nrun more iterations, at each iteration considering only inputs\nand outputs which are still free and wavelengths which are\nstill compliant with the k-legal constraint (cWw < k).\nTo maintain the initial pointers’ scheme which gives priority\nto input-output pairs on the anti-diagonal, at each time slot\npIi and pOj pointers are incremented (modulo N ) while pWw\npointers are decremented (modulo N ) regardless of the packet\nassignments. Furthermore, the search direction is reversed each\ntime slot to improve fairness in case of non uniform traffic [9].\nIV. RESULTS\nWe present here the performance of the proposed algorithms\nthrough simulations. Each input is equipped with N queues,\none for each output, with a capacity of 10000 cells at each\nqueue. C-iSLIP, D-iSLIP and λ-RDSRR iterates log2(N)\nwhich usually is enough to ensure that the found matching\nis maximal. By definition, λ-2DRR iterates N times. In the\nreported plots N indicates the switch number of ports, k\nindicates the k-legal constraint and I denotes the number of\niterations each algorithm runs at each time slot.\nDue to space limitations, we present results for uniform\ntraffic scenario. Let ρi be the load at input port i with 0 ≤\ni ≤ N − 1, and ρij the traffic that input port i transmits to\noutput port j. In uniform traffic, ρij = ρiN .\nFig. 4 shows the delays for the different algorithms we\npropose under uniform traffic conditions when the switching\n978-1-4244-5638-3/10/$26.00 ©2010 IEEE\nThis full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE Globecom 2010 proceedings.\n10-2\n10-1\n100\n101\n102\n103\n104\n105\n 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1\nM\nea\nn \nde\nla\ny \n[ce\nlls\n]\nInput traffic load\nC-iSLIP (N=31, k=1, I=5)\nD-iSLIP (N=31, k=1, I=5)\nλ-RDSRR (N=31, k=1, I=5)\nλ-2DRR (N=31, k=1)\nC-iSLIP (N=32, k=1, I=5)\nD-iSLIP (N=32, k=1, I=5)\nλ-RDSRR (N=32, k=1, I=5)\nλ-2DRR (N=32, k=1)\nFig. 4. Delay performance for AWG-based switch with an even number of\nports (N = 32) and an odd number of ports (N = 31)\nfabric presents either an even number of ports (N = 32 -\nwhite-filled markers) or an odd number of ports (N = 31 -\nblack-filled markers). For odd N switching fabrics, all the\nschedulers ensure good performance, the λ-RDSRR shows\nhigher delay because of the fixed pointer updating scheme. The\nD-iSLIP, the λ-2DRR and the λ-RDSRR ensure almost 100%\nof throughput, while the delays for C-iSLIP saturate for an\ninput load around 0.9. Performance worsens for AWG-based\nswitching fabrics with an even N and no algorithm is able to\nachieve 100% of throughput. Again the one performing the\nworst is the λ-RDSRR followed by the λ-2DRR. Note that,\nthese results agree with the theoretical results presented in\n[6] where the authors showed that for AWG-based switching\nfabric with an even N , the maximum achievable throughput\nis limited to 1− 1/N if no speed up is available.\n10-2\n10-1\n100\n101\n102\n103\n104\n105\n 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1\nM\nea\nn \nde\nla\ny \n[ce\nlls\n]\nInput traffic load\nC-iSLIP (N=31, k=1, I=5)\nD-iSLIP (N=31, k=1, I=5)\nλ-RDSRR (N=31, k=1, I=5)\nλ-2DRR (N=31, k=1)\nC-iSLIP (N=31, k=2, I=5)\nD-iSLIP (N=31, k=2, I=5)\nλ-RDSRR (N=31, k=2, I=5)\nλ-2DRR (N=31, k=2)\nFig. 5. Delay performance of the different algorithms under uniform traffic\nwhen either 1-legal or 2-legal permutations are allowed.\nFig. 5 shows the delay performances for a N = 31\nports switch when either 1-legal (black-filled markers) or 2-\nlegal (white-filled markers) permutations are allowed. The\nrelaxation of the k-legal constraint reduces delays since it\neasier for the scheduler to find an allowed matching. The λ-\nRDSRR is the algorithms whose performance improves the\nmost passing from k = 1 to k = 2, since it becomes easier\nto find input-output permutations which do not belong strictly\nto the anti-diagonals. Indeed, the λ-RDSRR pointer updating\nrule is fixed and does not depend on the packets forwarded by\nthe switch. C-iSLIP performance improvement is remarkable\ntoo, since it is not able to achieve 100% when k = 1, while\nit ensures 100% of throughput when k = 2. The D-iSLIP and\nthe λ-2DRR schedulers proved themselves less sensitive to the\nk-legal constraint, indeed, only a marginal improvement in the\ndelay can be observed.\nV. CONCLUSIONS\nWe presented four heuristics based on maximal size algo-\nrithms solutions to control optical backplanes and we evaluated\ntheir performance. All the proposed algorithms are simple\nenough to be easily implemented in hardware and, in par-\nticular, the D-iSLIP algorithm achieves the best performance\nin term of throughput and delay. Thus, even though sub-\noptimal, the proposed algorithms seem to be good candidates\nto schedule packets in future optical switching fabrics. Finally,\nthe proposed heuristics allow to drastically overcome the in-\nband crosstalk limitation to scalability of AWG-based switch-\ning fabrics, without worsening performance neither increasing\nsignificantly the scheduler complexity; thus, making AWG a\nviable solution to build future all-optical switching fabrics.\nACKNOWLEDGMENT\nSpecial thanks are due to David Hay for helpful discussions\nand his valuable insights.\nThis work was partially supported by the BONE project, a\nNetwork of Excellence funded by the European Commission\nwithin the 7th Framework Programme, and by the PHOBOS\n09TIC014CT grant (Xunta de Galicia, Spain).\nREFERENCES\n[1] E. Bonetto, L. Chiaraviglio, D. Cuda, G. Gavilanes, and F. Neri, “Optical\ntechnologies can improve the energy efficiency of networks,” in 35th\nEuropean Conference on Optical Communication, Vienna, Austria, 2009.\n[2] J. Gripp, M. Duelk, J. Simsarian, A. Bhardwaj, P. Bernasconi,\nO. Laznicka, and M. Zirngibl, “Optical switch fabrics for ultra-high-\ncapacity IP routers,” Journal of Lightwave Technology, vol. 21, no. 11,\npp. 2839–2850, Nov 2003.\n[3] J. M. Finochietto, R. Gaudino, G. A. Gavilanes, , and F. Neri, “Simple\noptical fabrics for scalable terabit packet switches,” in IEEE ICC, Beijing,\nChina, 2008.\n[4] H. Takahashi, K. Oda, and H. Toba, “Impact of crosstalk in an arrayed-\nwaveguide multiplexer on N × N optical interconnection,” Journal of\nLightwave Technology, vol. 14, no. 6, pp. 1097–1105, Jun 1996.\n[5] M. Rodelgo-Lacruz, C. Lo´pez-Bravo, F. J. G.-C. no, and H. J. Chao,\n“Practical Scalability of wavelength routing switches,” in IEEE ICC,\nDresden, Germany, 2009.\n[6] A. Bianco, D. Hay, and F. Neri, “Crosstalk-Preventing scheduling in\nAWG-Based Cell Switches,” in IEEE Globecom, Honolulu, Hawaii, USA,\n2009.\n[7] R. . LaMaire and D. N. Serpanos, “Two-dimensional round-robin sched-\nulers for packet switches with multiple input queues,” IEEE/ACM Trans-\nactions on Networking, vol. 2, no. 5, pp. 471–482, Oct 1994.\n[8] N. McKeown, “iSLIP: A scheduling algorithm for input-queued\nswitches,” IEEE/ACM Transactions on Networking, vol. 7, no. 2, pp. 188–\n201, Apr 1999.\n[9] Y. Jiang and M. Hamdi, “A fully desynchronized round-robin matching\nscheduler for a VOQ packet switch architecture,” in IEEE workshop on\nhigh performance switching and routing, Paris, France, Jun 2001, pp.\n407–412.\n978-1-4244-5638-3/10/$26.00 ©2010 IEEE\nThis full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE Globecom 2010 proceedings.\n",
            "id": 4825989,
            "identifiers": [
                {
                    "identifier": "234878760",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1109/glocom.2009.5425775",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:iris.polito.it:11583/2353600",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "11415145",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:porto.polito.it:2353600",
                    "type": "OAI_ID"
                }
            ],
            "title": "Crosstalk-Preventing Scheduling in AWG-Based Cell Switches",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:iris.polito.it:11583/2353600",
                "oai:porto.polito.it:2353600"
            ],
            "publishedDate": "2009-01-01T00:00:00",
            "publisher": "IEEE",
            "pubmedId": null,
            "references": [
                {
                    "id": 6458420,
                    "title": "A fully desynchronized round-robin matching scheduler for a VOQ packet switch architecture,” in IEEE workshop on high performance switching and routing,",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1109/hpsr.2001.923670",
                    "raw": "Y. Jiang and M. Hamdi, “A fully desynchronized round-robin matching scheduler for a VOQ packet switch architecture,” in IEEE workshop on high performance switching and routing, Paris, France, Jun 2001, pp. 407–412. 978-1-4244-5638-3/10/$26.00 ©2010 IEEE This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE Globecom 2010 proceedings.",
                    "cites": null
                },
                {
                    "id": 6458399,
                    "title": "Crosstalk-Preventing scheduling in AWG-Based Cell Switches,”",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1109/glocom.2009.5425775",
                    "raw": "A. Bianco, D. Hay, and F. Neri, “Crosstalk-Preventing scheduling in AWG-Based Cell Switches,” in IEEE Globecom, Honolulu, Hawaii, USA, 2009.",
                    "cites": null
                },
                {
                    "id": 6458395,
                    "title": "Impact of crosstalk in an arrayedwaveguide multiplexer on",
                    "authors": [],
                    "date": "1996",
                    "doi": "10.1109/50.511611",
                    "raw": "H. Takahashi, K. Oda, and H. Toba, “Impact of crosstalk in an arrayedwaveguide multiplexer on N × N optical interconnection,” Journal of Lightwave Technology, vol. 14, no. 6, pp. 1097–1105, Jun 1996.",
                    "cites": null
                },
                {
                    "id": 6458417,
                    "title": "iSLIP: A scheduling algorithm for input-queued switches,”",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1109/90.769767",
                    "raw": "N. McKeown, “iSLIP: A scheduling algorithm for input-queued switches,” IEEE/ACM Transactions on Networking, vol. 7, no. 2, pp. 188– 201, Apr 1999.",
                    "cites": null
                },
                {
                    "id": 6458391,
                    "title": "Optical switch fabrics for ultra-highcapacity IP routers,”",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1109/jlt.2003.819150",
                    "raw": "J. Gripp, M. Duelk, J. Simsarian, A. Bhardwaj, P. Bernasconi, O. Laznicka, and M. Zirngibl, “Optical switch fabrics for ultra-highcapacity IP routers,” Journal of Lightwave Technology, vol. 21, no. 11, pp. 2839–2850, Nov 2003.",
                    "cites": null
                },
                {
                    "id": 6458389,
                    "title": "Optical technologies can improve the energy efﬁciency of networks,”",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1109/ecoc.2010.5621487",
                    "raw": "E. Bonetto, L. Chiaraviglio, D. Cuda, G. Gavilanes, and F. Neri, “Optical technologies can improve the energy efﬁciency of networks,” in 35th European Conference on Optical Communication, Vienna, Austria, 2009.",
                    "cites": null
                },
                {
                    "id": 6458397,
                    "title": "Practical Scalability of wavelength routing switches,”",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1109/icc.2009.5199357",
                    "raw": "M. Rodelgo-Lacruz, C. L´ opez-Bravo, F. J. G.-C. no, and H. J. Chao, “Practical Scalability of wavelength routing switches,” in IEEE ICC, Dresden, Germany, 2009.",
                    "cites": null
                },
                {
                    "id": 6458393,
                    "title": "Simple optical fabrics for scalable terabit packet switches,”",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1109/icc.2008.1000",
                    "raw": "J. M. Finochietto, R. Gaudino, G. A. Gavilanes, , and F. Neri, “Simple optical fabrics for scalable terabit packet switches,” in IEEE ICC, Beijing, China, 2008.",
                    "cites": null
                },
                {
                    "id": 6458401,
                    "title": "Two-dimensional round-robin schedulers for packet switches with multiple input queues,”",
                    "authors": [],
                    "date": "1994",
                    "doi": "10.1109/90.336324",
                    "raw": "R. . LaMaire and D. N. Serpanos, “Two-dimensional round-robin schedulers for packet switches with multiple input queues,” IEEE/ACM Transactions on Networking, vol. 2, no. 5, pp. 471–482, Oct 1994.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "https://iris.polito.it/retrieve/handle/11583/2353600/52646/globe_awg.pdf"
            ],
            "updatedDate": "2020-08-03T20:11:15",
            "yearPublished": 2009,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/pdf/11415145.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/11415145"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/11415145/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/11415145/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/4825989"
                }
            ]
        },
        {
            "acceptedDate": "2010-09-29T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Benso, Alfredo"
                },
                {
                    "name": "Di Carlo, Stefano"
                },
                {
                    "name": "Politano, Gianfranco Michele Maria"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/11415867",
                "https://api.core.ac.uk/v3/outputs/234879484",
                "https://api.core.ac.uk/v3/outputs/209888694"
            ],
            "createdDate": "2013-07-10T14:52:04",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 12601,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/12601",
                    "logo": "https://api.core.ac.uk/data-providers/12601/logo"
                },
                {
                    "id": 351,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/351",
                    "logo": "https://api.core.ac.uk/data-providers/351/logo"
                }
            ],
            "depositedDate": "2011-05-01T00:00:00",
            "abstract": "Despite great advances in discovering cancer molecular profiles, the proper application of microarray technology to routine clinical diagnostics is still a challenge. Current practices in the classification of microarrays' data show two main limitations: the reliability of the training data sets used to build the classifiers, and the classifiers' performances, especially when the sample to be classified does not belong to any of the available classes. In this case, state-of-the-art algorithms usually produce a high rate of false positives that, in real diagnostic applications, are unacceptable. To address this problem, this paper presents a new cDNA microarray data classification algorithm based on graph theory and is able to overcome most of the limitations of known classification methodologies. The classifier works by analyzing gene expression data organized in an innovative data structure based on graphs, where vertices correspond to genes and edges to gene expression relationships. To demonstrate the novelty of the proposed approach, the authors present an experimental performance comparison between the proposed classifier and several state-of-the-art classification algorithm",
            "documentType": "research",
            "doi": "10.1109/tcbb.2010.90",
            "downloadUrl": "https://core.ac.uk/download/pdf/11415867.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "A cDNA Microarray Gene Ex-\npression Data Classif ier for Clini-\ncal Diagnostics Based on Graph \nTheory\nAuthors: Benso A., Di Carlo S., Politano G,\nPublished in the IEEE/ACM Transactions on Computational Biology and Bioinformatics, Vol. 8, No. 3, \n2011, pp. 577-591.\nN.B. This is a copy of the ACCEPTED version of the manuscript. The final \nPUBLISHED manuscript is available on IEEE Xplore®:\nURL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5582075\nDOI: 10.1109/TCBB.2010.90\n© 2011 IEEE. Personal use of this material is permitted. Permission from IEEE must be \nobtained for all other uses, in any current or future media, including reprinting/republishing \nthis material for advertising or promotional purposes, creating new collective works, for resale \nor redistribution to servers or lists, or reuse of any copyrighted component of this work in \nother works.\n!Politecnico di Torino\n1A cDNA Microarray Gene Expression Data\nClassifier for Clinical Diagnostics based on\nGraph Theory\nAlfredo Benso, IEEE Senior Member, Stefano Di Carlo, IEEE Member and Gianfranco Politano\nAbstract—Despite great advances in discovering cancer molecular profiles, the proper application of microarray technology to routine\nclinical diagnostics is still a challenge. Current practices in the classification of microarrays’ data show two main limitations: the reliability\nof the training data sets used to build the classifiers, and the classifiers’ performances, especially when the sample to be classified does\nnot belong to any of the available classes. In this case, state-of-the-art algorithms usually produce a high rate of false positives that, in\nreal diagnostic applications, are unacceptable. To address this problem, this paper presents a new cDNA microarray data classification\nalgorithm based on graph theory and able to overcome most of the limitations of known classification methodologies. The classifier\nworks by analyzing gene expression data organized in an innovative data structure based on graphs, where vertices correspond to\ngenes and edges to gene expression relationships. To demonstrate the novelty of the proposed approach, the authors present an\nexperimental performance comparison between the proposed classifier and several state-of-the-art classification algorithms.\nIndex Terms—microarray, gene expression, classification, clinical diagnostics, graph theory\nF\n1 INTRODUCTION\nDNA microarrays are one of the fastest-growing tech-\nnologies for genetic research. They are small solid sup-\nports, e.g., membranes or glass slides, on which se-\nquences of DNA are fixed in an orderly arrangement.\nTens of thousands of DNA probes can be attached to a\nsingle slide and used to analyze and measure the activity\nof genes. Scientists are using DNA microarrays to inves-\ntigate several phenomena (e.g., cancer, pest control, etc.)\nby measuring changes in gene expression and thereby\nlearning how cells respond to a disease or to a partic-\nular treatment [1], [2]. Even if microarrays represent a\npowerful source of biological information, using gene\nexpression data to classify diseases on a molecular level\nfor clinical diagnostic remains a challenging research\nproblem. It involves assessing gene expression levels\nfrom different experiments, determining genes whose\nexpression is relevant (feature extraction) and then apply-\ning accurate and readily interpretable classification rules\nproviding biological insight of the target phenomenon\n(classification) [3].\nClassifying microarray data poses several challenges\nto typical machine learning methods. In particular, mi-\ncroarray classification faces the “small N, large P” prob-\nlem of statistical learning, where the number P of vari-\nables (gene expressions) is typically much larger than the\nnumber N of available samples. This disparity impacts\nthe major aspects of the classifier design: the classifica-\ntion rule, the error estimation and the feature selection.\nMoreover, when considering clinical diagnostic, one of\nA. Benso, S. Di Carlo, and G. Politano are with the Department of Control\nand Computer Engineering of Politecnico di Torino, Corso Duca degli Abruzzi\n24, Torino, Italy. e-mail: {firstname.lastname}@polito.it\nthe main problems of traditional machine-learning tech-\nniques concerns the ability of properly detecting false\npositives, i.e., samples erroneously assigned to a class\neven if they do not belong to the class library used to\ntrain the classifier. This misbehavior is clearly unaccept-\nable since it would very likely lead to a misdiagnosis.\nThis paper tries to leverage these problems by pre-\nsenting a new classification algorithm based on a graph-\nbased data structure, called Gene Expression Graph (GEG),\nused to represent gene expression data. GEGs clearly\nexpress relationships among expressed and silenced\ngenes in different conditions (e.g., healthy vs. diseased).\nThey are therefore specifically suited for the analysis of\ncomplementary DNA (cDNA) microarrays that provide\non a single support information for both healthy and\ndiseased specimens. The concept of using a graph-based\ndata structure to represent gene expression data, first\npublished by the authors in [4] and in [5], allows to\nbuild a classifier based on a topological comparison\nbetween graphs representing known classes, and graphs\nrepresenting samples to analyze.\nOne of the main contributions of the classifier stems\nin the ability of combining in a single algorithm high\naccuracy in the classification process together with the\nability of detecting samples not belonging to any of\nthe trained classes, thus drastically reducing the number\nof false positive classification outcomes. To validate the\nefficiency of the proposed approach, the paper presents\nan experimental comparison between the GEG-based\nclassifier and several generic state-of-the-art multi-class\nand one-class classification methods on a set of cDNA\nmicroarray experiments for fifteen well known and doc-\numented diseases. Experimental results show that the\nGEG-based classifier is able to reach the same perfor-\n2mances reached by multi-class classifiers when dealing\nwith samples belonging to the considered class library,\nwhile it outperforms one-class classifiers in the ability\nof detecting samples not belonging to any of the trained\nclasses.\nThe paper is organized as follows: section 2 describes\nthe proposed classification approach, and section 3 pro-\nposes an overview of existing classification methods in\norder to highlight the main differences with the pro-\nposed approach. Section 4 presents the experimental\nresults, and section 5 concludes the paper suggesting\nfuture developments of this work.\n2 METHODOLOGY\nClassifying diseases based on DNA Microarray gene\nexpression information usually requires:\n1) signal pre-processing of raw microarray scans,\n2) data modeling,\n3) prediction (e.g., classification), and\n4) validation.\nThe signal pre-processing stage elaborates the raw image\nobtained from the scanning process of a microarray (sam-\nple) in order to calculate the expression level of each DNA\nprobe. It compensates for any kind of acquisition errors,\nhardware damages (scratches, dust, etc.) and procedural\nissues (stains, drops, spots out of focus, etc). While this\nstep is out of the scope of this paper, the next subsec-\ntions will focus on the problem of efficiently modelling\nmicroarray data, and on the definition of an efficient\nprediction algorithm for classification of diseases.\n2.1 Data modelling\nThe result of the scanning process and signal processing\nof a microarray containing probes (spots) for P different\ngenes is a gene expression profile:\n~s = (expr1, expr2, . . . , exprP ) (1)\nwith exprj representing the gene expression level of the\njth gene (gj) of the sample (j 2 [1, P ] ⇢ N).\nDepending on the microarray technology exprj may\nidentify an absolute expression level or a set of ex-\npression levels measured in different conditions. cDNA\nmicroarrays, the main target of this paper, provide for\neach spot two expression levels using two fluorescence\nintensities: the first labeled Cy5 producing a red flu-\norescence associated with a diseased condition, and\nthe second labeled Cy3 producing a green fluorescence\nassociated with a healthy condition. We can therefore\nformally define exprj as an index function:\nexprj : {Cy5, Cy3}! R (2)\nand denote with ~s [j] [Cy5] and ~s [j] [Cy3] the two expres-\nsion levels of gene gj in sample ~s.\nGene expression profiles obtained from N samples\nare usually organized in the form of a matrix called\nGene Expression Matrix (GEM) [6] where the ith row\n(i 2 [1, N ] ⇢ N) represents the gene expression profile of\nthe ith sample of the considered set. The GEM obtained\nfrom the scanning process of a set of microarrays is a raw\ndata-set containing noise, missing values and system-\natic variations arising from the experimental procedure.\nTechniques such as the one proposed in [7] are widely\napplied to mitigate the effect of these problems and\nto improve the overall data quality. GEMs are a data\nmodel widely used to build classification and prediction\nalgorithms. In this paper we consider a multi-class classi-\nfication problem. A classifier or predictor C forK classes,\neach one representing a disease, is a map from the space\nS of all possible gene expression profiles into a set Y of\nK classes:\nC : S ! Y = {y1, y2, . . . , yK} (3)\nbuilt over a training set\nTr = {t1 = (~s1, C (~s1)) , . . . , tT = ( ~sT , C ( ~sT ))} (4)\nof T previously labeled samples. Since classification\nmethods rely on gene expression profiles to perform pre-\ndictions, several pre-processing steps such as between-\nmicroarray normalization are usually applied to make\nquantitative comparison of two or more microarrays\npossible [8], [9]. Each of these steps may strongly impact\nthe efficiency and accuracy of the classifier. Moreover,\nthey require rebuilding the overall data model every\ntime new samples have to be analyzed, or added to\nthe training set to enhance the learning capability of the\nclassifier. This operation is computationally expensive,\nand might be not affordable if performance is one of the\ntarget requirements.\nTo cope with these problems we propose a new graph\nbased data model for groups of gene expression profiles,\nbuilt on raw gene expression measures. The model is\nconstructed in order to allow efficient classification and\nto avoid the influence of pre-processing steps on the\nprediction process.\nTo better explain the proposed approach, let us split\nthe training set Tr into K sub-sets:\nTr = {Tr1, T r2, . . . , T rK} (5)\nwhere each sub-set Tri =\n{( ~sx, C ( ~sx)) 2 Tr | C ( ~sx) = yi} characterizes a separate\nclass yi and therefore groups samples of individuals\naffected by the same disease.\nEach sub-set Tri can be modeled by a non-oriented\nweighted graph (Gene Expression Graph) GEGi =\n(Vi, Ei) where:\n• each vertex vx 2 Vi, with x 2 [1, P ] ⇢ N represents\ngene gx of samples belonging to Tri, labeled using\nits UnigeneID [10]. Only vertexes representing rele-\nvant genes are included in the graph (the concept of\ngene relevance will be discussed in section 2.1.1);\n3• each edge (u, v) 2 Ei ✓ Vi ⇥ Vi connects pairs\nof vertexes representing genes that are co-relevant,\ni.e., concurrently relevant, within a single sample\n~sx | ( ~sx, C ( ~sx)) 2 Tri. It therefore models re-\nlationships among relevant genes of a sample. If\nn genes are co-relevant in the same sample, each\ncorresponding vertex will be connected with an\nedge to the remaining n \u0000 1 ones, thus creating a\nclique. This structural property is very important\nto identify features connected to each training sub-\nset Tri and thus to construct an efficient classifier.\nThis consideration is based on the hypothesis that,\nwhen considering a statistically significant number\nof experiments, genes that are co-relevant within the\nsame sample are likely to have a biological meaning\nin characterizing the target disease (e.g., [11]);\n• the weight wu,v of each edge (u, v) 2 Ei corresponds\nto the number of times genes u and v are co-relevant\nin the same sample over the set of samples compos-\ning Tri. In a graph built over a single experiment,\neach edge will be weighted as 1. Adding additional\nmicroarrays will modify the graph by introducing\nadditional edges and/or by modifying the weight\nof existing ones.\n2.1.1 Relevant genes selection\nThe identification of relevant genes is a key factor to\nbuild gene expression graphs. Depending on the experi-\nmental setup and on the specific microarray technology,\ndifferent strategies can be exploited. In [4] we adopted\nthe absolute gene expression level of diseased tissues.\nThis approach presents two drawbacks: (i) it requires the\nidentification of a threshold representing a Boolean cut-\noff between relevant and not relevant genes and (ii) it\ndoes not allow to model both expressed and silenced\ngenes.\nSeveral studies on cDNA microarrays have shown\nthat it is possible to identify relevant genes for a target\ndisease by searching for spots that are differentially-\nexpressed between healthy and diseased conditions [12].\nThe predominance of the Cy3 or Cy5 component of\na spot indicates the abundance of the corresponding\nDNA sequence, allowing us to introduce the concept\nof over-expressed or silenced genes. On the other hand,\nequal intensities indicate no peculiar information for the\ncorresponding gene. Based on this concept it is possible\nto define different metrics to evaluate differential ex-\npressions and to identify relevant genes for constructing\nGEGs.\nIn [5] we considered the use of a linear difference\nbetween diseased and healthy gene expressions. De-\nspite greatly improving the data model quality w.r.t. [4]\nand allowing a significant noise reduction, this process\nsuffered again from the need of defining a differential\nexpression threshold to distinguish between relevant and\nnon relevant genes. Being linear differences of gene\nexpressions of a single sample not heavy-tailed dis-\ntributed, it was impossible to define the threshold based\non a strong mathematical model. Moreover, considering\ngenes with equal differential-expression, it was impossi-\nble to discriminate between genes with high Cy5 and\nCy3 components from genes with low Cy5 and Cy3\ncomponents. Loosing this information may reduce the\nefficiency of the classification process.\nSeveral publications on microarray data suggest to use\nthe (binary) logarithm of the ratio Cy5/Cy3 (log-ratio) to\nmeasure differential-expression of genes. The use of the\nlog-ratio seems to be a better solution to identify rele-\nvant genes for GEGs. It takes into account the absolute\nintensity of the two channels thus overcoming one of the\nproblems of the linear difference. Moreover, log-ratios of\na microarray tend to be Normally distributed [13], thus\nallowing to build a more rigorous mathematical model to\nbetter identify relevant genes. Relevant and non-relevant\ngenes of a sample can be therefore selected considering\nwhether the corresponding log-ratio is null (non-relevant\ngene) or not (relevant gene). This simple rule can be\nfurther refined by considering the sign of the log-ratio.\nGenes that exhibit a positive log-ratio are up-regulated\nin the diseased condition (Cy5 component) compared to\nthe healthy one (Cy3) identifying over-expressed relevant\ngenes. On the other hand, genes with a negative log-ratio\nare down-regulated in the diseased condition w.r.t. the\nhealthy one, identifying silenced relevant genes.\nWhen applying this rule, one has to consider that\nexperimental conditions may introduce systematic biases\non the experimental data able to shift or scale expres-\nsion levels of a sample. To compensate these problems\nand to allow comparison of different samples, within-\nmicroarray normalization should be applied. Among the\ndifferent methods proposed in literature, we applied\nhere the standard score (z score) normalization [14]. The\nstandard score transformation expresses log-ratios for\nindividual genes of a sample as a unit of the standard\ndeviation from the normalized mean of zero:\nz(Cy5, Cy3, µ,\u0000) =\nlog2\nCy5\nCy3 \u0000 µ\n\u0000\n(6)\nwhere µ and \u0000 denote the mean and the standard\ndeviation of log-ratios of all genes within the consid-\nered sample. Correction is done before sample-to-sample\ncomparison and is therefore comparison-independent.\nFinally, considering a sharp cut-off between over-\nexpressed and silenced genes limits the ability of iden-\ntifying non-relevant genes. In fact, the amount of genes\nthat will exhibit a perfectly null standard score will be\nusually really small. A threshold \" could be therefore\nintroduced to enlarge the non-relevance area. Different\napproaches can be considered to define this threshold. It\ncan be connected to the intrinsic error introduced when\nmeasuring expression levels of genes, or it can be defined\nbased on well established methods to identify differen-\ntially expressed genes such as fold-change, or t-test [15].\nEven if this approach still requires the introduction of a\nthreshold, we will show that its influence on the accuracy\nof the proposed classifier is negligible (see section 4).\n4In a formal way, by indicating with ER ⇢ R (Ex-\npression Range) the full scale range of Cy3 and Cy5 for\nthe target technology, the relevance can be defined as\na function that maps these two components into one\nof three possible values indicating: (1) over-expressed\nrelevant genes, (-1) silenced relevant genes and (0) non-\nrelevant genes:\nRel\",µ,\u0000 : ER⇥ ER! {0, 1,\u00001} (7)\nRel\",µ,\u0000(Cy5, Cy3)!\n8><>:\n1 z(Cy5, Cy3, µ,\u0000) > \"\n0 \u0000\"  z(Cy5, Cy3, µ,\u0000)  \"\n\u00001 z(Cy5, Cy3, µ,\u0000) < \"\n(8)\nBased on this definition, each node vx 2 Vi of GEGi\nassociated with gene gx can be additionally labeled with\na so called Cumulative Relevance Count:\nCRCx =\nX\n8j|(~sj ,C(~sj))2Tri\nRel\",µ,\u0000 (~sj [x] [Cy5] ,~sj [x] [Cy3])\n(9)\nA node with positive CRC identifies a gene over-\nexpressed in the majority of the samples, while a node\nwith negative CRC identifies a gene silenced in the\nmajority of the samples.\nIn a similar way, the weight wu,v of each edge (u, v) 2\nEi of GEGi can be formally expressed as:\nwu,v =\nX\n8j|(~sj ,C(~sj))2Tri\n(|Rel\",µ,\u0000 (~sj [u] [Cy5] ,~sj [u] [Cy3])|^\n|Rel\",µ,\u0000 (~sj [v] [Cy5] ,~sj [v] [Cy3])|)\n(10)\nwhere ^ denotes the logical and operator returning\n1 if both its operands are equal to 1, while |·| denotes\nthe absolute value of the related relevance value. Each\nsample will therefore provide a unitary contribution to\nwu,v if both gu and gv will exhibit a relevance (in absolute\nvalue) equal to 1.\nFig. 1 shows an example of GEG construction from a\nset Tr of six samples (Fig. 1-a). Each sample includes 4\ngenes and, for each gene, the Cy5 and Cy3 components\nare provided. Fig. 1-b shows the log-ratio calculated\nfor each gene in each sample and the indication of\nover-expressed relevant genes, silenced relevant genes\nand non-relevant genes. Relevant genes are identified\naccording to eq. 8 with a threshold \" = 1 (two-fold\n[15]). Starting from these values, Fig. 1-c shows the\ncorresponding GEG where each vertex corresponds to a\ngene that is relevant in at least one experiment. To give\nan example of how to compute the CRC for each vertex,\nand the weight of each arc, let us look in more details\nat vertexes C and D. Looking at the log-ratio table, one\ncan see that gene C is over-expressed in 2 experiments\n(Exp. 1, and 5), silenced in two experiments (Exp. 3,\nand 4), and non-relevant in two experiments (Exp. 2,\nand 6). The CRC of node C in the GEG is therefore\nCRCC = 2 \u0000 2 = 0. Gene D, instead, is silenced in 3\nexperiments, and over-expressed in 2 experiments: its\nCRC is therefore -1. To compute the weight of the edge\n(C,D) it is enough to count the number of experiments\nin which both genes are relevant (this time without\ntaking into account the sign). They are experiments 1,\n4, and 5; the weight wC,D is therefore 3.\nGEM =\nExp/Gene\nExp.1\nExp.2\nExp.3\nExp.4\nExp.5\nExp.6\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\nA(Cy5, Cy3) B(Cy5, Cy3) C(Cy5, Cy3) D(Cy5, Cy3)\n5000, 10 20, 11100 15000, 80 90, 13000\n8000, 20 20, 12000 1000, 1050 100, 12000\n10000, 10099 30, 30000 11000, 30 40, 1900\n1200, 20 15, 10 10, 100 8000, 50\n5000, 100 20, 4500 10500, 30 12500, 70\n7000, 15 70, 5500 10100, 10050 40, 12500\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\nExp/Gene\nExp.1\nExp.2\nExp.3\nExp.4\nExp.5\nExp.6\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\nA B C D\n8.97 \u00009.12 7.55 \u00007.17\n8.64 \u00009.23 \u00000.07 \u00006.91\n\u00000.02 \u00009.97 8.52 \u00005.57\n5.90 0.58 \u00003.32 7.32\n5.64 \u00007.82 8.45 7.48\n8.87 \u00006.30 0.01 \u00008.29\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n(b) Log-ratios matrix and relevance with ε=1. \n(a) Initial training set expression levels represented as a gene expression matrix\nA\nCRC:\n5\nB\nCRC:\n-5\nC\nCRC:\n0\nD\nCRC:\n-1\n4\n3\n4\n3\n3\n4\n(c) Gene expression graphs.Silenced nodes indicate nodes with negative CRC, i.e., nodes silenced in \nthe majority of the samples, while over expressed node represent nodes with positive CRC, i.e., \nnodes over-expressed in the majority of the samples.\nover-expressed\nsilenced\nover-expressed\nsilenced\nμ=0.05 σ=9.52\nμ=-1.89 σ=8.02\nμ=-6.02 σ=4.39\nμ=2.62 σ=4.90\nμ=3.43 σ=7.59\nμ=-1.42 σ=7.72\nFigure 1. GEG construction example starting from the initial set of\ngene expression profiles, to the final graph construction\nIf new samples become available from new experi-\nments referring to the same pathology, the related infor-\nmation can be easily added to the corresponding GEG\nwithout any additional memory requirement. GEGs’\nmemory occupation is in fact determined by the number\nof considered genes, only, and is independent of the\nnumber of experiments in the data-set.\n2.2 Classification\nGene Expression Graphs represent an excellent data\nstructure for building efficient classifiers. The classifier\npresented in this paper works by structurally compar-\ning pairs of GEGs: one representing a given pathology\n(GEGpat), built from a corresponding training set Trpat\n(eq. 4), and one representing the sample ~s to classify\n(GEGs). This comparison measures how much GEGs\nis similar (or can be overlapped) to GEGpat in terms\nof over-expressed/silenced genes (CRC of vertexes) and\nrelationships among gene expressions (weight of edges).\nThe result of this operation is a proximity score (Ps 2\n[\u00001, 1] ⇢ R), computed according to eq. 11, measuring\nthe similarity between the two graphs.\n5Ps(GEGpat, GEGs) =\nSMS(GEGpat, GEGs)\nMMS(GEGpat)\n(11)\nSMS (sample matching score) analyzes the similarity\nof GEGpat and GEGs considering those vertexes (genes)\nappearing in both graphs, only. SMS is computed as:\nSMS(GEGpat, GEGs) =\nX\n8(i,j)2Es\\Epat\n(12)✓\nZi · wi,j · |Zi||Zi|+ |Zj |\n◆\n+\n✓\nZj · wi,j · |Zj ||Zi|+ |Zj |\n◆\nwhere (i, j) are edges appearing in both GEGs and\nGEGpat, while the term Zx (Z-term) for vertex vx is\ncomputed as:\nZx = CRCxpat · CRCxs (13)\nBy construction, each vertex vx of a GEG has CRCx <\n0 if gx is silenced in the majority of the samples of its\ntraining set, CRCx = 0 if gx is actually not relevant in\nits training set, or CRCx > 0 if gx is over-expressed in\nthe majority of the samples of its training set. Zx may\ntherefore assume the following values:\n• Zx > 0: if gx is silenced/over-expressed in both\nGEGs and GEGpat;\n• Zx < 0: if gx is silenced in GEGs and over-expressed\nin GEGpat, or viceversa;\n• Zx = 0: if gx is not relevant either in GEGs, or in\nGEGpat.\nThe purpose of this term is to quantify to what extent the\nexpression of gx in ~s is \"similar\" to the expression of the\nsame gene in Trpat. The more genes have a positive Z-\nterm, the higher will be the similarity and therefore the\nSMS of the sample w.r.t. the considered GEGpat. The two\nterms of eq. 12 are the Z-term of the two genes connected\nby the considered edge ((i, j)), each one multiplied by\na portion of the weight of the edge. This portion is\ncomputed as the percentage of the Z-term of the gene\nover the total Z-term of the pair.\nMMS (maximum matching score) is the maximum\nSMS that would be obtained with all genes in GEGs\nperfectly matching all genes in GEGpat, with Z-term of\neach gene always positive. It can be therefore computed\nas:\nMMS(GEGpat) =\nX\n8(i,j)2Epat\n \nwi,j ·\nCRC2i + CRC2j\n|CRCi|+ |CRCj |\n!\n(14)\nThe proximity score is used as a measure of ~s to belong\nto the class (disease) characterized by GEGpat. Posi-\ntive values indicate the existence of similarities between\nthe two graphs, with higher scores indicating higher\nsimilarity between the sample and the class. Negative\nvalues indicate opposite structural information between\nthe two graphs, thus identifying high dissimilarity be-\ntween the sample and the class. Considering the multi-\nclass classification problem of eq. 3 and given a set of\nK graphs, GEG1, GEG2, . . . , GEGK , with GEGi built\nusing the sub-set Tri ⇢ Tr (eq. 5) and characterizing a\ngiven disease, we can compute a proximity vector for a\nsample ~s as:\n~PVs = (Ps(GEG1, GEGs), . . . , Ps(GEGK , GEGs))\n(15)\nwhere ~PVs [i], with i 2 [1,K] represents the proximity\nscore of sample ~s with class yi, i.e., the measure of the\naffinity of ~s with the class yi.\nIt is important to highlight that the computation of\neach proximity score composing ~PVs is independent of\nthe number of classes considered in the problem, making\nit an absolute indicator of the similarity of a sample with\na given class. This is in contrast with several state-of-the-\nart classifiers (see section 3) where proximity measures\noften depend on the number of classes,and need to\nbe interpreted in relation with the measure of all the\nconsidered classes. This forces to completely rebuild the\nprediction model every time new classes are included in\nthe classification process.\n2.2.1 Classifier’s decision rule\nGiven the proximity vector of a sample ~s, the definition\nof the classifier C (eq. 3) passes through the definition of a\ndecision rule able to predict the correct class. In statistical\nclassification a decision rule uses information from a\nsample to decide between two or more hypotheses.\nState-of-the-art classifiers (see section 3) usually apply\nthe maximum proximity rule to identify the predicted\nclass. In our case this would mean choosing, as best\nprediction for a sample ~s, the class with the highest Ps:\nC(~s) = y\nargmax\ni\n( ~PVs[i]|8i2[1,K]) (16)\nThis rule is really effective when working on samples\nthat are known to belong to the available class library.\nNevertheless, when performing predictions for clinical\ndiagnostic, two situations should be considered: (i) the\nsample actually belongs to one of the classes the classifier\nhas been trained for (classifiable sample), or (ii) the\nsample does not belong to any class (out-of-class sample)\nbecause it is either a healthy sample or a sample showing\na disease not considered when the classifier was trained.\nThe classification problem should be therefore extended\nby introducing an out-of-class (yooc) condition as:\nC : S ! (Y = {y1, y2, . . . , yK})[(yooc = (y1 ^ y2 ^ . . . ^ yK))\n(17)\nwhere the notation yi stands for “not in class yi” and\n^ denotes the logical and.\nGiven this new classification problem, samples can be\ngrouped based on the result of the classification into four\ngroups:\n6• True Positives: classifiable samples classified in one of\nthe available classes yi 2 Y . This includes samples of\nclass yi 2 Y classified in the correct class (matches),\nand samples of class yi 2 Y classified in the wrong\nclass yj 2 Y , with yj 6= yi (mismatches).\n• True Negatives: out-of-class samples correctly classi-\nfied as yooc,\n• False Positives: out-of-class samples erroneously clas-\nsified as yi 2 Y , and\n• False Negatives: samples of class yi 2 Y erroneously\nclassified yooc.\nThese definitions will be used in the remaining of the pa-\nper to analyze the capability of the classifier to both iden-\ntify the correct class for classifiable samples (matches\nvs. mismatches), and distinguish between classifiable\nand out-of-class samples. In particular, failing to identify\nout-of-class samples creates a very high rate of false\npositives, dramatically reducing the applicability of the\nclassifier to a real diagnostic scenario where both false\npositives and false negatives has to be lowered as much\nas possible, if not completely removed.\nThe maximum probability rule proposed in eq. 16\nfails when trying to identify out-of-class samples. To\novercome this problem we propose an improved deci-\nsion rule based on the analisys of how proximity scores\ndistribute between classifiable and out-of class samples.\nLet us consider the training set Tr used to train the clas-\nsifier. A set of cross-validation experiments to analyze\nhow proximity scores distribute between classifiable and\nout-of-class samples can be setup as follows. K subsets\nof experiments are generated by removing one of the\navailable classes yi (i 2 [1,K]) from the training library\nand using the corresponding samples Tri as out-of-class\nsamples to classify. For each subset, several folds can\nbe generated by removing k samples from the training\nsets of each class of the considered library and k samples\nfrom the out-of-class set. Each fold will therefore contain\na test-set of k · (K \u0000 1) classifiable samples, and k out-\nof-class samples. The probability density function of the\nproximity scores for the given classification problem can\nbe finally estimated by performing a kernel density esti-\nmation on the obtained results [16]. Fig. 2 shows the ker-\nnel density estimate of the proximity scores probability\ndensity function for the dataset introduced in section 4.1\nto validate our method, using a gaussian kernel. MAX\nshows the distribution of the highest proximity score of\neach sample for all classifiable samples (true positives -\nsolid line), and all out-of-class samples (false positives\n- dotted line), while DIFF shows the difference between\nthe values of the two highest Ps for each sample, again\nfor both classifiable and out-of-class samples.\nLooking at Fig. 2, we propose to identify three distinct\nareas: (i) the maximum proximity area, (ii) the decision\narea, and (iii) the out-of-class area, delimited by two\nthresholds Psmax and Psooc (Psmax > Psooc ). Based\non this partitioning the following decision rules (Fig. 3)\ncan be exploited to solve the classification problem of eq.\n17:\nPsooc=0.3 Psdiff=0.15Psmax=0.7\nFigure 2. Kernel density estimate of proximity scores probability\ndensity function for the GEG-based classifier. Estimation is performed\nusing a Gaussian kernel.\nR1: IF ( 66 9i 2 [1, K] | ~PVs [i] > Psooc)THEN C (~s) = yooc\nR2: IF\n“\n9i 2 [1, K] | ~PVs [i] > Psmax\n”\nTHEN\nC (~s) = yargmax\ni\n( ~PVs[i]|8i2[1,K])\nR3: IF\n“\nR1 ^R2 ^C\n”\nTHEN C (˜s) = yi\nC =\n„\n9i, j | i = argmax\nx\n“\n~PVs [x] | 8x 2 [1,K]\n”\n^\nj = argmax\nx\n“\n~PVs [x] | 8x 2 [1,K] ^ x 6= i\n”\n^“\n~PVs [i]\u0000 ~PVs [j] > Psdiff\n””\nR4: IF\n“\nR1 ^ R2 ^ R3\n”\nTHEN C (~s) = uncertain\nFigure 3. Decision rules for a diagnostic classifier\n• R1 (maximize true negatives): to be able to pre-\ndict a class for a sample ~s at least one class yi\n(i 2 [1,K]) should exhibit a proximity score ~PVs [i]\nhigher Psooc. If this condition is not satisfied (out-\nof-class area), ~s is classified as yooc;\n• R2 (maximize true positives): if at least one class\nwith proximity scores higher than Psmax do exist\n(maximum probability area), the class with maxi-\nmum proximity score is predicted. This gives max-\nimum confidence to predictions with high score;\n• R3: in case neither R1 or R2 are satisfied the predic-\ntion should be identified among those classes with\nPs 2 [Psooc, Psmax] (decision area). In this case a\nprediction is provided if the two top ranked proxim-\nity scores differs at least of a minimum value Psdiff\nallowing to distinguish between the two classes. In\nfact this rule applies every time the prediction is not\ncertain, i.e., low proximity score. It avoids to provide\na result if the distinction between two classes is not\nsufficient to take a clear decision;\n• R4: whenever the first three rules cannot be applied\nthe prediction is considered uncertain and the clas-\nsifier does not produce any classification result. In\nalternative, multiple classification results can be also\nprovided to alert the user that the confidence in\nthe prediction is low, or the sample can be simply\ntreated as out-of-class.\nThe proposed decision rule tries to imitate a human\ncognitive process to perform predictions. It takes into\nconsideration both the absolute value of the proximity\nscores and their relative values. This property is very\nuseful for clinical diagnostics. In fact, in addition to the\ncapability of identifying out-of-class samples, it can also\n7provide important diagnostic information such as the\nidentification of genetic similarities with known diseases.\nThe three thresholds can be defined looking at the dis-\ntributions in Fig. 2:\n• Psmax: if the MAX plot shows a clear separation\nbetween the true and the false positive distributions,\nPsmax can be placed in such a way to have most\nof the true positives immediately detected by rule\nR2. Psmax defines the maximum probability area.\nLooking at Fig. 2, a good choice is Psmax t 0.7 . The\nmore the threshold is placed near 1.0 the less false\nnegatives will appear, but also the less true positives\nwill be detected using the maximum probability rule\n(R2);\n• Psooc: similarly to Psmax, looking at the MAX plot,\nPsooc can be defined in order to correctly identify\nfalse positives using rule R1, i.e., proximity score\nlower than the threshold. From Fig. 2 it is clear that\na good choice is Psooc t 0.3 ;\n• Psdiff : for all samples that fall in the decision area,\nthe DIFF graph can be used to define Psdiff . A\ngood heuristic is to consider the point where the\ntwo curves intersect. Fig. 2 shows that in general,\nand therefore also in the decision area, the two\ntop classes show very close Ps for false positives,\nonly (dotted line). In the case of true positives the\ndistinction is much higher (between 0.3, and 0.8).\nPsdiff t 0.15 maximizes, in this case, the number\nof true positives.\nWhenever new data are fit into the model these parame-\nters can be continuously tuned to improve the classifica-\ntion accuracy. The application of this decision rule allows\nus to better discriminate correct and uncertain results.\n3 STATE-OF-THE-ART\nMulti-class classification of gene expression profiles is\na widely addressed topic literature. However, the large\nand complex multivariate data-sets generated by mi-\ncroarray experiments still pose methodological and com-\nputational challenges. This section reviews a set of well\nestablished calssification methods applied in several pa-\npers to the classification of gene expression profiles and\nin particular cDNA microarray experiments.\nMicroarray classification can be treated as a regression\nproblem where the dependent variable is categorical, and\npredicted from a linear combination of gene expression\nlevels. Partial Least Squares (PLS) [17], [18] and Linear\nDiscriminant Analysis (LDA) [19], [20], [21] are common\nregression methods applied to cDNA microarray data\nclassification.\nThe k–nearest neighbors rule (KNN) [22] is used to\nclassify cancers based on gene expression data in several\npapers. KNN is an intuitive method that classifies unla-\nbeled samples based on their similarity with examples\nin the training set. It finds the k closest features in\nthe training set and assigns the sample to the class\nthat appears most frequently within the k-subset [18],\n[23], [24], [25]. The main limitations of KNN are three:\nstorage requirements, computational cost, and extreme\nsensitivity to gene expression data scaling.\nDecision trees [26] and Random Forests (RF) [27] are\npopular cancer classification approaches [28], [29], [30],\n[31]. According to Statnikov et al. [30], the capability of\nrandom forest to generate solutions from the interactivi-\nties of multiple decision trees allows to discard irrelevant\ngenes from the classification process.\nNeural Networks (NNET) are able to learn arbitrarily\ncomplex nonlinear regressions [32]. In particular, mul-\ntilayer perceptrons (MLPs) , are one the most popular\ntypes of artificial neural networks that have been applied\nto cDNA microarray classification problems [33], [34],\n[35], [36].\nSupport vector machines (SVM) are a popular classifica-\ntion algorithm because of their robustness and correct-\nness [37], [38]. Multicategory support vector machines\nare one of the most effective classifiers in performing\naccurate cancer diagnosis from gene expression data.\nSVMs often outperform to a remarkable degree other\npopular machine learning algorithms, such as KNN and\nNNET [39], [24], [40].\nApproaches that combine multiple classifiers (ensem-\nble) have also received much attention in the past decade,\nand are now a standard approach to improve classifi-\ncation performance in cDNA microarray classification\nproblems [41], [42], [43], [44], [45], [46], [47], [48].\nOne of the main drawbacks of the poposed methods\nis the need of high dimensionality reduction to avoid\nproblems associated with high-dimensional sparse data-\nsets as the one provided by microarray data. Some\nresearchers suggested that, in microarrays classification,\nthe choice of the dimensionality reduction method is\neven more important than the classification method\nitself [49]. Moreover, the proposed techniques are not\ndesigned to detect out-of-class samples, thus limiting\ntheir applicability in real clinical diagnostic applications.\nThe ability of detecting out-of-class samples, also re-\nferred to as novelty detection, outlier detection or one-\nclass classification, is an important aspect for a machine\nlearning system [50], [51]. Slight modifications in the\ndata distribution might indicate, for instance, a new class\nor a profile modification in a class that has already been\nmodeled. The term one-class refers to the fact that the\ntraining phase is performed on samples of a single class\nthat represents the normal profile.\nOne-class classification has been used in a number\nof applications, especially signal processing and image\nanalysis. Of the various approaches described in litera-\nture [50], Parzen Window, KNN, K-Means and PCA have\nbeen applied with some extent to cDNA microarray data\nclassification and gene expression analises [52], [53], [54],\n[55]. However, combination of multiple classes into a\nsingle one, high dimensionality feature spaces, noisy\nfeatures and quite often not enough samples make these\nproblems hard to solve, thus reducing the efficiency\nof available solutions and increasing computatational\n8costs. Mitigation techniques include dimensionality re-\nduction together with combination of different one-class\nclassifiers as proposed in [52]. However new dedicated\nsolutions for the specific application domain still need\nto be defined.\n4 EXPERIMENTAL RESULTS\nThis section proposes an experimental validation of the\nGEG-based classifier. The validation represents the last\nstep of the classifier definition process proposed in sec-\ntion 2.\n4.1 Experimental design\nOur experimental design involves a number of classifica-\ntion experiments on a large set of microarrays using both\nthe GEG-based classifier and a collection of state-of-the-\nart classification methods. The GEG-based classifier has\nbeen implemented in about 2,500 lines of ANSI C code.\nComparison with state-of-the-art classification methods\nhas been performed considering two distinct problems:\n1) accuracy of classifiers when working with classifi-\nable samples, and\n2) ability of dealing with out-of-class samples.\nObtained results are compared to better highlight the\nbenefits provided by the proposed methodology.\nA total of 15 pathologies is considered in this study.\nSamples have been downloaded from the cDNA Stan-\nford Microarray database [56]. All genes without a valid\nUnigeneID have been discarded. Moreover, since old\nmicroarray technologies often used spots duplication,\nduring the GEG generation we considered as relevant\nthose genes relevant in at least one of their replica on\nthe microarray.\nSix sets of samples have been downloaded form a\nlarger set of experiments aiming at performing Lym-\nphoma Classification [57], [58], including:\n• Diffuse Large B-Cell Lymphoma (DLBCL): a non-\nhodgkin lymphoma disease,\n• B-Cell Chronic Lymphocytic Leukemia Wait&Watch\n(CLLww),\n• B-Cell Chronic Lymphocytic Leukemia (CLL),\n• Follicular Lymphoma (FL): independent lymphon-\node samples on LymphoChip microarrays [59],\n• Hematopoietic Lymphoma (HL), and\n• Normal Lymphoid subset (NL): purified normal\nlymphocyte subpopulations under a range of acti-\nvation conditions, in normal human lymphonodes.\nThe remaining pathologies are:\n• Acute Lymphoblastic Leukemia (ALL),\n• Core Binding Factor Acute Myeloid Leukemia (CBF-\nAML): subgroups characterized by shorter overall\nsurvival [60],\n• Breast Cancer (BC): samples of predominantly ad-\nvanced primary breast tumor,\n• Cutaneous B-Cell Lymphomas (CBCL): a phenotype\nof B-cell lymphomas of the skin,\n• Healthy Blood (HB): blood samples from apparently\nhealthy human donors . A common reference RNA\n(Cy3-labeled) was mixed with the Cy5-labeled ex-\nperimental sample before hybridization to provide a\ncommon internal reference standard for comparison\nof relative gene expression levels across arrays [61],\n• Solid Ovarian tumor (SOT),\n• Solid Brain Tumor (SBT),\n• Solid Lung Tumor (SLT), and\n• Acute Myeloid Leukemia dataset (AML):\nperipheral-blood samples or bone marrow samples\nof intermediate-risk AML with a normal karyotype.\n11 out of 15 phenotypes are strictly related to blood dis-\neases. Samples will therefore include very similar sets of\ngenes and only a reduced subset of them will be able to\ndifferentiate the phenotypes. Moreover, two phenotypes\n(NL, and HB) are related to healthy specimens. These\npeculiarities of the data-set increase the complexity of\nthe classification and therefore make it a very good\ncandidate to validate the classifier.\nTable 1 shows the composition of the data-set. To\ntest the performance of our model with different cDNA\ntechnologies we considered different types of microarray\nchips: 9K (9,216 spots), 18K (18,432 spots), 24K (24,168\nspots), 37K (37,632 spots), 45K (43,196 spots).\nFrom the first 9 pathologies we extracted a training-set\nof 213 samples and a test-set of 74 classifiable samples,\nwhile the remaining 9 phenotypes have been used to\ncreate a test-set of 59 out-of-class samples. It is worth\nmentioning here that the considered training set does not\ninclude any of the samples of the test-sets. According to\n[62] this is a given requirement to avoid overoptimistic\nresults and therefore to honestly evaluate the classifica-\ntion performances.\nTable 1\nComposition of the experimental data-set in terms of number of test\nsamples (#Test), number of training samples (#Training), type of\nmicroarray (Chip)\nClassifiable samples Out-of-class samples\nDisease #Test #Train Chip Disease #Test Chip\nDLBCL 10 51 9k HL 10 9k\nCLLww 10 21 9k NL 9 18k\nCLL 9 12 9k SOT 11 24k\nALL 8 19 24k BT 12 24k\nCBF-AML 7 14 45k SLT 6 24k\nBC 8 21 9k AML 11 45k\nCBCL 6 10 45k\nFL 6 18 37k\nHB 10 47 37k\nTotal 74 213 59\n4.2 Proximity score analysis\nThe first analysis we performed on the results of the clas-\nsification experiments aims at understanding whether\nthe proximity score introduced in section 2.2 is actually\nable to measure the affinity of a sample with his related\n9DLBCLCLLww CLL ALL CBF-AML BC CBCL FL HB HL NL SOT BT SLT AML2\nDLBCL\nCLLww\nALL\nCLL\nCBF-AML\nBC\nCBCL\nFL\nHB\n(a) Average proximity score with ε=0 (1-folding)\nDLBCLCLLww CLL ALL CBF-AML BC CBCL FL HB HL NL SOT BT SLT AML2\nDLBCL\nCLLww\nALL\nCLL\nCBF-AML\nBC\nCBCL\nFL\nHB\n(a) Average proximity score with ε=1 (2-folding)\nPs ≥ 0.7 0.5 ≤Ps < 0.7 0.3 ≤Ps < 0.5 Ps < 0.3 \nMXIMUM PROXIMITY AREA DECISION AREA OUT OF CLASS AREA\nFigure 4. Average proximity score with different gene relevance thresholds for the considered data-set. Columns identify test samples grouped\nby diseases, while rows identify the considered classes. Bullets in the intersections of the grid represent the average proximity score of test samples\nof a given disease (column) for each of the available classes (rows).\nclass or not, and to highlight similarities among classes\n(diseases).\nFig. 4 graphically shows the average proximity score\nfor the test samples of each of the 15 considered diseases\n(columns) against the 9 considered classes (rows). The\ntwo subfigures report results obtained by using two\nthresholds for the identification of relevant genes (eq.\n8): \" = 0 (1-folding) for Fig. 4-a, and \" = 1 (2-folding) for\nFig. 4-b. Mismatchings are included in the average cal-\nculation. Different colors and shapes of bullets identify\ndifferent ranges of the proximity score strictly related to\nthe three decision areas used to predict the target class\n(see section 2.2).\nBy first looking at Fig. 4-a, the high score of the\nelements on the diagonal, together with the lowest scores\nof the other elements on the related columns, perfectly\nfollow the expected outcome: each disease is correctly\nidentified with a high proximity score in correspondence\nto its related class. In fact, in most of the cases, the\nscore falls in the maximum proximity area, thus allowing\nto have a prediction with maximum confidence. In the\nthree cases where the maximum proximity rule cannot\nbe applied (ALL, BC, CBCL) all the other classes have\nan average proximity score in the out-of-class area, thus\nguaranteeing a difference higher than the chosen Psdiff\nthreshold and therefore allowing to take a clear decision.\nTable 2 is another view of the same data in which it can\nbe better appreciated the level of discrimination of the\npresented approach and the overall low level of noise\namong non-matching sets. The only noticeable noise that\nappears is among NL and DLBCL, CLL and CLLww\n(Ps ⇡ 0.23), although the result itself is too low and\ndoes not have a sufficient discrimination w.r.t. the other\nscores to move the sample in one of the classes. It is also\nvery interesting to note that the classifier is correctly able\nto distinguish all out-of-class samples (HL, NL SOT, BT,\nSLT, AML) that have been in general scored very low or\neven with negative scores.\nIn addition to this result, Fig. 4-a also reveals how the\nproximity score is able to highlight similarities among\nclasses. The CLLww and CLL columns show a small\ncluster of elements with relatively high proximity scores.\nThis result can be reasonably interpreted as a sort of\nsimilarity among the pathologies confirmed by the fact\nthat CLL, CLLww, and DLBCL are actually variations\nof a disease of blood B-cells [63]. This information is\nvery interesting especially because it confirms how the\nproximity score can be used as an indicator of biological\nsimilarities of diseases. It also suggests that GEGs are\nable, by construction, to give more weight to genes\nand gene relationships that unequivocally identify a\nparticular pathology. Obviously this ability also depends\non the quality of the training set, but this is a common\nproblem to all supervised classification methods. Besides\nthis property the classifier is still able to correctly dis-\ncriminate among these similar classes (see section 4.3).\nLooking at Fig. 4-b it is possible to evaluate the effect\nof the threshold \" used to identify relevant genes on\nthe overall behavior of the classifier. The figure shows\nthat the proximity score of the elements on the diagonal\nincreases, thus increasing the confidence the classifier\nhas in placing samples in the related classes. Moreover,\nthe similarity among blood B-Cells is emphasized. Nev-\nertheless, even if not producing classification errors, the\nnoise on the NL column increases. This suggests that\nthe used threshold was too high and probably some\ngenes that were relevant to distinguish between NL and\nblood B-Cells diseases have been lost. This does not\nrepresent a major problem. In fact, the first threshold\n(\" = 0), corresponding to a situation in which genes are\nconsidered not-relevant if they have exactly the same ex-\npression in both the healthy and diseased tissue, already\nproduced 100% of correct classifications. The threshold\nshould be therefore considered simply as a fine tuning\nof the proposed model.\n10\nTable 2\nAverage proximity score computed with \" = 0. Columns identify test samples grouped by diseases, while rows identify the considered classes.\nDLBCL CLLww CLL ALL CBF-AML BC CBCL FL HB HL NL SOT BT SLT AML\nDLBCL 0.776 0.279 0.459 -0.005 0.011 0.003 0.042 0.009 0.036 0.041 0.266 0.001 0.002 0.001 -0.001\nCLLww 0.145 0.76 0.447 0.003 0.018 0.001 0.038 0.014 0.05 -0.006 0.159 0.001 0.003 0.001 -0.001\nCLL 0.243 0.483 0.771 0.001 0.018 0.001 0.037 0.016 0.051 -0.001 0.262 0.001 0.003 0.001 -0.001\nALL -0.001 0 0 0.656 0.14 -0.004 0.041 0.011 0.039 0.001 0 0.007 0.015 0.027 0.008\nCBF-AML 0 0.001 0.001 0.078 0.901 -0.001 0.042 0.08 0.109 -0.001 0 0.012 0.025 0.021 -0.039\nBC 0 0.002 0 -0.026 -0.019 0.53 0.046 -0.009 0.004 0.001 0 0.003 -0.005 -0.001 -0.002\nCBCL 0.001 0.003 0.002 0.02 0.053 0.002 0.679 0 0.035 0 0.001 0.005 0.007 0.005 -0.003\nFL 0.001 -0 0 0.003 0.09 -0.001 -0.008 0.701 0.068 0 0 0.021 0.052 0.031 -0.085\nHB 0.004 0.006 0.006 0.057 0.226 0 0.091 0.179 0.842 -0.001 0.003 0.009 0.017 0.011 -0.014\n4.3 GEG vs State-of-the-art Classifiers\nThis section compares the performance of the GEG-\nbased classifier with a set of state-of-the-art classifica-\ntion algorithms and one-class classifiers. In order to\ncompare classification results all experiments have been\nperformed using log-ratios (see section 2.1) rather than\nraw gene expression measures.\nThe analysis shows that our model has all the charac-\nteristics not only to be considered a new valid microarray\nclassification tool, but also a very useful support for\nmedical diagnostics.\n4.3.1 Accuracy on classifiable samples\nAccording to section 3, the accuracy of the GEG-based\nclassifier when dealing with classifiable samples has\nbeen compared with the following state-of-the-art classi-\nfication algorithms applied in several papers to the clas-\nsification of cDNA microarray information: k–Nearest\nNeighbors (KNN), Neural Networks (NNET), Linear\nDiscriminant Analysis (LDA), Partial Least Square (PLS),\nSupport Vector Machines (SVM), Random Forests (RF)\nand a set of ensemble techniques built using combina-\ntions of these approaches. Since we are dealing with\nclassifiable samples, the keep-the-max rule has been\nconsidered for all classifiers, including the GEG-based\none.\nAccording to the literature proposed in section 3,\nensamble classifiers have been constructed on top of\nKNN, NNET and RF (EKNN, ENNET, ERF). For each\ntype of classifier we constructed as many models as\nthe number of samples of the training set (213 in our\ncase) using a leave-one-out (LOO) approach. The result\nof the ensemble classification has been then computed\nconsidering two different voting rules: majority voting\n(MV) and max average accuracy voting (MAAV). We fi-\nnally considered a last ensemble composed of a balanced\ncombination (ERC) of the three considered classifiers .\nAll algorithms have been implemented using the Clas-\nsification And REgression Training (CARET) package\nof R, a free and multiplatform programming language\nand software environment widely used for statistical\nsoftware development and data analysis [64][65].\nThe performance of the prediction model of each\nclassifier has been tuned and optimized by perform-\ning leave-group-out-cross-validation (LGOCV). For each\nclassifier, 30 folds of the training set have been generated\nwith 98% of samples used to train the model while the\nremaining ones used as test-set. The size of the grid used\nto search the tuning parameters space for each classefier\n(e.g., k for KNN) has been set to 5. This represents a\ngood compromise in terms of computational time of the\ntraining phase and optimization results.\nTable 3 summarizes the average accuracy estimated\nfrom the cross-validation experiments for each consid-\nered classifier, together with the specific parameters\napplied to reach this results. For each estimated accuracy\nthe corresponding confidence interval (CI) computed\nwith 95% level of confidence (LOC) is also provided.\nThe accuracy obtained on the considered test-set is\nreported in Table 4 for all considered classifiers. Ac-\ncording to Fig. 5, that shows in a single diagram both\nthe estimated accuracy with related CI and the test-set\nresults, the GEG-based classifier performs as the best\nstate-of-the-art algorithms.\nTable 4\nClassification results for classifiable samples using the keep-the-max\nrule for all classifiers.\nClassifier Matches Mismatches\nGEG 73/74 (98.65%) 1/74 (1.35%)\nRF 73/74 (98.65%) 1/74 (1.35%)\nKNN 69/74 (93.24%) 5/74 (6.76%)\nNNET 70/74 (94.59%) 4/74 (5.41%)\nLDA 73/74 (98.65%) 1/74 (1.35%)\nPLS 42/74 (56.76%) 32/74 (43.24%)\nSVM 73/74 (98.65%) 1/74 (1.35%)\nERF (MV) 73/74 (98.65%) 1/74 (1.35%)\nEKNN (MV) 68/74 (91.89%) 6/74 (8.11%)\nENNET (MV) 73/74 (98.65%) 1/74 (1.35%)\nERC (MV) 73/74 (98.65%) 1/74 (1.35%)\nERF (MAAV) 71/74 (95.95%) 3/74 (4.05%)\nEKNN (MAAV) 64/74 (86.49%) 10/74 (13.51%)\nENNET (MAAV) 46/74 (62.16%) 28/74 (37.84%)\nERC (MAAV) 53/74 (71.62%) 21/74 (28.38%)\n4.3.2 Ability of identifying out of class samples\nWhen considering both classifiable and out-class-\nsamples, the GEG-based classifer outperforms state-of-\nthe-art methods for out-of-class samples detection. In\nthis case the GEG-based classifier applies the decision\nrule presented in Fig. 3.\n11\nTable 3\nAccuracy estimation of considered classifiers, with related confidence interval (CI) for 95% Level of Confidence (LOC), and specific parameters\napplied to obtain this result.\nAvg. Acc. CI Specific parameters\nGEG 0.964 [ 0.940 , 0.989 ] ✏ = 0\nRF 0.966 [ 0.940 , 0.992 ] mtry=20 (Number of variables randomly sampled as candidates at each split)\nKNN 0.926 [ 0.896 , 0.956 ] k = 5 (Number of nearest neighbors)\nNNET 0.956 [ 0.903 , 1 ] size=7 (Number of units in the hidden layer), decay=0.1 (Parameter for weight decay)\nLDA 0.963 [ 0.934 , 0.992 ] no pars\nPLS 0.578 [ 0.472 , 0.685 ] ncomp = 5 (Number of components one wishes to fit)\nSVM 0.986 [ 0.923 , 1 ] Radial Kernel, C=10 (cost of constraints violation) sigma=0.0214 (Inverse kernel width)\nERF (MV) 0.961 [ 0.938 , 0.984 ] Ensemble of 213 RF classifiers with majority voting decision rule (MV)\nEKNN (MV) 0.892 [ 0.861 , 0.923 ] Ensemble of 213 KNN classifiers with majority voting decision rule (MV)\nENNET (MV) 0.964 [ 0.933 , 0.995 ] Ensemble of 213 NNET classifiers with majority voting decision rule (MV)\nERC (MV) 0.946 [ 0.901 , 0.991 ] Ensemble of 213 classifiers: 71 RF, 71 KNN, 71NNET with majority voting decision rule (MV)\nERF (MAAV) 0.949 [ 0.928 , 0.970 ] Ensemble of 213 RF classifiers with Max Average Accuracy Voting (MAAV)\nEKNN (MAAV) 0.842 [ 0.808 , 0.876 ] Ensemble of 213 KNN classifiers with Max Average Accuracy Voting (MAAV)\nENNET (MAAV) 0.598 [ 0.547 , 0.649 ] Ensemble of 213 NNET classifiers with Max Average Accuracy Voting (MAAV)\nERC (MAAV) 0.698 [ 0.657 , 0.739 ] Ensemble of 213 classifiers: 71 RF, 71 KNN, 71NNET with Max Average Accuracy Voting (MAAV)\nX\t\n\f\r    accuracy\t\n\f\r    on\t\n\f\r    the\t\n\f\r    test-­set\naccuracy’s\t\n\f\r    CI\t\n\f\r    from\t\n\f\r    cross-­\nvalidation\t\n\f\r    (95%LOC)\nFigure 5. Comparison of accuracy and confidence intervals\n(LOC=95%) for the considered classifiers\nResults are provided in terms of true positives (TP),\ntrue negatives (TN), false positives (FP), false nega-\ntives (FN), sensitivity, specificity, and f-score. Sensitivity,\nspecificity, and f-score are statistical measures of the\nperformance of a binary classification test. The sensitivity\nmeasures true positives, i.e., the proportion of classifiable\nsamples that are correctly identified as such (e.g., the\npercentage of sick people who are identified as having a\ndisease), the specificity measures true negatives, i.e., the\nproportion of correctly identified out-of-class samples\n(e.g., the percentage of healthy people who are identified\nas not having any disease), and the f-score is a measure of\nthe test’s accuracy that considers both precision (number\nof correct results divided by the number of returned\nresults) and recall (number of correct results divided by\nthe number of results that should have been returned).\nTable 5 proposes a confusion matrix that summarizes\nthe performance of the GEG-based classifier when deal-\ning with both classifiable and out-of-class samples of\nthe considered test-set. The most positive result is that\n100% of out-of-class samples (TN column) are correctly\nidentified. This aspect is a critical issue in biological\nclassification and, in particular, in clinical diagnostics.\nU indicates that 5 out of 59 samples have been placed in\nthis category but the decision rule produced an uncertain\nresult to warn the user that the confidence in these\npredictions is not maximum. The cost of the application\nof the decision rule is a very reduced amount of FN. 4\nout of 74 samples have been errouneously classified as\nout-of-class (with one of these classified as uncertain).\nThis of course derives from the impossibility of perfectly\nseparating the different decision areas with the consid-\nered thresholds. Neverthless, results are quite promising,\nwith the sensitivity equal to 0.95, specificity equal to 1,\nand f-score equal to 0.96.\nTable 5\nConfusion matrix for the GEG-based classifier with the considered\ntest-set (Psooc = 0.3, Psmax = 0.7, Psdiff = 0.15\nClassifiable Out-of-class\nClassifiable 70/74 with 1M (94.59% TP) 0/59 (0% FP)\nOut-of-class 4/74 with 1U (5.41% FN) 59/59 with 5 U (100% TN)\nThe prediction model of the GEG-based classifier\nwhen working with out-of-class samples has been cross-\nvalidated by performing the same set of experiments\nproposed in section 2.2.1 to identify the decision rule\nthresholds. Table 6 shows the average sensitivity and\nspecificity togheter with the related confidence intervals\nwith LOC=95%. The first row reports the results of\nthe complete cross-validation experiments. These results\nshow a good estimate of the sensitivity that confirms\nthe experimental results on the test-set, while they sig-\nnificantly downestimate the sensitivity. The reason of\nthis bias is in the type of performed cross-validation\nexperiments. The considered training-set (Table 1) con-\ntains two classes corresponding to two variants of the\n12\nsame desease (CLL and CLLww). Whenever during the\ncross-validation one of these two classes is removed from\nthe library, detecting its samples as out-of-class becomes\na very hard problem for the classifier, increasing the\nnumber of FP. This is confirmed by the second row of\nTable 6. In this case the sensitivity and specificity are\nestimated without considering CLL and CLLww as out-\nof-class samples. This downestimation of the specificity\nof the classifier when performing corss-validation may\nalso bias the selection of the thresholds proposed in\nsection 2.2.1. Different cross-validation approaches are\nunder-investigation to overcome these problems.\nTable 6\nEstimation of sensitivity and specificity of the GEG-based classifier by\nperforming cross-validation experiments. Confidence intervals are\nprovided with LOC=95%\nCross. Val. Sens. Sens. CI Spec. Spec. CI\nComplete 0.945 [ 0.931 , 0.960 ] 0.718 [ 0.547 , 0.867 ]\nNo Cll, Cllww 0.947 [ 0.932 , 0.961 ] 0.906 [ 0.824 , 0.989 ]\nThe performance of the GEG-based classifier in detect-\ning out-of-class samples has been compared with a set\nof four state-of-the-art one-class classification methods,\nnamely, Parzen, KNN, KMeans, and PCA (see section\n3), all implemented using Matlab’s DD_tools [66]. The\ntarget class has been constructed using the 9 classes of\nclassifiable samples with 5% rejection rate. According\nto [52] we also considered two ensembles of one-class\nclassifiers: the first one E1 composed of Parzen, KNN,\nand KMeans, while the second E2 composed of KNN,\nKMeans and PCA. Two different voitng rules have been\napplied for the ensembles: majority voting (MV), and\nmax score voting (MSV). Table 7 proposes the result of\nthe same type of cross-validation experiments applied\nto the GEG-based classifier (No Cll, Cllw), while Table\n8 summarizes the obtained classification results on the\ntest-set for all classifiers including the GEG-based one.\nTable 7\nEstimation of sensitivity and specificity of one-class classifiers by\nperforming cross-validation experiments. Confidence intervals are\nprovided with LOC=95%\nCross. Val. Sens. Sens. CI Spec. Spec. CI\nParzen* 0.254 [ 0.230 , 0.280 ] 0.744 [ 0.640, 0.848 ]\nKNN 0.938 [ 0.920 , 0.956 ] 0.03 [ 0 , 0.066 ]\nKMeans 0.829 [ 0.80 , 0.86 ] 0.155 [ 0.084 , 0.227 ]\nPCA 0.254 [ 0.23 , 0.28 ] 0.744 [ 0.638 , 0.850 ]\nE1 (MV) 0.826 [ 0.796 , 0.856 ] 0.159 [ 0.087 , 0.231 ]\nE2 (MV) 0.826 [ 0.796 , 0.856 ] 0.159 [ 0.087 , 0.231 ]\nE1 (MSV) 0.254 [ 0.227 , 0.281 ] 0.744 [ 0.638 , 0.850 ]\nE2 (MSV) 0.306 [ 0.28 , 0.34 ] 0.0692 [ 0 , 0.180 ]\nFig. 6 graphically compares in a set of Receiver Operat-\ning Characteristic (ROC) curves the results for all consid-\nered classifiers. The ROC curve for the GEG-based clas-\nsifier has been computed by varing the three tresholds\napplied in the proposed decision rules, while the one for\nthe one-class classifier is computed by changing the con-\nTable 8\nOne-class classification results\nClassifier TP TN FN FP Sens Spec F-score\nGEG 70 59 4 0 0.95 1 0.96\nParzen 44 39 30 20 0.59 0.66 0.64\nKNN 74 0 0 59 1 0 0.71\nKMeans 72 0 2 59 0.97 0 0.70\nPCA 44 30 30 29 0.59 0.51 0.60\nE1 (MV) 72 0 2 59 0.97 0 0.70\nE2 (MV) 72 0 2 59 0.97 0 0.70\nE1 (MSV) 44 39 30 20 0.59 0.66 0.64\nE2 (MSV) 57 7 17 52 0.77 0.11 0.62\nsidered rejection boundary. The black dots represent the\nperformance of the classifier in terms of true positive rate\n(i.e., sensitivity) and false positive rate (i.e., 1-specificity)\non the considered test-set, while the squares represent\nthe confidence intervals of the true/false positive rates\nobtained from the cross-validation.\nIt is evident how the proposed approach is the one\nthat better allows to drastically reduce the number of FP\nand FN conditions that would make a diagnostic tool\nunusable in real medical setups. Moreover it provides\nresults on the test-set that are more coherent with the one\nobtained by cross-validating the model. This is an impor-\ntant characteristic to early estimate the performance of a\ngiven classifier based on the available training data.\nAmong the different one-class classifiers, Parzen\nseems to provide the best compromise in terms of sen-\nsitivity, specificity, f-score, and computational complex-\nity. Therefore, to deal with both classifiable and out-\nof-class samples, we combined this one-class classifier\nwith the state-of-the-art multi-class classification meth-\nods proposed in Table 4. The one-class classifier is used\nto filter out-of-class samples, while classifiable samples\nare then submitted to the considered classifiers. Table 9\nreports how the number (and percentage) of mismatches\nchanges when samples are first filtered by the one-class\nclassifier. The reduction of the number of mismatches\nshould not be considered here as an improvement of the\nclassifiers’ accuracy. In fact, it is correlated to the fact that\nthe one-class classifier erroneously rejectes some of those\nclassifiable sample that would generate mismatchings.\n4.3.3 Computational Time\nOne important aspect to take into account when compar-\ning the performance of different classifiers is the overall\ncomputation time. Several steps are required to complete\nthe classification of a test-set: data pre-processing (e.g.,\nNear ZeroVariance (NZV) used to identify and eliminate\nnear zero-variance genes in the data-set and PCA used\nfor dimensionality reduction), training, and classifica-\ntion. Table 10 summarizes the execution time required\nby each step for the GEG-based classifier, and for the\ncombination of state-of-the-art classifiers with the Parzen\none-class classifier.\nTable 10 illustrates the only real criticality of the\nproposed approach. In fact, while the training phase for\n13\n0.0 0.2 0.4 0.6 0.8 1.0\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nParzen\nFalse positive rate\nTr\nue\n p\nos\niti\nve\n ra\nte\nParzen\nCI Parzen\nGEG\n0.0 0.2 0.4 0.6 0.8 1.0\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nKNN\nFalse positive rate\nTr\nue\n p\nos\niti\nve\n ra\nte\nKNN\nCI KNNGEG\n0.0 0.2 0.4 0.6 0.8 1.0\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nKMeans\nFalse positive rate\nTr\nue\n p\nos\niti\nve\n ra\nte\nKMeans\nCI KMeans\nGEG\n0.0 0.2 0.4 0.6 0.8 1.0\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nPCA\nFalse positive rate\nTr\nue\n p\nos\niti\nve\n ra\nte\nPCA\nCI PCA\nGEG\n0.0 0.2 0.4 0.6 0.8 1.0\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nE1(MV)\nFalse positive rate\nTr\nue\n p\nos\niti\nve\n ra\nte\nE1(MV)\nCI E1(MV)\nGEG\n0.0 0.2 0.4 0.6 0.8 1.0\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nE2(MV)\nFalse positive rate\nTr\nue\n p\nos\niti\nve\n ra\nte\nE2(MV)\nCI E2(MV)\nGEG\n0.0 0.2 0.4 0.6 0.8 1.0\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nE1(MSV)\nFalse positive rate\nTr\nue\n p\nos\niti\nve\n ra\nte\nE1(MSV)\nCI E1(MSV)\nGEG\n0.0 0.2 0.4 0.6 0.8 1.0\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nE2(MSV)\nFalse positive rate\nTr\nue\n p\nos\niti\nve\n ra\nte\nE2(MSV)\nCI E2(MSV)\nGEG\nFigure 6. GEG-based classifier vs one-class classifier. Black dots represent performances on the considered test-set.\nTable 9\nClassification results after filtering out-of-class samples with the Parzen one-class classifier\nRF KNN NNET LDA PLS SVM E_KNN E_NNET E_RF E_KNN E_NNET E_RF E_RC E_RC\nMV MV MV MAAV MAAV MAAV MV MAAV\nMismatches 1 5 2 1 19 1 5 1 1 8 21 3 1 15\n2.27% 11.36% 4.55% 2.27% 43.18% 2.27% 11.36% 2.27% 2.27% 18.18% 47.73% 6.82% 2.27% 34.09%\nthe GEG-based classifier is negligeble, thus allowing an\neasy update of the classification model, the classification\ntime is an order of magnitude higher than the one of the\nother classifiers. Two are the main motivations for this\ndifference. First, the GEG-based classifier works without\nperforming any dimensionality reduction (see section\n4.3.4); while this has positive effects on the classification\nperformances, it strongly increases the computational\ncosts. Second, the available implementation is a pre-\nliminary prototype and several optimizations can be\nintroduced in the code to reduce the computation time\nand increase its efficiency.\nTable 10\nComparison of execution time for the different classifiers\nGEG PLS + Others + Ensemble +\nOne-Class One-Class One-Class\npre-processing - 36’19” ~38’09” 38’09”\nTraining 00’04” 05’33” ~03’04” 312’40”\nClassification 103’00” 05’19” ~02’28” 5’36\nTOTAL 103’04” 48’11” 43’41” 356’25’\n4.3.4 Data Reduction\nIn microarray analysis, genes are the information. Di-\nmensionality reduction is usually applied for making the\nproblem treatable by the available computational setup,\nbut there is no guarantee that the discarded data may\nnot become useful or interesting. The GEG approach\ndoes not need heavy dimensionality reduction. This\nallows to keep most of the information in the GEGs,\nmaking it easier to reuse the same data model for further\nanalysis of the gene expression data. An example of\nthis is the development of clustering and feature extrac-\ntion algorithms, able to extract from the GEG structure\nthe pathological genetic markers, again by analyzing\nthe topological structure and the weights of the GEGs.\nClearly, limiting the dimensionality reduction impacts on\nthe execution time, but never to a point to make the\nalgorithm unusable (even without any dimensionality\nreduction as can be seen in section 4.3.3).\nThe number of variables, either log-ratios of gene ex-\npression levels or principal components (PC), considered\nby each tested classifier are summarized in Table 11. It\nclearly appears how the number of variables mantained\nin the GEG after the training is far higher than in all\nother methods.\nTable 11\nComparison of data dimensionality reduction between the GEG-based\nand the state-of-the-art classifiers\nGEG PLS Others One-Class\nStart Genes 62,300* 62,300* 62,300* 62,300*\nAfter reduction 59,878 genes 5PC 77 PC 232 PC\n* Because of the gene overlapping among microchips\n5 CONCLUSIONS & FUTURE WORKS\nIn this paper we have presented a new classification\nalgorithm designed to be used in real clinical diagnostic\n14\napplications. The classifier is based on a new graph-\nbased data model used to organize large amounts of\ngene expression data coming from microarray experi-\nments. This new data model is very flexible and it makes\nthe implementation of classification, clustering, and fea-\nture extraction algorithms easier and less “abstracted”\nfrom the biological problem. The classifier is not only\nable to correctly classify samples in the corresponding\nclasses, but it is also able to correctly detect out-of-class\nsamples, thus drastically reducing the false positive rate.\nExperiments on a set of cDNA microarrays provided\nvery good results. In order to reduce the computational\ntime, one of the main limitations of the proposed tool,\nand to make it freely available to other researchers\nfor additional testing and valdiation, we are currently\nworking on implementing the proposed classification\ninto the CARET package of R .\nREFERENCES\n[1] G. Gibson, “Microarray analysis,” PLoS Biology, vol. 1, no. 1, pp.\n28–29, Oct. 2003.\n[2] P. Larranaga, B. Calvo, R. Santana, C. Bielza, J. Galdiano, I. Inza,\nJ. A. Lozano, R. Armananzas, A. Santafe, G. ad Perez, and\nV. Robles, “Machine learning in bioinformatics,” Briefings in Bioin-\nformatics, vol. 7, no. 1, pp. 86–112, Feb. 2006.\n[3] E. R. Dougherty, “The fundamental role of pattern recognition\nfor gene-expression/microarray data in bioinformatics,” Pattern\nRecognition, vol. 38, no. 12, pp. 2226–2228, Dec. 2005.\n[4] A. Benso, S. Di Carlo, G. Politano, and L. Sterpone, “A graph-\nbased representation of gene expression profiles in dna microar-\nrays,” in IEEE Symposium on Computational Intelligence in Bioinfor-\nmatics and Computational Biology (CIBCB), Sept. 2008, pp. 75–82.\n[5] ——, “Differential gene expression graphs: A data structure for\nclassification in dna microarrays,” in 8th IEEE International Con-\nference on BioInformatics and BioEngineering (BIBE), Oct. 2008, pp.\n1–6.\n[6] D. Jiang, C. Tang, and A. Zhang, “Cluster analysis for gene\nexpression data: a survey,” IEEE Trans. Knowl. Data Eng., vol. 16,\nno. 11, pp. 1370–1386, Nov. 2004.\n[7] O. Troyanskaya, M. Cantor, G. Sherlock, P. Brown, T. Hastie,\nR. Tibshirani, D. Botstein, and R. Altman, “Missing value estima-\ntion methods for dna microarrays,” Bioinformatics, vol. 17, no. 6,\npp. 520–525, Jun. 2004.\n[8] A. Hill, E. Brown, M. Whitley, G. Tucker-Kellog, C. Hunter, and\nS. D., “Evaluation of normalization procedures for oligonucletide\narray data based on spiked crna controls,” Genome Miology, vol. 2,\nno. 12, Nov. 2001.\n[9] J. Schuchhardt, D. Beule, A. Malik, E. Wolski, H. Eickhoff,\nH. Lehrach, and H. Herzel, “Normalization strategies for cdna\nmicroarrays,” Nucleic Acids Research, vol. 28, no. 10, p. E47, May\n2000.\n[10] Unigene. [Online]. Available:\nhttp://www.ncbi.nlm.nih.gov/sites/entrez?db=unigene\n[11] J. Stuart, E. Segal, D. Koller, and S. Kom, “A gene-coexpression\nnetwork for global discovery of conserved genetic modules,”\nScience, vol. 302, no. 5643, pp. 249–255, Oct. 2003.\n[12] D. B. Allison, X. Cui, G. P. Page, and M. Sabripour, “Microar-\nray data analysis: from disarray to consolidation to consensus,”\nNature Reviews: Genetics, vol. 7, no. 1, pp. 55–65, May 2006.\n[13] M. K. Kerr, M. Martin, and G. A. Churchill, “Analysis of variance\nfor gene expression microarray data,” Journal of Computational\nBiology, vol. 7, no. 6, pp. 819–837, Dec. 2000.\n[14] C. Cheadle, M. P. Vawter, W. J. Freed, and K. G. Becker, “Analysis\nof Microarray Data Using Z Score Transformation,” J Mol Diagn,\nvol. 5, no. 2, pp. 73–81, 2003.\n[15] D. M. Witten and R. Tibshirani. A comparison of fold-change and\nthe t-statistic for microarray data analysis. [Online]. Available:\nhttp://www-stat.stanford.edu/ tibs/ftp/FCTComparison.pdf\n[16] E. Parzen, “On estimation of a probability density function and\nmode,” The Annals of Mathematical Statistics, vol. 33, no. 3, pp.\n1065–1076, 1962.\n[17] D. Nguyen and D. Rocke, “Tumor classification by partial least\nsquares using microarray gene expression data.” Bioinformatics,\nvol. 18, no. 1, pp. 39–50, Jan. 2002.\n[18] J. W. Lee, J. B. Lee, M. Park, and S. H. Song, “An extensive\ncomparison of recent classification tools applied to microarray\ndata,” Computational Statistics and Data Analysis, vol. 48, no. 4, pp.\n869 – 885, 2005.\n[19] L. Sheng, R. Pique-Regi, S. Asgharzadeh, and A. Ortega, “Mi-\ncroarray classification using block diagonal linear discriminant\nanalysis with embedded feature selection,” Acoustics, Speech, and\nSignal Processing, IEEE International Conference on, vol. 0, pp. 1757–\n1760, 2009.\n[20] Y. Guo, T. Hastie, and R. Tibshirani, “Regularized linear discrim-\ninant analysis and its application in microarrays,” Biostat, vol. 8,\nno. 1, pp. 86–100, 2007.\n[21] P. Xu, G. N. Brock, and R. S. Parrish, “Modified linear discrim-\ninant analysis approaches for classification of high-dimensional\nmicroarray data,” Computational Statistics and Data Analysis,\nvol. 53, no. 5, pp. 1674 – 1687, 2009.\n[22] B. V. Dasarathy, Ed., Nearest neighbor (NN) norms: Nn pattern\nclassification. IEEE Computer Society, 1991.\n[23] L. Li, C. R. Weinberg, T. A. Darden, and L. G. Pedersen, “Gene\nselection for sample classification based on gene expression data:\nstudy of sensitivity to choice of parameters of the GA/KNN\nmethod,” Bioinformatics, vol. 17, no. 12, pp. 1131–1142, 2001.\n[24] A. Statnikov, C. F. Aliferis, I. Tsamardinos, D. Hardin, and\nS. Levy, “A comprehensive evaluation of multicategory classifica-\ntion methods for microarray gene expression cancer diagnosis,”\nBioinformatics, vol. 21, no. 5, pp. 631–643, 2005.\n[25] J. Liang and S. Kachalo, “Computational analysis of microarray\ngene expression profiles: Clustering, classification, and beyond,”\nChemometrics and Intelligent Laboratory Systems, vol. 62, no. 2, pp.\n199–216, 2002, cited By (since 1996) 13.\n[26] J. Breiman, Leo ad Friedman, C. J. Stone, and R. Olshen, Classifi-\ncation and Regression Trees. Taylor and Francis, Inc, 1984.\n[27] L. Breiman, “Random forests,” Machine Learning, vol. 1, no. 45,\npp. 5–32, 2001.\n[28] H. Zhang, C.-Y. Yu, and B. Singer, “Cell and tumor classification\nusing gene expression data: construction of forests.” Proc Natl\nAcad Sci U S A, vol. 100, no. 7, pp. 4168–4172, Apr. 2003.\n[29] H. Zhang, C.-Y. Yu, B. Singer, and M. Xiong, “Recursive parti-\ntioning for tumor classification with gene expression microarray\ndata,” in Proceedings of the National Academy of Sciences of the United\nStates of America, vol. 98, no. 12, Jun. 2001, pp. 6730–6735.\n[30] A. Statnikov, L. Wang, and C. Aliferis, “A comprehensive\ncomparison of random forests and support vector machines\nfor microarray-based cancer classification,” BMC Bioinformatics,\nvol. 9, no. 1, p. 319, 2008.\n[31] R. Diaz-Uriarte and S. Alvarez de Andres, “Gene selection and\nclassification of microarray data using random forest,” BMC\nBioinformatics, vol. 7, no. 1, p. 3, 2006.\n[32] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classification, 2nd\ned. Wiley-Interscience, 2000.\n[33] F. Azuaje, “A computational neural approach to support the\ndiscovery of gene function and classes of cancer,” IEEE Trans.\nBiomed. Eng., vol. 48, no. 3, pp. 332–339, 2001.\n[34] C.-J. Huang and W.-C. Liao, “Application of probabilistic neural\nnetworks to the class prediction of leukemia and embryonal\ntumor of central nervous system,” Neural Process. Lett., vol. 19,\nno. 3, pp. 211–226, 2004.\n[35] J. Khan, J. S. Wei, M. Ringner, L. H. Saal, M. Ladanyi, F. Wester-\nmann, F. Berthold, M. Schwab, C. R. Antonescu, C. Peterson, and\nP. S. Meltzer, “Classification and diagnostic prediction of cancers\nusing gene expression profiling and artificial neural networks,”\nNat Med, vol. 7, no. 6, pp. 673–679, Jun. 2001.\n[36] G. Bloom, I. V. Yang, D. Boulware, K. Y. Kwong, D. Coppola,\nS. Eschrich, J. Quackenbush, and T. J. Yeatman, “Multi-platform,\nmulti-site, microarray-based human tumor classification,” Ameri-\ncan Journal of Pathology, vol. 164, no. 1, pp. 9–16, 2004.\n[37] V. N. Vapnik, “An overview of statistical learning theory,” IEEE\nTrans. Neural Netw, vol. 10, no. 5, pp. 988–999, Sept. 1999.\n[38] G. Natsoulis, L. El Ghaoui, G. R. Lanckriet, A. M. Tolley, F. Leroy,\nS. Dunlea, B. P. Eynon, C. I. Pearson, S. Tugendreich, and K. Jar-\nnagin, “Classification of a large microarray data set: Algorithm\n15\ncomparison and analysis of drug signatures,” Genome Research,\nvol. 15, no. 5, pp. 724–736, 2005.\n[39] K. Crammer and Y. Singer, “On the algorithmic implementation\nof multiclass kernel-based vector machines,” J. Mach. Learn. Res.,\nvol. 2, pp. 265–292, 2002.\n[40] T. S. Furey, N. Cristianini, N. Duffy, D. W. Bednarski, M. Schum-\nmer, and D. Haussler, “Support vector machine classification and\nvalidation of cancer tissue samples using microarray expression\ndata,” Bioinformatics, vol. 16, no. 10, pp. 906–914, 2000.\n[41] A.-L. Boulesteix, C. Porzelius, and M. Daumer, “Microarray-based\nclassification and clinical predictors: on combined classifiers and\nadditional predictive value,” Bioinformatics, vol. 24, no. 15, pp.\n1698–1706, 2008.\n[42] L. Frey, M. Edgerton, D. Fisher, and S. Levy, “Ensemble stump\nclassifiers and gene expression signatures in lung cancer.” Stud\nHealth Technol Inform, vol. 129, no. Pt 2, pp. 1255–1259, 2007.\n[43] J.-H. Hong and S.-B. Cho, “The classification of cancer based on\ndna microarray data that uses diverse ensemble genetic program-\nming.” Artif Intell Med, vol. 36, no. 1, pp. 43–58, Jan 2006.\n[44] B. Liu, Q. Cui, T. Jiang, and S. Ma, “A combinational feature\nselection and ensemble neural network method for classification\nof gene expression data.” BMC Bioinformatics, vol. 5, p. 136, Sep\n2004.\n[45] Y. Peng, “A novel ensemble machine learning for robust microar-\nray data classification.” Comput Biol Med, vol. 36, no. 6, pp. 553–\n573, Jun 2006.\n[46] A. Statnikov, C. F. Aliferis, I. Tsamardinos, D. Hardin, and\nS. Levy, “A comprehensive evaluation of multicategory classifica-\ntion methods for microarray gene expression cancer diagnosis.”\nBioinformatics, vol. 21, no. 5, pp. 631–643, Mar 2005.\n[47] K.-J. Kim and S.-B. Cho, “Ensemble classifiers based on correla-\ntion analysis for dna microarray classification,” Neurocomputing,\nvol. 70, no. 1-3, pp. 187 – 199, 2006.\n[48] S. B. Cho and H.-H. Won, “Cancer classification using ensemble of\nneural networks with multiple significant gene subsets,” Applied\nIntelligence, vol. 26, no. 3, pp. 243–250, 2007.\n[49] S. Deegalla and H. Boström, “Classification of microarrays with\nknn: Comparison of dimensionality reduction methods,” in LNCS:\nIntelligent Data Engineering and Automated Learning (IDEAL), vol.\n4881, 2007, pp. 800–809.\n[50] M. Markou and S. Singh, “Novelty detection: A review - part 1:\nStatistical approaches,” Signal Processing, vol. 83, p. 2003, 2003.\n[51] V. Hodge and J. Austin, “A survey of outlier detection method-\nologies,” Artificial Intelligence Review, vol. 22, no. 2, pp. 85–126,\nOct 2004.\n[52] E. Spinosa and A. de Carvalho, “Combining one-class classifiers\nfor robust novelty detection in gene expression data,” Advances\nin Bioinformatics and Computational Biology, pp. 54–64, 2005.\n[53] X. Yun and R. G. Brereton, “Diagnostic pattern recognition on\ngene-expression profile data by using one-class classification,”\nJournal of chemical information and modeling, vol. 45, no. 5, pp. 1392–\n1401, 2005.\n[54] P. Juszczak, D. M. Tax, E. P. Kalska, and R. P. Duin, “Minimum\nspanning tree based one-class classifier,” Neurocomputing, vol. 72,\nno. 7-9, pp. 1859 – 1869, 2009.\n[55] V. Gesù, G. Bosco, and L. Pinello, “A one class classifier for signal\nidentification: A biological case study,” in KES ’08: Proceedings\nof the 12th international conference on Knowledge-Based Intelligent\nInformation and Engineering Systems, Part III. Berlin, Heidelberg:\nSpringer-Verlag, 2008, pp. 747–754.\n[56] cdna stanford’s microarray database. [Online]. Available:\nhttp://genome-www.stanford.edu/\n[57] A. A. Alizadeh, M. B. Eisen, R. E. Davis, C. Ma, I. S. Lossos,\nA. Rosenwald, J. C. Boldrick, H. Sabet, T. Tran, X. Yu, J. I. Powell,\nL. Yang, G. E. Marti, T. Moore, J. J. Hudson, L. Lu, D. B. Lewis,\nR. Tibshirani, G. Sherlock, W. C. Chan, T. C. Greiner, D. D.\nWeisenburger, J. O. Armitage, R. Warnke, R. Levy, W. Wilson,\nM. R. Grever, J. C. Byrd, D. Botstein, P. O. Brown, and L. M.\nStaudt, “Distinct types of diffuse large b-cell lymphoma identified\nby gene expression profiling.” Nature, vol. 403, no. 6769, pp. 503–\n511, Feb. 2000.\n[58] C. Palmer, M. Diehn, A. Alizadeh, and P. O. Browncorrespond-\ning, “Cell-type specific gene expression profiles of leukocytes in\nhuman peripheral blood,” BMC Genomics, vol. 7, no. 115, 2006.\n[59] S. P. Bohen, O. G. Troyanskaya, O. Alter, R. Warnke, D. Botstein,\nP. O. Brown, and R. Levy, “Variation in gene expression patterns\nin follicular lymphoma and the response to rituximab.” Proc Natl\nAcad Sci U S A, vol. 100, no. 4, pp. 1926–1930, Feb 2003.\n[60] B. et al., “Gene-expression profiling identifies distinct subclasses\nof core binding factor acute myeloid leukemia,” Blood, vol. 110,\nno. 4, pp. 1291–1300, 2007.\n[61] A. R. Whitney, M. Diehn, S. J. Popper, A. A. Alizadeh, J. C.\nBoldrick, D. A. Relman, and P. O. Brown, “Individuality and\nvariation in gene expression patterns in human blood.” Proc Natl\nAcad Sci U S A, vol. 100, no. 4, pp. 1896–1901, Feb 2003.\n[62] C. Ambroise and M. G. J., “Selection bias in gene extraction on the\nbasis of microarray gene-expression data,” Proc. National Academy\nof Science USA, vol. 99, pp. 6562–6566, 2002.\n[63] A. Rosenwald, A. A. Alizadeh, G. Widhopf, R. Simon, R. E. Davis,\nX. Yu, L. Yang, O. K. Pickeral, L. Z. Rassenti, J. Powell, D. Botstein,\nJ. C. Byrd, M. R. Grever, B. D. Cheson, N. Chiorazzi, W. H. Wilson,\nT. J. Kipps, P. O. Brown, and L. M. Staudt, “Relation of Gene\nExpression Phenotype to Immunoglobulin Mutation Genotype in\nB Cell Chronic Lymphocytic Leukemia,” J. Exp. Med., vol. 194,\nno. 11, pp. 1639–1648, 2001.\n[64] The r project for statistical computing: http://www.r-\nproject.org/.\n[65] K. Kuhn, “Building predictive models in r using the caret pack-\nage,” Journal of Statistical Software, vol. 28, no. 5, pp. 1–26, Aug.\n2008.\n[66] D. M. J. Tax. Ddtools, the data description tool-\nbox for matlab. [Online]. Available: http://www-\nict.ewi.tudelft.nl/ davidt/dd_tools.html\nPLACE\nPHOTO\nHERE\nAlfredo Benso currently holds a tenured Asso-\nciate Professor position in computer Engineering\nat Politecnico di Torino, Italy. His research inter-\nests include pattern recognition techniques for\nBioinformatics. Benso has an MS in Computer\nEngineering and a PhD in Information Technolo-\ngies, both from Politecnico di Torino. He is also\nactively involved in the Computer Society, where\nhe has been the leading volunteer for several\nprojects. He is a Computer Society Golden Core\nMember, and an IEEE Senior Member. .\nPLACE\nPHOTO\nHERE\nStefano Di Carlo received the MS degree in\ncomputer engineering and the PhD degree in\ninformation technologies from the Politecnico di\nTorino, Torino, Italy. Since 2008, he has been\nan assistant professor in the Department of\nControl and Computer Engineering, Politecnico\ndi Torino. His research interests include pattern\nrecognition techniques for Bioinformatics and\nchemical sensors. He is a Golden Core member\nof the IEEE Computer Society and a member of\nthe IEEE.\nPLACE\nPHOTO\nHERE\nGianfranco Politano received the MS degree in\ncomputer engineering in 2007 from the Politec-\nnico di Torino, Torino, Italy. Since 2008, he has\nbeen a PhD student at the Department of Con-\ntrol and Computer Engineering, Politecnico di\nTorino. His main research topics are microarray\ndata analysis and pattern recognition techniques\nfor bioinformatics.\n",
            "id": 4823973,
            "identifiers": [
                {
                    "identifier": "11415867",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "209888694",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1109/tcbb.2010.90",
                    "type": "DOI"
                },
                {
                    "identifier": "234879484",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:iris.polito.it:11583/2366472",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "oai:porto.polito.it:2366472",
                    "type": "OAI_ID"
                }
            ],
            "title": "A cDNA Microarray Gene Expression Data Classifier for Clinical Diagnostics Based on Graph Theory",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:iris.polito.it:11583/2366472",
                "oai:porto.polito.it:2366472"
            ],
            "publishedDate": "2011-01-01T00:00:00",
            "publisher": "IEEE Computer Society",
            "pubmedId": null,
            "references": [
                {
                    "id": 6444332,
                    "title": "A combinational feature selection and ensemble neural network method for classiﬁcation of gene expression data.”",
                    "authors": [],
                    "date": "2004",
                    "doi": null,
                    "raw": "B. Liu, Q. Cui, T. Jiang, and S. Ma, “A combinational feature selection and ensemble neural network method for classiﬁcation of gene expression data.” BMC Bioinformatics, vol. 5, p. 136, Sep 2004.",
                    "cites": null
                },
                {
                    "id": 6444288,
                    "title": "A comparison of fold-change and the t-statistic for microarray data analysis.",
                    "authors": [],
                    "date": null,
                    "doi": null,
                    "raw": "D. M. Witten and R. Tibshirani. A comparison of fold-change and the t-statistic for microarray data analysis. [Online]. Available: http://www-stat.stanford.edu/ tibs/ftp/FCTComparison.pdf",
                    "cites": null
                },
                {
                    "id": 6444312,
                    "title": "A comprehensive comparison of random forests and support vector machines for microarray-based cancer classiﬁcation,”",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1186/1471-2105-9-319",
                    "raw": "A. Statnikov, L. Wang, and C. Aliferis, “A comprehensive comparison of random forests and support vector machines for microarray-based cancer classiﬁcation,” BMC Bioinformatics, vol. 9, no. 1, p. 319, 2008.",
                    "cites": null
                },
                {
                    "id": 6444301,
                    "title": "A comprehensive evaluation of multicategory classiﬁcation methods for microarray gene expression cancer diagnosis,”",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1093/bioinformatics/bti033",
                    "raw": "A. Statnikov, C. F. Aliferis, I. Tsamardinos, D. Hardin, and S. Levy, “A comprehensive evaluation of multicategory classiﬁcation methods for microarray gene expression cancer diagnosis,” Bioinformatics, vol. 21, no. 5, pp. 631–643, 2005.",
                    "cites": null
                },
                {
                    "id": 6444334,
                    "title": "A comprehensive evaluation of multicategory classiﬁcation methods for microarray gene expression cancer diagnosis.”",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1093/bioinformatics/bti033",
                    "raw": "A. Statnikov, C. F. Aliferis, I. Tsamardinos, D. Hardin, and S. Levy, “A comprehensive evaluation of multicategory classiﬁcation methods for microarray gene expression cancer diagnosis.” Bioinformatics, vol. 21, no. 5, pp. 631–643, Mar 2005.",
                    "cites": null
                },
                {
                    "id": 6444315,
                    "title": "A computational neural approach to support the discovery of gene function and classes of cancer,”",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1109/10.914796",
                    "raw": "F. Azuaje, “A computational neural approach to support the discovery of gene function and classes of cancer,” IEEE Trans. Biomed. Eng., vol. 48, no. 3, pp. 332–339, 2001.",
                    "cites": null
                },
                {
                    "id": 6444285,
                    "title": "A gene-coexpression network for global discovery of conserved genetic modules,”",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1126/science.1087447",
                    "raw": "J. Stuart, E. Segal, D. Koller, and S. Kom, “A gene-coexpression network for global discovery of conserved genetic modules,” Science, vol. 302, no. 5643, pp. 249–255, Oct. 2003.",
                    "cites": null
                },
                {
                    "id": 6444275,
                    "title": "A graphbased representation of gene expression proﬁles in dna microarrays,”",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1109/cibcb.2008.4675762",
                    "raw": "A. Benso, S. Di Carlo, G. Politano, and L. Sterpone, “A graphbased representation of gene expression proﬁles in dna microarrays,” in IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB), Sept. 2008, pp. 75–82.",
                    "cites": null
                },
                {
                    "id": 6444333,
                    "title": "A novel ensemble machine learning for robust microarray data classiﬁcation.”",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1016/j.compbiomed.2005.04.001",
                    "raw": "Y. Peng, “A novel ensemble machine learning for robust microarray data classiﬁcation.” Comput Biol Med, vol. 36, no. 6, pp. 553– 573, Jun 2006.",
                    "cites": null
                },
                {
                    "id": 6444343,
                    "title": "A one class classiﬁer for signal identiﬁcation: A biological case study,”",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1007/978-3-540-85567-5_93",
                    "raw": "V. Gesù, G. Bosco, and L. Pinello, “A one class classiﬁer for signal identiﬁcation: A biological case study,” in KES ’08: Proceedings of the 12th international conference on Knowledge-Based Intelligent Information and Engineering Systems, Part III. Berlin, Heidelberg: Springer-Verlag, 2008, pp. 747–754.",
                    "cites": null
                },
                {
                    "id": 6444339,
                    "title": "A survey of outlier detection methodologies,”",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1007/s10462-004-4304-y",
                    "raw": "V. Hodge and J. Austin, “A survey of outlier detection methodologies,” Artiﬁcial Intelligence Review, vol. 22, no. 2, pp. 85–126, Oct 2004.",
                    "cites": null
                },
                {
                    "id": 6444291,
                    "title": "An extensive comparison of recent classiﬁcation tools applied to microarray data,”",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1016/j.csda.2004.03.017",
                    "raw": "J. W. Lee, J. B. Lee, M. Park, and S. H. Song, “An extensive comparison of recent classiﬁcation tools applied to microarray data,” Computational Statistics and Data Analysis, vol. 48, no. 4, pp. 869 – 885, 2005.",
                    "cites": null
                },
                {
                    "id": 6444322,
                    "title": "An overview of statistical learning theory,”",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1109/72.788640",
                    "raw": "V. N. Vapnik, “An overview of statistical learning theory,” IEEE Trans. Neural Netw, vol. 10, no. 5, pp. 988–999, Sept. 1999.",
                    "cites": null
                },
                {
                    "id": 6444287,
                    "title": "Analysis of variance for gene expression microarray data,”",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1089/10665270050514954",
                    "raw": "M. K. Kerr, M. Martin, and G. A. Churchill, “Analysis of variance for gene expression microarray data,” Journal of Computational Biology, vol. 7, no. 6, pp. 819–837, Dec. 2000.",
                    "cites": null
                },
                {
                    "id": 6444317,
                    "title": "Application of probabilistic neural networks to the class prediction of leukemia and embryonal tumor of central nervous system,”",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1023/b:nepl.0000035613.51734.48",
                    "raw": "C.-J. Huang and W.-C. Liao, “Application of probabilistic neural networks to the class prediction of leukemia and embryonal tumor of central nervous system,” Neural Process. Lett., vol. 19, no. 3, pp. 211–226, 2004.",
                    "cites": null
                },
                {
                    "id": 6444353,
                    "title": "Building predictive models in r using the caret package,”",
                    "authors": [],
                    "date": "2008",
                    "doi": null,
                    "raw": "K. Kuhn, “Building predictive models in r using the caret package,” Journal of Statistical Software, vol. 28, no. 5, pp. 1–26, Aug. 2008.",
                    "cites": null
                },
                {
                    "id": 6444336,
                    "title": "Cancer classiﬁcation using ensemble of neural networks with multiple signiﬁcant gene subsets,”",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1007/s10489-006-0020-4",
                    "raw": "S. B. Cho and H.-H. Won, “Cancer classiﬁcation using ensemble of neural networks with multiple signiﬁcant gene subsets,” Applied Intelligence, vol. 26, no. 3, pp. 243–250, 2007.",
                    "cites": null
                },
                {
                    "id": 6444344,
                    "title": "cdna stanford’s microarray database.",
                    "authors": [],
                    "date": null,
                    "doi": "10.1007/springerreference_34602",
                    "raw": "cdna stanford’s microarray database. [Online]. Available: http://genome-www.stanford.edu/",
                    "cites": null
                },
                {
                    "id": 6444308,
                    "title": "Cell and tumor classiﬁcation using gene expression data: construction of forests.”",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1073/pnas.0230559100",
                    "raw": "H. Zhang, C.-Y. Yu, and B. Singer, “Cell and tumor classiﬁcation using gene expression data: construction of forests.” Proc Natl Acad Sci U S A, vol. 100, no. 7, pp. 4168–4172, Apr. 2003.",
                    "cites": null
                },
                {
                    "id": 6444346,
                    "title": "Cell-type speciﬁc gene expression proﬁles of leukocytes in human peripheral blood,”",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "C. Palmer, M. Diehn, A. Alizadeh, and P. O. Browncorresponding, “Cell-type speciﬁc gene expression proﬁles of leukocytes in human peripheral blood,” BMC Genomics, vol. 7, no. 115, 2006.",
                    "cites": null
                },
                {
                    "id": 6444305,
                    "title": "Classiﬁ-cation and Regression Trees.",
                    "authors": [],
                    "date": "1984",
                    "doi": "10.2307/2530946",
                    "raw": "J. Breiman, Leo ad Friedman, C. J. Stone, and R. Olshen, Classiﬁ-cation and Regression Trees. Taylor and Francis, Inc, 1984.",
                    "cites": null
                },
                {
                    "id": 6444318,
                    "title": "Classiﬁcation and diagnostic prediction of cancers using gene expression proﬁling and artiﬁcial neural networks,”",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1240/sav_gbm_2002_h_000061",
                    "raw": "J. Khan, J. S. Wei, M. Ringner, L. H. Saal, M. Ladanyi, F. Westermann, F. Berthold, M. Schwab, C. R. Antonescu, C. Peterson, and P. S. Meltzer, “Classiﬁcation and diagnostic prediction of cancers using gene expression proﬁling and artiﬁcial neural networks,” Nat Med, vol. 7, no. 6, pp. 673–679, Jun. 2001.",
                    "cites": null
                },
                {
                    "id": 6444324,
                    "title": "Classiﬁcation of a large microarray data set: Algorithm15 comparison and analysis of drug signatures,”",
                    "authors": [],
                    "date": "2005",
                    "doi": null,
                    "raw": "G. Natsoulis, L. El Ghaoui, G. R. Lanckriet, A. M. Tolley, F. Leroy, S. Dunlea, B. P. Eynon, C. I. Pearson, S. Tugendreich, and K. Jarnagin, “Classiﬁcation of a large microarray data set: Algorithm15 comparison and analysis of drug signatures,” Genome Research, vol. 15, no. 5, pp. 724–736, 2005.",
                    "cites": null
                },
                {
                    "id": 6444337,
                    "title": "Classiﬁcation of microarrays with knn: Comparison of dimensionality reduction methods,”",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1007/978-3-540-77226-2_80",
                    "raw": "S. Deegalla and H. Boström, “Classiﬁcation of microarrays with knn: Comparison of dimensionality reduction methods,” in LNCS: Intelligent Data Engineering and Automated Learning (IDEAL), vol. 4881, 2007, pp. 800–809.",
                    "cites": null
                },
                {
                    "id": 6444281,
                    "title": "Cluster analysis for gene expression data: a survey,”",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1109/tkde.2004.68",
                    "raw": "D. Jiang, C. Tang, and A. Zhang, “Cluster analysis for gene expression data: a survey,” IEEE Trans. Knowl. Data Eng., vol. 16, no. 11, pp. 1370–1386, Nov. 2004.",
                    "cites": null
                },
                {
                    "id": 6444340,
                    "title": "Combining one-class classiﬁers for robust novelty detection in gene expression data,”",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1007/11532323_7",
                    "raw": "E. Spinosa and A. de Carvalho, “Combining one-class classiﬁers for robust novelty detection in gene expression data,” Advances in Bioinformatics and Computational Biology, pp. 54–64, 2005.",
                    "cites": null
                },
                {
                    "id": 6444303,
                    "title": "Computational analysis of microarray gene expression proﬁles: Clustering, classiﬁcation, and beyond,”",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1016/s0169-7439(02)00014-x",
                    "raw": "J. Liang and S. Kachalo, “Computational analysis of microarray gene expression proﬁles: Clustering, classiﬁcation, and beyond,” Chemometrics and Intelligent Laboratory Systems, vol. 62, no. 2, pp. 199–216, 2002, cited By (since 1996) 13.",
                    "cites": null
                },
                {
                    "id": 6444341,
                    "title": "Diagnostic pattern recognition on gene-expression proﬁle data by using one-class classiﬁcation,”",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1021/ci049726v",
                    "raw": "X. Yun and R. G. Brereton, “Diagnostic pattern recognition on gene-expression proﬁle data by using one-class classiﬁcation,” Journal of chemical information and modeling, vol. 45, no. 5, pp. 1392– 1401, 2005.",
                    "cites": null
                },
                {
                    "id": 6444345,
                    "title": "Distinct types of diffuse large b-cell lymphoma identiﬁed by gene expression proﬁling.”",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1038/35000501",
                    "raw": "A. A. Alizadeh, M. B. Eisen, R. E. Davis, C. Ma, I. S. Lossos, A. Rosenwald, J. C. Boldrick, H. Sabet, T. Tran, X. Yu, J. I. Powell, L. Yang, G. E. Marti, T. Moore, J. J. Hudson, L. Lu, D. B. Lewis, R. Tibshirani, G. Sherlock, W. C. Chan, T. C. Greiner, D. D. Weisenburger, J. O. Armitage, R. Warnke, R. Levy, W. Wilson, M. R. Grever, J. C. Byrd, D. Botstein, P. O. Brown, and L. M. Staudt, “Distinct types of diffuse large b-cell lymphoma identiﬁed by gene expression proﬁling.” Nature, vol. 403, no. 6769, pp. 503– 511, Feb. 2000.",
                    "cites": null
                },
                {
                    "id": 6444335,
                    "title": "Ensemble classiﬁers based on correlation analysis for dna microarray classiﬁcation,”",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1016/j.neucom.2006.03.002",
                    "raw": "K.-J. Kim and S.-B. Cho, “Ensemble classiﬁers based on correlation analysis for dna microarray classiﬁcation,” Neurocomputing, vol. 70, no. 1-3, pp. 187 – 199, 2006.",
                    "cites": null
                },
                {
                    "id": 6444330,
                    "title": "Ensemble stump classiﬁers and gene expression signatures in lung cancer.”",
                    "authors": [],
                    "date": "2007",
                    "doi": null,
                    "raw": "L. Frey, M. Edgerton, D. Fisher, and S. Levy, “Ensemble stump classiﬁers and gene expression signatures in lung cancer.” Stud Health Technol Inform, vol. 129, no. Pt 2, pp. 1255–1259, 2007.",
                    "cites": null
                },
                {
                    "id": 6444283,
                    "title": "Evaluation of normalization procedures for oligonucletide array data based on spiked crna controls,”",
                    "authors": [],
                    "date": "2001",
                    "doi": null,
                    "raw": "A. Hill, E. Brown, M. Whitley, G. Tucker-Kellog, C. Hunter, and S. D., “Evaluation of normalization procedures for oligonucletide array data based on spiked crna controls,” Genome Miology, vol. 2, no. 12, Nov. 2001.",
                    "cites": null
                },
                {
                    "id": 6444278,
                    "title": "gene expression graphs: A data structure for classiﬁcation in dna microarrays,”",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1109/bibe.2008.4696689",
                    "raw": "——, “Differential gene expression graphs: A data structure for classiﬁcation in dna microarrays,” in 8th IEEE International Conference on BioInformatics and BioEngineering (BIBE), Oct. 2008, pp. 1–6.",
                    "cites": null
                },
                {
                    "id": 6444313,
                    "title": "Gene selection and classiﬁcation of microarray data using random forest,”",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "R. Diaz-Uriarte and S. Alvarez de Andres, “Gene selection and classiﬁcation of microarray data using random forest,” BMC Bioinformatics, vol. 7, no. 1, p. 3, 2006.",
                    "cites": null
                },
                {
                    "id": 6444300,
                    "title": "Gene selection for sample classiﬁcation based on gene expression data: study of sensitivity to choice of parameters of",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1093/bioinformatics/17.12.1131",
                    "raw": "L. Li, C. R. Weinberg, T. A. Darden, and L. G. Pedersen, “Gene selection for sample classiﬁcation based on gene expression data: study of sensitivity to choice of parameters of the GA/KNN method,” Bioinformatics, vol. 17, no. 12, pp. 1131–1142, 2001.",
                    "cites": null
                },
                {
                    "id": 6444348,
                    "title": "Gene-expression proﬁling identiﬁes distinct subclasses of core binding factor acute myeloid leukemia,”",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1182/blood-2006-10-049783",
                    "raw": "B. et al., “Gene-expression proﬁling identiﬁes distinct subclasses of core binding factor acute myeloid leukemia,” Blood, vol. 110, no. 4, pp. 1291–1300, 2007.",
                    "cites": null
                },
                {
                    "id": 6444349,
                    "title": "Individuality and variation in gene expression patterns in human blood.”",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1073/pnas.252784499",
                    "raw": "A. R. Whitney, M. Diehn, S. J. Popper, A. A. Alizadeh, J. C. Boldrick, D. A. Relman, and P. O. Brown, “Individuality and variation in gene expression patterns in human blood.” Proc Natl Acad Sci U S A, vol. 100, no. 4, pp. 1896–1901, Feb 2003.",
                    "cites": null
                },
                {
                    "id": 6444268,
                    "title": "Microarray analysis,”",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1371/journal.pbio.0000015",
                    "raw": "G. Gibson, “Microarray analysis,” PLoS Biology, vol. 1, no. 1, pp. 28–29, Oct. 2003.",
                    "cites": null
                },
                {
                    "id": 6444293,
                    "title": "Microarray classiﬁcation using block diagonal linear discriminant analysis with embedded feature selection,”",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1109/icassp.2009.4959944",
                    "raw": "L. Sheng, R. Pique-Regi, S. Asgharzadeh, and A. Ortega, “Microarray classiﬁcation using block diagonal linear discriminant analysis with embedded feature selection,” Acoustics, Speech, and Signal Processing, IEEE International Conference on, vol. 0, pp. 1757– 1760, 2009.",
                    "cites": null
                },
                {
                    "id": 6444286,
                    "title": "Microarray data analysis: from disarray to consolidation to consensus,”",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1038/nrg1749",
                    "raw": "D. B. Allison, X. Cui, G. P. Page, and M. Sabripour, “Microarray data analysis: from disarray to consolidation to consensus,” Nature Reviews: Genetics, vol. 7, no. 1, pp. 55–65, May 2006.",
                    "cites": null
                },
                {
                    "id": 6444329,
                    "title": "Microarray-based classiﬁcation and clinical predictors: on combined classiﬁers and additional predictive value,”",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1093/bioinformatics/btn262",
                    "raw": "A.-L. Boulesteix, C. Porzelius, and M. Daumer, “Microarray-based classiﬁcation and clinical predictors: on combined classiﬁers and additional predictive value,” Bioinformatics, vol. 24, no. 15, pp. 1698–1706, 2008.",
                    "cites": null
                },
                {
                    "id": 6444342,
                    "title": "Minimum spanning tree based one-class classiﬁer,”",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1016/j.neucom.2008.05.003",
                    "raw": "P. Juszczak, D. M. Tax, E. P. Kalska, and R. P. Duin, “Minimum spanning tree based one-class classiﬁer,” Neurocomputing, vol. 72, no. 7-9, pp. 1859 – 1869, 2009.",
                    "cites": null
                },
                {
                    "id": 6444282,
                    "title": "Missing value estimation methods for dna microarrays,”",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1093/bioinformatics/17.6.520",
                    "raw": "O. Troyanskaya, M. Cantor, G. Sherlock, P. Brown, T. Hastie, R. Tibshirani, D. Botstein, and R. Altman, “Missing value estimation methods for dna microarrays,” Bioinformatics, vol. 17, no. 6, pp. 520–525, Jun. 2004.",
                    "cites": null
                },
                {
                    "id": 6444296,
                    "title": "Modiﬁed linear discriminant analysis approaches for classiﬁcation of high-dimensional microarray data,”",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1016/j.csda.2008.02.005",
                    "raw": "P. Xu, G. N. Brock, and R. S. Parrish, “Modiﬁed linear discriminant analysis approaches for classiﬁcation of high-dimensional microarray data,” Computational Statistics and Data Analysis, vol. 53, no. 5, pp. 1674 – 1687, 2009.",
                    "cites": null
                },
                {
                    "id": 6444320,
                    "title": "Multi-platform, multi-site, microarray-based human tumor classiﬁcation,”",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1016/s0002-9440(10)63090-8",
                    "raw": "G. Bloom, I. V. Yang, D. Boulware, K. Y. Kwong, D. Coppola, S. Eschrich, J. Quackenbush, and T. J. Yeatman, “Multi-platform, multi-site, microarray-based human tumor classiﬁcation,” American Journal of Pathology, vol. 164, no. 1, pp. 9–16, 2004.",
                    "cites": null
                },
                {
                    "id": 6444298,
                    "title": "Nearest neighbor (NN) norms: Nn pattern classiﬁcation.",
                    "authors": [],
                    "date": "1991",
                    "doi": null,
                    "raw": "B. V. Dasarathy, Ed., Nearest neighbor (NN) norms: Nn pattern classiﬁcation. IEEE Computer Society, 1991.",
                    "cites": null
                },
                {
                    "id": 6444284,
                    "title": "Normalization strategies for cdna microarrays,”",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1063/1.1336843",
                    "raw": "J. Schuchhardt, D. Beule, A. Malik, E. Wolski, H. Eickhoff, H. Lehrach, and H. Herzel, “Normalization strategies for cdna microarrays,” Nucleic Acids Research, vol. 28, no. 10, p. E47, May 2000.",
                    "cites": null
                },
                {
                    "id": 6444338,
                    "title": "Novelty detection: A review - part 1:",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1016/j.sigpro.2003.07.019",
                    "raw": "M. Markou and S. Singh, “Novelty detection: A review - part 1: Statistical approaches,” Signal Processing, vol. 83, p. 2003, 2003.",
                    "cites": null
                },
                {
                    "id": 6444289,
                    "title": "On estimation of a probability density function and mode,”",
                    "authors": [],
                    "date": "1962",
                    "doi": "10.1214/aoms/1177704472",
                    "raw": "E. Parzen, “On estimation of a probability density function and mode,” The Annals of Mathematical Statistics, vol. 33, no. 3, pp. 1065–1076, 1962.",
                    "cites": null
                },
                {
                    "id": 6444326,
                    "title": "On the algorithmic implementation of multiclass kernel-based vector machines,”",
                    "authors": [],
                    "date": "2002",
                    "doi": null,
                    "raw": "K. Crammer and Y. Singer, “On the algorithmic implementation of multiclass kernel-based vector machines,” J. Mach. Learn. Res., vol. 2, pp. 265–292, 2002.",
                    "cites": null
                },
                {
                    "id": 6444307,
                    "title": "Random forests,”",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1007/0-387-21529-8_16",
                    "raw": "L. Breiman, “Random forests,” Machine Learning, vol. 1, no. 45, pp. 5–32, 2001.",
                    "cites": null
                },
                {
                    "id": 6444310,
                    "title": "Recursive partitioning for tumor classiﬁcation with gene expression microarray data,”",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1073/pnas.111153698",
                    "raw": "H. Zhang, C.-Y. Yu, B. Singer, and M. Xiong, “Recursive partitioning for tumor classiﬁcation with gene expression microarray data,” in Proceedings of the National Academy of Sciences of the United States of America, vol. 98, no. 12, Jun. 2001, pp. 6730–6735.",
                    "cites": null
                },
                {
                    "id": 6444295,
                    "title": "Regularized linear discriminant analysis and its application in microarrays,”",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1093/biostatistics/kxj035",
                    "raw": "Y. Guo, T. Hastie, and R. Tibshirani, “Regularized linear discriminant analysis and its application in microarrays,” Biostat, vol. 8, no. 1, pp. 86–100, 2007.",
                    "cites": null
                },
                {
                    "id": 6444351,
                    "title": "Relation of Gene Expression Phenotype to Immunoglobulin Mutation Genotype in",
                    "authors": [],
                    "date": "2001",
                    "doi": null,
                    "raw": "A. Rosenwald, A. A. Alizadeh, G. Widhopf, R. Simon, R. E. Davis, X. Yu, L. Yang, O. K. Pickeral, L. Z. Rassenti, J. Powell, D. Botstein, J. C. Byrd, M. R. Grever, B. D. Cheson, N. Chiorazzi, W. H. Wilson, T. J. Kipps, P. O. Brown, and L. M. Staudt, “Relation of Gene Expression Phenotype to Immunoglobulin Mutation Genotype in B Cell Chronic Lymphocytic Leukemia,” J. Exp. Med., vol. 194, no. 11, pp. 1639–1648, 2001.",
                    "cites": null
                },
                {
                    "id": 6444350,
                    "title": "Selection bias in gene extraction on the basis of microarray gene-expression data,”",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1073/pnas.102102699",
                    "raw": "C. Ambroise and M. G. J., “Selection bias in gene extraction on the basis of microarray gene-expression data,” Proc. National Academy of Science USA, vol. 99, pp. 6562–6566, 2002.",
                    "cites": null
                },
                {
                    "id": 6444327,
                    "title": "Support vector machine classiﬁcation and validation of cancer tissue samples using microarray expression data,”",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1093/bioinformatics/16.10.906",
                    "raw": "T. S. Furey, N. Cristianini, N. Duffy, D. W. Bednarski, M. Schummer, and D. Haussler, “Support vector machine classiﬁcation and validation of cancer tissue samples using microarray expression data,” Bioinformatics, vol. 16, no. 10, pp. 906–914, 2000.",
                    "cites": null
                },
                {
                    "id": 6444331,
                    "title": "The classiﬁcation of cancer based on dna microarray data that uses diverse ensemble genetic programming.”",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1016/j.artmed.2005.06.002",
                    "raw": "J.-H. Hong and S.-B. Cho, “The classiﬁcation of cancer based on dna microarray data that uses diverse ensemble genetic programming.” Artif Intell Med, vol. 36, no. 1, pp. 43–58, Jan 2006.",
                    "cites": null
                },
                {
                    "id": 6444271,
                    "title": "The fundamental role of pattern recognition for gene-expression/microarray data in bioinformatics,”",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1016/j.patcog.2005.03.008",
                    "raw": "E. R. Dougherty, “The fundamental role of pattern recognition for gene-expression/microarray data in bioinformatics,” Pattern Recognition, vol. 38, no. 12, pp. 2226–2228, Dec. 2005.",
                    "cites": null
                },
                {
                    "id": 6444352,
                    "title": "The r project for statistical computing: http://www.rproject.org/.",
                    "authors": [],
                    "date": null,
                    "doi": null,
                    "raw": "The r project for statistical computing: http://www.rproject.org/.",
                    "cites": null
                },
                {
                    "id": 6444290,
                    "title": "Tumor classiﬁcation by partial least squares using microarray gene expression data.”",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1093/bioinformatics/18.1.39",
                    "raw": "D. Nguyen and D. Rocke, “Tumor classiﬁcation by partial least squares using microarray gene expression data.” Bioinformatics, vol. 18, no. 1, pp. 39–50, Jan. 2002.",
                    "cites": null
                },
                {
                    "id": 6444347,
                    "title": "Variation in gene expression patterns in follicular lymphoma and the response to rituximab.”",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1073/pnas.0437875100",
                    "raw": "S. P. Bohen, O. G. Troyanskaya, O. Alter, R. Warnke, D. Botstein, P. O. Brown, and R. Levy, “Variation in gene expression patterns in follicular lymphoma and the response to rituximab.” Proc Natl Acad Sci U S A, vol. 100, no. 4, pp. 1926–1930, Feb 2003.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [],
            "updatedDate": "2021-04-26T12:11:28",
            "yearPublished": 2011,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "1545-5963"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/pdf/11415867.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/11415867"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/11415867/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/11415867/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/4823973"
                }
            ]
        },
        {
            "acceptedDate": "2012-08-09T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Domenico Maisano"
                },
                {
                    "name": "Fiorenzo Franceschini"
                },
                {
                    "name": "Garfield"
                },
                {
                    "name": "Glänzel"
                },
                {
                    "name": "Hirsch"
                },
                {
                    "name": "Hirsch"
                },
                {
                    "name": "ISI Web of Knowledge"
                },
                {
                    "name": "Kosmulski"
                },
                {
                    "name": "Kosmulski"
                },
                {
                    "name": "Luca Mastrogiacomo"
                },
                {
                    "name": "Maurizio Galetto"
                },
                {
                    "name": "Moed"
                },
                {
                    "name": "Vinkler"
                },
                {
                    "name": "Waltman"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/234893065",
                "https://api.core.ac.uk/v3/outputs/11429359",
                "https://api.core.ac.uk/v3/outputs/188390274"
            ],
            "createdDate": "2013-07-10T14:52:41",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 12601,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/12601",
                    "logo": "https://api.core.ac.uk/data-providers/12601/logo"
                },
                {
                    "id": 351,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/351",
                    "logo": "https://api.core.ac.uk/data-providers/351/logo"
                }
            ],
            "depositedDate": "2012-10-01T00:00:00",
            "abstract": "The aim of this brief communication is to reply to a letter by Kosmulski (Journal of Informetrics 6(3):368-369, 2012), which criticizes a recent indicator called \"success-index\". The most interesting features of this indicator, presented in Franceschini et al. (Scientometrics, in press), are: (i) allowing the selection of an \"elite\" subset from a set of publications and (ii) implementing the field-normalization at the level of an individual publication. We show that the Kosmulski's criticism is unfair and inappropriate, as it is the result of a misinterpretation of the indicato",
            "documentType": "research",
            "doi": "10.1016/j.joi.2012.07.005",
            "downloadUrl": "https://core.ac.uk/download/pdf/11429359.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "Brief Communication \nFurther clarifications about the Success-index \nFiorenzo Franceschini1, Maurizio Galetto, Domenico Maisano, Luca Mastrogiacomo \n1 fiorenzo.franceschini@polito.it \nPolitecnico di Torino, DISPEA (Department of Management and Production Engineering), \nCorso Duca degli Abruzzi 24, 10129, Torino (Italy) \nAbstract \nThe aim of this brief communication is to reply to a letter by Kosmulski (Journal of \nInformetrics 6(3):368-369, 2012), which criticizes a recent indicator called “success-index”. \nThe most interesting features of this indicator, presented in [Franceschini et al., to appear in \nScientometrics, DOI: 10.1007/s11192-011-0570-z], are: (i) allowing the selection of an “elite” \nsubset from a set of publications and (ii) implementing the field-normalization at the level of an \nindividual publication. We show that the Kosmulski’s criticism is unfair and inappropriate, as it \nis the result of a misinterpretation of the indicator. \nKeywords: success-index, citation propensity, field normalization, h-index. \nReconstruction of the dispute \nWith this brief communication we reply to a letter by Kosmulski (2012), who criticized the \nsuccess-index, presented in a recent article [Franceschini et al. 2012a]. We anticipate that this \ncriticism is unfair since it was based on a misinterpretation of the indicator. Let’s now try to trace \nthe genesis of this dispute. \n1. Kosmulski’s indicator (NSP). In 2011, Kosmulski (2011) presented a novel bibliometric \nindicator, denominated Number of Successful Papers (hereafter abbreviated as NSP). Precisely, \nfor a generic group of scientific publications examined—e.g., those associated to a scientist or a \njournal—the articles that have received more citations than those made are classified as \n“successful”. In other words, a score is associated to each (i-th) of the (P) publications of \ninterest:  \n1 when\n0 otherwise\n \n\ni\ni\nscore c r\nscore\ni i  (1) \nwhere ci are the citations received and ri the citations made by the i-th publication. \nNSP is defined as: \n1\n P i\ni\nNSP score . (2) \nIt can be noted that NSP—being an indicator based on the citations accumulated over a non-\nfixed reference time-window—is time dependent. \n 1\nAccording to the authors, NSP is very interesting for two reasons: (i) the indicator has a great \nsimplicity and immediate meaning—almost equivalent to those of the h-index [Hirsch, 2005]; \n(ii) the indicator can be applied to groups of publications from different disciplines, as it \n(potentially) implements a field-normalization at the level of a single publication. \nConsidering NSP from a broader perspective, it can be seen that—given a generic set of \npublications—this indicator allows to select an “elite” subset. This selection can also be made \nby other indicators in the literature: e.g., let us consider the h-core approach [Hirsch, 2007], the \nselection by π-indicator [Vinkler, 2011], the characteristic scores and scales (CSS) method \n[Glänzel, 2011] or the ESI’s Highly Cited Papers method [ISI Web of Knowledge, 2012]. We \nremark that, differently from NSP, the aforementioned methods require that the set of \npublications examined are necessarily within the same scientific discipline. \nUnfortunately, NSP has the serious defect of estimating the citation propensity of a publication \nin a very fragile way, in terms of statistical significance. In addition, it is prone to manipulation. \nMore details about these specific limitations, can be found in [Franceschini et al. 2012a]. \n2. The success-index. In [Franceschini et al. 2012a] we suggested a new indicator—i.e., the \nsuccess-index—which is inspired by NSP, but aimed at reducing its limitations. Here is the \ndefinition of the success-index: \n1 when\n0 otherwise\n \n\ni\ni\nscore c CT\nscore\ni i  (3) \n1\n-index\n\nP i\ni\nSuccess score . (4) \nIt can be noticed that, in Eq. 3, the term ri of Eq. 1 is replaced by CTi, i.e., a generic comparison \n(or normalization) term associated with the i-th publication; in other words CTi is an estimate of \nthe number of citations that a publication—in a certain scientific context and period of time—\nshould potentially achieve. Note that we have not put any constraint on the definition of the new \ncomparison term, provided that it must be based on a reasonably representative sample of \npublications, “close” to that one of interest [Franceschini et al., 2012a, 9th page].  \nOf course, determining the “(non-)success status” of an individual paper should not be intended \nas a comprehensive assessment of quality. Nevertheless, this does not mean that counting the \nnumber of papers from a set above/below some appropriate citation thresholds could not \nprovide useful information. Also, this is the basic idea of the highly cited publications indicator, \ntheorized by Waltman and Van Eck  (2012). \nWe remark that, for any indicator implementing the field-normalization (not necessarily the \nsuccess-index), it is essential to determine an appropriate procedure for constructing the \nnormalization term (CTi, in the case of the success-index). Three are the most critical issues in \n 2\ndoing this, as also described in [Franceschini et al., 2012b]: \n Defining the procedure for selecting the reference sample of publications. Possible \napproaches are: (i) the selection of papers published by the same journal, (ii) the use of \nsuperimposed classifications such as ISI subject categories, (iii) or the implementation of \n“adaptive” techniques in which the sample is determined considering the “neighbourhood” \nof the publication(s) of interest—typically consisting of the set of publications citing or \nbeing cited by them. \n Deciding whether to consider (i) the distribution of the number of references made or (ii) the \ndistribution of the citations received by the publications of the reference sample. \n Identifying a suitable (central tendency) indicator for obtaining CTi from the distribution of \ninterest, e.g., mean, median, harmonic mean, percentiles, etc.. \nThese three issues are valid for the construction of a generic field-normalized indicator, not \nnecessarily the success-index. The first issue is particularly critical and currently much debated \namong bibliometricians; the reason is that the sample must be large enough to be statistically \nrepresentative but, at the same time, should not be “polluted by outsider papers”, such as papers \nfrom other (sub-)disciplines. \nFranceschini et al. (2012a, 9th page) mention—for the mere purpose of example—some \nsimplified procedures for calculating CTi: \n ir , i.e., the number of citations made by the (i-th) publication concerned (case of the \nKosmulski’s NSP-index); \n  JY ir  or  JY ir , i.e., the mean or median number of references made by the articles \npublished in the same journal (J) and year (Y) of the (i-th) publication concerned; \n  JY ic  or  JY ic , i.e., the mean or median number of citations received by the articles \npublished in the same journal (J) and year (Y) of the (i-th) publication concerned; \n  N ir  or  N ir , i.e., the mean or median number of references made by a sample of \npublications representing the “neighbourhood” of the (i-th) publication concerned; \n  N ic  or  N ic , i.e., the mean or median number of citations received by a sample of \npublications representing the “neighbourhood” of the (i-th) publication concerned. \nWe remark again that choosing the optimum procedure is still an open question, as reported in \n[Franceschini et al., 2012a, 9th page]: the typical issues concerning (1) the sample selection and \n(2) the choice of a suitable indicator for denoting the propensity to cite remain still open.  \nAlso, we note that the first of the aforementioned alternative procedures is the one used for the \nNSP-index. \n 3\n3. The letter by Kosmulski. Let’s come now to the point. In a recent letter to the editor, \nKosmulski (2012, page 368) begins as follows: Franceschini, Galetto, Maisano, and \nMastrogiacomo (in press) defined a new bibliometric index representing the scientific output of \na scientist: success-index=∑scorei, where the sum is taken over all publications of a scientist, \nand scorei = 1 when the number of citations received by the publication i is greater than the \nmedian number of citations received by all articles published in the same journal and in the \nsame year—i.e.,  JY ic —and scorei = 0 otherwise (please, compare this definition—especially \nthe text underlined—with the original one [Franceschini et al., 2012a, 9th page] …any \ndiscrepancy?!).  \nThen follows a detailed criticism to this specific definition. In a nutshell, Kosmulski explains \nthat estimating the citation propensity of an article by  JY ic  leads to penalize the articles \npublished by prestigious journals—i.e., journals with articles of relatively high citation \nimpact—while would favour modestly cited articles published by low impact journals. The \nconcept can be interpreted through a metaphor that we introduce: for one star (publication) of \nmoderate shine (citation impact) is much easier to stand out in a constellation (journal) \nembracing not very bright stars (publications of low impact), than in a constellation with many \n“blinding” stars (publications of high impact). \nNext, using a mocking tone, Kosmulski renames the success-index as modesty-index, as it \nprimarily rewards publication of high-impact articles in low-impact journals [Kosmulski, 2012, \npage 368]. \nKosmulski’s criticism is reasonable because the median number of citations received by the \narticles of a scientific journal—specialized in a certain (sub-)discipline—provides a rather \ndistorted estimate of the citation propensity, which does not necessarily reflect the citation \npropensity of the totality of the publications in that (sub-)discipline. It is well known that there \nis a certain “bias” in the sense that a few prestigious journals tend to include most of the articles \nof great impact, while less prestigious journals tend to publish articles (almost exclusively) of \nlow impact [Garfield, 1979]. \nIt is worth recalling that the use of CTi = ri—as suggested by Kosmulski (2011)—also \nintroduces a bias, although of a different nature. \nFurther considerations \nThe real misunderstanding of the Kosmulski’s letter is not given by his reflections; instead, it is \nrepresented by the underhand alteration of the definition of the success-index [Kosmulski, 2012, \npage 368]. \nAs seen before, the success-index is constructed by associating each publication with a comparison \n 4\nterm CTi, which should represent the citation propensity of a statistically significant sample of \nhomologous publications. Despite the ample variety of options for constructing CTi (some of which \nmentioned before) it is stated that: estimating the propensity to cite by a sample of publications that \nrepresent the neighbourhood […] seems to be a more “adaptive” and accurate method \n[Franceschini et al, 2012a, 2nd page]. Also, we clarify that the “neighbourhood” of the \npublication(s) of interest has been defined as the set of publications citing or being cited by them \n[Franceschini et al, 2012a, 2nd page]. \nIn a recent paper the authors propose a more structured technique for selecting the sample of \nhomologous publications, referring to journal articles [Franceschini et al., 2012c]. From the \nperspective of an i-th paper of interest, this technique is based on the following steps (see Fig. 1):  \n(a) identification of the i-th paper of interest and the corresponding journal (J);  \n(b) identification of other articles published by J in the recent years (e.g., the last 5-10 years);  \n(c) definition of a reference sample consisting of the papers (from the whole scientific literature) \nthat cite the papers identified at point (b). \n(a) i-th paper \nof interest \n(b) Papers published by the same \njournal (J)  in the last years \n (c) Sample consisting of  the papers (from the whole \nliterature) citing the articles at point (b) \n… (published in J)  journal J \n \nFig. 1. Scheme of a possible technique for selecting a reference sample of publications, according to which to \nconstruct the CTi related to an i-th paper of interest. \nThis technique is inspired by a procedure by Moed (2010) to determine the Database Citation \nPotential (DCP), which is used as a normalization term for the Source Normalized Impact per \nPaper (SNIP), i.e., an annual field-normalized indicator for ranking scientific journals. \nThere are two key assumptions underlying this technique: (1) articles issued by the same journal (J) \nroughly concern the same (sub-)discipline and (2) articles citing other articles issued by J are \nrelatively similar as regards their citation propensity. Also, it is necessary to avoid inconsistencies \namong the different article types within the reference sample (e.g., research articles, reviews, brief \ncommunications, letters), due to the different propensity to cite. For instance, this can be done by \nlimiting the analysis to research articles only.  \nFor the purpose of example, Fig. 2 reports a structured comparison among three possible standards \nfor constructing CTi, i.e., ii rCT )1( ,  iJYi rCT ~)2(   and  icJi rCT ~)3(  —the last standard is founded \non the sample selection technique illustrated in Fig. 1.  \n   third or the     \n This example should not be intended as an\nempirical proof of the superiority of the second standard with respect to the first one, \n 5\nWe note that the use of different standards may entail considerable differences in the resulting CTi \nvalues. \n(b) Example of calculation of CTi values and the success-index, for an anonymous scientist \n(a) Comparison among three standards for calculating CTi  \nCTi ii rCT )1(   iJYi rCT ~)2(    icJrCTi ~)3(   \nDescription Number of references \nmade by the (i-th) paper \nof interest. \nMedian number of references \nmade by the articles issued in \nthe same journal (J) and year (Y) \nof the (i-th) publication \nconcerned. Selection should be \nlimited to articles of the same \ntype. \nMedian number of references made \nby the articles citing other articles \nissued (in the last years) by the same \njournal (J) of the (i-th) publication \nconcerned (see Fig. 1). Subscript “cJ” \nstands for “citing articles from J”. \nSelection should be limited to articles \nof the same type. \nWhich citing propensity \nis estimated? \nThat of the very single \narticle of interest. \nThat of the articles issued by \njournal J, in the year Y. \nThat of the articles (from the whole \nliterature) citing other articles issued \nby J. \nComputational load of \ndatabase queries \nLow: just count the \nnumber of references \nmade by the paper \nconcerned. \nMedium: the papers issued by J \nhave to be examined. \nHigh: apart from analysing the papers \nissued by J, it is necessary to analyse \nthe papers citing them. \n(Potential) drawbacks Statistically fragile and \nsomehow prone to \nmanipulation by authors. \nIt may reflect some potential \nparticularities of a journal, \ninstead of (sub-)field \ncharacteristics. \n- \n \ni-th article \nof interest Journ. (J) ci ii rCT \n)1(   iJYi rCT ~)2(    icJrCTi ~)3(   \n1 J1 117 11  27.0  20.0 \n2 J2 52 9  22.0  16.4 \n3 J3 21 21  29.0  24.2  \n4 J4 15 16  26.0  23.2  \n5 J4 11 10  26.0  23.2  \n6 J5 4 6  15.0  18.2  \n7 J6 1 26  25.0  22.8  \n8 J7 1 71  22.0  22.4  \n9 J1 0 3  27.0  20.0  \n  success\n(1)=NSP=3  success(2)=2  success(3)=2  \nFig. 2. Comparison among three possible standards to construct the CTi related to an i-th paper of interest: \n, ii rCT )1(  iJYi rCT ~)2(  ,  icJi rCT ~)3(  . Precisely, (a) reports a description of the major peculiarities, while (b) \nan example of calculation of CTi values (and next the success-index) according to the three standards. The \nexample refers to a portion of the scientific production of an anonymous scientist;  and  respectively denote \npapers included and not included in the so-called success-core [Franceschini et al., 2012a]. \nIt is worth remarking that the technique illustrated in Fig. 1 \n .   ,  -\nindex  ,    . \n,   CT . Of course, we are convinced that a\n   . , we are currently \ndeveloping an application able to    e.g.,   \n)\ncan be rather complex as regards the \namount of database queries Using the words of one of the referees the construction of the success\nentails a “herculean” effort much greater than that required for the construction of NSP\nHowever this is the price to pay for estimating i properly  \npractical prerequisite of the procedure is its automation For this reason\nautomatically querying bibliometric databases ( WoS or\nScopus . \nIn the three standards of Fig. 2, it may be also seen that CTi is constructed based on the citation \n 6\nmade by a reference sample of publications (which is unitary in the first case). Of course, other \npossible estimates of CTi can be based on the citations received; for instance, one could use the \nmean/median number of citations received by the articles of the reference sample. About this, in \n[Franceschini et al., 2012a, 6th page] the authors state that the indicators based on the distribution of \ncitations made—rather than those received—have several advantages: (1) the number of citations \nmade (related to a reference sample of publications) is fixed over time, while the number of \ncitations received tends to increase and requires a physiological accumulation period to stabilize—\ntypically, around 3–5 years depending on the disciplines. For this reason, indicators based on the \nnumber of references look more stable and robust, especially for relatively recent samples of \npublications. (2) This stability is also derived by the fact that the number of references is likely to \nbe, on average, less variable than the number of citations received. The estimation will therefore be \nless subject to fluctuations. (3) Certainly, the citations that a present publication will receive will \ncome from future publications. Therefore, it is somehow questionable to estimate the future \npropensity to cite by the present one. However, since changes in the propensity to cite generally \nrequire a large number of years (hardly less than 10–15 years, the result of this approximation is \nnot very distorted. \nWe believe that the information given here is sufficient to demonstrate how inappropriate and \nunjustified the Kosmulski’s criticism is, since it is based on a “far-fetched interpretation” of the \nsuccess-index. \nBut there’s more! In our article two simplified examples of application of the NSP- and the success-\nindex are presented; the first concerning four scientific journals and the second concerning two \nscientists from different disciplines [Franceschini et al., 2012a, Subsection Empirical application \nexamples of the success-index]. Before presenting the data, the following statement on the \ncalculation of CTi is made [Franceschini et al., 2012a, 9th page]: Despite the claimed “freedom” in \nthe construction of CTi, for the purpose of simplicity and practicality, it will be hereafter calculated \nas  JY ir . These examples show that the results obtained using the NSP- and the success-index may \nbe significantly different. \nHaving said that, the second example reported in the original paper [Franceschini et al., 2012a] is \nrecalled by Kosmulski (2012, Table 1) inappropriately, in order to support his criticism. \nThe authors thank Kosmulski for his “funny irony(!)” and his contribution to the criticism of one of \nthe possible alternative procedures for the construction of CTi—i.e., that one based on  JY ic . \nWhile Franceschini et al. (2012a) focused on the vulnerability of the estimation by CTi = ri, as \nproposed by Kosmulski (2011), Kosmulski (2012) focused on the fragility of the estimation by \nCTi = .  JY ic\n 7\n 8\nAccording to the authors, the intense debate on the success-index’s potential and the best strategy \nfor constructing CTi can lead to interesting ideas for tackling the problem of field-normalization in \ngeneral. \nReferences \nFranceschini, F., Galetto, M., Maisano, D., Mastrogiacomo, L. (2012a). The success-index: an alternative \napproach to the h-index for evaluating an individual’s research output. To appear in Scientometrics. DOI: \n10.1007/s11192-011-0570-z. \nFranceschini, F., Galetto, M., Maisano, D., Mastrogiacomo, L. (2012b). An informetric model for the \nsuccess-index. Submitted to Journal of Informetrics. \nFranceschini, F., Maisano, D., Mastrogiacomo, L. (2012c). Evaluating research institutions: the potential of \nthe success-index. Submitted to Scientometrics. \nGarfield, E. (1979). Is citation analysis a legitimate evaluation tool? Scientometrics, 1(4), 359–375. \nGlänzel, W. (2011). The application of characteristic scores and scales to the evaluation and ranking of \nscientific journals. Journal of Information Science, 37(1), 40–48. \nHirsch, J.E. (2005). An index to quantify an individual’s scientific research output, in Proceedings of the \nNational Academy of Sciences of the United States of America, 102, 16569–16572. \nHirsch, J.E. (2007). Does the h index have predictive power?  PNAS, 104(49), 19193–19198. \nKosmulski, M. (2011). Successful papers: A new idea in evaluation of scientific output. Journal of \nInformetrics, 5(3), 481–485. \nKosmulski, M. (2012). Modesty-index. Journal of Informetrics, 6(3), 368–369. \nISI Web of Knowledge (2012). Essential Science Indicators. http://thomsonreuters.com [11 July 2012] \nMoed, H.F. (2010). Measuring contextual citation impact of scientific journals. Journal of Informetrics, 4(3), \n265–277. \nWaltman, L., Van Eck, N.J. (2012). The inconsistency of the h-index. Journal of the American Society for \nInformation Science and Technology, 63(2), 406–415.  \nVinkler. P. (2011). Application of the distribution of citations among publications in scientometric \nevaluations. Journal of the American Society for Information Science and Technology, 62(10), 1963–\n1978. \n",
            "id": 4834184,
            "identifiers": [
                {
                    "identifier": "11429359",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2135318118",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "10.1016/j.joi.2012.07.005",
                    "type": "DOI"
                },
                {
                    "identifier": "234893065",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:iris.polito.it:11583/2502038",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "188390274",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:porto.polito.it:2502038",
                    "type": "OAI_ID"
                }
            ],
            "title": "Further clarifications about the success-index",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2135318118",
            "oaiIds": [
                "oai:porto.polito.it:2502038",
                "oai:iris.polito.it:11583/2502038"
            ],
            "publishedDate": "2012-01-01T00:00:00",
            "publisher": "Elsevier BV:PO Box 211, 1000 AE Amsterdam Netherlands:011 31 20 4853757, 011 31 20 4853642, 011 31 20 4853641, EMAIL: nlinfo-f@elsevier.nl, INTERNET: http://www.elsevier.nl, Fax: 011 31 20 4853598",
            "pubmedId": null,
            "references": [
                {
                    "id": 6446519,
                    "title": "An index to quantify an individual’s scientific research output,",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1073/pnas.0507655102",
                    "raw": "Hirsch, J.E. (2005). An index to quantify an individual’s scientific research output, in Proceedings of the National Academy of Sciences of the United States of America, 102, 16569–16572.",
                    "cites": null
                },
                {
                    "id": 6446513,
                    "title": "An informetric model for the success-index.",
                    "authors": [],
                    "date": "2012",
                    "doi": "10.1016/j.joi.2012.09.008",
                    "raw": "Franceschini, F., Galetto, M., Maisano, D., Mastrogiacomo, L. (2012b). An informetric model for the success-index. Submitted to Journal of Informetrics.",
                    "cites": null
                },
                {
                    "id": 6446528,
                    "title": "Application of the distribution of citations among publications in scientometric evaluations.",
                    "authors": [],
                    "date": "2011",
                    "doi": "10.1002/asi.21600",
                    "raw": "Vinkler. P. (2011). Application of the distribution of citations among publications in scientometric evaluations. Journal of the American Society for Information Science and Technology, 62(10), 1963– 1978.",
                    "cites": null
                },
                {
                    "id": 6446521,
                    "title": "Does the h index have predictive power?",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1073/pnas.0707962104",
                    "raw": "Hirsch, J.E. (2007). Does the h index have predictive power?  PNAS, 104(49), 19193–19198.",
                    "cites": null
                },
                {
                    "id": 6446514,
                    "title": "Evaluating research institutions: the potential of the success-index.",
                    "authors": [],
                    "date": "2012",
                    "doi": "10.1016/j.joi.2012.07.005",
                    "raw": "Franceschini, F., Maisano, D., Mastrogiacomo, L. (2012c). Evaluating research institutions: the potential of the success-index. Submitted to Scientometrics.",
                    "cites": null
                },
                {
                    "id": 6446516,
                    "title": "Is citation analysis a legitimate evaluation tool?",
                    "authors": [],
                    "date": "1979",
                    "doi": null,
                    "raw": "Garfield, E. (1979). Is citation analysis a legitimate evaluation tool? Scientometrics, 1(4), 359–375.",
                    "cites": null
                },
                {
                    "id": 6446525,
                    "title": "of Knowledge (2012). Essential Science Indicators.",
                    "authors": [],
                    "date": "2012",
                    "doi": null,
                    "raw": "ISI Web of Knowledge (2012). Essential Science Indicators. http://thomsonreuters.com [11 July 2012] Moed, H.F. (2010). Measuring contextual citation impact of scientific journals. Journal of Informetrics, 4(3), 265–277.",
                    "cites": null
                },
                {
                    "id": 6446523,
                    "title": "Successful papers: A new idea in evaluation of scientific output.",
                    "authors": [],
                    "date": "2011",
                    "doi": "10.1016/j.joi.2011.03.001",
                    "raw": "Kosmulski, M. (2011). Successful papers: A new idea in evaluation of scientific output. Journal of Informetrics, 5(3), 481–485.",
                    "cites": null
                },
                {
                    "id": 6446517,
                    "title": "The application of characteristic scores and scales to the evaluation and ranking of scientific journals.",
                    "authors": [],
                    "date": "2011",
                    "doi": "10.1177/0165551510392316",
                    "raw": "Glänzel, W. (2011). The application of characteristic scores and scales to the evaluation and ranking of scientific journals. Journal of Information Science, 37(1), 40–48.",
                    "cites": null
                },
                {
                    "id": 6446526,
                    "title": "The inconsistency of the h-index.",
                    "authors": [],
                    "date": "2012",
                    "doi": "10.1002/asi.21678",
                    "raw": "Waltman, L., Van Eck, N.J. (2012). The inconsistency of the h-index. Journal of the American Society for Information Science and Technology, 63(2), 406–415.",
                    "cites": null
                },
                {
                    "id": 6446511,
                    "title": "The success-index: an alternative approach to the h-index for evaluating an individual’s research output.",
                    "authors": [],
                    "date": "2012",
                    "doi": "10.1016/j.joi.2012.07.005",
                    "raw": "Franceschini, F., Galetto, M., Maisano, D., Mastrogiacomo, L. (2012a). The success-index: an alternative approach to the h-index for evaluating an individual’s research output. To appear in Scientometrics. DOI: 10.1007/s11192-011-0570-z.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [],
            "updatedDate": "2021-04-30T09:56:41",
            "yearPublished": 2012,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "1751-1577"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/pdf/11429359.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/11429359"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/11429359/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/11429359/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/4834184"
                }
            ]
        },
        {
            "acceptedDate": "2009-07-22T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Benso, Alfredo"
                },
                {
                    "name": "Di Carlo.S."
                },
                {
                    "name": "Politano, Gianfranco Michele Maria"
                },
                {
                    "name": "Sterpone, Luca"
                }
            ],
            "contributors": [],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/11412943",
                "https://api.core.ac.uk/v3/outputs/234876558"
            ],
            "createdDate": "2013-07-10T14:51:56",
            "dataProviders": [
                {
                    "id": 12601,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/12601",
                    "logo": "https://api.core.ac.uk/data-providers/12601/logo"
                },
                {
                    "id": 351,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/351",
                    "logo": "https://api.core.ac.uk/data-providers/351/logo"
                }
            ],
            "depositedDate": "2009-05-01T00:00:00",
            "abstract": "Gene expression is the fundamental control of the structure and functions of the cellular versatility and adaptability of any organisms. The measurement of gene expressions is performed on images generated by optical inspection of microarray devices which allow the simultaneous analysis of thousands of genes. The images produced by these devices are used to calculate the expression levels of mRNA in order to draw diagnostic information related to human disease. The quality measures are mandatory in genes classification and in the decision-making diagnostic. However, microarrays are characterized by imperfections due to sample contaminations, scratches, precipitation or imperfect gridding and spot detection. The automatic and efficient quality measurement of microarray is needed in order to discriminate faulty gene expression levels. In this paper we present a new method for estimate the quality degree and the data's reliability of a microarray analysis. The efficiency of the proposed approach in terms of genes expression classification has been demonstrated through a clustering supervised analysis performed on a set of three different histological samples related to the Lymphoma's cancer diseas",
            "documentType": "research",
            "doi": "10.1109/memea.2009.5167990",
            "downloadUrl": "https://core.ac.uk/download/pdf/11412943.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "Gene expression reliability estimation through  \ncluster-based analysis \nLuca Sterpone, Alfredo Benso, Stefano Di Carlo, Gianfranco Politano \nDipartimento di Automatica e Informatica (DAUIN) \nPolitecnico di Torino \nTorino, Italy \ncontact mail: luca.sterpone@polito.it \n \nAbstract—Gene expression is the fundamental control of the \nstructure and functions of the cellular versatility and adaptability \nof any organisms. The measurement of gene expressions is \nperformed on images generated by optical inspection of \nmicroarray devices which allow the simultaneous analysis of \nthousands of genes. The images produced by these devices are \nused to calculate the expression levels of mRNA in order to draw \ndiagnostic information related to human disease. The quality \nmeasures are mandatory in genes classification and in the \ndecision-making diagnostic. However, microarrays are \ncharacterized by imperfections due to sample contaminations, \nscratches, precipitation or imperfect gridding and spot detection. \nThe automatic and efficient quality measurement of microarray \nis needed in order to discriminate faulty gene expression levels. \nIn this paper we present a new method for estimate the quality \ndegree and the data’s reliability of a microarray analysis. The \nefficiency of the proposed approach in terms of genes expression \nclassification has been demonstrated through a clustering \nsupervised analysis performed on a set of three different \nhistological samples related to the Lymphoma’s cancer disease. \nKeywords-DNA microarray, profiling, gene expression, \nreliability classification \nI.  INTRODUCTION \nNowadays, the study of organisms are based essentially on  \ngenomic. Numerous genome related projects create several \nthousands of biological meaningful information and enable the \nexploration of gene functions belonging to Deoxyribonucleic \nAcid (DNA) sequences. Analyze the gene functionalities allow \nto determine the cellular process that have been disrupted or \ncompromised thus providing a window of the gene’s biological \nrole. Several methods have been used in the past to report the \ngene’s expression. Many of these methods are based on fairly \nlabor-intensive operations. Recently, the gene expression \nanalysis have been revolutionized by the introduction of DNA \nmicroarrays. These devices allow to analyze the gene \nexpression by the visual inspection of Ribonucleic Acid (RNA) \nproduced by thousands of genes to be monitored at once. By \nexamining the expression of several genes simultaneously, it is \npossible to identify and study the gene expression patterns of a \ntype of cellular’s tissue under a certain physiological condition \n[1]. Physically, a DNA microarray consists of a solid glass or \nsilicon surface, studded with a large number of DNA \nfragments, each containing a nucleotide sequence that serves as \na probe for a specific gene. DNA microarrays have been used \nto examine in particular the gene expression signature of \ndifferent types of human cancer cells, providing the study of \nadditional layer of information useful for predicting gene \nfunctions in relation to cancer diseases.  \nOnce the hybridization process is completed, the DNA \nmicroarray is visual inspected by an automated scanning-laser \nmicroscope that scans a microarray slide with several blocks of \ntwo dimensional (2-D) arrays where the DNA fragments are \nlocalized. The result is recorded in the form of an image, where \nthe most expressed genes are indicated by an higher intensity \nwith different color channels ranging from the green cyanine \ndyes, Cy3, and the red cyanine Cy5. The measurement of the \ngene expression levels is obtained analyzing the resulting \nimage. Nevertheless the DNA fragments contained in the \nmicroarray have prior known positions due to their regular \nstructure, several issues during the biological process influence \nthe quality of the measurement.  \nImperfections in microarray are due to sample \ncontamination, scratches, precipitation, imperfect gridding or \nsegmentation. These imperfections affect the extractions of the \ngene expression levels compromising the microarray \nclassification. Since the microarray analysis are performed on \nrecorded images, quality measurements is retrospective and \nthus need automatic tools to detect, censor or flagging specific \ngenes that are not correctly expressed and if considered will \ncontribute erroneously to the microarray classification [2]. \nAutomatic tools exist to estimate the DNA segment shape in \ncDNA array using a metric specialized in the automatic \ngridding and in the local qualifications of the spot [3] [4]. \nHowever, the major challenge remain in the identification of \nthe appearance irregularity of a grid and on the measurement of \nthe illumination noise that corrupts the expected characteristics \nof the genetic markers where DNA fragments are placed.  \nA previous work proposed a method for determining \nindividual DNA fragments and their borders in order to \nmaximize the detected DNA fragments and to compensate the \nerrors introduced by artifacts [5], however that algorithm does \nnot provide any evidence of the quality measures on the \nmicroarray images. A recent work proposes an algebraic \nframework for count faulty DNA fragments, however this \nmethod is able to produce only an average probability of failure \ndetected fragments, while no reliability information are given \nabout the level of confidence and the accuracy of the gene \nexpression levels computation [6].  \nIn this paper we propose a new method for measure and \nestimate the reliability of the gene expression levels through \nthe analysis of the DNA microarray fragments. The method is \nbased on an analysis flow consisting of spot finding and image \nsegmentation of DNA microarray images using the embedded \ndual core platform developed in [7] and integrated with two \nnovel software modules. A quality assurance module, able to \nindividuate, through a set of image rules, the imperfections in \nDNA microarray images, and a hierarchical clustering \nMeMeA 2009 - International Workshop on Medical Measurements and Applications \nCetraro, Italy \nMay 29-30, 2009\n978-1-4244-3599-9/09/$25.00 ©2009 IEEE 229\n \nFigure 2. The flow of the proposed \nmethod. \nalgorithm able to create a reliability metric on the set of \nanalyzed microarray samples and to compute the level of \naccuracy (in terms of quality and reliability) of the marker \ngenes.  \nThe paper presents also an experimental analysis using a set \nof DNA microarray images related to three different \nhistological classes: Normal Tissue, Follicular Lymphoma and \nDiffuse Large B-cell Lymphoma. The results demonstrated the \ncapability of the proposed method to provide an estimation of \nthe reliability degree of the gene expression levels.  \nII. DNA  MICROARRAY SEGMENTATION ERRORS \nAs illustrated in figure 1 a DNA microarray image is \ncharacterized by three main objects: the DNA fragments (or \nspots), the Sub-grids and the Background.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 1. An example of ideal slice of a DNA microarray image on the top-\nleft. An example of Low line error in the top-right. Some examples related to \nmissing segmentation, cross hybridization, scratches and margin lines \nphenomena that can happen during the segmentation phase.  \nDigital DNA microarray images are characterized by two \nmain problems: the noise level and the low pixel intensity. \nWhile in the first case, the integrity of the DNA fragment is \naffected by the neighborhood high intensity pixels, in the \nsecond case the DNA fragment is difficulty recognizable since, \nhaving low intensity level, is not discriminated with respect to \nthe background of the image. \nConsidering the different type of errors, we classified the \neffects considering their regions, noise and genetic \ncharacteristics. When the region characteristics are considered, \nthe classification is:  \n• Local: the errors are related only to a single grid, while the \nother grids are not affected. An example of this error is the \nmissing segmentation, as reported in figure 1. These errors \nare generally correctable by moving the interested grid in the \nright position.  \n• Expanded: the errors are related to more than a single grid, \ni.e. the errors overlap two neighboring grids. \nWhen the signal/noise ratio is considered, the errors are \nclassified  as: \n• Low line: the missing segmentation is placed to a central \nregion of the grid, as illustrated in figure 1. This is provoked \nby signal of low intensity internally to a grid. \n• Margin line: the missing segmentation is located at the \nmargin of a grid. This happens since the intensity of the DNA \nfragments progressively decrease along the x axis. Besides, in \nsome cases the geometry of the DNA microarray is \ncharacterized by a minor number of DNA fragments in the \nlast line of each grid. In this case the average intensity \ndecreases and thus the autocorrelation coefficient. An \nexample of this phenomena is illustrated in figure 1.c. \nFinally, considering the genetic characteristics of each \nDNA fragments, two further effects are considered: \n• Cross hybridization: the error is provoked for the annealing \nof a single-stranded DNA fragments to a single-stranded \ntarget DNA to which it is only partially complementary. \nThe results of this effects are two (or more) equally wrong \nexpressed neighborhood DNA fragments. \n• Scratches: this type of error is due for the inclusion in the \nDNA fragment segmentations of pixels with artifacts (i.e. \ndust particles, scratches or spot contaminants). The \nconsequence is a poor signal separation between the DNA \nfragments and the background, thus incrementing and \ndistorting the correct signal/background ratio.  \nThese type of effects are the most critical ones, since they \nare not identifiable through gridding or segmentations \nalgorithm and therefore require a further data analysis.  \nIII. THE PROPOSED METHOD \nThe flow of the present method is illustrated in figure 2. \nThe images of the DNA microarray samples under analysis are \nelaborated through the platform we developed in [7]. It \nperforms the gridding and the segmentation of the several \nDNA microarray images. Two new modules have been \ndeveloped: the \nsegmentation analysis and \nthe cluster reliability \nestimation. \nA database containing \nthe coordinates of the \nsegmentation of each \nimage is generated. The \ncoordinates are then used \nin order to perform a \nsegmentation analysis of \nthe region and signal/noise \neffects. For this purpose a \nset of segmentation rules \nhave been defined in order \nto classify each considered \nDNA fragment. Finally, a hierarchical clustering algorithm is \nperformed according to the marker genes of the considered \nhistological analysis, this algorithm is able to consider the \ncross-hybridization and the scratches effects. It generates the \nclassification of the microarray samples and a list of selected \ngene expression levels, each one referred to a single DNA \nmicroarray, reporting the correspondent level of quality and \nreliability. The method generates two parameter for each \nmarker gene: the quality and the reliability coefficients. The \nformer indicates the regularity of the DNA fragments in terms \nof shape and morphology, while the second indicates the \ninfluence of that gene in the final gene expression \nclassification.  \n230\nA. Fragments Segmentation Rules  \nThe Fragments Segmentation Rules are a set of typical \nshape used to compare each analyze spot. The set consists of:  \n1. Regular spot, centered without noise: the segmentation \nanalysis compares the shape of the spot with a circular \nregion.  \n2. Irregular spot, centered, without noise: the irregular \nmorphology of the spot does not affect the results, since the \nanalysis is performed considering only the spot’s intensity \nvalue.  \n3. Regular spot, not centered, without noise: the spot \nsegmentation is incorrect. The spot region must be moved \nin the corrected position in order to have the correct result. \n4. Regular spot, centered, with noise: the existence of noise \nrepresents the major drawback of the segmentation \nalgorithm. In this case the spot must be flagged as not \ncorrectly analyzable. \nB. Clustering Reliability Estimation Algorithm (CREA) \nThe CREA algorithm works on the basis of the final \nsegmentation and on the marker genes used for the analysis. \nStarting from the marker genes, it performs a hierarchical \nclustering with a supervised approach that uses the phenotypic \ninformation associated to each microarray sample.  \nThe algorithm executes the following steps: segmentation \nselection, prior clustering and classification. The first step \nconsists in analyzing the spots flagged by the segmentation \nanalysis and identifying their redundant elements within the \nmicroarray. In the case the flagged spots belong to the set of \nmarker genes, they are included in the reliability metric. The \nsecond phase executes the clustering of the samples basing on \nthe marker genes and their redundant elements, this is \nconsidered as the prior cluster. The third phase creates all the \npossible combination of clusters and computes the reliability \nparameter for each gene that is expressed as the ratio between \nthe number of clusters equal to the prior cluster and the total \nnumber of combination generated. By this way, the reliability \nparameter indicates the percentage of influence on the \nclassification of a selected gene. Lower is the percentage of the \nreliability parameter, major is the probability that an error on \nthe considered gene affects the classification.  \nThe results of the algorithm is a dendogram diagram and a \ngene expression quality and reliability estimation related to the \nuncorrected identified genes. By exploring the generated \ndendogram, it is possible to identify the classification’s group \nof the considered DNA microarray samples, while considering \nthe gene list is possible to evaluate the influence of that genes \non the classification. \nIV. EXPERIMENTAL RESULTS \nWe validated the proposed method on a set of real data, \nrelated to the Lymphoma disease. We analyzed 16 samples of \ntwo different histological cases related to the Follicular \nLymphoma (samples 14 – 16) and Diffuse Large B-Cell \nLymphoma (6 – 13) and related to a sane tissue, Normal Tissue \n(1 – 5). The samples are available from the Stanford \nMicroarray Database [8].  \nWe performed two analysis. The first analysis has been \nexecuted on the original samples in order to identify the quality \nand reliability estimation. We achieved the results reported in \nthe Figure 3.a and in the Table 1, the sample 13 results not \ncorrectly classified since it has been classified erroneously as \nFollicular Lymphoma. The table 1 shows the list of quality and \nreliability gene’s characteristics computed by the CREA \nalgorithm. Four marker genes related to the sample 13 has been \nidentified and two of them (Oncogen ETV6 and CD22) has a \nlower level of reliability.  \nTABLE I.  GENE EXPRESSIONS QUALITY AND RELIABILITY ESTIMATION \nSample \n#ID \nQuality and Reliability Measures \nMarker Gene Identifier Quality Reliability \n13 Oncogen ETV6 4 12% \n13 CD 22 2 5% \n13 Transcriptor TFAP4 4 98% \n13 PAK1 4 94% \nWe removed for microarray redundant gene list of the \nsample 13 the erroneously identified genes ETV6 and the \nCD22 genes and we performed a second analysis. As results \nwe obtained the right classification of the three histological \nsamples, as reported in figure 3.b.  \n \n \n \n \n         (a)           (b) \nFigure 3. The figure reports the dendogram clustering diagram obtained (a) \nwithout applying the quality and reliability results, the sample #13 (B-cell \nLymphoma) is erroneously classified with the Follicular Lymphoma, and (b) \navoiding the false positive DNA fragments reported by the proposed method, \nthe sample #13 is correctly classified as B-cell Lymphoma. \nV. CONCLUSION \nIn this paper we presented a new method to estimate the \nquality and the reliability of gene expression analysis \nperformed on DNA microarray. The experimental results \nperformed on a real genomic set of DNA microarray images \nconfirm the validity of the developed method. \nREFERENCES \n[1] Amos Mosseri and Eitan Hirsh, “Analysis of Gene Expression Data”, \nLecture 3, Tel Aviv University, 2005. \n[2] D.A. Morrison and J.T. Ellis, “The design and analysis of microarray \nexperiments: applications in parasitology”, DNA Cell Biology, 22: pp  \n357 – 394, 2003. \n[3] C.A. Glasbey and P. Ghaazl, “Combinatorial image analysis of DNA \nmicroarray features”, Bioinformatics 19: 194 – 203, 2003. \n[4] M. Bakay, Y. W. Chen, R. Borup, P. Zhao, K. Nagaraju, and E. P. \nHoffman, “Sources of variability and effect of experimental approach on \nexpression profiling data interpretation”, BMC Bioinformatics 3: 4, \n2002. \n[5] K. Blekas, N. P. Galatsanos, A. Likas, and I.E. Lagaris, “Mixture Model \nAnalysis of DNA Microarray Images”, IEEE Transactions on Medical \nImaging, Vol. 24, No. 7, July 2005. \n[6] D. Huang, O. Milenkovic, “Superimposed coding for iterative detection \nof DNA microarray spot failures”, IEEE International Workshop on \nGenomic Signal Processing and Statistics, pp. 1 – 4, 2008. \n[7] L. Sterpone, M. Violante, “A new FPGA-based edge detection system \nfor the gridding of DNA microarray images”, IEEE Instrumentation amd \nMeasurement Technology Conference, pp. 1 6, 2007. \n[8] Stanford University, “Stanford Microarray Database”, Available: \nhttp://smd.stanford.edu/ \n \n231\n",
            "id": 4824481,
            "identifiers": [
                {
                    "identifier": "10.1109/memea.2009.5167990",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:iris.polito.it:11583/2295365",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "11412943",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "234876558",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:porto.polito.it:2295365",
                    "type": "OAI_ID"
                }
            ],
            "title": "Gene expression reliability estimation through cluster-based analysis",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:porto.polito.it:2295365",
                "oai:iris.polito.it:11583/2295365"
            ],
            "publishedDate": "2009-01-01T00:00:00",
            "publisher": "IEEE",
            "pubmedId": null,
            "references": [
                {
                    "id": 6438723,
                    "title": "A new FPGA-based edge detection system for the gridding of DNA microarray images”,",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1109/imtc.2007.379114",
                    "raw": "L. Sterpone, M. Violante, “A new FPGA-based edge detection system for the gridding of DNA microarray images”, IEEE Instrumentation amd Measurement Technology Conference, pp. 1 6, 2007.",
                    "cites": null
                },
                {
                    "id": 6438721,
                    "title": "B a k a y , Y . W . C h e n , R . B o r u p , P . Z h a o , K . N a g a r a j u , a n d E . P . Hoffman, “Sources of variability and effect of experimental approach on expression profiling data interpretation”,",
                    "authors": [],
                    "date": "2002",
                    "doi": null,
                    "raw": "M .  B a k a y ,  Y .  W .  C h e n ,  R .  B o r u p ,  P .  Z h a o ,  K .  N a g a r a j u ,  a n d  E .  P . Hoffman, “Sources of variability and effect of experimental approach on expression profiling data interpretation”, BMC Bioinformatics 3: 4, 2002.",
                    "cites": null
                },
                {
                    "id": 6438720,
                    "title": "Combinatorial image analysis of DNA microarray features”,",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1093/bioinformatics/19.2.194",
                    "raw": "C.A. Glasbey and P. Ghaazl, “Combinatorial image analysis of DNA microarray features”, Bioinformatics 19: 194 – 203, 2003.",
                    "cites": null
                },
                {
                    "id": 6438722,
                    "title": "Superimposed coding for iterative detection of DNA microarray spot failures”,",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1109/gensips.2008.4555682",
                    "raw": "D. Huang, O. Milenkovic, “Superimposed coding for iterative detection of DNA microarray spot failures”, IEEE International Workshop on Genomic Signal Processing and Statistics, pp. 1 – 4, 2008.",
                    "cites": null
                },
                {
                    "id": 6438719,
                    "title": "The design and analysis of microarray experiments: applications in parasitology”,",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1089/104454903767650658",
                    "raw": "D.A. Morrison and J.T. Ellis, “The design and analysis of microarray experiments: applications in parasitology”, DNA Cell Biology, 22: pp 357 – 394, 2003.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "https://iris.polito.it/retrieve/handle/11583/2295365/52099/2009-MEMEA-Microarray.pdf"
            ],
            "updatedDate": "2020-08-03T20:11:05",
            "yearPublished": 2009,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/pdf/11412943.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/11412943"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/11412943/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/11412943/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/4824481"
                }
            ]
        },
        {
            "acceptedDate": "2009-12-10T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Caramia, M."
                },
                {
                    "name": "Di Carlo, Stefano"
                },
                {
                    "name": "Fabiano, Michele"
                },
                {
                    "name": "Prinetto, Paolo Ernesto"
                }
            ],
            "contributors": [],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/11413019",
                "https://api.core.ac.uk/v3/outputs/234876636"
            ],
            "createdDate": "2013-07-10T14:51:57",
            "dataProviders": [
                {
                    "id": 12601,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/12601",
                    "logo": "https://api.core.ac.uk/data-providers/12601/logo"
                },
                {
                    "id": 351,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/351",
                    "logo": "https://api.core.ac.uk/data-providers/351/logo"
                }
            ],
            "depositedDate": "2009-11-01T00:00:00",
            "abstract": "Designing a mass-memory device (i.e., a solid-state recorder) is one of the typical issues of mission-critical space system applications. Flash-memories could be used for this goal: a huge number of parameters and trade-offs need to be explored. Flash-memories are nonvolatile, shock-resistant and power-economic, but in turn have different drawback: e.g., their cost is higher than normal hard disk and the number of erasure cycles is bounded. Moreover space environment presents various issues especially because of radiations: different and quite often contrasting dimensions need to be explored during the design of a flash-memory based solid-state recorder. No systematic approach has so far been proposed to consider them all as a whole: as a consequence a novel design environment currently under development is aimed at supporting the design of flash-based mass-memory device for space application",
            "documentType": "research",
            "doi": "10.1109/hldvt.2009.5340180",
            "downloadUrl": "https://core.ac.uk/download/pdf/11413019.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "  \n \nFLARE: a Design Environment for FLASH-based \nSpace Applications \nMaurizio CARAMIA(*), Stefano DI CARLO (+), Michele FABIANO(+), Paolo PRINETTO(+) \n \n(*) \nThales Alenia Space Italia \nTorino, Italy \n \n{Maurizio.Caramia}@thalesaleniaspace.com \n \n (+) \nPolitecnico di Torino \nDipartimento di Automatica e Informatica \nTorino, Italy \n{Stefano.Dicarlo, Michele.Fabiano, \nPaolo.Prinetto}@polito.it \n \nAbstract – Designing a mass-memory device (i.e., a solid-state \nrecorder) is one of the typical issues of mission-critical space \nsystem applications. Flash-memories could be used for this goal: \na huge number of parameters and trade-offs need to be explored. \nFlash-memories are nonvolatile, shock-resistant and power-\neconomic, but in turn have different drawback: e.g., their cost is \nhigher than normal hard disk and the number of erasure cycles \nis bounded. Moreover space environment presents various issues \nespecially because of radiations: different and quite often \ncontrasting dimensions need to be explored during the design of \na flash-memory based solid-state recorder. No systematic \napproach has so far been proposed to consider them all as a \nwhole: as a consequence a novel design environment currently \nunder development is aimed at supporting the design of flash-\nbased mass-memory device for space applications. \n \nI. INTRODUCTION \nNowadays processing power available for embedded \ntechnology and boards is absolutely overcoming the one \navailable just a few years ago. However, in space applications \nthe very strict requirements have often driven the design \nchoices toward older and/or lower-performing radiation-\ntolerant electronics. Although each new space application has \nits own story and requires increasing intelligence and \nautonomy [1], a typical mission-critical space system \napplication includes solid state recorder(s), redundant mission \ncomputers, flight guidance and navigation systems, health \nmonitoring computers and robotic manipulator controllers. \nThe issue of solid-state recorder(s) for space applications is \naddressed in this paper. In particular, we shall present a design \nenvironment currently under development, to support the \ndesign of flash-based hard disks (HDs) for space applications. \nFlash-memory based systems are gaining acceptance and \nusage not only in the consumer market but in space \napplications, as well, where they mainly play the role of high-\ncapacity storage devices: in fact flash-memory guarantees \nboth the non-volatility in case of power loss and a highest \nstorage density [2]. Moreover they are shock-resistant and \npower-economic: power consumption is always a critical issue \nin space applications. However designing flash-based systems \nfor space application requires both exploring a huge number \nof design dimensions and evaluating a huge amount of trade-\noffs among all such dimensions. The most relevant \ndimensions include, e.g., flash-memory technology, flash-\nmemory architecture, file management system, dependability \nenhancement strategies, power consumption, weight, physical \nsize and so on. A complex and powerful design environment \nis thus needed to properly evaluate the impact of the choices \nin each dimension and the related trade-offs. \nUnfortunately no such a design environment is today \navailable: in fact in the literature each paper is typically \ntackling just one specific issue in just one design dimensions. \nNo systematic approach has so far been proposed to consider \nthem all as a whole. Such a concurrent exploration capability \nis mandatory to provide the designers a powerful design \nenvironment, capable of supporting them through all the steps \nof the design cycle, including Architectural Exploration, \nDesign Validation & Verification, (Automatic) Test insertion, \nDependability evaluation and so on. \nThis paper presents the general architecture of a novel \ndesign environment currently under development to support \nthe design of flash-based mass memories, especially for space \napplications. As pointed out before, the project is mainly \npushed by the unavailability, at our best knowledge, of a \ncommercial tool capable of supporting a systematic analysis \nand exploration of the different possible alternatives. \nThe rest of the paper is organized as follows: Section 2 \naddresses the main flash-memory peculiarities, Section 3 \nexplores the dimensions of the issues of designing flash-based \nmass memory devices, focusing the attention also on the space \napplications, Section 4 deals with the modeling of flash-based \nHDs, while Section 5 proposes a possible architecture for a \ndesign environment to support the design of flash-based hard \ndisks for space applications. \nII. FLASH-MEMORY PECULIARITIES \nFlash-memories present several interesting features that \nproperly fit with the requirements of mass-memories for space \napplications, while possible alternatives need to be evaluated. \n14978-1-4244-4823-4/09/$25.00 ©2009 IEEE\n  \nOur solid-state recorder would need a relative high capacity: \nthe first most suitable solution could be DRAMs. On the one \nhand DRAMs are very fast, reliable and provide a very high \ndata rate, but on the other hand they need a battery pack-up to \nnot lose data and this issue generate an intricate balance \nbetween battery mass and data retention time: data retention \nover years is not feasible and count of battery charge cycles \nare limited. DRAMs are not discussed anymore in this paper. \nThe second and more attractive solution is the use of flash-\nmemories. There are two major types of flash-memory in the \ncurrent market: NOR and NAND flash-memory. NOR flash-\nmemory is for EEPROM replacement and is more suitable for \nprogram execution, while NAND flash-memory is more \nsuitable for storage systems [3], [4]: Table 1 briefly sums up \nthe main characteristic of these types of flash-memory. \nThis paper addresses only NAND flash-memories: in fact \nthey are the most suitable choice for HD replacement. \nOn the one hand, flash-memories are nonvolatile, shock-\nresistant, and power-economic: a pure flash-memory based \nsolution could guarantee unlimited data retention time and no \nneed of battery backup. On the other hand flash-memories are \nstill much more expensive than hard disk drive memory. Only \nrather large data structures could be accessed and, in addition, \nDDR2 SDRAMs provide higher write/read rate (e.g., 6 Gbit/s) \ncompared to the moderate one accomplished by flash-\nmemories (e.g., up to 80/200 Mbit/s) [2]. Moreover one of the \nmain challenging aspects of flash-memories is that the space \nalready written (i.e., programmed) with data usually cannot be \noverwritten unless it is erased from the flash-memory device. \nA NAND flash-memory is usually partitioned into blocks: \neach block has a fixed number of pages and each page has a \nfixed size. A block is the smallest unit for erase operations, \nwhile read and write operations are done in terms of pages, i.e., \na page can be erased only if its whole corresponding block is \nerased, whereas a page can be read/written independently.  \nIn addition flash-memory wears out after a certain number \nof erasure cycles (i.e., actually 106 for NAND flash-memory): \nif the erasure cycles of a block exceed this number, it becomes \na “bad block” and is not reliable for storing data anymore. \n \n \n NAND NOR \nStandby Power Low/Med Low \nActive Power Low Med/High \nCost per bit Low High \nRead Speed Med/High High \nWrite Speed High Low \nErase Speed High Medium \nCapacity High Low \nErase Cycles 10\n6 \n10\n5 \nFile Storage Use Easy Hard \nCode Execution Hard Easy \nInterface I/O-like SRAM-like \nTABLE 1 – NAND VS NOR FLASH-MEMORY \n \nFinally another peculiar aspect of flash-memories is that \ntechnology provides the possibility of storing more than one \nbit of information per cell: in fact traditional flash-memory \ndevices (Single-Level Cell or SLC) store only one bit per cell, \nwhile newer devices (Multi-Level Cell or MLC) are able to \nstore typically two bits per cell. \n \nIII. ISSUES IN DESIGNING FLASH-BASED HARD-DISKS FOR \nSPACE APPLICATIONS \nWhen a flash-based system for space application has to be \ndesigned, the investigation of a vast quantity of design \nparameters needs to be defined. A possible partial taxonomy \ncould involve Flash-memory Technology, Flash-memory \nArchitecture, Flash-memory Wearing, Testing and \nDependability and, finally, Using flash-memory as Hard-Disk. \nDesigner should discriminate among which technology to \nchoose (Flash-memory Technology) and they may also have to \nselect the most appropriate flash-memory chipset (Flash-\nmemory Architecture). [18] \nIn the problem of Wearing, a significant role is played by \nthe so called wear leveling techniques, which are aiming at \ndistributing data evenly across each memory block of the \nentire flash-memory to avoid single block to wear out. Several \ninteresting wear leveling techniques have been proposed [13] \n– [16] and could be considered or higher capacity flash-\nmemory devices could be used, then especially taking care of \nthe resulting drawbacks in terms of weight and volume [2]. \nAn effective comparative analysis of some wear leveling \nalgorithms could be found in [14]. \nFlash-memory Testing is quite different from testing other \nkinds of memory: disturbances or faults not conforming to any \nof the traditionally known fault models used in testing RAMs \ncould be experienced and specific fault models are needed to \nproperly represent the most frequent physical defects are \nneeded. Then efficient test algorithms are needed and Built-In \nSelf Test (BIST) and Built-In Self Diagnosis (BISD) circuits \nhave to been taken in consideration. [18] \nHowever wearing, testing and dependability are strictly \nlinked among each other and designers should evaluate the \nright trade-off among them. \nSeveral challenging aspects need to be addressed when \nusing a flash-memory as a mass-memory device. A possible \nincomplete taxonomy of these aspects could involve: \n \n\u0001 looking for proper solutions in order to let OS \nsuccessfully communicate with NAND flash-memory \ndevices (i.e., Operating System Management) [3] – [8] \n\u0001 implementing an efficient logical to physical address \ntranslation process for fast operations (i.e., Address \nTranslation) [18] \n\u0001 proper techniques to handle blocks exceeding the \nmaximum number of erase cycles (i.e., Bad Block \nManagement)  [19] \n\u0001 proper strategies for reclaiming invalidated space to be \nerased in order to free some space (i.e., Garbage \nCollection) [18] \n15\n  \n \nFinally designers should address Error Detection And \nCorrection (EDAC) techniques, evaluating the most proper \nchoice for their design. [18] \n[18] is a more detailed investigation about these main flash-\nmemory issues discussed above. \n \nA. Flash-based Hard-Disks in the Space Environment \nA solid state recorder for critical space missions needs to \nsatisfy many different constraints, including, among the \nothers, no loss of mass memory data and the guaranteed \navailability of storage capability at End-Of-Life (EOL). A \nwell-designed flash-based memory system can meet the \nrequirements of interplanetary missions, but its design must \ncompensate for flash’s shortcomings in speed, radiation \ntolerance, noise, and read/write cycle life and this \ncompensation leverage the costs. \nOn the one hand vendors should absolutely provide flash-\nmemories physically qualified to survive in the space \nenvironment with the help of proper strategies [9] – [12], \nwhile on the other hand data integrity, reliability, simplicity, \nmodularity, and autonomy are just some of the key features to \nfulfill (e.g., reliable storage of context data, also during \nspacecraft power outage). \nMoreover designers should evaluate the most proper choice \nfor accomplishing the level of dependability requested by their \ndesign, with the help of ECC algorithms for error checking \nand correction of NAND flash-memory. A more detailed \nsurvey about the most peculiar flash-memory design \ndimensions and trade-offs to tackle during the design of flash-\nbased hard disks for space applications could be found in [18]. \n \nIV. MODELING OF FLASH-BASED HARD-DISKS \nSeveral issues and aspects need to be addressed during the \nmodeling of a flash-based HD. First of all, Figure 1 shows a \nfirst possible high view of a flash-based mass-memory device. \nThere are three main functional blocks: a Non-Volatile \nMemory is needed to provide integrity of data, a Volatile \nMemory is used for performance reasons, while a Memory \nController is managing and controlling the overall system, \nproviding several features. The mass-memory device is \ninteracting with the requests of the external world, e.g., the \noperations of the OS in use. \n \n \nFigure 1 – A Flash-based Hard Disk Architecture \n \n    (a)                  (b)  \nFigure 2 – (a) High-level and (b) low-level view of flash architecture \nIt is possible to refine this first architectural view: in fact, \nduring the design of a flash-based HD, the OS and the \napplications want to successfully communicate with the bare \nflash-memory chip. Figure 2 (a) shows that an intermediate \nblock is needed to accomplish this task. On the one hand OS \nusually would like to exploit its typical system calls (e.g., \nopen, read, write) to work with the mass-memory device, \nwithout taking care of anything else. On the other hand, the \nflash-memory chip would like to receive the most proper \ncommands to accomplish the operations previously requested \nfrom the OS: Figure 2 (b) shows a possible view of the \ncommands a flash-memory chip usually would like to get. E.g., \n[16] briefly presents a typical layered system architecture of \npopular flash-memory-based file systems. \nSo designers should develop a sort of managing part to \ntackle all the typical issues of flash-memories, presented also \nin the previous paragraphs. This is the most important and \nchallenging part of designing a flash-based HD: many often \ncontrasting issues, parameters and dimensions are involved in \nthis part, which has to address them in the most proper way. \nThis managing part could be named “flash-memory \nmanager” and could be split into its composing functional \nblocks as Figure 3 shows: these blocks represent the main \nissues a flash-memory based system has to tackle and to solve \nin the more possible efficient way. Designers have to manage \nhow the logical to physical Address Translation is \naccomplished: reliability and efficiency are only two of the \nparameters of quality of this aspect. They need also to focus \non Wear Leveling and Garbage Collection techniques and \nstrategies. At the same time, designers need to distinguish \namong several EDAC strategies: a trade-off between needed \nreliability and related costs leads their choice over a particular \ncode rather than another one. In addition designers have to \nmanage Bad Blocks. \n \nV. THE FLARE DESIGN EVALUATION ENVIRONMENT \nIn this section the proposed FLash ARchitecture Evaluation \n(FLARE) design environment to support the design of flash-\nbased hard disks for space applications is introduced. FLARE \nis currently under development. \n16\n  \nActually there is no systematic support for the development \nof a flash-based hard disk qualified for space applications. \nDesigners have always to think about the most suitable choice \nfor the specific space applications they are dealing with: the \nhuge number of variables and parameters could easily lead to \nunverified scenarios and to delayed product release. \nIn fact the level of confidence with these parameters is \ndirectly linked with the designers’ skill, cleverness and \nexperience. As a result, a systematic tool to support the design \nof flash-based hard disks for space applications is needed. \n \nA. Evaluation parameters \nFLARE tool is intended to evaluate several aspects of the \ndesign of a flash-based system. Designers have to tackle many \ncritical issues: FLARE could help them to distinguish and \nidentify the peculiar features of these aspects and to evaluate \nthe most suitable solution for them. \nThe capacity of the flash-memory is the first fundamental \nparameter to set: designers should discuss about the physical \nquantity of flash-memory required by the design. This is a \ntypical issue of space applications: in fact space critical \nmissions require minimizing all the costs as much as possible \nand the dimension of the flash-memory is the first significant \nparameter that designers and their companies have to face. \nDesigners could have to discriminate among different flash-\nmemories of different capacities during the design of their \nsystem: FLARE could provide them with an overall \nevaluation of which capacity is more suitable for their design. \nDesigning a flash-based HD means dealing with NAND \nflash-memories, which are always partitioned in blocks and in \nturn each block is divided in pages: once capacity is set, \ndesigners have to address the dimension of each block and of \neach block or, in the same way, the number of blocks and \npages for each block. \nObviously, designers could decide the capacity from the \ndimension and the number of blocks and pages, but the issue \nis practically the same. FLARE could help the designers to do \nthis decision, in order to understand which level of granularity \nwould more properly fit with the current design and to decide \nthe most suitable flash-memory chipset. \n \n \nFigure 3 – The main functional block of Flash-memory manager \nAn essential parameter to evaluate is the percentage of \nwearing of each block: especially in mission-critical space \napplications, resources are always a key-point of the mission \nand it is desirable or, usually, mandatory that the percentage \nof wasted resources is as low as possible. E.g., on the one \nhand it could be enough 2GByte NAND flash-memory with \nsome kind of wear-leveling techniques or on the other hand a \nbigger NAND flash-memory device could be requested in \norder to accomplish mission requirements. At the same time, \ndesigners could need to explore several kinds of solutions, in \norder to find that one with the most fitting percentage of \nwearing. As a consequence, this parameter is strictly linked to \nthe adopted wear-leveling strategies: with the help of FLARE, \ndesigners could evaluate how this percentage varies as the \nwear-leveling techniques change. \nAs a consequence designers need to calculate the \npercentage of flash-memory which is not “dead”, i.e., the \npercentage of blocks which did not become bad blocks at the \nEnd-Of-Life (EOL). Mission-critical space applications \nsometimes could explicitly require a fixed amount of flash-\nmemory still alive at the EOL: designers have to evaluate the \npossible alternatives and to find the most affordable solution \nat the minimum possible cost for their design. \nDesigners have to provide a well-defined level of \ndependability according to their specific design. A \nfundamental role could be played by the so called Out-Of-\nBound (OOB) data [3]: they can be exploited also to store \nsome kind of ECC/EDAC codes, in order to accomplish the \nrequired dependability. The smart reader could get the \nunavoidable trade-off between spare data and user data: in fact \nit is true that a bigger OOB area could provide higher level of \ndependability, but at the same time would provide poor \nservice in term of user data storage. Designers have to tackle \nthis issue and find the most suitable solution for their \nparticular design. Moreover, designers have to evaluate \namong the Built-In Self Test (BIST) functionalities, evaluating \nat the same time the percentage of errors detected/corrected. \nThis is only a possible incomplete taxonomy of what is \nneeded to be evaluated during the design of a flash-based \nmass-memory device for space applications. Moreover all \nthese parameters are strictly linked together and they affect \neach other in a complex way: so an exploration of these \ndifferent and quite often contrasting dimensions is needed and \nno systematic approach has so far been proposed to consider \nthem all as a whole. \n \nB. FLARE Architecture \nThe proposed FLash ARchitecture Evaluation (FLARE) \ndesign environment is aimed at supporting designers through \nall the steps of the design cycle flash-based hard disk for \nspace applications, including Architectural Exploration, \nDesign Validation & Verification, (Automatic) Test insertion, \nDependability evaluation and so on. FLARE is currently \nunder development. \nFigure 4 shows the architecture of the system. \n \n \n17\n  \n \nFigure 4 – A detailed view of FLARE architecture \n1)  System Configuration Management \nThe System Configuration Management allows setting and \nexploring the possible alternatives and design dimensions: \ndesigners are able to easily modify the memory configuration \nblock (Architecture configuration), the Test infrastructure \n(Test configuration), and all the architectural solutions aimed \nat tackling Flash aging (Bad block, Garbage Collection, Wear \nLeveling Configuration). \nThe Architecture Configuration block is intended to contain \nall the details about the architecture of the flash-memory to \nemulate: capacity, number of blocks and number of pages are \nonly some of the main architectural parameters that the \ndesigners are able to set. \nWith the Test Configuration block, the designer can set all \nthe parameters for correctly testing the proposed flash-\nmemory: all the issues addressed in the previous paragraphs \nare taken into account and the proper fault-models and the \nspecific testing strategies can be specified, always according \nto the particular application flash-memory is used for. \nAs clearly showed previously, some wear-leveling \nstrategies are needed to spread writes over the flash-memory: \ndesigners are capable to exploit the Wear Leveling \nConfiguration block to specify all the details about the wear-\nleveling strategies to adopt during the emulation campaign. \nThe range of these details can be variable: designers could \nchoose a “simple” less/more aggressive wear-leveling strategy \namong the ones just provided with FLARE tool or developing \ntheir own wear-leveling algorithm could be a valid alternative, \nin order to evaluate it. \nIf wear leveling strategies aim to spread write operations \nover the flash-memory device, at a certain point invalidated \nspace should be reclaimed: in the Garbage Collection module, \ndesigners are able to specify the strategies to identify a block, \nto collect its good pages and to erase it. It is usually strictly \nconnected with wear-leveling strategies: it could even be \nconsidered that GC preferences are managed by wear-leveling \nstrategies, but these two issues are kept separated now. \nHowever, blocks exceeding the maximum number of \nerasure cycles are marked as bad: in Bad Block Configuration \nmodule designers can set the proper parameters to mark, \nidentify and exclude bad blocks from active space memory. \nSimple well-known strategies could be used (e.g., Skip Block \nMethod) as well as new approaches can be experimented and \nevaluated by designers. \nDependability of flash-memory need to be guaranteed: \ndesigners are able to specify in the EDAC Configuration block \nall the parameters needed to accomplish the required level of \ndata integrity and reliability. E.g., a reasonable question could \nbe if a CRC code is enough to accomplish the requested level \nof reliability or something more is needed. Maybe some ECC \nwould be absolutely necessary to accomplish the required \nlevel, e.g., Orthogonal Reed-Solomon Error Correction Code \nmight be the EDAC strategy designers were looking for. \nIn EDAC Configuration block, designers are capable of \ndefining, exploring and evaluating all possible EDAC \nstrategies for their particular design. \nThe designers’ configuration choices feed the so called \nConfiguration Manager block: this layer is thought to take \ncare of managing the “static” data coming from the various \ndimensions of the design of a flash-memory device (i.e., the \n“note” modules on the right) and to get it across the core \nfunctional blocks. This layer is essential for dispatching the \nupdated configuration modules discussed above to the \nappropriate managing blocks. The architectural choice of \nhaving this kind of layer is strictly linked to flexibility: in fact \non the one hand if some changes to parameters and algorithms \nare needed, designers can simply modify the proper module(s) \nnot interesting in the rest, because the Configuration Manager \nlayer will take care of dispatching the updated configuration(s) \nto the appropriate blocks. On the other hand, designers are \ncapable of developing new (compatible) configuration \nmodules, in case they felt like the existing modules were not \nenough for their needs: adding new configuration modules to \nthe whole architecture would turn in very few efforts, thanks \nto this way of partitioning, and would result in high \nmodularity and flexibility. \n \n2)  Flash Memory Simulator \nThe system kernel is the newly developed Flash Memory \nSimulator, charged of providing the designer the possibility of \nsimulating and evaluating all the parameters of interest. \nThe Flash-memory Simulator block is one of the most \nimportant functional blocks of the FLARE tool: in fact it is \nthought to emulate the behavior of the configured flash-\nmemory. The desired architecture is specified in the \nArchitecture Configuration module discussed before: a \n“customized” architectural configuration for the flash-memory \ndevice could be identified or a ready for use configuration \ncould be chosen from a developed library. Then the \nConfiguration Manager takes this information and advertises \nthe Flash-memory Simulator block about the architectural \ndetails of the flash-memory to emulate. \n \n18\n  \n3)  Dependability Evaluation \nIn addition to the overall architecture, some fault injection \ntechniques could be considered: a Fault Injector functional \nblock is added for this purpose. It is fed by a Fault Activation \nReadout Measure (FARM) Configuration block [17]: it sets all \nthe needed parameters for the fault injector to make it work as \nrequested. In this way a fault can be injected in the system to \nevaluate its effect in the emulated flash-memory. Fault \ninjection is an additional function of the FLARE tool: in fact it \nis represented surrounded by a dotted rounded rectangle in \norder to highlight this point, i.e., it is not essential to the \ncorrectness of the FLARE tool, but at the same time it could \nbe very useful for experimenting various fault injection \ntechniques and configurations. \nA Fault injection environment provides the designer to \nassess the target system dependability via a powerful manager \nof fault injection campaigns in all the part of the system itself. \n[17] \n \n4)  Utilities \nAs the name intuitively suggests, the Monitor and Control \nblock is monitoring and controlling the output of the previous \nFlash-memory Emulator block. Designers can have under \ncontrol all the events of the core blocks in order to get a more \ncomprehensive knowledge about the countermeasure to take \nin some specific cases. The Monitor and Control block is \npeculiarly different from the Data Warehouse Tool block: in \nfact the last one is a mean with which the user can extract \ninformation about the emulated flash-memory at the EOL \ntimeline, whereas the first one is a sort of automatic tool \ninforming the user about the most significant events of the \nactual emulation campaign. \nThe use of a Database is fundamental to gather all the \ninformation needed at the EOL timeline: its role is simply to \nstore data. The user is able to access the data with the help of \na Data Warehouse Tool: data and metadata can be extracted, \ntransformed and loaded, to easily accomplish all the designers’ \nrequests. \n \nVI. CONCLUSIONS AND FUTURE WORKS \nThis paper has proposed FLARE, a design environment to \nsupport the designers during the design of flash-based hard \ndisks for space applications. The composing blocks of the \nproposed architecture highlight the high-level of modularity \nand flexibility that this tool will be able to provide to \ndesigners: in fact each block is intended to be a sort of plug-in \nblock, which can simply be plugged-out and replaced by \nanother block when necessary, without taking care of the rest. \nAs a result, designers are provided with a powerful and \nflexible environment, able to clearly identify the best choices \nfor the current design. \nFLARE tool is currently under development and refinement: \nthe first implementation data of the tool are intended to be \nprovided soon. \n \nVII. REFERENCES \n \n[1] Anthony Lai: “Space-ready, radiation-tolerant processor modules: A \nCOTS technology strategy”, Military Embedded Systems Resource \nGuide, May 2005 \n[2] Cassel M., Walter D., Schmidt H., Gliem F., Michalik H., Stähle M., \nVögele K., Roos P. Casel.: \"NAND-Flash-memory Technology in \nMass Memory Systems for Space Applications\", Proceedings Data \nSystems In Aerospace (DASIA) 2008, Palma de Mallorca, Spain, 2008 \n[3] Chang L. P., Kuo T. W.: \"An efficient management scheme for large-\nscale flash-memory storage systems\", Proceedings of the 2004 ACM \nSymposium on Applied Computing , Nicosia, Cyprus, 862-868, 2004 \n[4] Hsieh Jen-Wei, Tsai Yi-Lin, Kuo Tei-Wei, Lee Tzao-Lin: \n\"Configurable Flash-Memory Management: Performance versus \nOverheads\" IEEE Transactions on Computers, Vol. 57, no. 11, 2008 \n[5] Intel Corporation, Technical Report: \"Understanding the Flash \nTranslation Layer (FTL) Specification\", December 1998 \n[6] Woodhouse D., Red Hat, Inc.: “JFFS : The Journalling Flash File \nSystem”, http://sources.redhat.com/jffs2/jffs2.pdf , 2001 \n[7] JFFS2, http://sourceware.org/jffs2/ \n[8] Aleph One Company, Cambridge, UK: “Yet Another Flash File \nSystem”, http://www.aleph1.co.uk/yaffs/index.html, 2002 \n[9] Brüggemann M., Schmidt H., Walter D., Gliem F., Michalik H.: \n“Further Heavy Ion and Proton SEE Evaluation of High Capacity \nNAND-Flash-memory Devices for Safeguard Data Recorder”, 8th \nESA/ESTEC D/TEC-QCA Final Presentation Day, February 2007 \n[10] Schmidt H., Walter D., Brüggemann M., Gliem F., Harboe-Sørensen R., \nVirtanen A.: \"Heavy Ion SEE Studies on 4-Gbit NAND-Flash-\nmemories\", Radiation Effects on Components and Systems (RADECS) \n2007, DWL-14, September 2007 \n[11] Schmidt H., Walter D., Gliem F., Nickson B., Harboe-Sorensen R., \nVirtanen A.: “TID and SEE Tests of an Advanced 8 Gbit NAND-\nFlash-memory”, Proc. IEEE Radiation Effects Data Workshop, 2008, \n38-41 \n[12] Brüggemann M., Schmidt H., Walter D., Gliem F., Harboe-Sørensen R., \nRoos P., Stähle M.: “SEE Tests of NAND Flash-memory Devices for \nUse in a Safeguard Data Recorder”, Radiation Effects on Components \nand Systems (RADECS) 2006, A-3, Volume A-3, 2006 \n[13] SanDisk Corporation, White Paper: “SanDisk Flash-memory Cards \nWear Leveling”, Doc. No. 80-36-00278, October 2003 \n[14] Chang Li-Pin: \"On Efficient Wear Leveling for Large-Scale Flash-\nMemoryStorage Systems\", Proceedings of the 22nd ACM Symposium \non Applied Computing, 2007 \n[15] M. L. Chiang, Paul C. H. Lee, R. C. Chang: \"Using Data Clustering To \nImprove Cleaning Performance For Flash Memory\", Software - \nPractice and Experience, 1999 \n[16] Chang Y.-H., Hsieh J.-W., Kuo T.-W.: “Endurance Enhancement of \nFlash-Memory Storage, Systems: An Efficient Static Wear Leveling \nDesign” Proc. 44th ACM/IEEE Design Automation Conference (DAC) \n'07, 212-217, 2007 \n[17] Benso A., Prinetto P.: “Fault Injection Techniques and Tools for \nEmbedded Systems Reliability Evaluation” – Kluver Academic \nPublishers, ISBN: 1-4020-7589-8, 2003 \n[18] Caramia M., Di Carlo S., Fabiano M., Prinetto P.: “Flash-memories in \nSpace Applications: Trends and Challenges”, East-West Design & Test \nSymposium (EWDTS) 2009, Moscow, Russia, September 18-21, to \nappear \n[19] Samsung, Application Note: “XSR1.5 Bad Block Management”, May \n2007 \n19\n",
            "id": 4821617,
            "identifiers": [
                {
                    "identifier": "11413019",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:iris.polito.it:11583/2296433",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "oai:porto.polito.it:2296433",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "10.1109/hldvt.2009.5340180",
                    "type": "DOI"
                },
                {
                    "identifier": "234876636",
                    "type": "CORE_ID"
                }
            ],
            "title": "FLARE: A design environment for FLASH-based space applications",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:iris.polito.it:11583/2296433",
                "oai:porto.polito.it:2296433"
            ],
            "publishedDate": "2009-01-01T00:00:00",
            "publisher": "IEEE",
            "pubmedId": null,
            "references": [
                {
                    "id": 6440571,
                    "title": "An efficient management scheme for largescale flash-memory storage systems&quot;,",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1145/967900.968076",
                    "raw": "Chang L. P., Kuo T. W.: &quot;An efficient management scheme for largescale flash-memory storage systems&quot;, Proceedings of the 2004 ACM Symposium on Applied Computing , Nicosia, Cyprus, 862-868, 2004",
                    "cites": null
                },
                {
                    "id": 6440585,
                    "title": "Application Note: “XSR1.5",
                    "authors": [],
                    "date": null,
                    "doi": null,
                    "raw": "Samsung, Application Note: “XSR1.5 Bad Block Management”, May",
                    "cites": null
                },
                {
                    "id": 6440570,
                    "title": "Casel.: &quot;NAND-Flash-memory Technology in Mass Memory Systems for Space Applications&quot;,",
                    "authors": [],
                    "date": "2008",
                    "doi": null,
                    "raw": "Cassel M., Walter D., Schmidt H., Gliem F., Michalik H., Stähle M., Vögele  K.,  Roos  P.  Casel.:  &quot;NAND-Flash-memory  Technology  in Mass  Memory  Systems  for  Space  Applications&quot;,  Proceedings  Data Systems In Aerospace (DASIA) 2008, Palma de Mallorca, Spain, 2008",
                    "cites": null
                },
                {
                    "id": 6440583,
                    "title": "Fault Injection Techniques and Tools for Embedded Systems Reliability Evaluation”",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1007/b105828",
                    "raw": "Benso  A.,  Prinetto  P.:  “Fault  Injection  Techniques  and  Tools  for Embedded  Systems  Reliability  Evaluation”  –  Kluver  Academic Publishers, ISBN: 1-4020-7589-8, 2003",
                    "cites": null
                },
                {
                    "id": 6440584,
                    "title": "Flash-memories in Space Applications: Trends and Challenges”,",
                    "authors": [],
                    "date": null,
                    "doi": "10.1109/hldvt.2009.5340180",
                    "raw": "Caramia M., Di Carlo S., Fabiano M., Prinetto P.: “Flash-memories in Space Applications: Trends and Challenges”, East-West Design & Test Symposium  (EWDTS)  2009,  Moscow,  Russia,  September  18-21,  to appear",
                    "cites": null
                },
                {
                    "id": 6440576,
                    "title": "Further Heavy Ion and Proton SEE Evaluation of High Capacity NAND-Flash-memory Devices for Safeguard Data Recorder”, 8th ESA/ESTEC D/TEC-QCA Final Presentation Day,",
                    "authors": [],
                    "date": "2007",
                    "doi": null,
                    "raw": "Brüggemann  M.,  Schmidt  H.,  Walter  D.,  Gliem  F.,  Michalik  H.: “Further  Heavy  Ion  and  Proton  SEE  Evaluation  of  High  Capacity NAND-Flash-memory  Devices  for  Safeguard  Data  Recorder”,  8th ESA/ESTEC D/TEC-QCA Final Presentation Day, February 2007",
                    "cites": null
                },
                {
                    "id": 6440577,
                    "title": "Heavy Ion SEE Studies on 4-Gbit NAND-Flashmemories&quot;,",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1109/radecs.2007.5205555",
                    "raw": "Schmidt H., Walter D., Brüggemann M., Gliem F., Harboe-Sørensen R., Virtanen  A.:  &quot;Heavy  Ion  SEE  Studies  on  4-Gbit  NAND-Flashmemories&quot;, Radiation Effects on Components and Systems (RADECS) 2007, DWL-14, September 2007",
                    "cites": null
                },
                {
                    "id": 6440574,
                    "title": "Inc.: “JFFS : The Journalling Flash File System”, http://sources.redhat.com/jffs2/jffs2.pdf ,",
                    "authors": [],
                    "date": "2001",
                    "doi": null,
                    "raw": "Woodhouse  D.,  Red  Hat,  Inc.:  “JFFS  :  The  Journalling  Flash  File System”, http://sources.redhat.com/jffs2/jffs2.pdf , 2001",
                    "cites": null
                },
                {
                    "id": 6440580,
                    "title": "On Efficient Wear Leveling for Large-Scale FlashMemoryStorage Systems&quot;,",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1145/1244002.1244248",
                    "raw": "Chang  Li-Pin:  &quot;On  Efficient  Wear  Leveling  for  Large-Scale  FlashMemoryStorage Systems&quot;, Proceedings of the 22nd ACM Symposium on Applied Computing, 2007",
                    "cites": null
                },
                {
                    "id": 6440578,
                    "title": "SEE Tests of NAND Flash-memory Devices for Use in a Safeguard Data Recorder”,",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "Brüggemann M., Schmidt H., Walter D., Gliem F., Harboe-Sørensen R., Roos P., Stähle M.: “SEE Tests of NAND Flash-memory Devices for Use in a Safeguard Data Recorder”, Radiation Effects on Components and Systems (RADECS) 2006, A-3, Volume A-3, 2006",
                    "cites": null
                },
                {
                    "id": 6440569,
                    "title": "Space-ready, radiation-tolerant processor modules: A COTS technology strategy”, Military Embedded Systems Resource Guide,",
                    "authors": [],
                    "date": "2005",
                    "doi": null,
                    "raw": "Anthony  Lai:  “Space-ready,  radiation-tolerant processor  modules:  A COTS  technology  strategy”,  Military  Embedded  Systems  Resource Guide, May 2005",
                    "cites": null
                },
                {
                    "id": 6440582,
                    "title": "T.-W.: “Endurance Enhancement of Flash-Memory Storage, Systems: An Efficient Static Wear Leveling Design”",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1145/1278480.1278533",
                    "raw": "Chang Y.-H., Hsieh J.-W., Kuo T.-W.: “Endurance Enhancement of Flash-Memory Storage, Systems:  An Efficient Static Wear  Leveling Design” Proc. 44th ACM/IEEE Design Automation Conference (DAC) '07, 212-217, 2007",
                    "cites": null
                },
                {
                    "id": 6440573,
                    "title": "Technical Report: &quot;Understanding the Flash Translation Layer (FTL) Specification&quot;,",
                    "authors": [],
                    "date": "1998",
                    "doi": null,
                    "raw": "Intel  Corporation,  Technical  Report:  &quot;Understanding  the  Flash Translation Layer (FTL) Specification&quot;, December 1998",
                    "cites": null
                },
                {
                    "id": 6440572,
                    "title": "Tsai Yi-Lin, Kuo Tei-Wei, Lee Tzao-Lin: &quot;Configurable Flash-Memory Management: Performance versus Overheads&quot;",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1109/tc.2008.61",
                    "raw": "Hsieh  Jen-Wei,  Tsai  Yi-Lin,  Kuo  Tei-Wei,  Lee  Tzao-Lin: &quot;Configurable  Flash-Memory  Management:  Performance  versus Overheads&quot; IEEE Transactions on Computers, Vol. 57, no. 11, 2008",
                    "cites": null
                },
                {
                    "id": 6440581,
                    "title": "Using Data Clustering To Improve Cleaning Performance For Flash Memory&quot;, Software -Practice and Experience,",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1002/(sici)1097-024x(199903)29:3<267::aid-spe233>3.0.co;2-t",
                    "raw": "M. L. Chiang, Paul C. H. Lee, R. C. Chang: &quot;Using Data Clustering To Improve  Cleaning  Performance  For  Flash  Memory&quot;,  Software  -Practice and Experience, 1999",
                    "cites": null
                },
                {
                    "id": 6440579,
                    "title": "White Paper: “SanDisk Flash-memory Cards Wear Leveling”,",
                    "authors": [],
                    "date": "2003",
                    "doi": null,
                    "raw": "SanDisk  Corporation,  White  Paper:  “SanDisk  Flash-memory  Cards Wear Leveling”, Doc. No. 80-36-00278, October 2003",
                    "cites": null
                },
                {
                    "id": 6440575,
                    "title": "Yet Another Flash File System”,",
                    "authors": [],
                    "date": "2002",
                    "doi": null,
                    "raw": "Aleph  One  Company,  Cambridge,  UK:  “Yet  Another  Flash  File System”, http://www.aleph1.co.uk/yaffs/index.html, 2002",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "https://iris.polito.it/retrieve/handle/11583/2296433/52110/2009-HLDVT-Flare-AuthorVersion.pdf"
            ],
            "updatedDate": "2020-08-03T20:11:05",
            "yearPublished": 2009,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/pdf/11413019.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/11413019"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/11413019/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/11413019/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/4821617"
                }
            ]
        },
        {
            "acceptedDate": "2012-08-12T00:00:00",
            "arxivId": "1208.2754",
            "authors": [
                {
                    "name": "Bas Luttik"
                },
                {
                    "name": "Berdine"
                },
                {
                    "name": "Bovet"
                },
                {
                    "name": "de Dinechin"
                },
                {
                    "name": "Feng"
                },
                {
                    "name": "Hayo Thielecke"
                },
                {
                    "name": "Hutton"
                },
                {
                    "name": "Kerrisk"
                },
                {
                    "name": "Maxim Strygin"
                },
                {
                    "name": "Michel A. Reniers"
                },
                {
                    "name": "Milner"
                },
                {
                    "name": "Moggi"
                },
                {
                    "name": "Reynolds"
                },
                {
                    "name": "Robbins"
                },
                {
                    "name": "Sandra Loosemore and Richard M. Stallman and Roland McGrath and Andrew Oram and Ulrich Drepper"
                },
                {
                    "name": "Stevens"
                }
            ],
            "contributors": [
                "The Pennsylvania State University CiteSeerX Archives"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/189107961",
                "https://api.core.ac.uk/v3/outputs/26420122"
            ],
            "createdDate": "2012-08-30T13:52:54",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 145,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/145",
                    "logo": "https://api.core.ac.uk/data-providers/145/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 645,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/645",
                    "logo": "https://api.core.ac.uk/data-providers/645/logo"
                }
            ],
            "depositedDate": "2012-08-12T00:00:00",
            "abstract": "Signals are a lightweight form of interprocess communication in Unix. When a\nprocess receives a signal, the control flow is interrupted and a previously\ninstalled signal handler is run. Signal handling is reminiscent both of\nexception handling and concurrent interleaving of processes. In this paper, we\ninvestigate different approaches to formalizing signal handling in operational\nsemantics, and compare them in a series of examples. We find the big-step style\nof operational semantics to be well suited to modelling signal handling. We\nintegrate exception handling with our big-step semantics of signal handling, by\nadopting the exception convention as defined in the Definition of Standard ML.\nThe semantics needs to capture the complex interactions between signal handling\nand exception handling.Comment: In Proceedings EXPRESS/SOS 2012, arXiv:1208.244",
            "documentType": "research",
            "doi": "10.4204/eptcs.89.11",
            "downloadUrl": "http://arxiv.org/abs/1208.2754",
            "fieldOfStudy": "computer science",
            "fullText": "B. Luttik and M.A. Reniers (Eds.): Combined Workshop on Expressiveness in\nConcurrency and Structural Operational Semantics (EXPRESS/SOS 2012)\nEPTCS 89, 2012, pp. 149–163, doi:10.4204/EPTCS.89.11\nOperational semantics for signal handling\nMaxim Strygin\nSchool of Computer Science\nUniversity of Birmingham\nM.Strygin@cs.bham.ac.uk\nHayo Thielecke\nSchool of Computer Science\nUniversity of Birmingham\nH.Thielecke@cs.bham.ac.uk\nSignals are a lightweight form of interprocess communication in Unix. When a process receives a\nsignal, the control flow is interrupted and a previously installed signal handler is run. Signal handling\nis reminiscent both of exception handling and concurrent interleaving of processes. In this paper, we\ninvestigate different approaches to formalizing signal handling in operational semantics, and compare\nthem in a series of examples. We find the big-step style of operational semantics to be well suited\nto modelling signal handling. We integrate exception handling with our big-step semantics of signal\nhandling, by adopting the exception convention as defined in the Definition of Standard ML. The\nsemantics needs to capture the complex interactions between signal handling and exception handling.\n1 Introduction\nIn operating systems, and specifically Unix and its descendants, signals provide a simple and efficient,\nif rather low-level, means of interprocess communication [10, 14, 16, 15, 3]. Put simply, a process can\ncause a branch of control in another process, causing it to run a signal handler in response to external\nevents. A well known example is the kill signal telling a process to shut down (perhaps after first\ndeallocating system resources, such as releasing memory).\nSignals resemble exceptions in that control jumps to a handler that can be installed by the program.\nNonetheless, there are some significant differences. Whereas exceptions typically abort from the con-\ntext, in which they were thrown rather than returning to it, signal handlers resume control after they have\nrun. Whereas exceptions are triggered at specific points by the code itself, signals arrive nondetermin-\nistically. In the literature on control constructs and their semantics, signals have received less attention\nthan exceptions, and far less than first-class continuations.\nExceptions have become amenable to semantic analysis by a focus on their key control features, while\nabstracting away from implementation details and restrictions (such as the entanglement of exceptions\nin C++ with the class hierarchy and memory management by destructors). For instance, the exceptions\nmonad [12] gives a highly idealized account of exceptions as functions A → (B+E) that may either\nreturn normally with a B or raise an exception of type E .\nThe aim of the present paper is to address signal handling at a level of generality and abstraction\ncomparable to that of other control constructs in the literature, idealizing where necessary and focusing\non some key semantic features. Our motivation for defining such a semantics, and exploring different\nstyles of definition, is to develop of a Hoare logic for signals. While program logic is beyond the scope\nof the paper, it is a reason for our investigating the big-step style of operational semantics. In big-step,\na command c takes a pre-state s1 to a post-state s2 in a judgment of the form s1,c ⇓ s2. This form of\njudgment is particularly convenient for proving the soundness of Hoare triples {P}c{Q}, since the pre-\ncondition P refers to the pre-state s1 and the postcondition Q to the post-state s2 in a big-step judgement.\n150 Operational semantics for signal handling\nOutline of the paper\nWe begin by reviewing the constructs that we will need, and how to define operational semantics for\nthem, in Section 2. We then combine these constructs and define the semantics for the whole language in\nSection 3. To validate our definition, we examine how signal and exception handling interact in a series\nof examples in Section 4. As an alternative to big-step semantics, we define a small-step semantics as\na stack machine in Section 5, and relate it to implementations. We compare the stack machine to the\nbig-step semantics in Section 6. Section 7 concludes.\n2 Language constructs\nBefore giving the formal definition of our operational semantics, we introduce the language constructs\nwith their intended meaning, as well as design choices and simplifying assumptions. We start from a\nsmall imperative base language. This language has a standard semantics in terms of how a command c\nchanges the state s1 into a new state s2. In a big-step operational semantics, the form of such judgements\nis\ns1,c ⇓ s2\nWhen the command c raises an exception e after producing the new state s2, we write\ns1,c ⇑ e,s2\nExceptions\nThe semantics of exceptions is fairly well understood, and it is greatly simplified by the fact that\nexceptions are block structured. The more primitive non-local jumps in C (given via the library functions\nsetjmp() and longjmp()) would be much harder to formalize. Exception throwing and handling is easy\nto add to a big-step operational semantics. A classic example of such a semantics is the Definition of\nStandard ML [11], whose style we will follow.\nIn addition to the rules for the operations themselves, we also need to specify how the propagation\nof exceptions interacts with the other constructs of the language: this propagation will be done with the\nexception convention from the Definition of Standard ML. If the j-th premise of a big-step rule raises an\nexception, and the premises to its left do not, then the conclusion of the rule raises the same exception,\nand with the same state.\nMore precisely, suppose there is a big-step rule of the form\n. . .c1 ⇓ s1 . . .c j ⇓ s j . . .cn ⇓ sn\n. . .c ⇓ s\nThen we implicitly extend this case to propagating exception by adding a rule\n. . .c1 ⇓ s1 . . .c j ⇑ e,s j\n. . .c ⇑ e,s j\nTo illustrate the exception convention, we consider how exceptions are propagated in a sequential com-\nposition c1;c2.\ns1,c1 ⇑ e,s2\ns1,(c1;c2) ⇑ e,s2\ns1,c1 ⇓ s2 s2,c2 ⇑ e,s3\ns1,(c1;c2) ⇑ e,s3\nStrygin and Thielecke 151\nIntuitively, the first command c1 may raise an exception, in which case the second command c2 has not\nrun at all. Alternatively, c1 may terminate normally, and c2 may raise an exception. In either case, the\ncombined command raises the same exception.\nSignals\nThe main construct we aim to address is signal handling. Signal handling is a form of interprocess\ncommunication, so that for full generality we would have to address the concurrent interaction between\na signal sending and a signal handling process. To keep the semantics as simple as possible, we address\nonly the handling part of the signal mechanism, while the truly concurrent interaction between sender\nand receiver is left for future work. Rather than modelling the signal sender explicitly, only the point\nof view of the process receiving the signals will be assumed, so that signals arrive nondeterministically,\ncausing handlers to run unpredictably. In the authors’ view, this focus on signal handling still presents\nsufficient programming and semantics challenges. First, the nondeterministic interference by signal\nhandlers leads to the need to preserve resource invariants, much as interference between concurrent\nprocesses. Moreover, the assumptions a programmer can make about the delivery of signals are very\nweak, even if there is a specification of the sender’s behaviour (which there usually is not). In the worst\ncase, the signal sender may even be malicious, sending signals with the sole intent of causing damage\nvia the actions of the signal handlers. In that sense, a nondeterministic sender is a worst-case but realistic\nassumption that the signal receiver has to be able to cope with.\nAs a language construct, signal handlers resemble both concurrency and exception handling. Our\nmost significant idealization of signal handlers is directly inspired by exceptions in contrast to the un-\nstructured longjmp that exceptions were designed to replace. We define an idealized block-structured\nform of signal handling in which a signal handler is installed at the beginning of the block and uninstalled\nat the end. It relates to sigaction the way exceptions related to setjmp and atomic synchronized\nblocks related to locking and unlocking.\nFor the operational semantics, we define a big-step semantics. This style of semantics appears partic-\nularly apt for the signals and exceptions kind of constructs. Essentially, the meaning of a block becomes\na subtree of a larger derivation tree, which is convenient for keeping track of pre- and post-states. In the\nsame way, the derivation tree of one-sided signal handler could be easily injected into a larger tree.\nOne may think of addressing one-sided interleaving with the same approach as complete interleaving.\nThis is true to some extent, but there are important differences between them. The interaction between\nfully concurrent processes is symmetric, but there is no such symmetry between the signal body and the\nhandler. Only the signal handler may interrupt the body, but not vice versa. This allows using a simpler\napproach for addressing signal handling. On the other hand, the general approach used for the fully\nconcurrent interleaving might not be suitable, as the interaction is non-symmetric.\nFigure 1 depicts the symmetric interleaving of concurrent processes compared to the one-sided inter-\nleaving of a process by its signal handlers. Dashed horizontal lines represent control flow; dotted vertical\nlines represent switches in the control flow due to interleaving. In both cases, the state σi that a process\nsees at some point could has been changed to some state σi+1 by interleaved actions. These state changes\nneed to be limited in some way, as otherwise no assumptions could be made by the process about the\nstate, including resource invariants.\nOne-shot and persistent signals\nSignal handlers can have two different control flow semantics, which we call persistent and one-shot.\nA persistent signal handler can be run any number of times as long as it is installed. By contrast, a one-\nshot signal handler can be run at most once, as it becomes automatically uninstalled after being run the\nfirst time. In Unix, the system call for installing handlers takes a parameter that determines which of\nthese behaviours is chosen.\n152 Operational semantics for signal handling\nRely for concurrent processes:\nProcess 1\nProcess 2\nR1 R2\nσ1 σ2\nσ2 σ3\nRely for signal handlers:\nBody\nHandler\nR R\nσ1 σ2 σ3 σ4\nFigure 1: Processes vs signal handlers\nOperational semantics\nIn the operational semantics, the evaluation of a command c starting from a state s1 will now take\nplace relative to a signal binding. Moreover, the signal binding is subdivided into two parts: persistent\nsignals S, and one-shot signals O. Persistent handlers may run any number of times during the evaluation\nof the command c, whereas one-shot handlers may run at most once. The form of a big-step judgement\nwith signal bindings is:\nS;O ⊢ s1,c ⇓ s2\nNote that the signal binding behaves like an environment (for variables bound via let) rather than a\nmutable state (for variables updated via :=). The judgement produces an updated state s2, but it does not\nupdate S or O.\nAnalogous to binding an exception handler, we have two binding constructs for signals: one for\npersistent and one for one-shot handlers, where z is a signal name, cb is a command, and ch is a handler\ncommand.\nbindztoch incb and bind/1ztoch incb\nTo support signal disabling in a scope, we introduce two blocking constructs for signals:\nblockzincb and block/1zincb\nNote that there is no need for an analogue of throwe (a command that throws an exception e), as we\nassume that signals arrive nondeterministically from other, unspecified processes. The idea of using two\ncontexts with a binder for each is loosely inspired by Barber and Plotkin’s Dual Intuitionistic Linear\nLogic (DILL) [1].\n3 Operational semantics for block-structured signals and exceptions\nDefinition 3.1 The syntax of the language with signal and exception handling is given in Figure 2.\nWe let s range over states, c over commands, x over variables, v over values, e over exception names,\nz over signal names, and E over expressions, using subscripts where needed, e.g., s1, e2, E3, c4 or ch. s is\na function from variables to values, such that s(x) returns a value v.\nStrygin and Thielecke 153\nc ::= while(E)doc (while construct)\n| x := E (Assignment)\n| c1;c2 (Sequential composition)\n| throwe (Exception throwing)\n| try c1 handle e by c2 (Exception handling)\n| bindztoc1 inc2 (Binding persistent signal handler)\n| bind/1ztoc1 inc2 (Binding one-shot signal handler)\n| blockzinc (Blocking persistent signal)\n| block/1zinc (Blocking one-shot signal)\nE ::= x | E +E | . . . (Expressions)\nFigure 2: The syntax of the language\nS [z 7→ c1 ];O ⊢ s1,c2 ⇓ s2\nS;O ⊢ s1,bindztoc1 inc2 ⇓ s2\nS;O [z 7→ c1 ] ⊢ s1,c2 ⇓ s2\nS;O ⊢ s1,bind/1ztoc1inc2 ⇓ s2\nS− z;O ⊢ s1,c ⇓ s2\nS;O ⊢ s1,blockzinc ⇓ s2\nS;O− z ⊢ s1,c ⇓ s2\nS;O ⊢ s1,block/1zinc ⇓ s2\nS;O ⊢ s,throwe ⇑ e,s\nS;O1 ⊢ s1,c1 ⇑ e,s2 S;O2 ⊢ s2,c2 ⇓ s3\nS;O1 ∗O2 ⊢ s1,try c1 handle e by c2 ⇓ s3\nS;O ⊢ s1,c1 ⇓ s2\nS;O ⊢ s1,try c1 handle e by c2 ⇓ s2\nS;O1 ⊢ s1,c1 ⇑ e,s2 S;O2 ⊢ s2,c2 ⇑ e2,s3\nS;O1 ∗O2 ⊢ s1,try c1 handle e by c2 ⇑ e2,s3\nS;O ⊢ s1,c1 ⇑ e2,s2 e2 6= e\nS;O ⊢ s1,try c1 handle e by c2 ⇑ e2,s2\ns ⊢ E ↓ v\nS;O ⊢ s,x:=E ⇓ s [x 7→ v ]\nS;O1 ⊢ s1,c1 ⇓ s2 S;O2 ⊢ s2,c2 ⇓ s3\nS;O1 ∗O2 ⊢ s1,(c1;c2) ⇓ s3\nS;O ⊢ s1,c1 ⇓ s2 S(z) = c2 /0; /0 ⊢ s2,c2 ⇓ s3\nS;O ⊢ s1,c1 ⇓ s3\nS;O− z ⊢ s1,c1 ⇓ s2 O(z) = c2 /0; /0 ⊢ s2,c2 ⇓ s3\nS;O ⊢ s1,c1 ⇓ s3\nS(z) = c2 /0; /0 ⊢ s1,c2 ⇓ s2 S;O ⊢ s2,c1 ⇓ s3\nS;O ⊢ s1,c1 ⇓ s3\nO(z) = c2 /0; /0 ⊢ s1,c2 ⇓ s2 S;O− z ⊢ s2,c1 ⇓ s3\nS;O ⊢ s1,c1 ⇓ s3\nFigure 3: Big-step semantics rules for exceptions and signal handling\n154 Operational semantics for signal handling\nO1 [z 7→ ch ](z) = ch /0; /0 ⊢ s1,ch ⇓ s2 S;O1 − z ⊢ s2,c1 ⇓ s3\nS;O1 [z 7→ ch ] ⊢ s1,c1 ⇓ s3 S;O2 ⊢ s3,c2 ⇓ s4\nS;(O1 ∗O2) [z 7→ ch ] ⊢ s1,(c1 ; c2) ⇓ s4\nS;O1 ∗O2 ⊢ s1,bind/1ztoch in(c1 ; c2) ⇓ s4\nFigure 4: Splitting of the O binding in seq. composed commands\nSome auxiliary definitions will be required for the operational semantics. For a partial function f , we\nwrite f [x 7→ v ] for the function that maps x to v and coincides with f on all other arguments. In particular,\nwe use this notation for updating states or signal bindings. We write dom( f ) for the domain of definition\nof a partial function. For x ∈ dom( f ), we write f − x for the restriction of f to (dom( f )\\{x}). A signal\nbinding is a finite partial function from signal names z to commands c. We will need a partial operation\non signal bindings. In fact, this definition is the same as the separating conjunction from separation\nlogic [13].\nDefinition 3.2 Given two signal bindings O1 and O2, we define a partial operation ∗ as follows:\n• If dom(O1)∩dom(O2) = /0, we write O1 ∗O2 for O1∪O2.\n• If dom(O1)∩dom(O2) 6= /0, then O1 ∗O2 is undefined.\nIt is this splitting of a signal binding, analogous to the heap-splitting of separation logic, that gives one-\nshot behaviour to signals. Specifically, in a sequential composition (c1;c2), the one-shot signals are split\nnon-deterministically between the commands c1 and c2. Moreover, every time a one-shot signal arrives\nand is handled, it is removed from the one-shot binding O. Thus, a one-shot signal may never be handled\ntwice.\nDefinition 3.3 Given two signal bindings S and O, the form of a big-step judgement is either\nS;O ⊢ s1,c ⇓ s2\nfor normal termination, or\nS;O ⊢ s1,c ⇑ e,s2\nfor exception throwing. The rules are given in Figure 3. The exception convention is assumed implicitly.\n4 Examples\nWe examine how signal and exception handling interact in a series of examples, and discuss the question\nof priority between them.\nExamples for signals\nThe aim of the Figure 4 and Figure 5 is to show how one-shot and persistent signal bindings are\n\"shared\" between sequentially composed commands, and highlight the core difference between them\n(splitting versus copying).\nIn Figure 4, the one-shot signal binding O = O1 ∗O2 (Definition 3.2) is split non-deterministically\nbetween commands c1 and c2. When the new signal z is registered, it becomes an element of the domain\n(O1 ∗O2) [z 7→ ch ]. However, is z ∈ dom(O1 [z 7→ ch ]) or z ∈ dom(O2 [z 7→ ch ]) will be determined\nStrygin and Thielecke 155\nS [z 7→ ch ](z) = ch /0; /0 ⊢ s1,ch ⇓ s2 S [z 7→ ch ];O ⊢ s2,c1 ⇓ s3\nS [z 7→ ch ];O ⊢ s1,c1 ⇓ s3 D\nS [z 7→ ch ];O ⊢ s1,(c1 ; c2) ⇓ s6\nS;O ⊢ s1,bindztoch in(c1 ; c2) ⇓ s6\nD =\nS [z 7→ ch ](z) = ch /0; /0 ⊢ s3,ch ⇓ s4 F\nS [z 7→ ch ];O ⊢ s3,c2 ⇓ s6\nF =\nS [z 7→ ch ](z) = ch /0; /0 ⊢ s4,ch ⇓ s5 S [z 7→ ch ];O ⊢ s5,c2 ⇓ s6\nS [z 7→ ch ];O ⊢ s4,c2 ⇓ s6\nFigure 5: Multiple persistent signal handling in seq. composed commands\nO [z 7→ ch ](z) = ch /0; /0 ⊢ s1,ch ⇓ s2 S;O− z ⊢ s2,c ⇓ s3\nS;O [z 7→ ch ] ⊢ s1,c ⇓ s3\nS;O ⊢ s1,bind/1ztoch inc ⇓ s3\nFigure 6: One-shot signal handling before the command\nduring the run time only. In this particular example, the signal z arrives in \"scope\" of the command c1\n(z ∈ dom(O1 [z 7→ ch ])) and the bound handler runs. According to the one-shot signal binding nature,\nthe binding for z is removed from O1 [z 7→ ch ] and consequently from (O1 ∗O2) [z 7→ ch ] as O1 [z 7→\nch ]⊆ (O1 ∗O2) [z 7→ ch ]. Therefore, z /∈ dom(O2) and if the signal z arrives during the execution of the\ncommand c2, it will be ignored.\nIn Figure 5, we focus on a persistent signal binding. The key difference with the one-shot binding is\nthat the binding is just copied to the every command without splitting or modification. Thus, the same\nsignal handler may run any number of times during the execution of the commands c1 and c2. This\nbehaviour is possible because triggering a persistent signal handler does not remove the corresponding\nbinding.\nExamples for signals and exceptions\nSuppose that a signal handler relies on some resource (valid pointer, open socket, active connection,\netc.) available in the particular scope. However, as a side effect of the handler execution, the resource be-\ncomes unavailable (freed pointer, closed socket, inactive connection). In this situation, multiple handler\nexecutions may lead to the program fail and abrupt termination.\nObviously, one-shot signal handlers are perfectly fit for purpose. In Figure 6, the one-shot signal\nhandler ch runs before the command c. Thus, when control flow returns to c, the one-shot signal binding\nno longer contains a binding for the handler ch. In Figure 7, the one-shot signal handler ch runs after the\ncommand c, and at that point the signal binding no longer contains a binding for ch. Note that the signal\nhandlers (persistent and one-shot) that are still bound might be triggered if the corresponding signal\narrives after the ch.\nOn the other hand, a persistent handler combined with an exception imitates one-shot signal handlers\n156 Operational semantics for signal handling\nS;O− z ⊢ s1,c ⇓ s2 O [z 7→ ch ](z) = ch /0; /0 ⊢ s2,ch ⇓ s3\nS;O [z 7→ ch ] ⊢ s1,c ⇓ s3\nS;O ⊢ s1,bind/1ztoch inc ⇓ s3\nFigure 7: One-shot signal handling after the command\nS [z 7→ (h ;throwe) ](z) = (h ;throwe)\n/0; /0 ⊢ s1,h ⇓ s2 /0; /0 ⊢ s2,throwe ⇑ e,s2\n/0; /0 ⊢ s1,(h ;throwe) ⇑ e,s2\nS [z 7→ (h ;throwe) ];O ⊢ s1,c ⇑ e,s2\nS;O ⊢ s1,(bindzto (h ;throwe)inc) ⇑ e,s2 S;O ⊢ s2,g ⇓ s3\nS;O ⊢ s1,try (bindzto(h ;throwe)inc) handle e by g ⇓ s3\nFigure 8: Persistent handler with an exception triggered before the command\nto some extent. The key trick is in adding of a \"throw\" command to the end of the persistent handler.\nBecause of a thrown exception, control leaves the signal block, so the persistent signal handler will not\nrun again.\nIn Figure 8, the persistent signal handler runs and throws an exception. As exception propagation\ntakes place, the command c does not run. In Figure 9, the command c runs before the persistent signal\nhandler has been triggered. Thus, the raising of the exception does not influence the command c at that\npoint.\nComparing the derivation trees in Figure 7 and Figure 9, we observe how similar they are. In both\ncases, the main command runs first and then the signal handler runs only once. The only difference is\nthat singular executions of the handler has been achieved by two different approaches.\nComparing the derivation trees from Figure 6 and Figure 8, we observe the next situation: in both\ncases the strict condition (singular execution) for the signal handlers is satisfied, but as a \"side effect\" of\nan exception propagation (Figure 8), the command c is skipped.\nS [z 7→ (h ;throwe) ];O ⊢ s1,c ⇓ s2 S [z 7→ (h ;throwe) ](z) = (h ;throwe) F\nS [z 7→ (h ;throwe) ];O ⊢ s1,c ⇑ e,s3\nS;O ⊢ s1,(bindzto (h ;throwe)inc) ⇑ e,s3 S;O ⊢ s3,g ⇓ s4\nS;O ⊢ s1,try (bind zto(h ;throwe)inc) handle e by g ⇓ s4\nF =\n/0; /0 ⊢ s2,h ⇓ s3 /0; /0 ⊢ s3,throwe ⇑ e,s3\n/0; /0 ⊢ s2,(h ;throwe) ⇑ e,s3\nFigure 9: Persistent handler with an exception triggered after the command\nStrygin and Thielecke 157\nS [z 7→ h ](z) = h /0; /0 ⊢ s1,h ⇓ s2 S [z 7→ h ];O ⊢ s2,throwe ⇑ e,s2\nS [z 7→ h ];O ⊢ s1,throwe ⇑ e,s2\nS;O ⊢ s1,(bind ztohinthrowe) ⇑ e,s2 S;O ⊢ s2,g ⇓ s3\nS;O ⊢ s1,try (bindztohinthrowe) handle e by g ⇓ s3\nFigure 10: Example of the derivation tree: a signal binding inside an exception block\nS [z 7→ h ](z) = h /0; /0 ⊢ s1,h ⇓ s2 S [z 7→ h ];O ⊢ s2,throwe ⇑ e,s2\nS [z 7→ h ];O ⊢ s1,throwe ⇑ e,s2 F\nS [z 7→ h ];O ⊢ s1,try (throwe) handle e by g ⇓ s4\nS;O ⊢ s1,bind ztohin (try (throwe) handle e by g) ⇓ s4\nF =\nS [z 7→ h ](z) = h /0; /0 ⊢ s2,h ⇓ s3 S [z 7→ h ];O ⊢ s3,g ⇓ s4\nS [z 7→ h ];O ⊢ s2,g ⇓ s4\nFigure 11: Derivation tree for the combined signals and exceptions\nInteraction between signal and exception handling\nThere is potentially a pitfall in combining signals and jumps (such as exceptions), in that a jump\ncould prevent a handler from being correctly uninstalled at the end of its scope. In fact the problem is\nquite general, and arises whenever resource management is combined with jumping. In our language as\ndefined in Definition 3.1, such a potential problem case is presented by the following code:\ntry (bindztohinthrowe) handle e by g\nThe intended meaning is that the signal z is bound locally inside the body of an exception block. The\nsignal handler may run immediately before the throwe command. However, once the exception has\npropagated to the exception handler, it has left the scope of the signal binding, so that the signal handler\nshould not be able to run. To see that the big-step semantics (Figure 3) correctly handles this case,\nconsider the derivation tree in Figure 10.\nIn a big-step semantics, block structure is handled correctly \"for free\". The extended signal binding\nS [z 7→ h ] is confined to the subtree of the body of the binding. When the body is left, the evaluation is\nresumed with the old S, which is what is used in the evaluation of g. Even when control leaves the signal\nblock abruptly via an exception, there is no danger that the signal handler escapes from its scope. By\ncontrast, in a small-step semantics (e.g.: abstract machine) the uninstalling of signal handlers needs to\nbe performed explicitly.\nThe question of priority\nIn our operational semantics, exception propagation has higher priority than exception handling.\nThus, a signal might be handled only before the exception has been thrown and after it has been caught\n(Figure 11). The command throw does not change the state itself, thus the state remains unchanged until\n158 Operational semantics for signal handling\nS [z 7→ h ];O ⊢ s1,throwe ⇑ e,s1 S [z 7→ h ](z) = h /0; /0 ⊢ s1,h ⇓ s2\nS [z 7→ h ];O ⊢ s1,throwe ⇑ e,s2\nS;O ⊢ s1,(bindztohinthrowe) ⇑ e,s2 S;O ⊢ s2,g ⇓ s3\nS;O ⊢ s1,try (bindztohinthrowe) handle e by g ⇓ s3\nFigure 12: Signal handler runs after the throw\nthe exception is caught, when there are different options: if no signal arrives then the exception handler\nruns, or else the signal handler runs first, and only then the exception handler proceeds.\nHowever, one can design an implementation where signal handling has higher priority. Thus, a signal\nhandler should be processed even if exception propagation takes place (Figure 12). In a semantics with\nsignal priority, the state is changed by the signal handler even during the exception propagation. One\ncan make a few interesting observation about it. During exception propagation, control flow exits nested\nblocks, which in turn may have different signal bindings. Thus, depending in which block a signal\narrives, the corresponding handler will interrupt the exception propagation. In addition, it might be the\ncase that the signal is blocked in that scope, thus propagation would not be interrupted.\n5 Stack machine for signal handlers\nWe define an abstract machine in order to highlight some of the issues that may arise in possible im-\nplementations of block-structured signals, such as managing the stack. The implementation of signal\nhandlers in our abstract machine was inspired by the real implementations of exceptions in contrast to\nthe unstructured longjmp that exceptions were designed to replace.\nThe defined block-structured form of signal handling requires a signal handler to be installed at\nthe beginning of the block and uninstalled at the end. Therefore, to keep track of signal handlers in a\nparticular scope, we use a signal stack. However, the addition of exceptions complicates the scoping\nof signal handlers. When control leaves a signal scope via a raised exception, the handler should be\nuninstalled. Thus, to implement the desired interaction between signal and exception scope, we keep\ntrack of signal handlers and exception handlers on the same stack. When an exception is raised, the stack\nis popped until the nearest enclosing handler for the exception name is found. The same popping of the\ncommon handler stack also removes any intervening signal handlers.\nA machine configuration is of the form 〈c | s | β | J | K 〉, where c is the expression that the machine\nis currently trying to evaluate, s is a state. The bit vector component β is used for keeping track of\ninstalled (not blocked) signals. J is a stack, which holds the signal and exception bindings. K is a\ncontinuation, which tells the machine what to do when it is finished with the current command c. The\ninitial continuation is a special instruction return. The special symbol \u0004 is used to represent an empty\nstack in the components J and K. When we get 〈return | s | β |\u0004 |\u0004〉, program execution is finished.\nThe full list of transition steps is given in Figure 13. To evaluate expression E in a state s, we apply the\nfunction eval (Defintion 5.1), which returns a value v.\nβ 0 stands for a null bit vector (which means blocking or ignoring of all signals). The system instruc-\ntion pop-upd(β ′) removes the top element from a stack J and updates β to β ′. The system instruction\nupdate(β ′) updates β to β ′. We define J as a data structure that follows stack discipline except in the\nStrygin and Thielecke 159\n〈c1;c2 | s1 | β1 | J1 | K1 〉  〈c1 | s1 | β1 | J1 | c2;K1 〉\n〈x := E | s1 | β1 | J1 | c′;K1 〉  〈c′ | s1 [x 7→ v ] | β1 | J1 | K1 〉\nwhere eval(E, s1)= v\n〈bindztohinc | s | β | J | K 〉  〈c | s | β + z | (z,h),J | pop-upd(β);K 〉\n〈bind/1ztohinc | s | β | J | K 〉  〈c | s | β + z | (z,h,0),J | pop-upd(β);K 〉\n〈pop-upd(β1) | s | β2 | (z,h),J | c;K 〉  〈c | s | β1 | J | K 〉\n〈c | s | β | J1,(z,h),J2 | K 〉  〈h | s | β 0 | J1,(z,h),J2 | update(β);c;K 〉\nhandling o f the persistent signal\n〈c | s | β | J1,(z,h,0),J2 | K 〉  〈h | s | β 0 | J1,(z,h,1),J2 | update(β − z);c;K 〉\nhandling o f the one− shot signal\n〈block zinc | s | β1 | J | K 〉  〈c | s | β1− z | J | update(β1);K 〉\n〈block/1 zinc | s | β1 | J | K 〉  〈c | s | β1− z | J | update(β1);K 〉\n〈update(β1) | s | β2 | J | c;K 〉  〈c | s | β1 | J | K 〉\n〈try cb handle e by h | s | β | J | K 〉  〈cb | s | β | (e,h),J | pop-upd(β);K 〉\n〈throwe1 | s | β | J1,(e1,h),J2 | K1 〉  〈h | s | β ′ | J2 | K2 〉\nwhere unwind(e1, (J1,(e1,h),J2), K1) = (h,β ′,J2,K2).\nFigure 13: Transition steps\ncase of one-shot signal handling. The J stack is manipulated by the system instructions that are pushed\nin and popped out from the continuation stack K.\nβ is a function from signal names z to Booleans. For each signal name z, β (z) tells us whether the\nsignal is currently enabled. Then β + z is a shorthand for β [z 7→ true] and β − z stands for β [z 7→ f alse].\nFor a throwe1 command, where e1 ∈ dom(J), we apply the unwind function (Definition 5.2), which\nreturns a quadruple that is used to construct the next machine configuration. If e1 /∈ dom(J), then the\nmachine gets stuck with an unhandled exception, in the sense that there is no transition for this configu-\nration, so that\n〈throwe1 | s | β | J | K 〉 6 \nAn exception binding tag has the form of (e,h), where e is an exception identifier, and h is a handler.\nA persistent signal binding tag has the form of (z,h), where z is a signal name, and h is a handler. A\none-shot signal binding tag has the form of (z,h,u), where z is a signal name, h is a handler, and u is a\nbit indicating that the handler has been used once (u=1) or not (u=0). Handling of the one-shot signals\nrequires update of the J stack; to be more precise, the bit u in (z,h,u) is updated.\n160 Operational semantics for signal handling\nDefinition 5.1 (eval function)\neval(x, s) = s(x)\neval(E1 +E2, s) = eval(E1, s)+eval(E2, s)\nDefinition 5.2 (unwind function)\nunwind(e1, J, c;K) = unwind(e1, J, K)\nunwind(e1, J, update(β);K) = unwind(e1, J, K)\nunwind(e1, ((z,h),J), pop-upd(β);K) = unwind(e1, J, K)\nunwind(e1, ((e1,h),J), pop-upd(β);K) = (h,β ,J,K)\nImplementation of signals\nWe compare how our idealized stack machine models features of real signal implementations.\nBit vector In our machine, β stands for the bit vector of installed not currently blocked signals; and β 0\nstands for a null bit vector that may be interpreted as \"all signals are blocked\" or \"no signals are installed\".\nThe use of this bit vector almost directly corresponds to the bit maps used in real implementations. In\nreal implementations, every signal has a default pre-assigned handler. To imitate the same behaviour,\nin our implementation it is possible to run a command inside of nested blocks in which all signals are\nbound to their default handlers.\nExceptions and signals In real implementations (as explained in [4], ISO/IEC 14882 [8, 9]), exception\nthrowing inside of signal handlers is not recommended, due to implementation restrictions. Moreover,\nthe existing implementation of signals is not block structured. By contrast, our abstract machine and\nbig-step semantics deal with block structured signals and allow signal handlers to throw exceptions.\nImplementation of exception handling In real implementations (e.g.: Itanium [5], and as described in\n[10, 4, 3]), exception handling is implemented by use of stack unwinding. Exception handling in our\nimplementation resembles handling in real implementations, except the fact that the abstract machine\nuses the extra stack J to keep track of block structures, and the J is manipulated by special instructions\nin the continuation K.\n6 Examples of the machine runs\nWe have already seen in previous examples (e.g.: Figure 8 and Figure 11) that the big-step semantics\ngives us block structure for free. This becomes very useful in studying block structured constructs and\ntheir interactions. By contrast, the machine needs to manage block structure explicitly with a help of the\nstack. The examples of corresponding machine runs are given in Figure 14 and Figure 15. Please note,\nthe pop-upd(β 0)2 stands for pop-upd(β 0);pop-upd(β 0).\nThe example in Figure 4 shows how the big-step syntax makes it easy to address one-shot signals\nwith splitting the bindings. On the contrary, the machine needs to perform extra administrative work\nwith the binding tags and the stack to implement one-shot signal handling (Figure 16).\nOne may observe that the abstract machine is more complex than the big-step semantics, as machine\nneeds to deal with many details explicitly. Overall, we see that the machine is closer to implementations,\nwhereas the big-step semantics is more convenient for abstract reasoning.\nStrygin and Thielecke 161\n〈try (bindzto(h ;throwe)inc) handle e by g | s1 | β 0 |\u0004 | return〉\n 〈bindzto (h ;throwe)inc | s1 | β 0 | (e,g) | pop-upd(β 0)〉\n 〈c | s1 | β 0 + z | (z,(h ;throwe)),(e,g) | pop-upd(β 0)2 〉\n 〈(h ;throwe) | s1 | β 0 | (z,(h ;throwe)),(e,g) | update(β 0 + z);c;pop-upd(β 0)2 〉\n 〈h | s1 | β 0 | (z,(h ;throwe)),(e,g) | throwe;update(β 0 + z);c;pop-upd(β 0)2 〉\n 〈throwe | s2 | β 0 | (z,(h ;throwe)),(e,g) | update(β 0 + z);c;pop-upd(β 0)2 〉\n 〈g | s2 | β 0 |\u0004 | return〉\n 〈return | s3 | β 0 |\u0004 |\u0004〉\nFigure 14: Binding inside of the try block\n〈bindztohin(try (throwe) handle e by g) | s1 | β 0 |\u0004 | return〉\n 〈try (throwe) handle e by g | s1 | β 0 + z | (z,h) | pop-upd(β 0)〉\n 〈throwe | s1 | β 0 + z | (e,g),(z,h) | pop-upd(β 0 + z);pop-upd(β 0)〉\n 〈h | s1 | β 0 | (e,g),(z,h) | update(β 0 + z);throwe;pop-upd(β 0 + z);pop-upd(β 0)〉\n 〈update(β 0 + z) | s2 | β 0 | (e,g),(z,h) | throwe;pop-upd(β 0 + z);pop-upd(β 0)〉\n 〈throwe | s2 | β 0 + z | (e,g),(z,h) | pop-upd(β 0 + z);pop-upd(β 0)〉\n 〈g | s2 | β 0 + z | (z,h) | pop-upd(β 0)〉\n 〈h | s2 | β 0 | (z,h) | update(β 0 + z);g;pop-upd(β 0)〉\n 〈update(β 0 + z) | s3 | β 0 | (z,h) | g;pop-upd(β 0)〉\n 〈g | s3 | β 0 + z | (z,h) | pop-upd(β 0)〉\n 〈pop-upd(β 0) | s4 | β 0 + z | (z,h) | return〉\n 〈return | s4 | β 0 |\u0004 |\u0004〉\nFigure 15: Exception handling inside of the binding\n〈bind/1 ztoch in(c1 ; c2) | s1 | β 0 |\u0004 | return〉\n 〈c1 ; c2 | s1 | β 0 + z | (z,h1,0) | pop-upd(β 0);return〉\n 〈c1 | s1 | β 0 + z | (z,h1,0) | c2;pop-upd(β 0);return〉\n 〈h1 | s1 | β 0 | (z,h1,1) | update(β 0);c1;c2;pop-upd(β 0);return〉\n 〈update(β 0) | s2 | β 0 | (z,h1,1) | c1;c2;pop-upd(β 0);return〉\n 〈c1 | s2 | β 0 | (z,h1,1) | c2;pop-upd(β 0);return 〉\n 〈c2 | s3 | β 0 | (z,h1,1) | pop-upd(β 0);return〉\n 〈pop-upd(β 0) | s4 | β 0 | (z,h1,1) | return〉\n 〈return | s4 | β 0 |\u0004 |\u0004〉\nFigure 16: Signal binding and seq. composed commands\n162 Operational semantics for signal handling\n7 Conclusions\nThe present paper idealizes signal handling in combination with the more familiar exception handling to\nfocus on some of their semantic and logical features. The semantics of one-shot handlers is reminiscent\nof linearly-used continuations [2] and the resource usage in separation logic [13]. The way we have\ntreated signal bindings in the big-step semantics borrows ideas from linear logic. Recall that we write\nS;O ⊢ s1,c ⇓ s2\nfor a judgement involving a persistent signal binding S and a one-shot signal binding O. As we have\nillustrated with the examples in Section 4, the signal binding S can be shared between two commands c1\nand c2 in a sequential composition, whereas O has to be split into disjoint parts O1 and O2. This splitting\nprevents a one-shot signal handler from being re-used and makes it a linear resource just like the contexts\nin a linear logic. In fact, Dual Intuitionistic Linear Logic [1] has two zones Γ and ∆ in the context, one\nwhich allows sharing and one which does not, as in the following rule that shares ∆ and splits Γ:\nΓ1;∆ ⊢ M : A⊸ B Γ2;∆ ⊢ N : A\nΓ1,Γ2;∆ ⊢ M N : B\nWe are not aware of previous operational semantics for signals, although Feng, Shao, Guo and Dong [6]\npresents a program logic for assembly language with interrupts, which are analogous to signals at the\nhardware level.\nHutton and Wright [7] study interruptions as asynchronous exceptions. By contrast, signals are a\nsoftware alternative to hardware interrupts, where signal handlers could be addressed as asynchronous\nsubroutine calls.\nSignals have been part of the long evolution of Unix, and are correspondingly complex. To implement\nblock-structured signal handling and integrate it with exceptions, the present signal mechanism may have\nto be revisited. The present implementations pose severe restrictions on programmers, for instance on\nusing non-local control in a handler. Removing such implementation restrictions would enable natural\nprogramming idioms. In further work, we hope to build on the operational semantics presented here for\nproving soundness of a Hoare logic for signals.\nThe formal connection between the big-step operational semantics and the signals abstract machine\nremains to be established. We conjecture that they are observationally equivalent and that this may be\nproved by way of a simulation relation.\nReferences\n[1] Andrew Barber & Gordon Plotkin (1998): Dual Intuitionistic Linear Logic. Technical Report, University of\nEdinburgh.\n[2] Josh Berdine, Peter W. O’Hearn, Uday Reddy & Hayo Thielecke (2002): Linear Continuation Passing.\nHigher-order and Symbolic Computation 15(2/3), pp. 181–208, doi:10.1023/A:1020891112409.\n[3] Daniel Bovet & Marco Cesati (2002): Understanding the Linux Kernel, Second Edition, 2 edition. O’Reilly\n& Associates, Inc., Sebastopol, CA, USA.\n[4] Christophe de Dinechin (2000): C++ exception handling for IA-64. In: Proceedings of the 1st conference on\nIndustrial Experiences with Systems Software - Volume 1, WIESS’00, USENIX Association, Berkeley, CA,\nUSA, pp. 8–8. Available at http://dl.acm.org/citation.cfm?id=1251503.1251511.\nStrygin and Thielecke 163\n[5] (2001): Itanium C++ ABI: Exception Handling. Available at http://www.codesourcery.com/cxx-abi/\nabi-eh.html. (Revision: 1.22).\n[6] Xinyu Feng, Zhong Shao, Yu Guo & Yuan Dong (2009): Certifying Low-Level Programs with Hard-\nware Interrupts and Preemptive Threads. J. Autom. Reasoning 42(2-4), pp. 301–347, doi:10.1007/\ns10817-009-9118-9.\n[7] Graham Hutton & Joel Wright (2007): What is the meaning of these constant interruptions? J. Funct.\nProgram. 17(6), pp. 777–792, doi:10.1017/S0956796807006363.\n[8] (2011): ISO/IEC 14882:2011 Information technology - Programming languages - C++. Available at\nhttp://www.iso.org.\n[9] (1999): ISO/IEC 14882:1999 Programming languages - C++. Available at www.iso.ch.\n[10] Michael Kerrisk (2010): The Linux Programming Interface: A Linux and UNIX System Programming Hand-\nbook, 1 edition. No Starch Press. Available at http://www.worldcat.org/isbn/1593272200.\n[11] Robin Milner, Mads Tofte, Robert Harper & David MacQueen (1997): The Definition of Standard ML (Re-\nvised). MIT Press. Available at http://www.worldcat.org/isbn/0262631814.\n[12] Eugenio Moggi (1989): Computational Lambda Calculus and Monads. In: Proceedings, Fourth Annual\nSymposium on Logic in Computer Science, pp. 14–23, doi:10.1109/LICS.1989.39155.\n[13] John C. Reynolds (2002): Separation Logic: A Logic for Shared Mutable Data Structures. In: Logic in\nComputer Science (LICS), IEEE, pp. 55–74, doi:10.1109/LICS.2002.1029817.\n[14] Kay Robbins & Steve Robbins (2003): UNIX Systems Programming: Communication, Concurrency and\nThreads (2nd Edition). Prentice Hall PTR. Available at http://www.worldcat.org/isbn/0130424110.\n[15] Sandra Loosemore and Richard M. Stallman and Roland McGrath and Andrew Oram and Ulrich Drep-\nper (2007): The GNU C Library Reference Manual, 0.12 edition. Available at http://www.gnu.org/\nsoftware/libc/manual/pdf/libc.pdf. last updated 2007-10-27, for version 2.8.\n[16] Richard W. Stevens & Stephen A. Rago (2005): Advanced Programming in the UNIX(R) Environment\n(2nd Edition). Addison-Wesley Professional. Available at http://www.informit.com/store/product.\naspx?isbn=0201433079.\n",
            "id": 4224827,
            "identifiers": [
                {
                    "identifier": "22200616",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:doaj.org/article:8bebb20f57d04b4eaaf28fd4ebe6a366",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "1208.2754",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "9259173",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "1975414984",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "10.4204/eptcs.89.11",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:citeseerx.psu:10.1.1.261.87",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "189107961",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "26420122",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:arxiv.org:1208.2754",
                    "type": "OAI_ID"
                }
            ],
            "title": "Operational semantics for signal handling",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "1975414984",
            "oaiIds": [
                "oai:citeseerx.psu:10.1.1.261.87",
                "oai:doaj.org/article:8bebb20f57d04b4eaaf28fd4ebe6a366",
                "oai:arxiv.org:1208.2754"
            ],
            "publishedDate": "2012-08-01T00:00:00",
            "publisher": "'Open Publishing Association'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://doaj.org/search?source=%7B%22query%22%3A%7B%22bool%22%3A%7B%22must%22%3A%5B%7B%22term%22%3A%7B%22id%22%3A%228bebb20f57d04b4eaaf28fd4ebe6a366%22%7D%7D%5D%7D%7D%7D",
                "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.261.87",
                "http://arxiv.org/abs/1208.2754"
            ],
            "updatedDate": "2021-05-03T17:33:23",
            "yearPublished": 2012,
            "journals": [
                {
                    "title": "Electronic Proceedings in Theoretical Computer Science",
                    "identifiers": [
                        "2075-2180"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1208.2754"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/4224827"
                }
            ]
        },
        {
            "acceptedDate": "2012-08-30T00:00:00",
            "arxivId": "1207.3582",
            "authors": [
                {
                    "name": "Ho, Tracey"
                },
                {
                    "name": "Leong, Derek"
                }
            ],
            "contributors": [
                "The Pennsylvania State University CiteSeerX Archives"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/192388384",
                "https://api.core.ac.uk/v3/outputs/216163089",
                "https://api.core.ac.uk/v3/outputs/103820924"
            ],
            "createdDate": "2012-07-19T14:00:47",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 145,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/145",
                    "logo": "https://api.core.ac.uk/data-providers/145/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 200,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/200",
                    "logo": "https://api.core.ac.uk/data-providers/200/logo"
                },
                {
                    "id": 2857,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/2857",
                    "logo": "https://api.core.ac.uk/data-providers/2857/logo"
                }
            ],
            "depositedDate": "2012-07-01T00:00:00",
            "abstract": "We consider a real-time streaming system where messages are created\nsequentially at the source, and are encoded for transmission to the receiver\nover a packet erasure link. Each message must subsequently be decoded at the\nreceiver within a given delay from its creation time. The goal is to construct\nan erasure correction code that achieves the maximum message size when all\nmessages must be decoded by their respective deadlines under a specified set of\nerasure patterns (erasure model). We present an explicit intrasession code\nconstruction that is asymptotically optimal under erasure models containing a\nlimited number of erasures per coding window, per sliding window, and\ncontaining erasure bursts of a limited length.Comment: Extended version of a conference paper in the IEEE International\n  Symposium on Information Theory (ISIT), July 2012. 12 pages, 3 figure",
            "documentType": "research",
            "doi": "10.1109/isit.2012.6284055",
            "downloadUrl": "http://arxiv.org/abs/1207.3582",
            "fieldOfStudy": "computer science",
            "fullText": "ar\nX\niv\n:1\n20\n7.\n35\n82\nv1\n  [\ncs\n.IT\n]  \n16\n Ju\nl 2\n01\n2\nErasure Coding for Real-Time Streaming\nDerek Leong\nDepartment of Electrical Engineering\nCalifornia Institute of Technology\nPasadena, California 91125, USA\nderekleong@caltech.edu\nTracey Ho\nDepartment of Electrical Engineering\nCalifornia Institute of Technology\nPasadena, California 91125, USA\ntho@caltech.edu\nAbstract—We consider a real-time streaming system where\nmessages are created sequentially at the source, and are encoded\nfor transmission to the receiver over a packet erasure link. Each\nmessage must subsequently be decoded at the receiver within\na given delay from its creation time. The goal is to construct\nan erasure correction code that achieves the maximum message\nsize when all messages must be decoded by their respective\ndeadlines under a specified set of erasure patterns (erasure\nmodel). We present an explicit intrasession code construction\nthat is asymptotically optimal under erasure models containing\na limited number of erasures per coding window, per sliding\nwindow, and containing erasure bursts of a limited length.\nI. INTRODUCTION\nWe consider a real-time streaming system where messages\nare created sequentially at the source, and are encoded for\ntransmission to the receiver over a packet erasure link. Each\nmessage must subsequently be decoded at the receiver within\na given delay from its creation time. We seek to construct\nan erasure correction code that withstands a specified set of\nerasure patterns (erasure model), allowing all messages to be\ndecoded by their respective deadlines.\nIn particular, we consider three erasure models: the first\nmodel limits the number of erasures in each coding window,\nthe second model limits the number of erasures in each sliding\nwindow, while the third model limits the length of erasure\nbursts. For each erasure model, the objective is to find an\noptimal code that achieves the maximum message size, among\nall codes that allow all messages to be decoded by their\nrespective deadlines under all admissible erasure patterns.\nWe present an explicit intrasession code construction which\nspecifies an allocation of link bandwidth or data packet space\namong the different messages; coding occurs within each mes-\nsage but not across messages. Intrasession coding is attractive\ndue to its relative simplicity, but it is not known in general\nwhen intrasession coding is sufficient or when intersession\ncoding is necessary. We show that for an asymptotic number of\nmessages, our code construction achieves the optimal message\nsize among all codes (intrasession or intersession) for the\nfirst and second models with any given maximum number of\nThis paper is an extended version of [1], which was presented at the\nISIT 2012 conference.\nThis work was supported in part by the Air Force Office of Scientific\nResearch under Grant FA9550-10-1-0166.\nFig. 1. Real-time streaming system for (c, d) = (3, 8). Each of the messages\n{1, . . . , 5} is assigned a unique color. Messages are created at the source at\nregular intervals of c time steps, and must be decoded at the receiver within d\ntime steps from their respective creation times. At each time step t, the source\nis allowed to transmit a single data packet of normalized unit size over the\nlink.\nerasures per window, and for the third model when the given\nmaximum erasure burst length is sufficiently short or long.\nIn related work, Martinian et al. [2], [3] provided construc-\ntions of streaming codes that minimize the delay required\nto correct erasure bursts of a given length. Streaming codes\nin which the decoding error probability decays exponentially\nwith delay, called tree codes or anytime codes, are considered\nin [4]–[6]. Tekin et al. [7] considered erasure correction coding\nfor a non-real-time streaming system where all messages are\ninitially present at the encoder.\nWe begin with a formal definition of the problem in\nSection II, and proceed to describe the construction of our\nintrasession code in Section III. We then demonstrate the opti-\nmality of this code under each erasure model in the subsequent\nsections. Proofs of theorems are deferred to Appendix B.\nII. PROBLEM DEFINITION\nConsider a discrete-time data streaming system comprising\na source and a receiver, with a directed unit-bandwidth packet\nerasure link from the source to the receiver. Independent\nmessages of uniform size s > 0 are created at the source at\nregular intervals of c ∈ Z+ time steps, and must be decoded\nat the receiver within d ∈ Z+ time steps from their respective\ncreation times. At each time step t ∈ Z+, the source is allowed\nto transmit a single data packet of normalized unit size over\nthe link. Fig. 1 depicts this real-time streaming system for an\n2instance of (c, d).\nMore precisely, each message k ∈ Z+ is created at time step\n(k − 1)c+ 1, and is to be decoded by time step (k − 1)c+ d.\nThe coded data transmitted at each time step t ∈ Z+ must be\na function of messages created at time step t or earlier. Let\ncoding window Wk be the interval of d time steps between\nthe creation time and the decoding deadline of message k, i.e.,\nWk , {(k − 1)c+ 1, . . . , (k − 1)c+ d}.\nWe shall assume that d > c so as to avoid the degenerate case\nof nonoverlapping coding windows for which it is sufficient\nto code individual messages separately.\nConsider the first n messages {1, . . . , n}, and the union of\ntheir (overlapping) coding windows Tn given by\nTn , W1 ∪ · · · ∪Wn = {1, . . . , (n− 1)c+ d}.\nAn erasure pattern E ⊆ Tn specifies a set of erased data\ntransmissions over the link. More precisely, if t ∈ E, then none\nof the data transmitted at time step t is received by the receiver\n(i.e., the entire data packet is erased); if t ∈ Tn\\E, then all of\nthe data transmitted at time step t is received by the receiver\nat time step t (i.e., the entire data packet is received without\ndelay). An erasure model specifies a set of erasure patterns\nthat an erasure correction code should withstand.\nFor a given pair of positive integers a and b, we define\nthe offset quotient qa,b and remainder ra,b to be the unique\nintegers satisfying the following three conditions:\na = qa,b b+ ra,b, qa,b ∈ Z\n+\n0 , ra,b ∈ {1, . . . , b},\nwhere Z+0 denotes the set of nonnegative integers, i.e.,\nZ\n+ ∪ {0}. Note that our definition departs from the usual\ndefinition of quotient and remainder in that ra,b can be equal\nto b but not 0.\nIII. CODE CONSTRUCTION\nWe present an intrasession code which codes only within\neach message and not across different messages. We begin\nby specifying the amount of link bandwidth or data packet\nspace allocated for the encoding of each message at each time\nstep. An appropriate code (e.g., random linear coding, MDS\ncode) is then applied to the allocation so that each message\ncan be decoded whenever the total amount of received data\nthat encodes that message is at least the message size s.\nThe allocation of link bandwidth follows a simple rule: the\nlink bandwidth at each time step is divided evenly among all\nactive messages. We say that message k is active at time step t\nif and only if t falls within its coding window, i.e., t ∈Wk.\nFig. 2 shows how much link bandwidth at each time step is\nallocated to each message, for two instances of (c, d).\nFor a given choice of (c, d), the messages that are encoded\nat a given time step t ∈ Z+ can be stated explicitly as follows:\nFirst, we define At to be the set of active messages at time\nstep t, i.e.,\nAt , {k ∈ Z\n+ : t ∈Wk}\n= {k ∈ Z+ : (k − 1)c+ 1 ≤ t ≤ (k − 1)c+ d}\n(a) (c, d) = (3, 9)\n(b) (c, d) = (3, 8)\nFig. 2. Allocation of link bandwidth at each time step t, in the encoding\nof messages {1, . . . , 6}, for (a) (c, d) = (3, 9) and (b) (c, d) = (3, 8). Each\nmessage is assigned a unique color. In (a), because d is a multiple of c, we\nhave qd,c + 1 = 3 active messages at each time step. In (b), because d is not\na multiple of c, we have either qd,c = 2 or qd,c + 1 = 3 active messages at\neach time step.\n=\n{\nk ∈ Z+ :\nt− d\nc\n+ 1 ≤ k ≤\nt− 1\nc\n+ 1\n}\n.\nTreating nonpositive messages 0,−1,−2, . . . as dummy mes-\nsages, we can write\nAt =\n{⌈\nt− d\nc\n+ 1\n⌉\n, . . . ,\n⌊\nt− 1\nc\n+ 1\n⌋}\n.\nExpressing this in terms of qd,c, rd,c, qt,c, rt,c yields\nAt =\n{\nqt,c + 1− qd,c +\n⌈\nrt,c − rd,c\nc\n⌉\n, . . . , qt,c + 1\n}\n.\nIt follows that the number of active messages |At| varies over\ntime depending on the value of rt,c; specifically, two cases are\npossible:\nCase 1: If rt,c ≤ rd,c, then\n−1 <\n1− c\nc\n≤\nrt,c − rd,c\nc\n≤ 0,\nwhich implies that\n⌈\nrt,c−rd,c\nc\n⌉\n= 0, and\nAt = {qt,c + 1− qd,c, . . . , qt,c + 1} .\nThe qd,c + 1 messages of At are therefore encoded at time\nstep t, with each message allocated 1\nqd,c+1\namount of link\nbandwidth.\nCase 2: If rt,c > rd,c, then\n0 <\nrt,c − rd,c\nc\n≤\nc− 1\nc\n< 1,\n3which implies that\n⌈\nrt,c−rd,c\nc\n⌉\n= 1, and\nAt = {qt,c + 1− (qd,c − 1), . . . , qt,c + 1} .\nThe qd,c messages of At are therefore encoded at time step t,\nwith each message allocated 1\nqd,c\namount of link bandwidth.\nNote that when d is a multiple of c, we have rt,c ≤ rd,c = c\nfor any t, which implies that qd,c + 1 messages are encoded\nat every time step.\nIn our subsequent performance analysis of this code, we\nmake repeated use of two key code properties; these are\npresented as technical lemmas in Appendix A.\nIV. PERFORMANCE UNDER z ERASURES\nPER CODING WINDOW\nFor the first erasure model, we look at erasure patterns\nthat have a limited number of erasures per coding window.\nConsider the first n messages {1, . . . , n}, and the union of\ntheir (overlapping) coding windows Tn. Let ECWn be the set of\nerasure patterns that have z or fewer erasures in each coding\nwindow Wk , i.e.,\nECWn ,\n{\nE ⊆ Tn : |E ∩Wk| ≤ z ∀ k ∈ {1, . . . , n}\n}\n.\nThe objective is to construct a code that allows all n mes-\nsages {1, . . . , n} to be decoded by their respective deadlines\nunder any erasure pattern E ∈ ECWn . Let sCWn be the maximum\nmessage size that can be achieved by such a code, for a given\nchoice of (n, c, d, z).\nWe observe that over a finite time horizon (i.e., when the\nnumber of messages n is finite), intrasession coding can be\nsuboptimal. The following example shows that an intersession\ncode can achieve a message size that is strictly larger than the\nmessage size achieved by an optimal intrasession code:\nExample (Finite time horizon). Suppose that (n, c, d, z) =\n(3, 1, 3, 1). The maximum message size that can be achieved\nby an intrasession code is 67 ; one such optimal intrasession\ncode, which can be found by solving a linear program, is\nas follows (the amount of link bandwidth allocated to each\nmessage is indicated in parentheses):\nThe following intersession code achieves a strictly larger\nmessage size of 1 (Mk denotes message k):\nUsing a simple cut-set bound argument, we can show that this\nis also the maximum achievable message size, i.e., sCWn = 1.\nHowever, it turns out that the intrasession code constructed\nin Section III is asymptotically optimal; the gap between the\nmaximum achievable message size sCWn and the message size\nachieved by the code vanishes as the number of messages n\ngoes to infinity:\nTheorem 1. The code constructed in Section III is asymp-\ntotically optimal in the following sense: the code achieves a\nmessage size of\nd−z∑\nj=1\nyj,\nwhich is equal to the asymptotic maximum achievable message\nsize limn→∞ sCWn , where y = (y1, . . . , yd) is defined as\ny ,\n( d entries︷ ︸︸ ︷\n1\nqd,c + 1\n, . . . ,\n1\nqd,c + 1︸ ︷︷ ︸\n(qd,c+1)rd,c entries\n,\n1\nqd,c\n, . . . ,\n1\nqd,c︸ ︷︷ ︸\nqd,c(c−rd,c) entries\n)\n.\nThe achievability claim of this theorem is a consequence of\nLemma 1; to prove the converse claim, we consider a cut-set\nbound corresponding to a specific worst-case erasure pattern\nin which exactly z erasures occur in every coding window.\nThis erasure pattern is chosen with the help of Lemma 2;\nspecifically, the erased time steps are chosen to coincide with\nthe larger blocks allocated to each message in the constructed\ncode.\nV. PERFORMANCE UNDER z ERASURES\nPER SLIDING WINDOW OF d TIME STEPS\nFor the second erasure model, we look at erasure patterns\nthat have a limited number of erasures per sliding window of\nd time steps. Consider the first n messages {1, . . . , n}, and the\nunion of their (overlapping) coding windows Tn. Let sliding\nwindow Lt denote the interval of d time steps beginning at\ntime step t, i.e.,\nLt , {t, . . . , t+ d− 1}.\nLet ESWn be the set of erasure patterns that have z or fewer\nerasures in each sliding window Lt, i.e.,\nESWn ,\n{\nE ⊆ Tn : |E ∩Lt| ≤ z ∀ t ∈ {1, . . . , (n− 1)c+ 1}\n}\n.\nThe objective is to construct a code that allows all n messages\n{1, . . . , n} to be decoded by their respective deadlines under\nany erasure pattern E ∈ ESWn . Let sSWn be the maximum message\n4size that can be achieved by such a code, for a given choice\nof (n, c, d, z).\nWe note that since ESWn ⊆ ECWn , we therefore have sSWn ≥ sCWn .\nFor the special case of c = 1, each sliding window is also\na coding window, and so this sliding window erasure model\nreduces to the coding window erasure model of Section IV,\ni.e., ESWn = ECWn . Over a finite time horizon, intrasession coding\ncan also be suboptimal for this erasure model; the illustrating\nexample from Section IV applies here as well.\nSurprisingly, the constructed intrasession code also turns out\nto be asymptotically optimal over all codes; the omission of\nerasure patterns in ESWn compared to ECWn has not led to an\nincrease in the maximum achievable message size (cf. Theo-\nrem 1):\nTheorem 2. The code constructed in Section III is asymp-\ntotically optimal in the following sense: the code achieves a\nmessage size of\nd−z∑\nj=1\nyj ,\nwhich is equal to the asymptotic maximum achievable message\nsize limn→∞ sSWn .\nProving the converse claim of this theorem requires a dif-\nferent approach from that of Theorem 1. When d is a multiple\nof c, we need only consider a cut-set bound corresponding\nto an obvious worst-case erasure pattern in which exactly z\nerasures occur in every sliding window, specifically, a periodic\nerasure pattern with alternating intervals of z erased time steps\nand d− z unerased time steps. When d is not a multiple\nof c, no single admissible erasure pattern provides a cut-set\nbound that matches the constructed code; instead, we need\nto combine different erasure patterns for different messages.\nTo pick these erasure patterns, we first choose a specific base\nerasure pattern E′ (which may not be admissible in general)\nwith the help of Lemma 2. We then derive admissible erasure\npatterns from E′ by taking its intersection with each coding\nwindow, i.e., (E′ ∩Wk) ∈ ESWn . These derived erasure patterns\nare used in the inductive computation of an upper bound for\nthe conditional entropy\nH\n(\nX [Wn\\E\n′]\n∣∣∣Mn1 , X(n−1)c1 ),\nwhere Xt is a random variable representing the coded data\ntransmitted at time step t, Mk is a random variable repre-\nsenting message k, and X [A] , (Xt)t∈A. Intuitively, this\nconditional entropy term expresses how much space is left in\nthe unerased data packets of the coding window for message n,\nafter encoding the first n messages, and conditioned on the\nprevious time steps. The nonnegativity of the conditional\nentropy leads us to a bound for sSWn that matches the message\nsize achieved by the constructed code in the limit n→∞.\nVI. PERFORMANCE UNDER ERASURE BURSTS\nOF z TIME STEPS\nFor the third erasure model, we look at erasure patterns\nthat contain erasure bursts of a limited number of time steps.\nConsider the first n messages {1, . . . , n}, and the union of\ntheir (overlapping) coding windows Tn. Let EBn be the set of\nerasure patterns in which each erasure burst is z or fewer time\nsteps in length, and consecutive bursts are separated by a gap\nof d− z or more unerased time steps, i.e.,\nEBn ,\n{\nE ⊆ Tn :\n(t∈E ∧ t+1/∈E)⇒ |E ∩ {t+1, . . . , t+d−z}| = 0,\n(t/∈E ∧ t+1∈E)⇒ |E ∩ {t+1, . . . , t+z+1}| ≤ z\n}\n.\nThe objective is to construct a code that allows all n messages\n{1, . . . , n} to be decoded by their respective deadlines under\nany erasure pattern E ∈ EBn. Let sBn be the maximum message\nsize that can be achieved by such a code, for a given choice\nof (n, c, d, z).\nUsing the proof technique of Theorem 2, we can show that\nthe constructed intrasession code is asymptotically optimal\nwhen d is a multiple of c, or when the maximum erasure\nburst length z is sufficiently short or long:\nTheorem 3. If\n1) d is a multiple of c, or\n2) d is not a multiple of c and z ≤ c− rd,c, or\n3) d is not a multiple of c and z ≥ d− rd,c = qd,c c,\nthen the code constructed in Section III is asymptotically\noptimal in the following sense: the code achieves a message\nsize of\nd−z∑\nj=1\nyj,\nwhich is equal to the asymptotic maximum achievable message\nsize limn→∞ sBn.\nWhen the maximum erasure burst length z takes on inter-\nmediate values, intersession coding may become necessary.\nWe are currently studying optimal convolutional codes for this\ncase.\nAPPENDIX A\nCODE PROPERTIES\nThe first property describes when it is possible to decode\neach message:\nLemma 1 (Achievability). Consider the code constructed in\nSection III for a given choice of (c, d). If message size s\nsatisfies the inequality\ns ≤\nℓ∑\nj=1\nyj ,\nwhere y = (y1, . . . , yd) is as defined in Theorem 1, then each\nmessage k ∈ Z+ can be decoded from the data at any ℓ time\nsteps in its coding window Wk .\nNote that the maximum message size s that can be supported\nby the code is given by\n∑d\nj=1 yj = c, which corresponds to\nthe choice of ℓ = d.\n5(a) (c, d) = (3, 9)\n(b) (c, d) = (3, 8)\nFig. 3. Partitioning of the set of time steps Tn into the d sets T (1)n , . . . , T\n(d)\nn ,\nin the encoding of messages {1, . . . , 7}, for (a) (c, d) = (3, 9) and\n(b) (c, d) = (3, 8). Each set T (i)n is assigned a unique color. The number i\nat the top of each time step t indicates the set T (i)n to which t belongs.\nThe second property describes a way of partitioning time\nsteps into sets with certain specific properties, which are used\nin our specification of the worst-case erasure patterns:\nLemma 2 (Partition of Coding Windows). Consider the code\nconstructed in Section III for a given choice of (c, d). Con-\nsider the first n messages {1, . . . , n}, and the union of their\n(overlapping) coding windows Tn. The set of time steps Tn\ncan be partitioned into d sets T (1)n , . . . , T (d)n , given by\nT (i)n ,\n\n\n{(\nj(qd,c + 1) + qi,c\n)\nc+ ri,c ∈ Tn : j ∈ Z\n+\n0\n}\nif ri,c ≤ rd,c,{(\nj qd,c + qi,c\n)\nc+ ri,c ∈ Tn : j ∈ Z\n+\n0\n}\nif ri,c > rd,c,\nwith the following properties:\n1) Over the time steps in the set T (i)n , each message\nk ∈ {1, . . . , n} is allocated 1\nqd,c+1\namount of link band-\nwidth if ri,c ≤ rd,c, and 1qd,c amount of link bandwidth\nif ri,c > rd,c.\n2) The allocated link bandwidth in T (i)n for each message\nk ∈ {1, . . . , n} is contained within a single time step\nin its coding window Wk (as opposed to being spread\nover multiple time steps or being outside of the coding\nwindow).\n3) The total amount of link bandwidth over all time steps\nin T (i)n , i.e.,\n∣∣T (i)n ∣∣, has the following upper bound:\n∣∣T (i)n ∣∣ <\n\n\nn\nqd,c + 1\n+ 2 if ri,c ≤ rd,c,\nn\nqd,c\n+ 2 if ri,c > rd,c.\nFig. 3 shows how the set of time steps Tn is partitioned into\nthe d sets T (1)n , . . . , T (d)n , for two instances of (c, d).\nAPPENDIX B\nPROOFS OF THEOREMS\nProof of Lemma 1: Consider a given message k ∈ Z+\nand its coding window\nWk =\n{\n(k − 1)c+ i : i ∈ {1, . . . , d}\n}\n.\nAt each time step t ∈Wk, message k is allocated either\n1\nqd,c+1\nor 1\nqd,c\namount of link bandwidth; at all other time\nsteps t /∈Wk , message k is allocated zero link bandwidth.\nLet xi be the amount of link bandwidth at time step t =\n(k − 1)c+ i that is allocated to message k. Writing t in terms\nof qi,c and ri,c produces\nt = (k − 1)c+ i = (k − 1 + qi,c)︸ ︷︷ ︸\nqt,c\nc+ ri,c︸︷︷︸\nrt,c\n.\nIt follows from the code construction that the value of xi\ndepends on ri,c; specifically, two cases are possible:\nCase 1: If ri,c ≤ rd,c, then xi = 1qd,c+1 . Since\ni ∈ {1, . . . , d}, this condition corresponds to the case\nwhere qi,c ∈ {0, . . . , qd,c} and ri,c ∈ {1, . . . , rd,c}. Therefore,\nmessage k is allocated 1\nqd,c+1\namount of link bandwidth per\ntime step for a total of (qd,c + 1)rd,c time steps in the coding\nwindow Wk .\nCase 2: If ri,c > rd,c, then xi = 1qd,c . Since\ni ∈ {1, . . . , d}, this condition corresponds to the case\nwhere qi,c ∈ {0, . . . , qd,c − 1} and ri,c ∈ {rd,c + 1, . . . , c}.\nTherefore, message k is allocated 1\nqd,c\namount of link\nbandwidth per time step for a total of qd,c(c− rd,c) time\nsteps in the coding window Wk.\nObserve that y is simply a vector containing the elements\nof {xi}di=1 sorted in ascending order. Since\n∑\ni∈U\nxi ≥\n|U|∑\nj=1\nyj ∀ U ⊆ {1, . . . , d},\nit follows that over any ℓ time steps in the coding window Wk,\nthe total amount of link bandwidth allocated to message k is\nat least\n∑ℓ\nj=1 yj . Therefore, as long as the message size s\ndoes not exceed\n∑ℓ\nj=1 yj , message k can always be decoded\nfrom the data at any ℓ time steps in Wk.\nProof of Lemma 2: The stated partition can be\nconstructed by assigning each time step t ∈ Tn to the\nset T (qi,cc+ri,c)n , where\nri,c = rt,c,\nqi,c =\n\n\nqt,c −\n⌊\nqt,c\nqd,c+1\n⌋\n(qd,c + 1) if rt,c ≤ rd,c,\nqt,c −\n⌊\nqt,c\nqd,c\n⌋\nqd,c if rt,c > rd,c.\nNote that index qi,c c+ ri,c ∈ {1, . . . , d} since\nqi,c ∈ {0, . . . , qd,c} when ri,c ∈ {1, . . . , rd,c}, and\nqi,c ∈ {0, . . . , qd,c − 1} when ri,c ∈ {rd,c + 1, . . . , c}.\nTo prove the required code properties, we consider two\nseparate cases:\n6Case 1: Consider the set T (i)n for a choice of i satisfying\nri,c ≤ rd,c. Since each time step t ∈ T (i)n can be expressed as\nt =\n(\nj(qd,c + 1) + qi,c\n)︸ ︷︷ ︸\nqt,c\nc+ ri,c︸︷︷︸\nrt,c\n, tj , where j ∈ Z+0 ,\nit follows from the code construction that the set of active\nmessages at each time step contains qd,c + 1 messages, and is\ngiven by\nAtj =\n{\nj(qd,c+1)+qi,c︸ ︷︷ ︸\nqt,c\n+1−qd,c, . . . , j(qd,c+1)+qi,c︸ ︷︷ ︸\nqt,c\n+1\n}\n.\nThe smallest time step in T (i)n corresponds to the choice of\nj = 0, which produces t0 = qi,c c+ ri,c = i and the set of\nactive messages\nAt0 = {qi,c + 1− qd,c, . . . , qi,c + 1}.\nNote that At0 contains message 1 since qi,c ∈ {0, . . . , qd,c},\nwhich implies that\nqi,c + 1− qd,c ≤ 1 ≤ qi,c + 1.\nAt the other extreme, let the largest time step in T (i)n corre-\nspond to the choice of j = j′; we therefore have\ntj′ ≤ (n− 1)c+ d < tj′+1, (1)\nand the final set of active messages\nAtj′ = {j\n′(qd,c+1)+qi,c+1−qd,c, . . . , j\n′(qd,c+1)+qi,c+1}.\nFrom the first inequality of (1), we obtain(\nj′(qd,c + 1) + qi,c\n)\nc+ ri,c ≤ (n− 1 + qd,c)c+ rd,c\n=⇒ n ≥\n⌈(\nj′(qd,c + 1) + qi,c + 1− qd,c\n)\nc+ ri,c − rd,c\nc\n⌉\n= j′(qd,c + 1) + qi,c + 1− qd,c +\n⌈\nri,c − rd,c\nc\n⌉\n= j′(qd,c + 1) + qi,c + 1− qd,c, (2)\nwhere the final step follows from the fact that\n1 ≤ ri,c ≤ rd,c ≤ c, which implies that\n−1 <\n1− c\nc\n≤\nri,c − rd,c\nc\n≤ 0 =⇒\n⌈\nri,c − rd,c\nc\n⌉\n= 0.\nFrom the second inequality of (1), we obtain\n(n− 1 + qd,c)c+ rd,c ≤\n(\n(j′+1)(qd,c+1) + qi,c\n)\nc+ ri,c − 1\n=⇒ n ≤\n⌊(\n(j′+1)(qd,c+1)+qi,c+1−qd,c\n)\nc+ ri,c−rd,c−1\nc\n⌋\n= (j′+1)(qd,c+1) + qi,c + 1− qd,c +\n⌊\nri,c−rd,c−1\nc\n⌋\n= j′(qd,c + 1) + qi,c + 1, (3)\nwhere the final step follows from the fact that\n1 ≤ ri,c ≤ rd,c ≤ c, which implies that\n− 1 =\n1− c− 1\nc\n≤\nri,c − rd,c − 1\nc\n≤ −\n1\nc\n< 0\n=⇒\n⌊\nri,c − rd,c − 1\nc\n⌋\n= −1.\nBy combining inequalities (2) and (3), we arrive at\nj′(qd,c + 1) + qi,c + 1− qd,c ≤ n ≤ j\n′(qd,c + 1) + qi,c + 1,\nwhich enables us to infer that Atj′ contains message n.\nFor any pair of consecutive time steps tj , tj+1 ∈ T (i)n , where\ntj =\n(\nj(qd,c + 1) + qi,c\n)\nc+ ri,c,\ntj+1 =\n(\n(j + 1)(qd,c + 1) + qi,c\n)\nc+ ri,c,\nwe observe that the smallest message in Atj+1 is exactly one\nlarger than the largest message in Atj , i.e.,\n(j + 1)(qd,c + 1) + qi,c + 1− qd,c\n= j(qd,c + 1) + qi,c + 1− qd,c + qd,c + 1\n=\n(\nj(qd,c + 1) + qi,c + 1\n)\n+ 1.\nThus, there are no overlapping or omitted messages among the\nsets of active messages corresponding to T (i)n . Properties 1 and\n2 therefore follow.\nThe total amount of link bandwidth over all time steps in\nT\n(i)\nn , i.e.,\n∣∣T (i)n ∣∣, can be computed by summing over the link\nbandwidth allocated to the n messages, and adding the unused\nlink bandwidth in the smallest time step (which is allocated\nto nonpositive dummy messages) and in the largest time step\n(which is allocated to messages larger than n); this produces\nthe required upper bound of Property 3.\nCase 2: Consider the set T (i)n for a choice of i satisfying\nri,c > rd,c. Since each time step t ∈ T (i)n can be expressed as\nt = (j qd,c + qi,c)︸ ︷︷ ︸\nqt,c\nc+ ri,c︸︷︷︸\nrt,c\n, tj , where j ∈ Z+0 ,\nit follows from the code construction that the set of active\nmessages at each time step contains qd,c messages, and is given\nby\nAtj =\n{\nj qd,c + qi,c︸ ︷︷ ︸\nqt,c\n+1− (qd,c − 1), . . . , j qd,c + qi,c︸ ︷︷ ︸\nqt,c\n+1\n}\n.\nThe smallest time step in T (i)n corresponds to the choice of\nj = 0, which produces t0 = qi,c c+ ri,c = i and the set of\nactive messages\nAt0 = {qi,c + 1− (qd,c − 1), . . . , qi,c + 1}.\nNote that At0 contains message 1 since\nqi,c ∈ {0, . . . , qd,c − 1}, and therefore\nqi,c + 1− (qd,c − 1) ≤ 1 ≤ qi,c + 1.\nAt the other extreme, let the largest time step in T (i)n corre-\nspond to the choice of j = j′; we therefore have\ntj′ ≤ (n− 1)c+ d < tj′+1, (4)\nand the final set of active messages\nAtj′ = {j\n′ qd,c + qi,c + 1− (qd,c − 1), . . . , j\n′ qd,c + qi,c + 1}.\n7From the first inequality of (4), we obtain\n(j′ qd,c + qi,c)c+ ri,c ≤ (n− 1 + qd,c)c+ rd,c\n=⇒ n ≥\n⌈\n(j′ qd,c + qi,c + 1− qd,c)c+ ri,c − rd,c\nc\n⌉\n= j′ qd,c + qi,c + 1− qd,c +\n⌈\nri,c − rd,c\nc\n⌉\n= j′ qd,c + qi,c + 1− (qd,c − 1), (5)\nwhere the final step follows from the fact that\n1 ≤ rd,c < ri,c ≤ c, which implies that\n0 <\nri,c − rd,c\nc\n≤\nc− 1\nc\n< 1 =⇒\n⌈\nri,c − rd,c\nc\n⌉\n= 1.\nFrom the second inequality of (4), we obtain\n(n− 1 + qd,c)c+ rd,c ≤\n(\n(j′ + 1)qd,c + qi,c\n)\nc+ ri,c − 1\n=⇒ n ≤\n⌊(\n(j′+1)qd,c + qi,c + 1− qd,c\n)\nc+ ri,c − rd,c − 1\nc\n⌋\n= (j′ + 1)qd,c + qi,c + 1− qd,c +\n⌊\nri,c − rd,c − 1\nc\n⌋\n= j′ qd,c + qi,c + 1, (6)\nwhere the final step follows from the fact that\n1 ≤ rd,c < ri,c ≤ c, which implies that\n0 =\n1− 1\nc\n≤\nri,c − rd,c − 1\nc\n≤\nc− 1− 1\nc\n< 1\n=⇒\n⌊\nri,c − rd,c − 1\nc\n⌋\n= 0.\nBy combining inequalities (5) and (6), we arrive at\nj′ qd,c + qi,c + 1− (qd,c − 1) ≤ n ≤ j\n′ qd,c + qi,c + 1,\nwhich enables us to infer that Atj′ contains message n.\nFor any pair of consecutive time steps tj , tj+1 ∈ T (i)n , where\ntj =\n(\nj qd,c + qi,c\n)\nc+ ri,c,\ntj+1 =\n(\n(j + 1) qd,c + qi,c\n)\nc+ ri,c,\nwe observe that the smallest message in Atj+1 is exactly one\nlarger than the largest message in Atj , i.e.,\n(j + 1)qd,c + qi,c + 1− (qd,c − 1)\n= j qd,c + qi,c + 1− (qd,c − 1) + qd,c\n=\n(\nj qd,c + qi,c + 1\n)\n+ 1.\nThus, there are no overlapping or omitted messages among the\nsets of active messages corresponding to T (i)n . Properties 1 and\n2 therefore follow.\nThe total amount of link bandwidth over all time steps in\nT\n(i)\nn , i.e.,\n∣∣T (i)n ∣∣, can be computed by summing over the link\nbandwidth allocated to the n messages, and adding the unused\nlink bandwidth in the smallest time step (which is allocated\nto nonpositive dummy messages) and in the largest time step\n(which is allocated to messages larger than n); this produces\nthe required upper bound of Property 3.\nProof of Theorem 1: Consider the code constructed in\nSection III for a given choice of (c, d). According to Lemma 1,\nif message size s satisfies the inequality\ns ≤\nd−z∑\nj=1\nyj ,\nthen each message k ∈ {1, . . . , n} can be decoded from the\ndata at any d− z time steps in its coding window Wk.\nTherefore, the code achieves a message size of\n∑d−z\nj=1 yj , by\nallowing all n messages {1, . . . , n} to be decoded by their\nrespective deadlines as long as there are z or fewer erasures\nin each coding window Wk , or equivalently, under any erasure\npattern E ∈ ECWn . To demonstrate the asymptotic optimality of\nthe code, we will show that this message size matches the\nmaximum achievable message size sCWn in the limit, i.e.,\nlim\nn→∞\nsCWn =\nd−z∑\nj=1\nyj. (7)\nTo obtain an upper bound for sCWn , we consider the cut-\nset bound corresponding to a specific erasure pattern E′ from\nECWn . Let {1, . . . , d} be partitioned into two sets V (1) and V (2),\nwhere\nV (1) ,\n{\ni ∈ {1, . . . , d} : ri,c ≤ rd,c\n}\n,\nV (2) ,\n{\ni ∈ {1, . . . , d} : ri,c > rd,c\n}\n.\nLet v = (v1, . . . , vd) be defined as v ,\n(\nv\n(1) | v(2)\n)\n, where\nv\n(1) is the vector containing the (qd,c + 1)rd,c elements of\nV (1) sorted in ascending order, and v(2) is the vector contain-\ning the qd,c(c− rd,c) elements of V (2) sorted in ascending\norder. Define the erasure pattern E′ ⊆ Tn as follows:\nE′ ,\nd⋃\nj=d−z+1\nT (vj)n ,\nwhere T (i)n is as defined in Lemma 2. The erased time steps\nin E′ have been chosen to coincide with the larger blocks\nallocated to each message in the constructed code. To show\nthat E′ is an admissible erasure pattern, we introduce the\nfollowing lemma:\nLemma 3. If A ⊆ {1, . . . , d}, then∣∣∣∣∣\n( ⋃\ni∈A\nT (i)n\n)\n∩Wk\n∣∣∣∣∣ = |A| ∀ k ∈ {1, . . . , n}, (8)\nwhere T (i)n is as defined in Lemma 2.\nProof of Lemma 3: Since the code constructed in\nSection III allocates a positive amount of link bandwidth to\neach message k ∈ {1, . . . , n} at every time step in its coding\nwindow Wk, it follows from Property 2 of Lemma 2 that for\neach i ∈ {1, . . . , d}, we have∣∣T (i)n ∩Wk∣∣ = 1 ∀ k ∈ {1, . . . , n}.\nEquation (8) therefore follows from the fact that T (1)n , . . . , T (d)n\nare disjoint sets.\n8Applying Lemma 3 with A = {vj}dj=d−z+1 produces∣∣E′ ∩Wk∣∣ = z ∀ k ∈ {1, . . . , n},\nand thus E′ is an admissible erasure pattern, i.e., E′ ∈ ECWn .\nNow, consider a code that achieves the maximum message\nsize sCWn . Such a code must allow all n messages {1, . . . , n} to\nbe decoded under the specific erasure pattern E′. We therefore\nhave the following cut-set bound for sCWn :\nn sCWn ≤\n∣∣Tn\\E′∣∣ ⇐⇒ sCWn ≤ 1n ∣∣Tn\\E′∣∣ = 1n\nd−z∑\nj=1\n∣∣T (vj)n ∣∣.\nApplying the upper bounds in Property 3 of Lemma 2, and\nwriting the resulting expression in terms of yj produces\nsCWn ≤\n1\nn\nd−z∑\nj=1\n∣∣T (vj)n ∣∣ ≤ 1n\nd−z∑\nj=1\n(n yj + 2).\nSince a message size of\n∑d−z\nj=1 yj is known to be achievable\n(by the constructed code), we have the following upper and\nlower bounds for sCWn :\nd−z∑\nj=1\nyj ≤ s\nCW\nn ≤\n1\nn\nd−z∑\nj=1\n(n yj + 2).\nThese turn out to be matching bounds in the limit as n→∞:\nd−z∑\nj=1\nyj ≤ lim\nn→∞\nsCWn ≤ lim\nn→∞\n1\nn\nd−z∑\nj=1\n(n yj + 2) =\nd−z∑\nj=1\nyj .\nWe therefore have (7) as required.\nProof of Theorem 2: Consider the code constructed in\nSection III for a given choice of (c, d). According to Lemma 1,\nif message size s satisfies the inequality\ns ≤\nd−z∑\nj=1\nyj,\nthen each message k ∈ {1, . . . , n} can be decoded from the\ndata at any d− z time steps in its coding window Wk.\nTherefore, the code achieves a message size of\n∑d−z\nj=1 yj ,\nby allowing all n messages {1, . . . , n} to be decoded by\ntheir respective deadlines as long as there are z or fewer\nerasures in each coding window Wk, which is indeed the case\nwhen there are z or fewer erasures in each sliding window\nLt, or equivalently, under any erasure pattern E ∈ ESWn . To\ndemonstrate the asymptotic optimality of the code, we will\nshow that this message size matches the maximum achievable\nmessage size sSWn in the limit, i.e.,\nlim\nn→∞\nsSWn =\nd−z∑\nj=1\nyj . (9)\nWe consider two cases separately, depending on whether d is\na multiple of c:\nCase 1: Suppose that d is a multiple of c. In this case, the\nmessage size achieved by the constructed code simplifies to\nd−z∑\nj=1\nyj =\nd− z\nqd,c + 1\n=\nd− z\nd\nc.\nTo obtain an upper bound for sSWn , we consider the cut-\nset bound corresponding to a specific periodic erasure pattern\nE′ ⊆ Tn given by\nE′ ,\n{\nj d+ i ∈ Tn : j ∈ Z\n+\n0 , i ∈ {1, . . . , z}\n}\n.\nSince E′ comprises alternating intervals of z erased time steps\nand d− z unerased time steps, we have exactly z erasures in\neach sliding window Lt; therefore, E′ is an admissible erasure\npattern, i.e., E′ ∈ ESWn .\nNow, consider a code that achieves the maximum message\nsize sSWn . Such a code must allow all n messages {1, . . . , n} to\nbe decoded under the specific erasure pattern E′. We therefore\nhave the following cut-set bound for sSWn :\nn sSWn ≤\n∣∣Tn\\E′∣∣ = ⌊(n− 1)c+ d\nd\n⌋\n(d− z) +max(r′ − z, 0),\nwhere\nr′ , (n− 1)c+ d−\n⌊\n(n− 1)c+ d\nd\n⌋\nd.\nFurther simplification produces\nsSWn ≤\n1\nn\n(n− 1)c+ 2d\nd\n(d− z) =\nd− z\nd\n(\nc+\n2d− c\nn\n)\n.\nSince a message size of d−z\nd\nc is known to be achievable (by\nthe constructed code), we have the following upper and lower\nbounds for sSWn :\nd− z\nd\nc ≤ sSWn ≤\nd− z\nd\n(\nc+\n2d− c\nn\n)\n.\nThese turn out to be matching bounds in the limit as n→∞:\nd− z\nd\nc ≤ lim\nn→∞\nsSWn ≤ lim\nn→∞\nd− z\nd\n(\nc+\n2d− c\nn\n)\n=\nd− z\nd\nc.\nWe therefore have (9) as required.\nCase 2: Suppose that d is not a multiple of c. Consider a\nspecific base erasure pattern E′ ⊆ Tn given by\nE′ ,\nd⋃\nj=d−z+1\nT (vj)n ,\nwhere T (i)n is as defined in Lemma 2, and v = (v1, . . . , vd) is\nas defined in the proof of Theorem 1. The erased time steps\nin E′ have been chosen to coincide with the larger blocks\nallocated to each message in the constructed code. From E′,\nwe derive the erasure patterns E′1, . . . , E′n given by\nE′k , E\n′ ∩Wk =\nd⋃\nj=d−z+1\n(\nT (vj)n ∩Wk\n)\n.\nApplying Lemma 3 with A = {vj}dj=d−z+1 produces∣∣E′k∣∣ = ∣∣E′ ∩Wk∣∣ = z ∀ k ∈ {1, . . . , n},\n9which implies that\n|E′k ∩ Lt| ≤ z ∀ t ∈ {1, . . . , (n− 1)c+ 1}\nfor each k ∈ {1, . . . , n}. Thus, E′k is an admissible erasure\npattern, i.e., E′k ∈ ESWn , for each k ∈ {1, . . . , n}.\nTo obtain an upper bound for sSWn , we introduce the follow-\ning lemma:\nLemma 4. Suppose that a code achieves a message size of\ns under a given set of erasure patterns E for a given choice\nof (n, c, d). Let Xt be a random variable representing the\ncoded data transmitted at time step t ∈ Tn, let Mk be a\nrandom variable representing message k ∈ {1, . . . , n}, and\ndefine X [A] , (Xt)t∈A. If E ⊆ Tn is such that E ∩Wk is\nan admissible erasure pattern, i.e., (E ∩Wk) ∈ E , for each\nk ∈ {1, . . . , n}, then for each k ∈ {1, . . . , n},\nH\n(\nX [Wk\\E]\n∣∣∣Mk1 , X(k−1)c1 ) ≤ ∣∣Tk\\E∣∣− k s. (10)\nProof of Lemma 4: We will prove by induction that\ninequality (10) holds for any k ∈ {1, . . . , n}.\n(Base case) Consider the case of k = 1. From the definition\nof mutual information, we have\nI\n(\nX [W1\\E] ; M1\n)\n= H\n(\nX [W1\\E]\n)\n−H\n(\nX [W1\\E]\n∣∣M1)\n= H\n(\nM1\n)\n−H\n(\nM1\n∣∣X [W1\\E]).\nRearranging terms produces\nH\n(\nX [W1\\E]\n∣∣M1) = H(X [W1\\E])−H(M1)\n+H\n(\nM1\n∣∣X [W1\\E]). (11)\nSince T1 = W1 and H(Xt) ≤ 1 for any t because of the unit\nlink bandwidth, we have\nH\n(\nX [W1\\E]\n)\n= H\n(\nX [T1\\E]\n)\n≤\n∣∣T1\\E∣∣. (12)\nFurthermore, since E ∩W1 is an admissible erasure pattern,\nmessage 1 must be decodable from the coded data at time\nsteps T1\\(E ∩W1) = W1\\(E ∩W1) = W1\\E, and so\nH\n(\nM1\n∣∣X [W1\\E]) = 0. (13)\nSubstituting (12), (13), and H(M1) = s into (11) yields\nH\n(\nX [W1\\E]\n∣∣M1) ≤ ∣∣T1\\E∣∣− s,\nas required.\n(Inductive step) Suppose that\nH\n(\nX [Wk\\E]\n∣∣∣Mk1 , X(k−1)c1 ) ≤ ∣∣Tk\\E∣∣− k s (14)\nfor some k ∈ {1, . . . , n− 1}. From the definition of condi-\ntional mutual information, we have\nI\n(\nX [Wk+1\\E] ; Mk+1\n∣∣∣Mk1 , Xkc1 )\n= H\n(\nX [Wk+1\\E]\n∣∣∣Mk1 , Xkc1 )\n−H\n(\nX [Wk+1\\E]\n∣∣∣Mk+11 , Xkc1 )\n= H\n(\nMk+1\n∣∣∣Mk1 , Xkc1 )\n−H\n(\nMk+1\n∣∣∣Mk1 , X [{1, . . . , kc} ∪ (Wk+1\\E)]).\nRearranging terms produces\nH\n(\nX [Wk+1\\E]\n∣∣∣Mk+11 , Xkc1 )\n= H\n(\nX [Wk+1\\E]\n∣∣∣Mk1 , Xkc1 )\n−H\n(\nMk+1\n∣∣∣Mk1 , Xkc1 )\n+H\n(\nMk+1\n∣∣∣Mk1 , X [{1, . . . , kc} ∪ (Wk+1\\E)]).\n(15)\nSince messages are independent and message k + 1 is created\nat time step kc+ 1, we have\nH\n(\nMk+1\n∣∣∣Mk1 , Xkc1 ) = H(Mk+1) = s. (16)\nFurthermore, since E ∩Wk+1 is an admissible erasure pattern,\nmessage k + 1 must be decodable from the coded data at\ntime steps Tk+1\\(E ∩Wk+1) = (Tk+1\\Wk+1) ∪ (Wk+1\\E)\n= {1, . . . , kc} ∪ (Wk+1\\E), and so\nH\n(\nMk+1\n∣∣∣Mk1 , X [{1, . . . , kc} ∪ (Wk+1\\E)]) = 0. (17)\nSubstituting (16) and (17) into (15) yields\nH\n(\nX [Wk+1\\E]\n∣∣∣Mk+11 , Xkc1 )\n= H\n(\nX [Wk+1\\E]\n∣∣∣Mk1 , Xkc1 )− s\n(a)\n≤ H\n(\nX\n[\n(Wk\\E) ∪ (Wk+1\\E)\n] ∣∣∣Mk1 , Xkc1 )− s\n(b)\n≤ H\n(\nX\n[\n(Wk\\E) ∪ (Wk+1\\E)\n] ∣∣∣Mk1 , X(k−1)c1 )− s\n(c)\n≤ H\n(\nX [Wk\\E]\n∣∣∣Mk1 , X(k−1)c1 )\n+H\n(\nX\n[\n(Wk+1\\E)\n∖\n(Wk\\E)\n] )\n− s\n(d)\n≤\n∣∣Tk\\E∣∣− k s+ ∣∣∣(Wk+1\\E)∖(Wk\\E)∣∣∣− s\n(e)\n=\n∣∣Tk+1\\E∣∣− (k + 1)s,\nas required, where\n(a) follows from the addition of random variables X [Wk\\E]\nin the entropy term;\n(b) follows from the removal of conditioned random variables\nXkc(k−1)c+1 in the entropy term;\n(c) follows from the chain rule for joint entropy, and the\nremoval of conditioned random variables X [Wk\\E], Mk1 ,\nand X(k−1)c1 in the second entropy term;\n(d) follows from the inductive hypothesis (14), and the fact\nthat H(Xt) ≤ 1 for any t because of the unit link\nbandwidth;\n(e) follows from the fact that∣∣Tk\\E∣∣+ ∣∣(Wk+1\\E)∖(Wk\\E)∣∣\n=\n∣∣Tk\\E∣∣+ ∣∣(Wk+1\\Wk)∖E∣∣\n10\n=\n∣∣Tk\\E∣∣+ ∣∣(Tk+1\\Tk)∖E∣∣\n=\n∣∣Tk+1\\E∣∣.\nApplying Lemma 4 with E = ESWn and E = E′ to an optimal\ncode that achieves a message size of sSWn produces\nH\n(\nX [Wk\\E\n′]\n∣∣∣Mk1 , X(k−1)c1 ) ≤ ∣∣Tk\\E′∣∣− k sSWn\nfor any k ∈ {1, . . . , n}. Since the conditional entropy term is\nnonnegative, it follows that for the choice of k = n, we have\n∣∣Tn\\E′∣∣−n sSWn ≥ 0 ⇐⇒ sSWn ≤ 1n ∣∣Tn\\E′∣∣ = 1n\nd−z∑\nj=1\n∣∣T (vj)n ∣∣.\nApplying the upper bounds in Property 3 of Lemma 2, and\nwriting the resulting expression in terms of yj produces\nsSWn ≤\n1\nn\nd−z∑\nj=1\n∣∣T (vj)n ∣∣ ≤ 1n\nd−z∑\nj=1\n(n yj + 2).\nSince a message size of\n∑d−z\nj=1 yj is known to be achievable\n(by the constructed code), we have the following upper and\nlower bounds for sSWn :\nd−z∑\nj=1\nyj ≤ s\nSW\nn ≤\n1\nn\nd−z∑\nj=1\n(n yj + 2).\nThese turn out to be matching bounds in the limit as n→∞:\nd−z∑\nj=1\nyj ≤ lim\nn→∞\nsSWn ≤ lim\nn→∞\n1\nn\nd−z∑\nj=1\n(n yj + 2) =\nd−z∑\nj=1\nyj.\nWe therefore have (9) as required.\nProof of Theorem 3: Observe that under each erasure\npattern E ∈ EBn, the coding window Wk for each message\nk ∈ {1, . . . , n} contains at most z erasures: if Wk intersects\nwith zero erasure bursts, then it contains zero erasures; if Wk\nintersects with exactly one erasure burst, then it contains at\nmost z erasures, i.e., the maximum length of a burst; if Wk\nintersects with two or more erasure bursts, then it contains a\ngap of at least d− z unerased time steps between consecutive\nbursts, and therefore contains at most z erasures.\nConsider the code constructed in Section III for a given\nchoice of (c, d). According to Lemma 1, if message size s\nsatisfies the inequality\ns ≤\nd−z∑\nj=1\nyj,\nthen each message k ∈ {1, . . . , n} can be decoded from the\ndata at any d− z time steps in its coding window Wk.\nTherefore, the code achieves a message size of\n∑d−z\nj=1 yj , by\nallowing all n messages {1, . . . , n} to be decoded by their\nrespective deadlines as long as there are z or fewer erasures\nin each coding window Wk, which is indeed the case under\nany erasure pattern E ∈ EBn. To demonstrate the asymptotic\noptimality of the code, we will show that this message size\nmatches the maximum achievable message size sBn in the limit,\ni.e.,\nlim\nn→∞\nsBn =\nd−z∑\nj=1\nyj, (18)\nfor the following three cases:\nCase 1: Suppose that d is a multiple of c. In this case, the\nmessage size achieved by the constructed code simplifies to\nd−z∑\nj=1\nyj =\nd− z\nqd,c + 1\n=\nd− z\nd\nc.\nTo obtain an upper bound for sBn, we consider the cut-set bound\ncorresponding to a specific periodic erasure pattern E′ ⊆ Tn\ngiven by\nE′ ,\n{\nj d+ i ∈ Tn : j ∈ Z\n+\n0 , i ∈ {1, . . . , z}\n}\n.\nSince E′ comprises alternating intervals of z erased time steps\nand d− z unerased time steps, it is an admissible erasure\npattern, i.e., E′ ∈ EBn.\nThe rest of the proof leading to the obtainment of (18) is\nthe same as that of Case 1 in the proof of Theorem 2, with\nsSWn replaced by sBn.\nCase 2: Suppose that d is not a multiple of c, and\nz ≤ c− rd,c. In this case, the message size achieved by the\nconstructed code simplifies to\nd−z∑\nj=1\nyj = c−\nd∑\nj=d−z+1\nyj = c−\nz\nqd,c\n.\nConsider a specific base erasure pattern E′ ⊆ Tn given by\nE′ ,\nd⋃\nj=d−z+1\nT (vj)n ,\nwhere T (i)n is as defined in Lemma 2, and v = (v1, . . . , vd) is\nas defined in the proof of Theorem 1. The erased time steps\nin E′ have been chosen to coincide with the larger blocks\nallocated to each message in the constructed code. In this case,\nE′ simplifies to\nE′ =\nc⋃\nri,c=c−z+1\nT\n((qd,c−1)c+ri,c)\nn\n=\n{(\n(j + 1)qd,c − 1\n)\nc+ ri,c ∈ Tn :\nj ∈ Z+0 , ri,c ∈ {c− z + 1, . . . , c}\n}\n,\nwhich follows from the definition of T (i)n and the fact that\nri,c > rd,c when ri,c ∈ {c− z + 1, . . . , c}. Observe that E′\ncomprises alternating intervals of z erased time steps and\nqd,c c− z unerased time steps, with each interval of erased\ntime steps corresponding to a specific choice of j ∈ Z+0 . Since\neach erased time step t ∈ E′ can be expressed as\nt =\n(\n(j + 1)qd,c − 1\n)︸ ︷︷ ︸\nqt,c\nc+ ri,c︸︷︷︸\nrt,c\n,\n11\nit follows from Section III that the set of active messages At\nat time step t is given by\nAt =\n{\n(j + 1)qd,c︸ ︷︷ ︸\nqt,c+1\n−(qd,c − 1), . . . , (j + 1)qd,c︸ ︷︷ ︸\nqt,c+1\n}\n.\nTherefore, the set of active messages At is the same at\nevery time step t in a given interval of z erased time steps\n(corresponding to a specific j).\nFrom E′, we derive the erasure patterns E′1, . . . , E′n given\nby\nE′k , E\n′ ∩Wk =\nd⋃\nj=d−z+1\n(\nT (vj)n ∩Wk\n)\n.\nApplying Lemma 3 with A = {vj}dj=d−z+1 produces∣∣E′k∣∣ = ∣∣E′ ∩Wk∣∣ = z ∀ k ∈ {1, . . . , n}.\nLet t′ ∈ E′k be one of the z erased time steps in Wk under\nerasure pattern E′k. As previously established, t′ belongs to an\ninterval of z erased time steps in E′ that have the same set\nof active messages At′ (which contains message k). It follows\nthat this interval of z erased time steps is also in E′k, and must\ntherefore constitute E′k itself. Thus, E′k is an admissible era-\nsure pattern, i.e., E′k ∈ EBn, for each k ∈ {1, . . . , n}, because\nit comprises a single erasure burst of z time steps.\nApplying Lemma 4 with E = EBn and E = E′ to an optimal\ncode that achieves a message size of sBn produces\nH\n(\nX [Wk\\E\n′]\n∣∣∣Mk1 , X(k−1)c1 ) ≤ ∣∣Tk\\E′∣∣− k sBn\nfor any k ∈ {1, . . . , n}. Since the conditional entropy term is\nnonnegative, it follows that for the choice of k = n, we have\n∣∣Tn\\E′∣∣ − n sBn ≥ 0 ⇐⇒ sBn ≤ 1n ∣∣Tn\\E′∣∣ = 1n\nd−z∑\nj=1\n∣∣T (vj)n ∣∣.\nThe rest of the proof leading to the obtainment of (18) is the\nsame as that of Case 2 in the proof of Theorem 2, with sSWn\nreplaced by sBn.\nCase 3: Suppose that d is not a multiple of c, and\nz ≥ d− rd,c = qd,c c. In this case, the message size achieved\nby the constructed code simplifies to\nd−z∑\nj=1\nyj =\nd− z\nqd,c + 1\n.\nConsider a specific base erasure pattern E′ ⊆ Tn given by\nE′ ,\nd⋃\nj=d−z+1\nT (vj)n ,\nwhere T (i)n is as defined in Lemma 2, and v = (v1, . . . , vd) is\nas defined in the proof of Theorem 1. The erased time steps\nin E′ have been chosen to coincide with the larger blocks\nallocated to each message in the constructed code. In this case,\nE′ simplifies to\nE′ = Tn\n∖( d−z⋃\nri,c=1\nT (ri,c)n\n)\n= Tn\n∖{(\nj(qd,c + 1)\n)\nc+ ri,c ∈ Tn :\nj ∈ Z+0 , ri,c ∈ {1, . . . , d− z}\n}\n,\nwhich follows from the definition of T (i)n and the fact that\nri,c ≤ rd,c when ri,c ∈ {1, . . . , d− z}. Observe that E′ com-\nprises alternating intervals of d− z unerased time steps and\n(qd,c + 1) c− (d− z) = c− rd,c + z erased time steps, with\neach interval of unerased time steps corresponding to a specific\nchoice of j ∈ Z+0 . Since each unerased time step t ∈ Tn\\E′\ncan be expressed as\nt =\n(\nj(qd,c + 1)\n)︸ ︷︷ ︸\nqt,c\nc+ ri,c︸︷︷︸\nrt,c\n,\nit follows from Section III that the set of active messages At\nat time step t is given by\nAt =\n{\nj(qd,c + 1)︸ ︷︷ ︸\nqt,c\n+1− qd,c, . . . , j(qd,c + 1)︸ ︷︷ ︸\nqt,c\n+1\n}\n.\nTherefore, the set of active messages At is the same at every\ntime step t in a given interval of d− z unerased time steps\n(corresponding to a specific j).\nFrom E′, we derive the erasure patterns E′1, . . . , E′n given\nby\nE′k , E\n′ ∩Wk =\nd⋃\nj=d−z+1\n(\nT (vj)n ∩Wk\n)\n.\nApplying Lemma 3 with A = {vj}dj=d−z+1 produces∣∣E′k∣∣ = ∣∣E′ ∩Wk∣∣ = z ∀ k ∈ {1, . . . , n}.\nLet t′ ∈ Wk\\E′k be one of the d− z unerased time steps in\nWk under erasure pattern E′k. As previously established, t′\nbelongs to an interval of d− z unerased time steps in Tn\\E′\nthat have the same set of active messages At′ (which contains\nmessage k). It follows that this interval of d− z unerased time\nsteps is also in Wk\\E′k, and must therefore constitute Wk\\E′k\nitself. Thus, E′k is an admissible erasure pattern, i.e., E′k ∈ EBn,\nfor each k ∈ {1, . . . , n}, because it comprises either a single\nerasure burst of z time steps, or two erasure bursts with a\ncombined length of z time steps separated by a gap of d− z\nunerased time steps.\nApplying Lemma 4 with E = EBn and E = E′ to an optimal\ncode that achieves a message size of sBn produces\nH\n(\nX [Wk\\E\n′]\n∣∣∣Mk1 , X(k−1)c1 ) ≤ ∣∣Tk\\E′∣∣− k sBn\nfor any k ∈ {1, . . . , n}. Since the conditional entropy term is\nnonnegative, it follows that for the choice of k = n, we have\n∣∣Tn\\E′∣∣− n sBn ≥ 0 ⇐⇒ sBn ≤ 1n ∣∣Tn\\E′∣∣ = 1n\nd−z∑\nj=1\n∣∣T (vj)n ∣∣.\nThe rest of the proof leading to the obtainment of (18) is the\nsame as that of Case 2 in the proof of Theorem 2, with sSWn\nreplaced by sBn.\n12\nREFERENCES\n[1] D. Leong and T. Ho, “Erasure coding for real-time streaming,” in Proc.\nIEEE Int. Symp. Inf. Theory (ISIT), Cambridge, Massachusetts, USA, Jul.\n2012.\n[2] E. Martinian and C.-E. W. Sundberg, “Low delay burst erasure correction\ncodes,” in Proc. IEEE Int. Conf. Commun. (ICC), May 2002, pp. 1736–\n1740.\n[3] E. Martinian and M. Trott, “Delay-optimal burst erasure code construc-\ntion,” in Proc. IEEE Int. Symp. Inf. Theory (ISIT), Jun. 2007, pp. 1006–\n1010.\n[4] L. J. Schulman, “Coding for interactive communication,” IEEE Trans. Inf.\nTheory, vol. 42, no. 6, pp. 1745–1756, Nov. 1996.\n[5] A. Sahai, “Anytime information theory,” Ph.D. dissertation, Massachusetts\nInstitute of Technology, 2001.\n[6] R. T. Sukhavasi, “Distributed control and computing: Optimal estimation,\nerror correcting codes, and interactive protocols,” Ph.D. dissertation,\nCalifornia Institute of Technology, 2012.\n[7] ¨O. F. Tekin, S. Vyetrenko, T. Ho, and H. Yao, “Erasure correction\nfor nested receivers,” in Proc. Annu. Allerton Conf. Commun., Control,\nComput. (Allerton), Sep. 2011, pp. 1454–1461.\n",
            "id": 4085891,
            "identifiers": [
                {
                    "identifier": "10.1109/isit.2012.6284055",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:authors.library.caltech.edu:33625",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "103820924",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10210752",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "216163089",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:arxiv.org:1207.3582",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "2031335939",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "oai:citeseerx.psu:10.1.1.261.208",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "22193262",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "8822579",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:citeseerx.psu:10.1.1.723.6232",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "1207.3582",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "192388384",
                    "type": "CORE_ID"
                }
            ],
            "title": "Erasure Coding for Real-Time Streaming",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2031335939",
            "oaiIds": [
                "oai:citeseerx.psu:10.1.1.723.6232",
                "oai:arxiv.org:1207.3582",
                "oai:authors.library.caltech.edu:33625",
                "oai:citeseerx.psu:10.1.1.261.208"
            ],
            "publishedDate": "2012-01-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1207.3582",
                "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.261.208",
                "http://www1.i2r.a-star.edu.sg/%7Edleong/pub/2012-ISIT-Streaming-LH-Long.pdf"
            ],
            "updatedDate": "2021-07-22T05:46:40",
            "yearPublished": 2012,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1207.3582"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/4085891"
                }
            ]
        },
        {
            "acceptedDate": "2011-07-13T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Mendizábal Vicente, Ainhoa"
                },
                {
                    "name": "Salgado Álvarez de Sotomayor, Luis"
                }
            ],
            "contributors": [],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/12000781",
                "https://api.core.ac.uk/v3/outputs/148662089"
            ],
            "createdDate": "2013-07-17T17:16:12",
            "dataProviders": [
                {
                    "id": 2824,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/2824",
                    "logo": "https://api.core.ac.uk/data-providers/2824/logo"
                },
                {
                    "id": 431,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/431",
                    "logo": "https://api.core.ac.uk/data-providers/431/logo"
                }
            ],
            "depositedDate": "2011-05-01T00:00:00",
            "abstract": "In the field of detection and monitoring of dynamic objects in quasi-static scenes, background subtraction techniques where background is modeled at pixel-level, although showing very significant limitations, are extensively used. In this work we propose a novel approach to background modeling that operates at region-level in a wavelet based multi-resolution framework. Based on a segmentation of the background, characterization is made for each region independently as a mixture of K Gaussian modes, considering the model of the approximation and detail coefficients at the different wavelet decomposition levels. Background region characterization is updated along time, and the detection of elements of interest is carried out computing the distance between background region models and those of each incoming image in the sequence. The inclusion of the context in the modeling scheme through each region characterization makes the model robust, being able to support not only gradual illumination and long-term changes, but also sudden illumination changes and the presence of strong shadows in the scen",
            "documentType": "research",
            "doi": "10.1109/icassp.2011.5946557",
            "downloadUrl": "https://core.ac.uk/download/148662089.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "A REGION BASED APPROACH TO BACKGROUND MODELING IN A WAVELET  \nMULTI-RESOLUTION FRAMEWORK \nAinhoa Mendizabal, Luis Salgado \nGrupo de Tratamiento de Imágenes, Universidad Politécnica de Madrid \nABSTRACT \nIn the field of detection and monitoring of dynamic objects \nin quasi-static scenes, background subtraction techniques \nwhere background is modeled at pixel-level, although \nshowing very significant limitations, are extensively used. \nIn this work we propose a novel approach to background \nmodeling that operates at region-level in a wavelet based \nmulti-resolution framework. Based on a segmentation of the \nbackground, characterization is made for each region \nindependently as a mixture of K Gaussian modes, \nconsidering the model of the approximation and detail \ncoefficients at the different wavelet decomposition levels. \nBackground region characterization is updated along time, \nand the detection of elements of interest is carried out \ncomputing the distance between background region models \nand those of each incoming image in the sequence. The \ninclusion of the context in the modeling scheme through \neach region characterization makes the model robust, being \nable to support not only gradual illumination and long-term \nchanges, but also sudden illumination changes and the \npresence of strong shadows in the scene.  \nIndex Terms— Region analysis, wavelet transform, \nBackground modeling, Mixture of Gaussians.\n1. INTRODUCTION \nMany applications such as traffic monitoring or surveillance \nsystems are based on the unsupervised analysis of quasi \nstatic video sequences. This analysis is focused mainly in \nthe detection and monitoring of elements of interest \nappearing in the scene. In this context, background \nsubtraction techniques are extensively used: a description \nand characterization of the scene in the absence of elements \nof interest is carried out (background modeling), and objects \nare detected through the analysis of the scene deviations \nwith respect to this model. As the scene background evolves \nalong time, due to illumination changes or new static objects \nincorporated or subtracted from the scene, models should be \nadaptive. \nMost background modeling techniques proposed in the \nliterature operate at pixel level: The color of each \nbackground pixel is modeled independently. A first \napproach was proposed in [1], where the intensity value of \neach background pixel is modeled by a Gaussian \ndistribution. But this model does not support the presence of \nmoving objects in cluttered areas like swaying trees. To \novercome this drawback, Stauffer et al. [2], model each \npixel value as a mixture of Gaussians, supporting then \nbimodal background pixels. Non Gaussian probability \ndensity function was considered in [3]:  Kernel Density \nEstimation techniques are incorporated to the \ncharacterization. Although these strategies base the decision \nwhether one pixel belongs to the background or not on the \nprobability of its color value, other non-parametric methods \npredict the value of each background pixel based on its \nrecent history values. If a new value differs enough from its \nprediction, it is considered as a foreground pixel. In [4] the \nprediction of a pixel value is obtained as a weighted sum of \nits p previous values; [5] proposed a cluster scheme where \neach cluster has its representative value and a weighting \nfactor that represents its similarity to the background; and in \n[6] a codebook is constructed for each background pixel that \nregisters each pixel value appearance frequency.  \nAlthough all these pixel-based models support gradual and \nlong-term illumination changes, they underperform in the \npresence of sudden ones. To reduce their impact, some \nauthors incorporate the idea of context in the \ncharacterization, introducing the neighborhood in the \nanalysis. For example, in [7] it is used not only the intensity \nvalue of the pixel but also its gradient magnitude. Sudden \nillumination changes alter meaningfully the intensity but not \nthe gradient information, fact that makes the \ncharacterization stronger. In general, border information is \nless sensitive to sudden illumination changes. In this sense, \n[8] and [9] propose using this information in the \ncharacterization, and detect the changes in a \"map of \nborders\". In [10], based on Pattern Recognition techniques, \na vector of characteristics using Binary Local Patterns is \nobtained for each pixel; and in [11] the changes on NxN\npixel blocks are analyzed, based on the idea that \nneighborhood pixels change similarly throughout time.  \nNevertheless, the pixel oriented model of all these methods \nbound the usefulness of context information. Attempts to \ntackle sudden illumination changes obtain limited results, \nand no solutions are given to the presence of shadows. As a \nresult, accuracy in the detection suffers from model \ninstabilities, thus requiring ad-hoc post-processing \nstrategies. \n929978-1-4577-0539-7/11/$26.00 ©2011 IEEE ICASSP 2011\nTo overcome the abovementioned limitations, in this paper \nwe present a novel approach to background modeling that \noperates at region-level in a wavelet based multi-resolution \nframework. Based on a segmentation of the background, \ncharacterization is made for each region independently as a \nmixture of K Gaussian modes, considering not only the \nmodel of the approximation coefficients but also the model \nof the detail coefficients at the different decomposition \nlevels. Background region characterization is updated along \ntime, and the detection of elements of interest is carried out \ncomputing the distance between background region models \nand those of each incoming image in the sequence. The \ninclusion of the context in the modeling scheme through \neach region characterization makes the model robust, being \nable to support not only gradual illumination and long-term \nchanges, but also sudden illumination changes and the \npresence of strong shadows in the scene.  \n2. STRATEGY OVERVIEW\nA block diagram of the proposed strategy is shown in Fig. 1. \nFigure 1: Block diagram of the proposed strategy \n2.1. Wavelet Decomposition \nThe proposed strategy works at region level, so a partition \nof the background is assumed to have been previously \ncomputed based on any color or texture similarity criterion. \nThis partition provides the topology of the regions that form \nthe background, and its modeling and update is carried out \nthrough the analysis of the incoming images.\nFor each incoming image, a wavelet transformation is \napplied. This decomposition produces a set of coefficients \nwhich provide information at different resolution levels. \n\"Approximation\" coefficients are related with a low pass \nfiltered version of the original image meanwhile \"detail\" \ncoefficients are related with the high frequencies present in \nthe scene. Detail coefficients also provide information about \nthe orientation of the details in three directions: vertical (v), \nhorizontal (h) and diagonal (d).  \n2.2. Region characterization \nFor each region of the background partition, a set of \napproximation and detail coefficients are associated – those \nlying within the region boundaries.  Parametric models are \napplied to these coefficients thus resulting in a set of \nparameters at different resolution levels. This vector of \nparameters characterizes an instantiation of the region at a \nspecific point in time (see Section 3). Whenever a new \nincoming image is analyzed, a new instantiation of the \nregion is characterized. \n2.3. Background modeling and update \nIn this strategy, a background region is modeled as a \nmixture of K Gaussian modes (MoG) in which one of them \nrepresents the region when it is considered to be part of the \nbackground. In our approach, each mode of the mixture \nmodels the deviations of the different instantiations from the \nprediction of the region throughout the time. Therefore, the \nstandard deviation of each Gaussian mode represents the \nstability of its associated region. Besides, each mode has an \nassociated factor that weights its relevance in the recent \nhistory of the region model. As a consequence, the mode \nholding the lowest value of the standard deviation and the \nhigher weighting factor is likely to be the background \nrepresentation of the region. \nAs it was mentioned in the introduction, the background \nmodel has to be updated in order to support long term and \nillumination changes. Based on the strategy proposed by \nStauffer at al. [2], for each instantiation of the region, the \nparameters associated to the closest mode in the MoG are \nupdated (parametric model of the estimated region and its \nstandard deviation), and the weighting factors of all the \nmodes are modified (see Section 4).  \n3. REGION CHARACTERIZATION \nParametric models are applied to the approximation and \ndetail coefficients. The distribution of the approximation \ncoefficients in the lowest resolution level for each region is \nmodeled as a Gaussian distribution:  \n݂ሺݔሻ ൌ ͳߪξʹߨ ݁\nିଵ ଶൗ ቀ\n௫ିఓ\nఙ ቁ\nమ\n(1) \nIn this case, the parameters involved in the characterization \nprocess are the mean and the standard deviation values of \nthe distribution, {ȝc,ıc}c=[R,G,B], where R, G, B represents the \ndifferent color bands.  \nBased on the work by Mallat [12], for the detail coefficients \nand at each resolution level a generalized Gaussian \ndistribution is assumed:  \n݂ሺݔሻ ൌ \u0003ܭ݁ିቀȁ௫ȁ ఈൗ ቁ\nഁ\n(2) \nIn this case, the parameters involved in the characterization \nprocess are {(Įhl, ȕhl), (Įvl, ȕvl), (Įdl, ȕdl)}i=[1,2…n], where l\nrepresents the resolution level and h, v and d, the horizontal, \nvertical and diagonal detail coefficients respectively. \nSome characterization results for a textured region are \nshown in Fig. 2. As it can be observed, the parameterized \ndistribution for approximation and detail coefficients \nfollows the real distribution tendency. \nIn order to measure the similarity between two regions, the \nKullback-Leibler distance is proposed [13]: for each band \nand at each resolution level, the similarity between the two \ndistributions involved is computed.\n930\nFigure 2: (a) Analyzed Region (b) In blue, RGB approximation \ncoefficients distribution, in red, characterized distribution. (c) In \nblue, vertical detail coefficients distribution (level 1, 2 y 3), in red, \ncharacterized distribution. \n4. BACKGROUND MODEL UPDATE \nIn this strategy, a background region is modeled as mixture \nof K gaussian modes. Each mode models the region \ndeviations from the region estimation throughout the time. \nܲሺܴሺ݅Ǣ ݐሻሻ ൌ ෍߱௞ሺ݅Ǣ ݐሻ כ ߟ൫݀௕௞ሺ݅Ǣ ݐሻǡ Ͳǡ ȭ௞ǡ௧൯\n௄\n௞ୀଵ\n(3) \nwhere R(i;t) represents the i-th region of the image in the \ninstant t, Ȧk(i;t) represents the weight of the k-mode,  Ȉk,t is \nthe covariance matrix of the mixture,  dbk(i;t) is the distance \nof the instantiation of the region R(i;t) in the instant t with \nrespect to the estimation of the k-mode Чbk(i;t) and Ș is the \ngaussian probability of belonging to each mode associated \nto the instantiation. \nThe distance for the approximation and detail coefficients is \ncomputed separately as an addition of the partial distances \nobtained for each set of coefficients at different scales, \ndirections and bands. This approximation does not affect the \neffectiveness of the algorithm. \n݀௔௣௣௞ ሺ݅Ǣ ݐሻ ൌ ෍ ݀௖௞ሺ݅Ǣ ݐሻ\n௖ୀሾோǡீǡ஻ሿ\n(4)  \n݀ௗ௘௧௞ ሺ݅Ǣ ݐሻ ൌ ෍ ሺ݀௛௟௞ ሺ݅Ǣ ݐሻ ൅\n௟ୀሾଵǡଶǡǥǡ௡ሿ\n݀௩௟௞ ሺ݅Ǣ ݐሻ ൅ ݀ௗ௟௞ ሺ݅Ǣ ݐሻሻ\nWe expect the detail coefficients to be less sensitive to \nsudden illumination changes so, approximation and detail \ncoefficients are combined through a weighting constant ߛ\nas: \n݀௕௞ሺ݅Ǣ ݐሻ ൌ \u0003ߛ݀௔௣௣௞ ሺ݅Ǣ ݐሻ ൅ ሺͳ െ ߛሻ݀ௗ௘௧௞ ሺ݅Ǣ ݐሻ (5) \nWhen a new instantiation of a region arrives, it is compared \nwith the K-gaussian modes. A region matches with one of \nthe modes if the distance between the region estimation and \nits instantiation is below a threshold value: \n݀௕௞ሺ݅Ǣ ݐሻ ൏ ௛ܶߪ௕௞ሺ݅Ǣ ݐሻ (6) \nAfterwards, all weighting factors are updated as: \n߱௞ሺ݅Ǣ ݐሻ ൌ ሺͳ െ ߙሻ߱௞ሺ݅Ǣ ݐ െ ͳሻ ൅ ߙሺܯ௞ሺ݅Ǣ ݐሻሻ    (7) \nwhere Į is the learning constant;  Mk(i;t)  is 1 if the region \nmatches the k-mode and 0 otherwise. If the region matches \nthe k-mode, the standard deviation of this mode is updated:  \nቀߪ௕௞ሺ݅Ǣ ݐሻቁ\nଶ ൌ ሺͳ െ ߙሻሺߪ௕௞ሺ݅Ǣ ݐ െ ͳሻሻଶ ൅ ߙሺ݀௕௞ሺ݅Ǣ ݐሻሻଶ (8) \nand the parameters of the region estimation, Чbk(i;t) as: \nߤƸ௖௞ሺ݅Ǣ ݐሻ ൌ ሺͳ െ ߩሻߤƸ௖௞ሺ݅Ǣ ݐ െ ͳሻ ൅ ߩߤ௖ሺ݅Ǣ ݐሻ\nሺߪො௖௞ሺ݅Ǣ ݐሻሻଶ ൌ ሺͳ െ ߩሻሺߪො௖௞ሺ݅Ǣ ݐ െ ͳሻሻଶ ൅ ߩሺߪ௖ሺ݅Ǣ ݐሻሻଶ (9) \nߙො௟௞ሺ݅Ǣ ݐሻ ൌ ሺͳ െ ߩሻߙො௟௞ሺ݅Ǣ ݐ െ ͳሻ ൅ ߩߙ௟ሺ݅Ǣ ݐሻ\nߚመ௟௞ሺ݅Ǣ ݐሻ ൌ ሺͳ െ ߩሻߚመ௟௞ሺ݅Ǣ ݐ െ ͳሻ ൅ ߩߚ௟ሺ݅Ǣ ݐሻ\n\u0003\u0003ߩ is a weighting constant computed as: \nߩ ൌ ߙߟ൫݀௕௞ሺ݅Ǣ ݐሻǡ Ͳǡ ߪ௕௞ሺ݅Ǣ ݐሻ൯ (10) \nAs we operate at region-level, only one mode, B, is \nrepresentative of the background, where: \nܤ ൌ ୩ ቆ\n߱௞ሺ݅Ǣ ݐሻ\nߪ௕௞ሺ݅Ǣ ݐሻ\nቇ \u0003\u0003\u0003\u0003\u0003݇ ൌ ͳǡǥ ǡ ܭ (11) \n5. RESULTS \nTo test the response of the proposed background modeling \napproach, different indoor video surveillance sequences \nhave been used. Illumination changes have been introduced \nin the scene covering from gradual to sudden ones, new \nstatic objects are included (objects entering and becoming \npart of the background), and shadows cast either from \nobjects and from moving objects are analyzed. \nAs expected, in long-term and gradual illumination changes, \nthe proposed approach behaves adapting smoothly the \nmodel of the background regions. In case of changes in the \nscene configuration (appearance of new static objects), those \nare accurately detected, as pixel-based oriented approaches \ndo, thus forcing a new mode to appear in the region model \nthat after some time is identified as the background one \n(when its ratio B becomes the highest).  \nFigure 3: Behavior in a sudden illumination change (ߛ=0.1).  The \ndistance to the principal mode increases with the change, but no \nnew mode appears in the mixture. \nIn the presence of sudden illumination changes, detail \ncoefficients show significantly less sensitiveness than the \napproximation ones. Thus, the significance of detail \ncoefficients in the distance computation (modeled through \nthe value ߛ in (5)) can be used to tackle this situation \nefficiently. Low values of this parameter prevent the \ndistance computation to recognize an illumination change as \n(a) (b) (c) \n1 \n931\na reconfiguration in the scene. Fig. 3 shows the distance \ncomputation in case of Ȗ=0.1, and how when the \nillumination changes abruptly (time indicated as 1 in the \nFig.3), the distance changes but fast converges again to very \nlow values.  In pixel-based approaches, this situation would \nhave been always identified as new object. Obviously, an \ninadequate use of this parameter (i.e. Ȗ=0.5 in Fig. 4) would \nlead to the identification of a new mode in the MoG (in \ngreen) which would be initially identified as a new object in \nthe scene. \nFigure 4: Behavior in a sudden illumination change (ߛ=0.5). In red, \nbehavior of the initial principal mode. (1) Illumination change, \nappearance of a new mode (green). (2) The principal mode of the \nmixture changes. \nIn the presence of shadows, also the response of the detail \ninformation is more stable absorbing them in the model. Fig. \n5 shows the distance evolution for a region which is affected \nby them (Fig. 6):  although the distance to the representative \nmode increases, the model update strategy adapts to the new \nsituation, thus preventing from the creation of a new mode \nin the MoG. Therefore shadows are not detected as new \nobjects (as pixel-based approaches would do) and the \nproposed update strategy allows adapting the background \nmodel to its presence.  \nFigure 5: Distance to the principal mode in presence of shadows.  \nFigure 6: Video Sequence of a region affected by shadows. (a) \nRegion free of shadows (b),(c),(d) Region in presence of shadows. \n6. CONCLUSIONS \nA novel approach to background modeling that operates at \nregion-level in a wavelet based multi-resolution framework \nhas been presented. The joint consideration of \napproximation and detail coefficients to model the regions \nas a MoG allows handling efficiently illumination changes \n(including sudden ones) the appearance of new objects \nbecoming part of the background and shadows cast either by \nbackground objects or by moving objects in the scene. The \ninformation gathered in the proposed framework shows high \npotential not only for background modeling but also for \nintelligent analysis of the scene evolution. \nACKNOWLEDGEMENTS \nThis work has been partially supported by the Ministerio de \nCiencia e Innovación of the Spanish Government under \nprojects TEC2007-67764 (SmartVision) and TEC2010-\n20412 (Enhanced 3DTV). \n7. REFERENCES \n[1] C.R Wren, A. Azarbayejani, T. Darrel, A.P. Pentland, “Pfinder: \nreal-time tracking of the human body” IEEE Trans. on PAMI , Vol. \n19, pp. 780-785, 1997. \n[2] C. Stauffer, W.E. Grimson, “Adaptive background mixture \nmodels for real-time tracking” IEEE Conf. on Computer Vision \nand Pattern Recognition, Vol. 2, 1999. \n[3] A. Elgammal, D. Harwood, “Non-parametric model for \nbackground subtraction” Proc. of the 6th. European Conf. on \nComputer Vision, Vol. 2, pp. 751-767, 2000. \n[4] K. Toyama, J. Kurm, B. Brummit, B. Meyers, “Wallflower: \nprinciples and practice of background maintenance” Proc. of the \n7th Internat. Conf. on Computer Vision, Vol. 1, pp. 255-261, 1999. \n[5] D. Butler, S. Sridharan, V.M. Bove, “Real-Time adaptive \nbackground segmentation” Proc. of the IEEE Internat. Conf. on \nAcoustic, Speech & Signal Processing, Vol. 3, pp. 349-352, 2003. \n[6] K. Kim, T.H. Chaldabhongse, D. Harwood, L. Davis, \n“Background modeling and subtraction by codebook construction” \nIEEE Intern. Conf. on Image Proc., Vol. 5, pp. 3061-3064, 2004. \n[7] O.Javed, K.Shafique, M.Shah “A hierarchical approach to \nrobust background subtraction using color and gradient info.” \nWorkshop on Motion and Video Computing. pp. 22-27, 2002. \n[8] J Zhang, L. Zhang, Tai Heng-Hing “Efficient video object \nsegmentation using adaptive background registration and edge-\nbased change detection techniques” IEEE Internat. Conf. on \nMultimedia and Expo, Vol. 2, pp. 1467-1470, 2004. \n[9] Yee-Hong Yang, M.D. Levine, “The background primal \nsketch: An approach for tracking moving objects” Machine Vision \nand Applications, 2005. \n[10] M. Heikkila M. Pietikainen, “A texture-based method for \nmodeling the background and detecting moving objects” IEEE \nTrans. on PAMI, Vol. 28, pp. 657-662, 2006. \n[11] M. Seki, T. Wada, H. Fujiwara, K. Sumi, “Background \nsubtraction based on coocurrence of image variation” Proc. IEEE \nConf Computer Vision & Pattern Recog. Vol. 2, pp. 65-72, 2003. \n[12] G. Mallat, “A theory for multiresolution signal \ndecomposition: the wavelet representation” IEEE Trans. on PAMI, \nVol. 2, no. 7, pp. 674-693, 1989. \n[13] G.V. Wouwer, P. Scheunders, D. V. Dyck, “Statistical texture \ncharacterization from Discrete Wavelet Representations” IEEE \nTrans. on Image Processing, Vol. 8, no. 4, pp. 592-598, 1999. \n1 \n1 \n2 \n(a) (b) (c) (d) \n932\n",
            "id": 5176736,
            "identifiers": [
                {
                    "identifier": "10.1109/icassp.2011.5946557",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:oa.upm.es:13466",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "12000781",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "148662089",
                    "type": "CORE_ID"
                }
            ],
            "title": "A region based approach to background modeling in a wavelet multi-resolution framework",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:oa.upm.es:13466"
            ],
            "publishedDate": "2011-01-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 9356091,
                    "title": "A texture-based method for modeling the background and detecting moving objects”",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/tpami.2006.68",
                    "raw": "M. Heikkila M. Pietikainen, “A texture-based method for modeling the background and detecting moving objects” IEEE Trans. on PAMI, Vol. 28, pp. 657-662, 2006.",
                    "cites": null
                },
                {
                    "id": 9356093,
                    "title": "A theory for multiresolution signal decomposition: the wavelet representation”",
                    "authors": [],
                    "date": "1989",
                    "doi": "10.1109/34.192463",
                    "raw": "G. Mallat, “A theory for multiresolution signal decomposition: the wavelet representation” IEEE Trans. on PAMI, Vol. 2, no. 7, pp. 674-693, 1989.",
                    "cites": null
                },
                {
                    "id": 9356074,
                    "title": "Adaptive background mixture models for real-time tracking”",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1109/cvpr.1999.784637",
                    "raw": "C. Stauffer, W.E. Grimson, “Adaptive background mixture models for real-time tracking” IEEE Conf. on Computer Vision and Pattern Recognition, Vol. 2, 1999.",
                    "cites": null
                },
                {
                    "id": 9356082,
                    "title": "Background modeling and subtraction by codebook construction”",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1109/icip.2004.1421759",
                    "raw": "K. Kim, T.H. Chaldabhongse, D. Harwood, L. Davis, “Background modeling and subtraction by codebook construction” IEEE Intern. Conf. on Image Proc., Vol. 5, pp. 3061-3064, 2004.",
                    "cites": null
                },
                {
                    "id": 9356092,
                    "title": "Background subtraction based on coocurrence of image variation”",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1109/cvpr.2003.1211453",
                    "raw": "M. Seki, T. Wada, H. Fujiwara, K. Sumi, “Background subtraction based on coocurrence of image variation” Proc. IEEE Conf Computer Vision & Pattern Recog. Vol. 2, pp. 65-72, 2003.",
                    "cites": null
                },
                {
                    "id": 9356088,
                    "title": "M.Shah “A hierarchical approach to robust background subtraction using color and gradient info.”",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1109/motion.2002.1182209",
                    "raw": "O.Javed, K.Shafique, M.Shah “A hierarchical approach to robust background subtraction using color and gradient info.” Workshop on Motion and Video Computing. pp. 22-27, 2002.",
                    "cites": null
                },
                {
                    "id": 9356076,
                    "title": "Non-parametric model for background subtraction”",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1007/3-540-45053-x_48",
                    "raw": "A. Elgammal, D. Harwood, “Non-parametric model for background subtraction” Proc. of the 6th. European Conf. on Computer Vision, Vol. 2, pp. 751-767, 2000.",
                    "cites": null
                },
                {
                    "id": 9356072,
                    "title": "Pfinder: real-time tracking of the human body”",
                    "authors": [],
                    "date": "1997",
                    "doi": "10.1109/afgr.1996.557243",
                    "raw": "C.R Wren, A. Azarbayejani, T. Darrel, A.P. Pentland, “Pfinder: real-time tracking of the human body” IEEE Trans. on PAMI , Vol. 19, pp. 780-785, 1997.",
                    "cites": null
                },
                {
                    "id": 9356080,
                    "title": "Real-Time adaptive background segmentation”",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1109/icassp.2003.1199481",
                    "raw": "D. Butler, S. Sridharan, V.M. Bove, “Real-Time adaptive background segmentation” Proc. of the IEEE Internat. Conf. on Acoustic, Speech & Signal Processing, Vol. 3, pp. 349-352, 2003.",
                    "cites": null
                },
                {
                    "id": 9356094,
                    "title": "Statistical texture characterization from Discrete Wavelet Representations”",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1109/83.753747",
                    "raw": "G.V. Wouwer, P. Scheunders, D. V. Dyck, “Statistical texture characterization from Discrete Wavelet Representations” IEEE Trans. on Image Processing, Vol. 8, no. 4, pp. 592-598, 1999. (a) (b)  (c)  (d)",
                    "cites": null
                },
                {
                    "id": 9356089,
                    "title": "Tai Heng-Hing “Efficient video object segmentation using adaptive background registration and edgebased change detection techniques”",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1109/icme.2004.1394512",
                    "raw": "J Zhang, L. Zhang, Tai Heng-Hing “Efficient video object segmentation using adaptive background registration and edgebased change detection techniques” IEEE Internat. Conf. on Multimedia and Expo, Vol. 2, pp. 1467-1470, 2004.",
                    "cites": null
                },
                {
                    "id": 9356090,
                    "title": "The background primal sketch: An approach for tracking moving objects” Machine Vision and Applications,",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1007/bf01213527",
                    "raw": "Yee-Hong Yang, M.D. Levine, “The background primal sketch: An approach for tracking moving objects” Machine Vision and Applications, 2005.",
                    "cites": null
                },
                {
                    "id": 9356078,
                    "title": "Wallflower: principles and practice of background maintenance”",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1109/iccv.1999.791228",
                    "raw": "K. Toyama, J. Kurm, B. Brummit, B. Meyers, “Wallflower: principles and practice of background maintenance” Proc. of the th Internat. Conf. on Computer Vision, Vol. 1, pp. 255-261, 1999.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "http://oa.upm.es/13466/",
                "http://oa.upm.es/13466/1/INVE_MEM_2011_112770.pdf"
            ],
            "updatedDate": "2022-02-28T00:07:51",
            "yearPublished": 2011,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/148662089.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/148662089"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/148662089/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/148662089/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/5176736"
                }
            ]
        },
        {
            "acceptedDate": "2009-09-02T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Araceli Fernandez-Cortes"
                },
                {
                    "name": "Octavio Martinez"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/189498483",
                "https://api.core.ac.uk/v3/outputs/288782"
            ],
            "createdDate": "2012-02-17T10:56:38",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 77,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/77",
                    "logo": "https://api.core.ac.uk/data-providers/77/logo"
                }
            ],
            "depositedDate": "2009-09-02T00:00:00",
            "abstract": "When assembling a large quantity of reads in a genomic shotgun project a serious limitation is the amount of random access memory (RAM) of the computers used in the project. This arises because all assembling programs must look at all the overlaps between reads at the same time, using RAM in order to construct contigs, and the memory of the computer can be filled up during this step, causing the abortion of the assembling process.&#xd;&#xa;Here we propose an algorithm that is capable of overcoming any memory limitation by using redundancy of processing and thus producing an increase in computing time but overcoming the memory limitation.&#xd;&#xa;The proposed algorithm consists in dividing the reads in a set of groups which size is half the maximum capability in memory of the computer used and performing assembling for all the possible combination pairs of such groups. After eliminating the redundancy of the set of contigs obtained in the previous step, the process is iterated until a set of contigs of manageable size is obtained such that the set can be handled by the assembler in a final step.&#xd;&#xa;Each step of the procedure increases the time of computing from k to approximately k + k(k-1)/2, but in many practical cases only one step is needed to finish the assembling process. The procedure is suitable for any kind of assembler and was successfully applied to the assembly of a very large set of reads from the maize genome",
            "documentType": "research",
            "doi": "10.1038/npre.2009.3712.1",
            "downloadUrl": "https://core.ac.uk/download/pdf/288782.pdf",
            "fieldOfStudy": "computer science",
            "fullText": " 1 \nMUEGANO: A divide and conquer algorithm to overcome \nmemory limitations when assembling shotgun projects \n \nOctavio Martínez de la Vega1 and \nAraceli Fernández Cortés1 \nAbstract  \nWhen assembling large quantity of reads in a genomic shotgun project a \nserious limitation is the amount of random access memory (RAM) of the \ncomputers used in the project. This arises because all assembling programs \nmust look at all the overlaps between reads at the same time, using RAM in \norder to construct contigs, and the memory of the computer can be filled up \nduring this step, causing the abortion of the assembling process. \nHere we propose an algorithm that is capable of overcoming any memory \nlimitation by using redundancy of processing and thus producing an increase in \ncomputing time but overcoming the memory limitation. \nThe proposed algorithm consists in dividing the reads in a set of groups which \nsize is half the maximum capability in memory of the computer used and \nperforming assembling for all the possible combination pairs of such groups. \nAfter eliminating the redundancy of the set of contigs obtained in the previous \nstep, the process is iterated until a set of contigs of manageable size is \nobtained such that the set can be handled by the assembler in a final step. \nEach step of the procedure increases the time of computing from k to \napproximately k + k(k-1)/2, but in many practical cases only one step is needed \nto finish the assembling process. The procedure is suitable for any kind of \nassembler and was successfully applied to the assembly of a very large set of \nreads from the maize genome. \n \nResults \nThe algorithm proposed here, called MUEGANO2 is capable of overcoming any \nmemory limitation by using redundancy of processing and thus producing an \nincrease in computing time. Even with an increase in computational time, this \nprocess could be the only practical way to process a very large number of reads \nin a shotgun assembling project that surpass the memory capacity of a given \ncomputer. The algorithm is warranted to recover all contigs formed by two or \nmore reads. \nThe algorithm is carried out in steps that can be repeated until finishing the \nassembly, say: \n1 - A set of redundant groups of reads of the maximum size permitted by the \namount of RAM available are obtained and assembled producing a first order \nset of redundant contigs (contigs1). \n \n                                                 \n1 Laboratorio Nacional de Genómica para la Biodiversidad (Langebio), CINVESTAV Irapuato. \n Km 9.6 Libramiento Norte Carretera Irapuato-León, 36500 Irapuato México. \n2 From Multiple Unrelated Element Genome Assembler in a Non-random Order. \n 2 \n2 - A filtering procedure is applied to the set contigs1 to eliminate the \nredundancy, obtaining a set of filtered first order non-redundant contigs \n(Fcontigs1). \n3 - If the number of contigs in Fcontigs1 is small enough to be processed by a \nsingle run of the assembler then we proceed to step 4, otherwise step 1 is \nrepeated using as reads the sequences of the set Fcontigs1. \n4 - The set of non redundant contigs produced is feed to the assembler \nproducing a final assembly. \n \nAssume that the maximum number of reads that can be processed in one run \nby the assembler program is no more than 2n and also that we have a total of \nN reads to process, N>>n. Let k be the largest natural number that fulfils \ncondition k ≥ N/n; that is, the number of groups of reads of size n that can be \nformed. To be precise let say that we number the reads from 1 to N and denote \nthem by ri from i = 1 to N. Consider the k groups of n distinct reads denoted by \nGj, j = 1, 2, … k where G1 is formed by the first n reads, G2  by the following n \nreads and so on until Gk that contain the last n reads, or less than n reads if k > \nN/n. Now consider forming all the possible combinations of two groups, say Emn \n= {Gm, Gn}, where m<n; m = 1, 2, … k-1, n = 2, 3, … k. Clearly there are k(k-\n1)/2 of such groups, because the first component of Emn can be selected from \nany of the k groups and the second member can be selected from the \nremaining (k-1) and we divide by 2 because only sets with different sub index \nare desired. Each one of these Emn groups can be assembled in one run of the \nassembler given that they have at most 2n reads (see Figure 1). \nThe set of contigs obtained from running the assembler on the groups {Emn}, \nsay contig1, will contain all possible contigs of at least two reads and will also \nbe redundant, because there are (multiple) cases where the same contig will be \nobtained in more than one group (see Proposition 1). We can filter this set to \nobtain a non-redundant set of contigs, Fcontig1. This can be done, for example, \nby using the blastclust program of the BLAST suit, coupled with a simple ad-hoc \nscript. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 1. Example of the first step of MUEGANO. The total number of reads \ncan be divided in four groups, and each possible assembly (1,2), (1,3), … (3,4) \nis performed giving the sets of (redundant) contigs c12, c13, … c34.  \n  C 12   C 13   C 14   C 23   C 24   C 34 \n1 4 2 3 \nOriginal set of reads \n1 2 1 3 1 4 3 2 3 2 4 4 \n 3 \nAll contigs obtained (Fcontig1) can be used as if they were simple reads to \ncontinue the assembling process, obtaining contigs2 and Fcontigs2, etc. If in \nany step the number of contigs is less than 2n they can be feed to the \nassembler to conclude the process; otherwise the procedure continues until the \nsize of Fcontigs-t is of size 2n or less and can then be processed in one single \nrun of the assembler producing the final set of contigs in step t+1. Only the \n“singletons” obtained in the first cycle will be real singletons (formed by one \nread); “singletons” obtained in the subsequent cycles will be in fact contigs \nformed by more than one read that did not increased in number of reads and \nwill not increase in any subsequent cycle because no other read (singleton or \ncontig) overlaps with it. \nThis procedure guarantees that all contigs will be recovered (see propositions 2 \nand 3). \nIf in a given step, say u, no new contigs are obtained in any of the sets (say \nthat Fcontigs-u is empty) the procedure stops because there are not new \nsignificant overlaps to take into account. \n \nIn all propositions below it is assumed that the assembler program is perfect, \nthat is, if an overlap of a given size s exist between any two reads x and y and \nif such reads are feed to the assembler it will recover the contig formed by such \nreads, say cxy. \n \nProposition 1 \nAll contigs of exactly two reads will be present in at least one of the Emn \nassemblies. \nProof \nTake any two reads, say read x and read y, that can be assembled into a contig \ncxy because they overlap in a significant part. We need to see that contig cxy will \nbe present in at least one of the Emn assemblies. \nThere are two possible cases, say: 1) Reads x and y are both in one of the \ngroups, say Gm, or 2) Reads x and y are in two different groups, say that x is in \ngroup Gm, and y is in group Gn. In case one the contig cxy will be present in the \nresult of assembling all Emr groups; r = m+1, m+2, … k. In the second case the \ncontig cxy will be present only in assembly Emn. Given that not constrain was \nimposed in the selection of x and y this proves the proposition. (See Figure 2). \n \n \n \n \n \n \n \n \n \n \nFigure 2. Assume that contig c124 can be formed with reads that are in groups \n1, 2 and 4. Then in the first step of MUEGANO only contigs c12 and c24 will be \nformed. However in further steps, contig c124 will be recovered. \nC 24 \nC 12 \nC 124 \n1 2 \n2 4 \n 4 \nProposition 2 \nAll contigs of exactly three reads will be present in at least one of the contigs \nresulting of the assembly of the first cycle (Fcontigs1) or at most in the second \ncycle of assembling (Fcontigs2). \nProof \nSay that there are three reads, x, y and z for which x and y overlap and thus \ncould form contig cxy and also y and z overlap, thus contig cyz could be formed \nbut x and z do not overlap and so contig cxz does not exist (cannot be formed; \nsee Figure 2). \nIf the three reads, x, y and z are all together in the same group Gm in cycle \none, then assembly cxyz will be formed in that cycle and will be present in \nFcontigs1. The same occurs if the three reads are together in any of the Emn \nassemblies during the first cycle. But this is not warranted by the algorithm. \nHowever what is proved in Proposition 1 is that after the first cycle contigs cxy \nand cyz will be formed and they will be present in Fcontigs1 and will proceed to \ncycle two as reads where, again, applying Proposition 1, they will form contigs \ncxyz that will be present in Fcontigs1. This line of reasoning can be generalized \nto prove that all contigs with any number of reads will be obtained in a finite \nnumber of steps (see Figure 2). \n \nProposition 3 \nAll contigs existent in the original set of N reads are recovered by the \nMUEGANO algorithm. \nProof \nLet c be a contig with r reads. Then the minimum number of independent \noverlaps in this contig must be r-1. Note that this contig will surly be formed if \nall the reads or sub-contigs forming it are feed in a single run to the assembler. \nGiven that the procedure continues until either: a single run of the assembler is \nneeded to process all reads (contigs) or until no new contigs are formed. In the \nfirst case, i.e., when all reads (contigs) are feed to the assembler the \nproposition is proved because this step must include at least all sub-contigs that \nform c. In the second case the contig c must be formed in a previous step. \n \nNotes \nOf course, if in any cycle no new contigs are formed the procedure stops \nbecause then there are not any overlapping reads. If the procedure continues it \nis because there were new contigs formed. \nAlso notice that there is no need to keep or process the singletons3 obtained in \nany of the steps; this is because if a singleton read did not found a pair to be \nassembled in a particular assembly, then, either it found a pair in other of the \nassemblies -and is already forming part of a contig, or it is a true singleton that \ncannot be assembled. A disadvantage of the procedure is that we lost count of \nthe true singletons.  \n \n \n \n                                                 \n3 A “singleton” is defined as a read that was not assembled in a particular assembly. \n 5 \nApplying MUEGANO to assemble maize reads \n \nIn a maize project at Langebio we had more than 64 millions of reads, 63 of \nthem in small reads of 100 bp from 210 runs of the GS20-454 pyrosequencing \napparatus, plus 555,000 reads of 250 pb from three runs of the Flex-454 \nmachine and around one million of Sanger reads with an average length of 800 \nbp, giving a total of more than 7,000 millions of bp of genomic DNA sequenced. \nNo assembler program could cope with that quantity of reads in a single run \ngiven limitations in the RAM memory of the computers, thus there was a need \nto apply MUEGANO to this dataset. \n \nFor the assembly of the maize sequences we used two assemblers: the 454 \nassembler (version  1.0.53.17 of the the “off instrument software”) and the 64-\nbit version of the PCAP assembler (Huang, et. al., 2003). The 454 assembler \ndoes not allow parameter adjusting and the PCAP assembler was always run \nwith the values recommended to tolerate highly repetitive overlaps, say: Min \ndepth of coverage for repeats, -l=75 and Adjusted overlap score cutoff, -\ns=4500 (Huang, X. and S.-P. Yang, 2005) \n \nThe assemblers where run in a cluster of 11 dual Xeon processors in which 10 \nnodes have 8 Gb of RAM and one node has 16 Gb of RAM. The maximum \nassembling capacity of the 454 assembler running on the cluster was calculated \nto be around 24 runs of 300,000 reads (7.2 millions of reads), while the \nmaximum number of reads that the PCAP assembler was able to assemble was \naround 1.6 millions of reads. The details of the MUEGANO algorithm applied to \nthis dataset are described in the following steps. \n \na) With the 213 runs of the GS20 and Flex machines we formed 18 sets \nwith a maximum of 12 runs each (17 sets of 12 runs and one set of 9 \nruns). Given that the number of reads in each run of the 454 machines \nvary from 250,000 up to 500,000 reads (with a mean of around 300,000 \nreads) the sets were not formed at random but selecting runs with small \nand large number of reads to obtain the 18 groups, each one with \naround 3.6 millions of reads. \nb) We did 18 x 17 / 2 = 153 runs of the 454 assembler with all possible pair \ncombinations of the distinct sets. Each individual assembling comprised \naround 7,200,000 individual reads and produced around 55,000 contigs. \nc) All contigs of at least 148 bp obtained from the 153 individual assemblies \n(8,028,352 contigs) were feed to the NCBI BLASTCLUST (BLAST score-\nbased single-linkage clustering) program with parameters adjusted to \ncluster only identical sequences. The output of this procedure resulted in \na non-redundant set of 4,821,833 contigs. \nd) The set of non-redundant contigs was compared via blastn (Altschul, et \nal., 1997) to a dataset containing the reported sequences for the maize \nmitochondria and chloroplast. All sequences with a 95% or more identity \nwith the maize organelles were set apart, obtaining a set of 4,681,391 \nnon-redundant filtered contigs. The PHRED (Ewing, B. and P. \n 6 \nGreen,1998) quality of this set of contigs was reduced by a factor of 3 to \nallow flexibility in the assembly. \ne) A run of PCAP with all 956,634 Sanger sequences produced a set of \n100,407 contigs and 468,202 singletons after filtering by organelles. \nf) The sets of sequences obtained in (d) and (e) where randomly allocated \nto seven sets of around 750,000 sequences, and all possible pair \ncombinations (7 x 6 / 2 = 21) of assemblies were performed using PCAP. \nThese assemblies produced a total of 3,909,724 contigs that after \nfiltering by redundancy (see “c” above) were reduced to a set of \n2,348,894 non-redundant contigs. \ng) The set of non-redundant contigs obtained in step f was divided in 3 \ngroups of around 782,964 sequences and the assemblies with all \npossible pair combinations (3 x 2 / 2 = 3) were performed. This \nprocedure produced a total of 702,514 contigs that after filtering by \nredundancy were reduced to a set of 606,187 non-redundant contigs. \n \nFinally, the PCAP assembler was run with the 606,187 non-redundant contigs \nget in step g, obtaining the set of final contigs. \n \nReferences \nHuang, X., J. Wang, et al. (2003). \"PCAP: A Whole-Genome Assembly \nProgram.\" Genome Res. 13(9): 2164-2170. \nHuang, X. and S.-P. Yang (2005). Generating a Genome Assembly with PCAP. \nCurrent Protocols in Bioinformatics. A. D. Baxevanies, D. B. Davison, R. \nD. M. Pageet al. NY, John Wiley & Sons, Inc. 2: 11.3.1-11.3.23. \nAltschul, S. F., T. L. Madden, et al. (1997). \"Gapped BLAST and PSI-BLAST: a \nnew generation of protein database search programs.\" Nucl. Acids. Res. \n25(17): 3389-3402. \nEwing, B. and P. Green (1998). \"Base-Calling of Automated Sequencer Traces \nUsing Phred. II. Error Probabilities.\" Genome Research 8: 185-194. \n \n",
            "id": 313301,
            "identifiers": [
                {
                    "identifier": "oai:nature.com:10.1038/npre.2009.3712.1",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "10.1038/npre.2009.3712.1",
                    "type": "DOI"
                },
                {
                    "identifier": "189498483",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2014884361",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "288782",
                    "type": "CORE_ID"
                }
            ],
            "title": "MUEGANO: A divide and conquer algorithm to overcome memory limitations when assembling shotgun projects",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:nature.com:10.1038/npre.2009.3712.1"
            ],
            "publishedDate": "2009-09-02T14:47:08",
            "publisher": "",
            "pubmedId": null,
            "references": [
                {
                    "id": 8398773,
                    "title": "Base-Calling of Automated Sequencer Traces Using Phred. II. Error Probabilities.&quot;",
                    "authors": [],
                    "date": "1998",
                    "doi": null,
                    "raw": "Ewing, B. and P. Green (1998). &quot;Base-Calling of Automated Sequencer Traces Using Phred. II. Error Probabilities.&quot; Genome Research 8: 185-194.",
                    "cites": null
                },
                {
                    "id": 8398772,
                    "title": "Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.&quot;",
                    "authors": [],
                    "date": "1997",
                    "doi": null,
                    "raw": "Altschul, S. F., T. L. Madden, et al. (1997). &quot;Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.&quot; Nucl. Acids. Res. 25(17): 3389-3402.",
                    "cites": null
                },
                {
                    "id": 8398771,
                    "title": "Generating a Genome Assembly with PCAP. Current Protocols in",
                    "authors": [],
                    "date": "2005",
                    "doi": null,
                    "raw": "Huang, X. and S.-P. Yang (2005). Generating a Genome Assembly with PCAP. Current Protocols in Bioinformatics. A. D. Baxevanies, D. B. Davison, R. D. M. Pageet al. NY, John Wiley & Sons, Inc. 2: 11.3.1-11.3.23.",
                    "cites": null
                },
                {
                    "id": 8398770,
                    "title": "PCAP: A Whole-Genome Assembly Program.&quot; Genome Res.",
                    "authors": [],
                    "date": "2003",
                    "doi": null,
                    "raw": "Huang, X., J. Wang, et al. (2003). &quot;PCAP: A Whole-Genome Assembly Program.&quot; Genome Res. 13(9): 2164-2170.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "http://precedings.nature.com/documents/3712/version/1"
            ],
            "updatedDate": "2021-04-30T09:08:31",
            "yearPublished": 2009,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "1756-0357"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/pdf/288782.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/288782"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/288782/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/288782/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/313301"
                }
            ]
        },
        {
            "acceptedDate": "2010-06-14T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Aniket Kittur"
                },
                {
                    "name": "Christian Seppa"
                },
                {
                    "name": "Eric Miller"
                },
                {
                    "name": "Fred W. Sabb"
                },
                {
                    "name": "Russell A. Poldrack"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/289191",
                "https://api.core.ac.uk/v3/outputs/190415078"
            ],
            "createdDate": "2012-02-17T10:58:03",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 77,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/77",
                    "logo": "https://api.core.ac.uk/data-providers/77/logo"
                }
            ],
            "depositedDate": "2010-06-14T00:00:00",
            "abstract": "The Cognitive Atlas is a collaborative knowledge-building project that aims to develop an ontology that characterizes the current conceptual framework among researchers in cognitive science and neuroscience. The project objectives from the beginning focused on usability, simplicity, and utility for end users. Support for Semantic Web technologies was also a priority in order to support interoperability with other neuroscience projects and knowledge bases. Current off-the-shelf semantic web or semantic wiki technologies, however, do not often lend themselves to simple user interaction designs for non-technical researchers and practitioners; the abstract nature and complexity of these systems acts as point of friction for user interaction, inhibiting usability and utility. Instead, we take an alternate interaction design approach driven by user centered design processes rather than a base set of semantic technologies. This paper reviews the initial two rounds of design and development of the Cognitive Atlas system, including interactive design decisions and their implementation as guided by current industry practices for the development of complex interactive systems",
            "documentType": "research",
            "doi": "10.1038/npre.2010.4532.1",
            "downloadUrl": "https://core.ac.uk/download/pdf/289191.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "The Cognitive Atlas: Employing Interaction Design \nProcesses to Facilitate Collaborative Ontology Creation \nEric Miller, Christian Seppa \nSquishymedia Inc \n524 E Burnside Ste 410 \nPortland OR 97214 \n+1 503 488 5951 \n{eric,christian}@squishymedia.com\nAniket Kittur \nCarnegie Mellon University \n5000 Forbes Ave. \nPittsburgh, PA 15218 \n+1 412 268 7505 \nnkittur@cs.cmu.edu  \n \nRussell A. Poldrack \nUniversity of Texas \n3925-B W. Braker Lane \nAustin, TX 78704 \n+1 512 232 9504 \npoldrack@mail.utexas.edu \n \nFred W Sabb \nUCLA \n760 Westwood Plaza  \nRm C8-854 \nLos Angeles CA 90095 \n+1 310 844 6458 \nfsabb@mednet.ucla.edu  \n \nABSTRACT \nThe Cognitive Atlas is a collaborative knowledge-building project \nthat aims to develop an ontology that characterizes the current \nconceptual framework among researchers in cognitive science and \nneuroscience.  The project objectives from the beginning focused \non usability, simplicity, and utility for end users.  Support for \nSemantic Web technologies was also a priority in order to support \ninteroperability with other neuroscience projects and knowledge \nbases. Current off-the-shelf semantic web or semantic wiki \ntechnologies, however, do not often lend themselves to simple \nuser interaction designs for non-technical researchers and \npractitioners; the abstract nature and complexity of these systems \nacts as point of friction for user interaction, inhibiting usability \nand utility.  Instead, we take an alternate interaction design \napproach driven by user centered design processes rather than a \nbase set of semantic technologies.  This paper reviews the initial \ntwo rounds of design and development of the Cognitive Atlas \nsystem, including interactive design decisions and their \nimplementation as guided by current industry practices for the \ndevelopment of complex interactive systems.  \nCategories and Subject Descriptors \nD.2.2 [Software Engineering]: Design Tools and Techniques – \nEvolutionary prototyping, Software libraries, User interfaces \nH.5.2 [Information Interfaces and Presentation]: User \nInterfaces – Graphical user interfaces, screen design, user-\ncentered design \nH.5.3 [Information Interfaces and Presentation]: Group and \nOrganization Interfaces – Collaborative computing, Web-based \ninteraction \nGeneral Terms \nDesign, Experimentation, Human Factors. \nKeywords \nSemantic Web; Ontologies; Collaboration; Cognitive Science; \nUser Interface; User Experience Design. \n \n1. INTRODUCTION \nBiomedical research has become increasingly reliant on semantic \ninfrastructure and ontologies to guide analysis and enable new \ndiscoveries.  One challenge to the development of knowledge \nbases in research domains arises in trying to elicit knowledge \nfrom researchers, who are not schooled in the technicalities of \nontology development, and who are unlikely to use any system \nthat requires them to learn a complex interaction design.  We have \ndeveloped the Cognitive Atlas project \n(http://www.cognitiveatlas.org) to address these issues in the \ndomain of cognitive science and neuroscience. \nThe system aims to develop a cohesive ontology from researchers \nwho are geographically and temporally distributed, and who do \nnot share a common mental model of what the ontology should \nlook like.  Thus, in contrast to systems like GO [1] in which the \nbuilding blocks of the domain are relatively well specified and \ncontributions are carefully managed, Cognitive Atlas needs to \nenable end users not just to add information but also to participate \nin the meta-level tasks of discussion, debate, gathering evidence, \nand building consensus. \nIn turn, the Cognitive Atlas is designed to contribute to the larger \ninformational ecosystem of neuroinformatics and scientific \ncollaboration projects. The Cognitive Atlas is contributing content \nto the NeuroLex [2] and the Neuroscience Information \nFramework [3], and is designed and built to be consistent with the \napproaches used by related projects such as the Semantic Web \nApplications in Neuromedicine (SWAN) project [4]. \nThe Cognitive Atlas is somewhat unique in that it is a web \napplication concerned almost entirely with building semantic \nrelationships but is deliberately designed to minimize user \ninteraction with the Semantic Web technologies employed by the \nsystem.  In part this is because of the nature of the subject matter \nto be captured: the system needed to support subjectivity, \ndifferences of opinion, overlap across domains, and ambiguity. \nUltimately, our approach is designed to address the issue of \ndaunting interactive complexity in semantic web applications by \nusing user-centered design processes to promote the elicitation \nand discussion of key semantic relations within the domain \nwithout requiring users to articulate their contributions using \nformal ontological structures.  Our approach is to create a Copyright is held by the author/owner(s). \nWWW 2010, April 24-30, 2010, Raleigh, North Carolina. \nFWCS 2010, April 26, 2010, Raleigh, North Carolina. \n \nN\nat\nur\ne \nPr\nec\ned\nin\ngs\n : \ndo\ni:1\n0.\n10\n38\n/n\npr\ne.\n20\n10\n.4\n53\n2.\n1 \n: P\nos\nte\nd \n11\n J\nun\n 2\n01\n0\nknowledge base containing enough structured information to \nsupport a process where we can refactor the knowledge base for \nexpression as RDF.  In effect, the system is designed to meet the \nusers halfway; it asks them to express their knowledge in an \ninteractive format structured around how they understand their \ndomain knowledge, while the underlying system is designed to \nsupport the translation of the structured contributions into \nformally articulated Semantic Web content.  \nThe project’s focus on ‘soft’ design considerations and standard \ncommercial design processes described in this paper may strike a \nmore technically minded reader as somewhat superficial.  The \nauthors would suggest that such an interpretation misses a key \npoint regarding the importance of a structured interaction design \nprocess.  Any description of a design process risks the appearance \nof triviality. Or to rephrase a commonly quoted aphorism about \nmusic criticism, \n“Writing about design is like dancing about architecture.” \nDesign is a process with a tendency towards subjectivity.  No \nsingle methodology, approach, or template will work for every \nsituation. We believe that an elegant concept alone is inadequate \nto ensure the success of an application; it must also be \nsuccessfully translated into a workable and useful embodiment \nthat offers value to the user community and is easy to use, and a \nstructured design process offers a way to enhance the likelihood \nof success. \nWe believe that the scientific community should take advantage \nof established interactive design practices and conventions in \norder to create effective and useful scientific collaboration \napplications.   Otherwise, an unnecessary degree of interactive \n‘friction’ may exist in poorly designed interactions, discouraging \nuse and impairing the usefulness and impact of an otherwise \nviable scientific project.  And an unusable application does not \nmake anyone happy – the principals, the users, or the funders. \n2. PROJECT OBJECTIVES \nCreating an ontology of mental concepts and their relations with \neach other has special value for cognitive science for a number of \nreasons.  First, as in many other domains, there are differences in \nterminology such that the same term could mean different things \nto different people, and conversely different terms could refer to \nthe same underlying concept.  Second, there are fundamental \ndifferences in opinion of what certain concepts and processes \nactually are: for example, some researchers conceive of the \nconcept \"working memory\" as a system with different \ncomponents (e.g., the phonological buffer), whereas others think \nof it from an attentional perspective (e.g., what is currently in the \nfocus of attention).  Both of these types of differences have \ncascading effects to other research that is based on cognition: for \nexample, for a neuroscientist to claim that an area of the brain is \nassociated with working memory, it is necessary to be explicit \nabout what the concept of working memory refers to.  Thus an \nontology of cognitive processes could provide a foundation for \nmany different areas of research related to cognition, ranging from \nstudies of disease and mental illness to the biology that underlies \nthem, including brain systems, signalling pathways, and even \ngenetic markers. \nIn addition to the utility of the ontology itself, there are a number \nof benefits that come about from the process of collaboratively \nconstructing the ontology.  In contrast to the cathedral approach to \nknowledge building, in which a small group of individuals work \naccording to a single plan, the Cognitive Atlas is based on the \nbazaar approach, in which very many people each contribute \nbased on their own expertise and interests [5] [6].  By combining \nprinciples from low-cost distributed knowledge production \nsystems such as Wikipedia with structural elements from more \nformal ontologies we aim to create a new type of knowledge \nbuilding approach that harnesses the strengths of both.  Some of \nthe goals enabled by this include: \n \n• Group sensemaking.  One key goal for the Cognitive Atlas \nis for it to serve as a tool for scientists to make sense of their \ndomain.  As described above, this led us to make different \ndesign decisions than if the goal was simply the creation of \nan ontology.  For example, coordination and consensus \nbuilding have been found to be critical to the effectiveness of \nlarge scale collaboration systems such as Wikipedia and open \nsource software [7][8].  Thus designing to support discussion \nand debate was critical to enable a distributed group of \nscientists to engage in sensemaking together.  \n \n• Individual sensemaking. At the individual level, we aimed \nto make the ontology useful for scientists or graduate \nstudents unfamiliar with a domain to understand it more \neasily, for example by having annotated and curated lists of \nrelevant citations for each concept as well as the relations \nbetween them.  Making the system useful for those actively \nengaged in using it (as opposed to the potential for being \nuseful sometime in the future for others) has been repeatedly \nfound to be a key factor in the success of groupware systems \n[9], so this aim is synergistic with the development of the \nontology itself. \n  \n• Capturing scientific discussion.  Currently, most forms of \nscientific discussion take place either extremely slowly (e.g., \njournal articles) or are limited in their reach (e.g., informal \ndiscussions at conferences, or journal clubs).  While there \nhave been some efforts to address this by allowing scientists \nto comment on papers online (e.g., Cell, PLoS ONE), \nengagement with these venues has been minimal.  By \nproviding a central place for these discussions and making \nthem useful in the context of a larger structure (the \ndevelopment of an ontology of cognition) we hope to capture \nand share scientific discourse in a way that will benefit other \nresearchers interested in a topic. \n \n• Promoting interdisciplinary research.  By having many \ndifferent kinds of researchers (e.g., cognitive psychologists, \nneuroscientists) developing, annotating and creating links \nbetween an underlying set of concepts we aim to increase the \nlikelihood of interdisciplinary insight.  Furthermore, making \nit easier for scientists to make sense of an unfamiliar area can \npromote the chances of their finding important connections \nto their own areas of expertise. \n3. DESIGN RATIONALES \nSince the target audience for the Cognitive Atlas is made up of \nresearchers with limited ontology creation experience, we decided \nto employ familiar and easy to use web application interaction \ndesign metaphors to the maximum extent possible. \nInitially the project team expected to use the Semantic MediaWiki \npackage as the foundation for the system.  After the first couple of \nmonths of discussions and research, however, it became clear that \nthe existing open-source packages were not a fit for the needs of \nN\nat\nur\ne \nPr\nec\ned\nin\ngs\n : \ndo\ni:1\n0.\n10\n38\n/n\npr\ne.\n20\n10\n.4\n53\n2.\n1 \n: P\nos\nte\nd \n11\n J\nun\n 2\n01\n0\nthis specific project.  Ultimately, three key issues steered us \ntowards custom development.  We realized that our ‘fuzzy’ \ndomain-specific semantics and knowledge structures would not \neasily fit into a pre-existing package; we knew that budget \nrestrictions would limit our ability to adequately customize the \nexisting frameworks to our needs; and we believed the success of \nour project depended on differentiating our application from other \nwiki-based knowledge elicitation initiatives by making our \napplication visually unique and memorable.  As a result of this \ndecision, the first two releases of Cognitive Atlas have been \npurpose-built using the open-source LAMP development stack. \nBy developing a purpose-designed custom framework for the \nelicitation of semantic knowledge we gained additional flexibility \nto wrap the user interaction design around the mental models and \nprocesses of our audience.  Whenever possible we wanted to \navoid asking users to shift out of their practitioner-oriented \nconceptual framework into a technology-driven abstract and \nformalized framework. \n4. DESIGN PRINCIPLES \n• Know the users.  Users of Cognitive Atlas will almost \nexclusively be researchers in the fields of cognitive \npsychology, cognitive science, and neuroscience.  This is a \nrelatively small and specific audience, so we can assume \ncertain shared characteristics of the user base as a foundation \nfor more sophisticated interaction design.   We can use \nsophisticated domain-specific language to precisely guide \ninteraction, for example, instead of mainstream audience \noriented general language.  \n \n• Don't make users think.  This usability principle, \npopularized by Steve Krug's mainstream interactive design \nbook [10], applies even to complex systems where a \nsophisticated user base is assumed.  Specifically, in the case \nof applications with a semantic web component, we were \naware that many Semantic Web and Semantic Wiki tools \nwould require end users to mentally shift into a kind of  'RDF \nmindset' in order to contribute to and benefit from the \nknowledge base.   We wanted to avoid forcing users to set \naside their existing conceptual frameworks that characterize \ntheir understanding of cognitive science.  In addition, we \nassumed that any requirements to learn a new language for \nspecifying knowledge, even something as simple as \nWikiText, would substantially reduce the level of \nparticipation. \n \n• Minimize interactive friction as much as possible.  Every \ntime a user needs to stop and think about a particular \ninteractive element in order to understand its functionality, a \npoint of 'friction' has been introduced into the interaction.  \nEven an overly heavy single pixel horizontal rule can inhibit \ncommunicative effectiveness as a user scans a page [11].  \nThe cumulative effect of these points of friction is to \ndiscourage end users since the system requires more in effort \nfrom the user than it provides in value.   \n \n• Ensure that the user interaction design is driven by user-\ncentered design goals, and not driven by the technologies \nand data models. We want to ensure that end users are \nfocused on the content that they are viewing and \ncontributing, and are not forced to think in terms of the \nunderlying technologies, technology-driven vocabularies, or \ndatabase structure. \n \n• Make the best use of all available design techniques.  \nHTML form elements are only one component of an \ninteractive designer's toolbox.  User interactions can be \nshaped and guided using layout, typography, negative space, \nrelative proportion, alignment, color vocabularies, contrast, \nsequencing, foregrounding and backgrounding, motion, \nchange over time, and other 'tricks' from the interactive \ndesign trade.  Collectively these techniques can be \norchestrated to provide subtle (or not so subtle) interaction \ncues and affordances for the user instead of explicitly \narticulating interaction requirements using the written word. \n \n• Engage with users throughout the design process.  \nIndustry standard techniques including wireframing, rapid \nprototyping, and agile development practices help the \ndevelopment team to elicit valuable feedback early and often \nfrom end users. \n \n• Ensure that the site 'sets a hook' in users.  The project will \nonly be a success if we get users to contribute to the system \nand to return on a regular basis.  We need to ensure that \nevery interaction with the system encourages users to return \nand to continue participating in the construction of the \nknowledge base.  Some planned features include “knowledge \nmanagement” functions that would provide researchers with \nadded value from their contributions to the system, \nindividual “dashboards” comprised of content relevant to the \nuser, and personalization features inspired by social \nnetworking sites.  Much of the planned development for \n2010 is oriented towards this particular design principle. \n \n• Avoid overdesign.  We believe it is best to start by designing \na simple system capable of supporting most usage scenarios \nbut not necessarily every possible use case.  A highly usable \nsystem supporting 80% of practitioner-provided information \nis better than a more complex system that is harder to use but \naccommodates a wide variety of edge cases.   \n \n• Use industry best practices.  Many commercial design and \ndevelopment best practices are also applicable and valuable \nwhen developing noncommercial applications.  Structured \nrequirements gathering processes, information architecture \nand wireframing processes, rapid prototyping, brand and \ndesign vocabulary development, agile development \ntechniques, structured project management, and other \napproaches commonly used by the private sector all apply. \n5. IMPLEMENTATION \nWe used a rapid and iterative design, prototyping and \ndevelopment process to step through and refine our interactive \ndesign approaches for the system.  We started with flat PDF \n'wireframe' style prototypes; developed medium-fidelity HTML \nprototypes to explore specific design issues; then used rapid \nrelease iterations on the alpha system during development to \nallow quick review and refinement cycles by the project team.  \nThis Agile-like process has continued into the production cycle as \npractitioners begin using the tool and provide feedback to the \nproject managers and development team. \nN\nat\nur\ne \nPr\nec\ned\nin\ngs\n : \ndo\ni:1\n0.\n10\n38\n/n\npr\ne.\n20\n10\n.4\n53\n2.\n1 \n: P\nos\nte\nd \n11\n J\nun\n 2\n01\n0\nThe project team also invested in the development of an identity \nand design vocabulary system.  Developed by a branding \nprofessional, the result was a clearly documented design \nvocabulary that served as a rulebook for a consistent user interface \ndesign language employed throughout Cognitive Atlas.  The \nbenefits are not merely aesthetic; usability benefits when users \nfamiliar with the system internalize the design conventions and \nare able to quickly understand meaning implied by subtle color \nand typographical cues. \nTerminology was also a key to making interaction intuitive for \nend users.  Cognitive Atlas uses the term \"assertions\" to describe \nuser contributions that characterize the relationship between two \nelements in the knowledge base.  Typical assertions supported by \nthe system are expressions like: \n• Declarative memory is the same as explicit memory \n• Memory retrieval is a part of declarative memory \n• PMID:12345 (a published research paper) provides empirical \nevidence that working memory is related to attention  \n• Working memory is measured by the comparison of 0-back \nand 1-back conditions on the item recognition task \nA relatively comprehensive example of a concept definition with \nrelated assertions can be viewed on the current system:  \nhttp://cognitiveatlas.org/concept/working_memory \nThe task of creating assertions is made more challenging when we \ntake into account that definitions for different concepts, tasks, or \ntest results (\"indicators\") can vary from paper to paper, or from \nresearcher to researcher.  So Cognitive Atlas supports the ability \nto cite existing definitions or to create new definitions for any of \nthe elements cited in an assertion. \n The need to express precise semantic relationships still remains; \nso to facilitate usability and clarity for the end user we designed \nthe user interaction to separate semantic linking activities from \nother interactions with the knowledge base.  \n6. CURRENT STATUS \nThe core functionality of the Cognitive Atlas system is \nfunctionally complete and core contributors began building the \nknowledge base in 2009.  Phase II of the system launched in late \n2009 and included full support for interoperability via Semantic \nWeb technologies, more robust integration with third-party \ninformation sources including PubMed and PubBrain, and many \nrefinements to the interactivity of the system. \nThe primary user activity as of this writing is centered on \ncontribution and discussion of definitions for the approximately \n800 terms in the knowledge base.  Once a substantial number of \nterms have been defined, the assertions (semantic relations) \ncomponent of the site will be substantially more useful for end \nusers. \nWe have identified two specific challenges to be addressed in the \nnear future.  One challenge is to ensure that the system provides \nimmediate and substantial value to the end user starting with the \nfirst interaction.  Many sites and systems compete for practitioner \nmindshare, so we need to make sure that the system is useful from \nthe beginning or we risk losing users and our traction within the \ncommunity.  Secondly, we need to refine our backend Semantic \nWeb tools to facilitate interoperability with other cognitive \nscience initiatives, in particular the NeuroLex [2] project. \nEnhancements planned for the 2010 releases of the system include \nadditional personalization, ‘dashboard’ tools, data mining tools \nfor knowledge discovery based on a user’s published abstracts, \ndevelopment of a version for mobile device access to the \nknowledge base, and visualization tools intended to maximize \nutility and informational value by avoiding common pitfalls [12]. \n7. CONCLUSIONS \nWe would like to be clear that the process described here was \nsuited to this for this particular application and would not be \napplicable in all situations.  The design abstractions we employed \nas well as the \"lite\" semantics employed in the application are not \nglobally applicable to Semantic Web-based systems where more \nformal mechanisms would be more appropriate. However, we do \nbelieve that some of the processes, practices, and design \ntechniques described here may be of benefit to the user interface \ndesign community and the Semantic Web community, particularly \nwith regard to the elicitation of expert knowledge from a broader \naudience of practitioners.  We also hope that some of the \nprinciples employed here may be useful for the design of systems \naimed at helping scientists and practitioners to make sense of the \nstate of knowledge in their areas for a diverse range of domains. \n8. ACKNOWLEDGMENTS \nThe development described here was supported by grant \nRO1MH082795 from the National Institute of Mental Health. \n9. REFERENCES \n[1] Ashburner M, Ball CA et al (2000) Gene ontology: tool for \nthe unification of biology. The Gene Ontology Consortium. \nNat Genet. 2000 May;25(1):25-9.  \n[2] Bug WJ, Ascoli GA et al (2008) The NIFSTD and BIRNLex \nvocabularies: building comprehensive ontologies for \nneuroscience.  Neuroinformatics. 2008 Sep;6(3):175-94. \nEpub 2008 Oct 31. \n[3]  Gardner D, Akil H et al (2008). The Neuroscience \nInformation Framework: A Data and Knowledge \nEnvironment for Neuroscience. Neuroinformatics. 2008 \nSep;6(3):149-60. Epub 2008 Oct 23.  \n[4] Ciccarese P, Wu E et al. (2008) The SWAN Biomedical \nDiscourse Ontology. J Biomed Inform. 2008 Oct;41(5):739-\n51. Epub 2008 May 4.  \n[5] Benkler, Y. (2002) Coase's Penguin, or, Linux and the \nNature of the Firm. Yale Law Journal, 112 (3). 367-445. \n[6] Raymond, E. (1999). The Cathedral and the Bazaar. \nKnowledge, Technology, and Policy, 12 (3). 23-49. \n[7] Kittur, A., & Kraut, R. E. (2008). Harnessing the wisdom of \ncrowds in Wikipedia: Quality through coordination. CSCW \n2008, San Diego, CA. \n[8] Mockus, A., Fielding, R. T., & Herbsleb, J. D. (2002). Two \ncase studies of open source software development: Apache \nand mozilla. ACM Transactions on Software Engineering \nand Methodology 11 (3), 309-346. \n[9] Grudin, J. (1994). Eight challenges for developers. \nCommunications of the ACM, 37, 93-105. \n[10] Krug, S. (2005). Don't Make Me Think: a Common Sense \nApproach to the Web (2nd Edition). New Riders Publishing. \n[11] Tufte, E. (1990). Envisioning Information. Graphics Press. \n[12] Karger, D. (2006) The Pathetic Fallacy of RDF. SWUI06.\n \nN\nat\nur\ne \nPr\nec\ned\nin\ngs\n : \ndo\ni:1\n0.\n10\n38\n/n\npr\ne.\n20\n10\n.4\n53\n2.\n1 \n: P\nos\nte\nd \n11\n J\nun\n 2\n01\n0\n",
            "id": 314476,
            "identifiers": [
                {
                    "identifier": "2024117744",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "10.1038/npre.2010.4532.1",
                    "type": "DOI"
                },
                {
                    "identifier": "289191",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "190415078",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:nature.com:10.1038/npre.2010.4532.1",
                    "type": "OAI_ID"
                }
            ],
            "title": "The Cognitive Atlas: Employing Interaction Design Processes to Facilitate Collaborative Ontology Creation",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:nature.com:10.1038/npre.2010.4532.1"
            ],
            "publishedDate": "2010-06-11T22:39:03",
            "publisher": "",
            "pubmedId": null,
            "references": [
                {
                    "id": 8403288,
                    "title": "Akil H et al (2008). The Neuroscience Information Framework: A Data and Knowledge Environment for Neuroscience. Neuroinformatics.",
                    "authors": [],
                    "date": "2008",
                    "doi": null,
                    "raw": "Gardner D, Akil H et al (2008). The Neuroscience Information Framework: A Data and Knowledge Environment for Neuroscience. Neuroinformatics. 2008 Sep;6(3):149-60. Epub 2008 Oct 23.",
                    "cites": null
                },
                {
                    "id": 8403286,
                    "title": "Ascoli GA et al (2008) The NIFSTD and BIRNLex vocabularies: building comprehensive ontologies for neuroscience. Neuroinformatics.",
                    "authors": [],
                    "date": "2008",
                    "doi": null,
                    "raw": "Bug WJ, Ascoli GA et al (2008) The NIFSTD and BIRNLex vocabularies: building comprehensive ontologies for neuroscience.  Neuroinformatics. 2008 Sep;6(3):175-94. Epub 2008 Oct 31.",
                    "cites": null
                },
                {
                    "id": 8403284,
                    "title": "Ball CA et al (2000) Gene ontology: tool for the unification of biology. The Gene Ontology Consortium. Nat Genet.",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "Ashburner M, Ball CA et al (2000) Gene ontology: tool for the unification of biology. The Gene Ontology Consortium. Nat Genet. 2000 May;25(1):25-9.",
                    "cites": null
                },
                {
                    "id": 8403291,
                    "title": "Coase's Penguin, or, Linux and the Nature of the Firm.",
                    "authors": [],
                    "date": "2002",
                    "doi": null,
                    "raw": "Benkler, Y. (2002) Coase's Penguin, or, Linux and the Nature of the Firm. Yale Law Journal, 112 (3). 367-445.",
                    "cites": null
                },
                {
                    "id": 8403298,
                    "title": "Don't Make Me Think: a Common Sense Approach to the Web (2nd Edition).",
                    "authors": [],
                    "date": "2005",
                    "doi": null,
                    "raw": "Krug, S. (2005). Don't Make Me Think: a Common Sense Approach to the Web (2nd Edition). New Riders Publishing.",
                    "cites": null
                },
                {
                    "id": 8403297,
                    "title": "Eight challenges for developers.",
                    "authors": [],
                    "date": "1994",
                    "doi": null,
                    "raw": "Grudin, J. (1994). Eight challenges for developers. Communications of the ACM, 37, 93-105.",
                    "cites": null
                },
                {
                    "id": 8403299,
                    "title": "Envisioning Information.",
                    "authors": [],
                    "date": "1990",
                    "doi": null,
                    "raw": "Tufte, E. (1990). Envisioning Information. Graphics Press.",
                    "cites": null
                },
                {
                    "id": 8403295,
                    "title": "Harnessing the wisdom of crowds in Wikipedia: Quality through coordination. CSCW",
                    "authors": [],
                    "date": "2008",
                    "doi": null,
                    "raw": "Kittur, A., & Kraut, R. E. (2008). Harnessing the wisdom of crowds in Wikipedia: Quality through coordination. CSCW 2008, San Diego, CA.",
                    "cites": null
                },
                {
                    "id": 8403293,
                    "title": "The Cathedral and the Bazaar.",
                    "authors": [],
                    "date": "1999",
                    "doi": null,
                    "raw": "Raymond, E. (1999). The Cathedral and the Bazaar. Knowledge, Technology, and Policy, 12 (3). 23-49.",
                    "cites": null
                },
                {
                    "id": 8403300,
                    "title": "The Pathetic Fallacy of RDF. SWUI06. N a t u r e P r e c e d i n g s : d o i : .",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "Karger, D. (2006) The Pathetic Fallacy of RDF. SWUI06. N a t u r e P r e c e d i n g s : d o i : . / n p r e . . . : P o s t e d J u n",
                    "cites": null
                },
                {
                    "id": 8403289,
                    "title": "The SWAN Biomedical Discourse Ontology. J Biomed Inform.",
                    "authors": [],
                    "date": "2008",
                    "doi": null,
                    "raw": "Ciccarese P, Wu E et al. (2008) The SWAN Biomedical Discourse Ontology. J Biomed Inform. 2008 Oct;41(5):739-51. Epub 2008 May 4.",
                    "cites": null
                },
                {
                    "id": 8403296,
                    "title": "Two case studies of open source software development: Apache and mozilla.",
                    "authors": [],
                    "date": "2002",
                    "doi": null,
                    "raw": "Mockus, A., Fielding, R. T., & Herbsleb, J. D. (2002). Two case studies of open source software development: Apache and mozilla. ACM Transactions on Software Engineering and Methodology 11 (3), 309-346.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "http://precedings.nature.com/documents/4532/version/1"
            ],
            "updatedDate": "2021-04-30T09:09:18",
            "yearPublished": 2010,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "1756-0357"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/pdf/289191.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/289191"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/289191/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/289191/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/314476"
                }
            ]
        },
        {
            "acceptedDate": "2007-06-20T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Constandinou, T G"
                },
                {
                    "name": "Drakakis, E M"
                },
                {
                    "name": "Eftekhar, A"
                },
                {
                    "name": "Toumazou, C"
                },
                {
                    "name": "Triantis, I F"
                }
            ],
            "contributors": [
                "Amir"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/294982",
                "https://api.core.ac.uk/v3/outputs/190679874"
            ],
            "createdDate": "2012-02-19T10:48:03",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 105,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/105",
                    "logo": "https://api.core.ac.uk/data-providers/105/logo"
                }
            ],
            "depositedDate": "2007-05-01T00:00:00",
            "abstract": "Published versio",
            "documentType": "research",
            "doi": "10.1109/cne.2007.369703",
            "downloadUrl": "",
            "fieldOfStudy": "computer science",
            "fullText": "Towards a reconﬁgurable sense-and-stimulate neural interface\ngenerating biphasic interleaved stimulus\nAmir Eftekhar∗‡, Timothy G. Constandinou∗, Iasonas F. Triantis∗, Chris Toumazou∗ and Emmanuel M. Drakakis†\n∗Institute of Biomedical Engineering, †Department of Bioengineering, ‡Department of Electrical and Electronic Engineering\nImperial College of Science, Technology and Medicine, London SW7 2AZ, UK\nEmail: amir.eftekhar@imperial.ac.uk\nAbstract—This work presents an eight channel neural stimulator\ninterface based on an address event communication protocol. The system\ndeveloped decodes, integrates and interleaves the spike patterns using a\ncontinuous interleave sampling (CIS) strategy, which drives a complex\nbiphasic current waveform to a bipolar electrode conﬁguration. This\npaper describes the basic concept, circuit design, simulation results and\nhardware implementation in a standard 0.35μm CMOS technology.\nI. INTRODUCTION\nThe ﬁeld of neuroprosthetics has been evolving over the last ﬁve\ndecades and its application base is expanding considerably. From\nfunctional electrical stimulation (FES) restoring muscular movement\nin paraplegics and stroke patients, to the mainstream medical device\nindustries developing pacemakers, cochlear and soon retinal implants,\nneuroprosthetics have revolutionised treatment of previously incur-\nable conditions. This is further emphasised with the development of\nneuromodulation devices for rehabilitation after spinal cord injury,\npain relief and treatment of epilepsy and depression [1].\nMost commercially available stimulators are open-loop, featuring\nexternally-controlled or pre-programmed stimuli. However, results\nindicate that they are power consuming and therapeutically ineffective\ndue to their inability to activate muscles or organs automatically when\nthe need arises.\nFor this reason it is desirable to achieve a sense-and-stimulate\nsystem, both for closed-loop FES as well as for neural lesion\nbypass devices, where activation of a muscle affected by a severed\nneural path is triggered by command bio-signals generated by nerves\npreceding the injury.\nThe idea of closed-loop stimulators and neural-bypass systems\nwas ﬁrst expressed in the sixties, as illustrated in Fig. 1 [2], and\nat the time it was mostly viewed as non-implementable. However,\nresearch in neurostimulation coupled with advances in integrated\ntechnologies has made such systems technically feasible and in the\nlast two decades closed-loop systems are the main research focus in\nthis ﬁeld.\nMaking a such a system implantable is not a trivial task, with\nthe front-end (neural-monitoring sensors) suffering from biocompat-\nibility issues, low SNR and poorly documented speciﬁcations. The\nprocessing stage is limited by power and area constraints and the\noutput stage has to satisfy issues of power, safety and selectivity. In\naddition, the need for wireless transcutaneous communication with\nan external device offering supply and data transmission increases\nthe overall complexity.\nOther than these constraints, important factors hindering devel-\nopment in this ﬁeld is the intense diversity between devices being\ndeveloped for different applications. According to Troyk et. al [3],\nresearch in stimulation in the past 20 years has been erroneously\nfocusing mainly on studying particular neurogenic disorders. This\nthen yields the need for speciﬁc technologies to be developed,\nrather than developing appropriate advanced devices that can then\nbe tailored for speciﬁc conditions. This yields a variety of electrode\nconﬁgurations, as well as different stimulus current amplitudes and\nwaveforms for different types of interfaces. Moreover various signal\nprocessing and transmission methods, all with their own advantages\nhave been developed [4].\nThus it is evident that the development of a generic sense-and-\nstimulate device featuring multiple re-conﬁgurable channels would\nrevolutionise neuroprostheses leading to much more coherent re-\nsearch and delivering highly efﬁcient rehabilitation practices.\nFig. 1. Reproduction of a 1964 ﬁgure from [2] illustrating an ”electronic\nbypass for motor neuron lesion”\nThe work presented here is part of a project that considers the\ndevelopment of a generic sense-and-stimulate system with adaptable\narchitecture whose purpose is to offer an alternative to single-use\ndevices in the ﬁeld. In the context of the overall project, parallel\ndevelopment of all three stages of signal sensing, processing and\ntransmission as well as stimulation takes place. The portion of the\nproject presented in this paper is concerned with the last two stages,\nwith the front end brieﬂy described in the next section. The rest of\nthe paper is organized as follows: Part II provides some theoretical\nbackground regarding stimulation parameters, while Part III outlines\nthe system architecture. Part IV describes the implementation of\nthe various blocks, Part V presents system simulation results, and\nﬁnally, Part VI summarizes the system submitted for fabrication and\ndiscusses future work.\nII. STIMULATION THEORY\nTypically, in electrical neural stimulation, a minimum of two\nelectrodes are used to produce a nerve activation current. The pair\ncan be used in monopolar or bipolar conﬁguration [5]. In multipolar\nstimulation schemes, the bipolar scheme (active and reference elec-\ntrode are placed close to the nerve) is preferred as it allows greater\nactivation selectivity because each pair generates a more localized\nﬁeld [5] (compared to monopolar).\nStimulus pulse parameters include frequency, amplitude and du-\nration. The former affects the smoothness of muscle contractions\nand needs to be adjusted to prevent muscle fatigue. The latter two\nProceedings of the 3rd International\nIEEE EMBS Conference on Neural Engineering\nKohala Coast, Hawaii, USA, May 2-5, 2007\nFrD3.30\n1-4244-0792-3/07/$20.00©2007 IEEE. 438\nparameters affect the strength of contraction by altering the charge\ninjected.\nAs established by [6], in order to avoid harmful electrochemical\nprocesses, the stimulus waveform has to be biphasic. In such a\nwaveform the ﬁrst pulse causes activation, followed by a second one\nwith opposite polarity to balance the charge delivered by the ﬁrst [5].\nHowever, it has been reported [1] that two opposite pulses back to\nback actually act to prevent the generation of an action potential. To\novercome this, a short time delay needs to be introduced between\nthe pulses [1]. Additionally, using an extended anodic pulse with\nreduced amplitude can compensate for charge distribution and thus\nreduce fatigue.\nIII. SYSTEM OVERVIEW\nThe system was developed such that virtually any sense signal\ncan be interfaced to the front-end providing the input to the system.\nThe requirement for the system is that at the sensor output (i.e.\nsystem input), an event must occur, causing a trigger. The trigger\nis transmitted via an asynchronous digital bus, in the form of a pulse\nusing an Address Event Representation (AER) architecture [7]. Input\nevents causing the trigger can be of any form, for example, a threshold\ncrossing, frequency detection, etc.\nThe overall system developed incorporated an ion-selective ﬁeld-\neffect transistor (ISFET) to sense Potassium (K) ion concentration,\nmodulating a spiking neuron circuit that in turn provides the required\npulse stream stimulus to an address-event transmitter. The back-\nend stimulation interface described here consists of an address-\nevent receiver (i.e. event demultiplexer) feeding multi-channel stim-\nulus generators. Each of these includes a ripple counter, digital-to-\nanalogue converter (DAC), CIS controller and biphasic waveform\ndriver.\nThe full system architecture is shown in Fig. 2, with the back-\nend portion annotated. The system described uses a pulse stream\nto generate a stimulus pulse whose magnitude is linearly related to\nthe number of trigger events. Although the system does not include\nhardware to pre-process the input stimulus, for example, by scaling\nor providing non-linear compression, such functions can be easily\nimplemented by intercepting the AER bus and performing pulse\nstream processing [8].\nThe stimulus pulse itself has been chosen to follow a biphasic\nproﬁle and to stimulate one or more bipolar electrode conﬁgurations.\nTo generate this stimulus the system is designed in three stages.\nFirstly, the spike (or pulse) rate (indicating the stimulus trigger\nmagnitude) is measured and converted to a current level. This current\n  Spiking\ngenerator\n                  AER\n         - Transmitter\none fixed pulse \nper event\n8-\nb\nit\n c\nu\nrr\nen\nt \ng\nen\ner\nat\no\nr\n(Fig. 2)\nChannel 1\nChannel 2\nChannel 3\nChannel 4\nChannel 5\nChannel 6\nChannel 7\nChannel 8\nCIS enable pulse Fig. 3\nE\nN\n1\nE\nN\n2\nE\nN\n3\nE\nN\n4\nE\nN\n5\nE\nN\n6\nE\nN\n7\nE\nN\n8\nB\nip\nh\nas\nic\n P\nu\nls\ne \nG\nen\ner\nat\no\nr\n(Fig. 4)\no\nn\ne \no\nn\n e\nac\nh\n c\nh\nan\nn\nel\n8-\nb\nit\n a\nd\nd\nre\nss\n e\nve\nn\nt \nco\nu\nn\nte\nr\n(0\n-2\n55\n)\nS\nti\nm\nu\nla\nti\no\nn\n E\nle\nc\ntr\no\nd\ne\ns\n*\no\nn\ne \no\nn\n e\nac\nh\n c\nh\nan\nn\nel\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\nSystem Presented\n                  AER\n            - Receiver\n8 \nC\nh\nan\nn\nel\ns\nIS\nF\nE\nT\n C\nh\ne\nm\nic\na\nl \nS\ne\nn\ns\nin\ng\no\nr\nO\nth\ne\nr \nn\ne\nu\nra\nl \nre\nc\no\nrd\nin\ng\n i\nn\nte\nrf\na\nc\ne\ns\n*Reconfigurability of stimulation strategy and channel selection allows for a large range of stimulation electrode\ntypes for both the central or peripheral nervous system\nFig. 2. General system architecture illustrating three main blocks: An 8-\nbit asynchronous ripple counter (capturing the number of events in each\nintegration cycle), a binary-weighted current mirror that converts the number\nof pulses to a current level and a CIS biphasic waveform generator.\ngenerated directly provides the stimulation magnitude to be delivered\nvia the electrodes. In the sections following the sequence of events\nfrom trigger pulses to biphasic waveform will be described.\nIV. IMPLEMENTATION\nA. Spike-rate to current conversion\nThe spike-rate capture was implemented by means of an asyn-\nchronous ripple counter that is reset each stimulation cycle. Although\nthere exist several alternative methods of achieving this, the chosen\nmethod, generating a binary representation can then directly control\na binary weighted current mirror. Depending upon this value, binary\nweighted currents can be switched in and summed at a central node\n(Fig 3) to generate the required current. By doing this, using an\naccurate current mirror implementation, a monotonically increasing\ncurrent conversion is achievable, representing the pulse rate, essen-\ntially integrated over each CIS cycle.\nIin\nIout\nInxt\nx2 Block\nIin\nIout\nInxt\nx2 Block\nIin\nIout\nInxt\nx2 Block\nIsource\nIout\n2x Isource\n4x Isource 2n x Isource\nn1 2\nCntrl1 Cntrl2 Cntrln\ncontrol pulses from counter\nFig. 3. The method used to double the current reference in each of n-stages.\nThe output current, Iout, is fed to a summing junction if its binary weighting\nis high. For example, for an input 01, Ctrl1 will be high (i.e. feeds current)\nand Ctrl2 would be low.\nDue to the relatively variable efﬁciency for artiﬁcial electrical\nstimulation, a total of 256 output levels (in linear increments) is\nsufﬁcient for this system and thus an 8-bit counter was required.\nThis has been implemented using a cascade of asynchronous t-ﬂip-\nﬂops with common asynchronous reset for clearing the counter on\ncompletion of the output sequence for that CIS cycle. Furthermore,\nthis counter is disabled during the readout phase in order to prevent\nvariation in the output current during stimulation. In a similar fashion,\nthis inhibit signal is used at several points to prevent unnecessary\ncurrent ﬂow and therefore minimise power consumption. For exam-\nple, the current mirrors feeding the stimulation current are enabled\nonly during each channels stimulation slot. This spike-rate to current\nconversion hardware has been repeated for each channel. Although\nthis is not completely necessary for this CIS implementation, (i.e. the\noutput of the 8 counters could have been multiplexed such that only\na single binary-weighted current tree is required), a repeatable block\nis preferred to simplify timing constraints and maintain scalability.\nB. Continuous Interleave Sampling Controller\nThe CIS strategy is an attractive scheme for systems requiring\nmulti-channel stimulus delivered to stimulation points within close\nproximity. In comparison to alternative strategies, including com-\npressed analog (CA) and spectral peak (SPEAK), the CIS strategy\nhas been shown to be beneﬁcial [9], particularly in cochlear implant\ntechnology. One of its fundamental features is the reduction in cross-\ntalk between adjacent channels caused by interaction of electric ﬁelds\nfrom simultaneous electrode stimulation. Although conventional CIS\nstrategy is not highly accurate in residual charge balancing, introduc-\ning an electrode shorting phase in the stimulation cycle can eliminate\nthis effect.\n439\nIt is with this insight and ease of implementation that the CIS\nstrategy is chosen for this system. This was implemented by simply\ncycling through the different channels and sequentially driving the\nstimulation electrodes with their respective current stimulus. Thus\nno two channels are ever simultaneously active, reducing power\ndissipation. This is achieved by generating an enable sequence as\nshown in Fig. 4. In case more than two channels need to be active,\nrespective modules like the system described here can be combined\nin a single chip, featuring fewer individual channels to keep power\nconsumption low.\nEN1\nEN2\nEN8\n2A\nA\nT 2T\nBiphasic Pulse Adjustable Parameters:\nT = time period of clock\nA = stimulus current  amplitude\nFig. 4. Implemented CIS timing illustrating channel selection using a\nsequential enable phase. The biphasic output waveform is also shown.\nTo implement the sequence in Fig. 4, a 5-bit ripple counter is\nused, with the three most signiﬁcant bits being used to provide the\nvarious channel enable signals, therefore achieving four clock cycle\nstimulation phases as described below. With this clocking sequence,\nthe eight channels can be easily interleaved by using an address\ndecoder to provide the ”enable signal” for each channel. Modifying\nthe digital logic that governs the ”Enable Pulse Generator” (Fig. 5)\noperation will allow variation of the stimulation strategy.\n3-bit Counter\nEnable Pulse\nGenerator*\nCLK OUT\nEN\nCLK\nOUT1a\nOUT1b\nEN\nCLK\nOUT2a\nOUT2b\nEN\nCLK\nOUT8a\nOUT8b\nBiphasic pulse\ngenerators\n1\n2\n8\n3\n*Reconfigurable to allow channel and stimulation strategy selection\nFig. 5. Block diagram illustrating the implementation of the CIS strategy\nusing a 3-bit counter.\nC. Stimulation Stage\nIn light of the required stimulus parameters discussed earlier, the\nsystem was developed to generate a four phase stimulation waveform\nas shown in Fig. 4. In order to achieve such a four phase biphasic\nwaveform it is necessary to generate the necessary control signal to\ndrive the H-bridge conﬁguration as shown in Fig. 6. These switches\nare controlled by a 2-bit ripple counter, used to generate four phases\n(11,00,01,10).\nWhen the channel enable signal (shown in Fig. 4) is high for a\nparticular channel, then the clock signal is applied to the counter,\ngenerating the four phases (hence the need for four clock cycles,\nmentioned in IV-B). Combinational logic is included to guarantee\nthe start-up conditions, i.e. power-on-reset. The external enable signal\nIin\nEN EN\nE1 E2\nx1\nx2\nEN\n      \nx1\nx2\nx1\nx2S1 S2\nS3 S4\nFig. 6. Circuit schematic for the biphasic waveform generation using an H-\nbridge conﬁguration. The control signals are generated using the remaining\n2-bit counting sequence and the enable pulse to short the electrodes when\nchannel not in use (for residual charge removal).\nTABLE I\nCOMBINATIONAL LOGIC TO CONTROL BIPHASIC CURRENT STIMULATION\nE1 E2 S1 S2 S3 S4\nφ1 0 1 0 1 1 0\nφ1 0 0 0 0 1 1\nφ1 1 0 1 0 0 1\nφ1 1 0 1 0 0 1\nis used to ensure that the electrodes are shorted (as described in IV-\nB).\nThe combinational logic required to control the switches such\nthat the current sourcing and sinking electrodes are conﬁgured as\nrequired is shown in Table. I. Furthermore, during the last two phases\nit is necessary to halve the stimulus current, due to the waveform\nproﬁle described previously, i.e. to maintain area equivalence on both\npulses. This is achieved by switching in (in the last two phases) a\ncurrent mirror stage between the current generating stage (described\nin section IV-A) and the biphasic current control switches (Fig. 6).\nThe method followed here will be similar if the system has to\nbe designed with the charge-balancing pulse longer and lower in\namplitude. Moreover, the stimulus current amplitude, duration and\nfrequency can be adjusted, according to the application.\nV. SIMULATION RESULTS\nThis design is implemented in the AMS C35 CMOS technology\nusing the HITKIT (3.70) design kit under the Cadence IC (5.1.41)\ndesign environment. The ﬂoorplan and layout of the design submitted\nfor fabrication is shown in Fig. 7.\nThis section presents key simulation results, demonstrating in-\ntended system operation. Firstly, Fig. 8 illustrates a monotonically\nincreasing spike-to-current conversion over the entire output range\n(256 levels). One observation is regarding the charge injection during\nswitching, caused by the release of charge built-up during the off-\nstate. This is introduced at two levels, ﬁrstly at the switched binary-\nweighted current mirror, and secondly at the channel selection switch.\nHowever, although the peak charge-injection current magnitude\nseems signiﬁcant, the transition time is extremely short. Hence, the\nactual amount of charge injection is negligible (Fig. 8b). Furthermore,\nas previously mentioned, all inactive electrodes are routinely shorted\nduring the electrode reset phase, ensuring that no residual charge\nremains after stimulation.\nNext, Fig. 9 illustrates the biphasic waveform proﬁle generated\nin the CIS sequence. This simulation incorporates a neural interface\n440\nCh\nan\nne\nl 1\n: S\npi\nke\n c\nou\nnt\ner\n &\nbi\nph\nas\nic\n c\nur\nre\nnt\n g\nen\ner\nat\nor\nCIS sequencer\nCh\nan\nne\nl 4\n: S\npi\nke\n c\nou\nnt\ner\n &\n \nbi\nph\nas\nic\n c\nur\nre\nnt\n g\nen\ner\nat\nor\nCh\nan\nne\nl 5\n: S\npi\nke\n c\nou\nnt\ner\n &\n \nbi\nph\nas\nic\n c\nur\nre\nnt\n g\nen\ner\nat\nor\nCh\nan\nne\nl 3\n: S\npi\nke\n c\nou\nnt\ner\n &\n \nbi\nph\nas\nic\n c\nur\nre\nnt\n g\nen\ner\nat\nor\nCh\nan\nne\nl 2\n: S\npi\nke\n c\nou\nnt\ner\n &\n \nbi\nph\nas\nic\n c\nur\nre\nnt\n g\nen\ner\nat\nor\nBias\nFig. 7. Chip ﬂoorplan and lay of the spike-driven neural stimulator\n(a)\n(b)\nFig. 8. Monotonicity of the event-count to current conversion, illustrating:\n(a) switched current output, and (b) current integral, i.e. charge transfer\nmacromodel assuming a 1KΩ equivalent electrode impedance [10].\nThe pulse waveform is clearly visible to a have four phase proﬁle,\nfollowing: (i) a single period anodic current, (ii) a single period\nbreak and (iii) a two period attenuated (halved) cathodic current.\nAlso the CIS strategy can be observed, with each channel cycled\nthrough sequentially with no overlapping outputs.\nFig. 9. Biphasic pulse waveform generated sequentially using a CIS strategy.\nThe pulses have a four phase proﬁle, but with equal anodic/cathodic charge\ndistribution for removal of residual charge on the nerves\nVI. DISCUSSION\nThe design speciﬁcations for this system are summarised in Ta-\nble II. An architecture was presented for generating multi-channel\nbiphasic stimulus for neural stimulation using the CIS strategy. The\nsystem developed is the core of a generic, closed-loop sense-and-\nstimulate system architecture. The system is designed such that a\nTABLE II\nDESIGN SPECIFICATIONS FOR THE SPIKE-DRIVEN NEURAL STIMULATOR\nTechnology AMS 0.35μm 1P4M CMOS\nSupply voltage 3.3V\nSystem device count 4106\nChannels 8\nStimulation strategy Continuous Interleave Sampling (CIS)\nElectrode conﬁguration Bipolar\nStimulus proﬁle Complex biphasic waveform\nStimulus Range 100μA-5mA (tunable)\nInput/output dynamic range 48dB (8-bits)\nRefresh rate 100Hz-20kHz (tunable)\nArea 3mm2\nPower Consumption 0.7-25.2mW (stimulus current dependant)\nrange of input neural recording interfaces can be used at the front-\nend. The detected neural activity is then used to trigger stimulation.\nThe key stages described can be conﬁgured to generate a variety\nof stimulus waveform characteristics, and a combination of different\nelectrode conﬁgurations through different connection arrangements,\nmaking it appropriate for use on both central and peripheral nerves.\nIt was shown how an event-triggered address-encoder can be used\nto directly generate a biphasic current pulse whose magnitude is\nproportional to the input activity. The output stimulus pulse train\nwas optimum for tissue safety and can be incorporated in multipolar\nelectrode combinations. Its amplitude range can be externally pro-\ngrammed for different applications.\nBeing reconﬁgurable, the system presented here may lead to co-\nherence in neuroprosthetics research and will deliver highly efﬁcient\nrehabilitation practices.\nACKNOWLEDGMENT\nThe authors would like to thank Li Xiaoying and Pantelis Georgiou\nfor developing the complimentary sensor and transmitter hardware\nessential to complete the neural bridge. Also, we acknowledge Dr.\nJulius Georgiou of the University of Cyprus for providing us access\nto silicon fabrication for this work.\nREFERENCES\n[1] B. M. Bugbee, An Implantable Stimulation for Selective Stimulation\nof Nerves, schoolSepartment of Medical Physics and Bioengineering,\nUniversity College London. PhD thesis.\n[2] U. Stanic, “History of functional electrical stimulation,” INS & IFNESS\nJoint Congress, 16-20 September.\n[3] P. R. Troyk, N.de N. Donaldson, “Implantable fes stimulation systems:\nWhat is needed?,” International Neuromodulation Society, vol. 4, no. 3,\npp. 196–204, 2001.\n[4] R. W. B. Wilson, D. Lawson and S. Brill, “Speech processors for\nauditory prostheses,” tech. rep., Research Triangle Institute, 1999.\n[5] J. K. P.H. Peckham, “Functional electrical stimulation for neuromuscular\napplications,” Annual Review of Biomedical Engineering, no. 3, pp. 327–\n360, 2005.\n[6] J. L. et al, “Brief, noninjurious electric waveform stimulation of the\nbrain,” Science, vol. 121, no. 3144, pp. 468–469, 1955.\n[7] P. Ha¨ﬂiger, A Spike-based Learning Rule and its Implementation in\nAnalog Hardware. PhD thesis, ETH Zu¨rich, Switzerland, 2000.\n[8] A. F. Murray and A. V. W. Smith, “Asynchronous VLSI neural networks\nusing pulse-stream arithmetic,” IEEE Journal of Solid-State Circuits,\nvol. 23, no. 3, pp. 688–697, 1988.\n[9] B. S. Wilson et al, “Design and evaluation of a continuous interleaved\nsampling (CIS) processing strategy for multichannel cochlear implants,”\nJournal of Rehabilitation Research and Development, vol. 30, no. 1,\npp. 110–116, 1993.\n[10] Iasonas F. Triantis et al, “On Cuff Imbalance and Tripolar ENG Am-\npliﬁer Conﬁgurations,” IEEE Transactions on Biomedical Engineering,\nvol. 52, no. 2, 2005.\n441\n",
            "id": 317799,
            "identifiers": [
                {
                    "identifier": "190679874",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2099238986",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "294982",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:spiral.imperial.ac.uk:10044/1/510",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "10.1109/cne.2007.369703",
                    "type": "DOI"
                }
            ],
            "title": "Towards a Reconfigurable Sense-and-Stimulate Neural Interface Generating Biphasic Interleaved Stimulus",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2099238986",
            "oaiIds": [
                "oai:spiral.imperial.ac.uk:10044/1/510"
            ],
            "publishedDate": "2007-01-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 17774931,
                    "title": "A Spike-based Learning Rule and its Implementation in Analog Hardware.",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "P. H¨ aﬂiger, A Spike-based Learning Rule and its Implementation in Analog Hardware. PhD thesis, ETH Z¨ urich, Switzerland, 2000.",
                    "cites": null
                },
                {
                    "id": 17774925,
                    "title": "An Implantable Stimulation for Selective Stimulation of Nerves,",
                    "authors": [],
                    "date": null,
                    "doi": "10.1016/s1350-4533(01)00013-3",
                    "raw": "B. M. Bugbee, An Implantable Stimulation for Selective Stimulation of Nerves, schoolSepartment of Medical Physics and Bioengineering, University College London. PhD thesis.",
                    "cites": null
                },
                {
                    "id": 17774932,
                    "title": "Asynchronous VLSI neural networks using pulse-stream arithmetic,”",
                    "authors": [],
                    "date": "1988",
                    "doi": "10.1109/4.307",
                    "raw": "A. F. Murray and A. V. W. Smith, “Asynchronous VLSI neural networks using pulse-stream arithmetic,” IEEE Journal of Solid-State Circuits, vol. 23, no. 3, pp. 688–697, 1988.",
                    "cites": null
                },
                {
                    "id": 17774930,
                    "title": "Brief, noninjurious electric waveform stimulation of the brain,”",
                    "authors": [],
                    "date": "1955",
                    "doi": "10.1126/science.121.3144.468",
                    "raw": "J. L. et al, “Brief, noninjurious electric waveform stimulation of the brain,” Science, vol. 121, no. 3144, pp. 468–469, 1955.",
                    "cites": null
                },
                {
                    "id": 17774933,
                    "title": "Design and evaluation of a continuous interleaved sampling (CIS) processing strategy for multichannel cochlear implants,”",
                    "authors": [],
                    "date": "1993",
                    "doi": null,
                    "raw": "B. S. Wilson et al, “Design and evaluation of a continuous interleaved sampling (CIS) processing strategy for multichannel cochlear implants,” Journal of Rehabilitation Research and Development, vol. 30, no. 1, pp. 110–116, 1993.",
                    "cites": null
                },
                {
                    "id": 17774929,
                    "title": "Functional electrical stimulation for neuromuscular applications,”",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1146/annurev.bioeng.6.040803.140103",
                    "raw": "J. K. P.H. Peckham, “Functional electrical stimulation for neuromuscular applications,” Annual Review of Biomedical Engineering, no. 3, pp. 327– 360, 2005.",
                    "cites": null
                },
                {
                    "id": 17774926,
                    "title": "History of functional electrical stimulation,”",
                    "authors": [],
                    "date": null,
                    "doi": "10.1007/978-88-470-2916-3_26",
                    "raw": "U. Stanic, “History of functional electrical stimulation,” INS & IFNESS Joint Congress, 16-20 September.",
                    "cites": null
                },
                {
                    "id": 17774927,
                    "title": "Implantable fes stimulation systems: What is needed?,”",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1046/j.1525-1403.2001.00196.x",
                    "raw": "P. R. Troyk, N.de N. Donaldson, “Implantable fes stimulation systems: What is needed?,” International Neuromodulation Society, vol. 4, no. 3, pp. 196–204, 2001.",
                    "cites": null
                },
                {
                    "id": 17774928,
                    "title": "Speech processors for auditory prostheses,” tech. rep., Research Triangle Institute,",
                    "authors": [],
                    "date": "1999",
                    "doi": null,
                    "raw": "R. W. B. Wilson, D. Lawson and S. Brill, “Speech processors for auditory prostheses,” tech. rep., Research Triangle Institute, 1999.",
                    "cites": null
                },
                {
                    "id": 17774934,
                    "title": "Triantis et al, “On Cuff Imbalance and Tripolar ENG Ampliﬁer Conﬁgurations,”",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1109/tbme.2004.840470",
                    "raw": "Iasonas F. Triantis et al, “On Cuff Imbalance and Tripolar ENG Ampliﬁer Conﬁgurations,” IEEE Transactions on Biomedical Engineering, vol. 52, no. 2, 2005.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "http://spiral.imperial.ac.uk/bitstream/10044/1/510/1/108_0112.pdf"
            ],
            "updatedDate": "2021-10-27T10:24:19",
            "yearPublished": 2007,
            "journals": [],
            "links": [
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/317799"
                }
            ]
        },
        {
            "acceptedDate": "2007-10-06T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Afshari, F"
                },
                {
                    "name": "Jones, R"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/294969",
                "https://api.core.ac.uk/v3/outputs/264636850"
            ],
            "createdDate": "2012-02-19T10:47:56",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 105,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/105",
                    "logo": "https://api.core.ac.uk/data-providers/105/logo"
                }
            ],
            "depositedDate": "2007-10-02T00:00:00",
            "abstract": "Accepted versio",
            "documentType": "research",
            "doi": "10.1108/00330330710831567",
            "downloadUrl": "",
            "fieldOfStudy": "computer science",
            "fullText": "1Developing an integrated institutional repository at Imperial College London\nFereshteh Afshari and Richard Jones\nAuthors: \nFereshteh Afshari, IT Officer (Applications and Projects), Central Library, Imperial\nCollege, London. E­mail: f.afshari@imperial.ac.uk\nRichard Jones, Web and Database Technology Specialist, Imperial College, London.\nE­mail: richard.d.jones@imperial.ac.uk\nResearch Paper\nPurpose\nTo demonstrate how a highly integrated approach to repository development and\ndeployment can be beneficial in producing a successful archive.\nDesign/methodology/approach\nImperial College London undertook a significant specifications process to gather and\nformalise requirements for its repository system.  This was done through an initial\nproposal stage, and then the engagement of groups of College members with interest\nin the project to elucidate the requirements and allow the specification of a system\nthat would be of genuine benefit.  Then, using well understood technology for\ndistributed systems, such as web services, and a well understood repository platform\n(DSpace), it was possible to undertake that work inside a structured project.\nFindings\nDemonstrates the advantages of producing integrated systems, especially with regard\nto lowering adoption barriers through easing academics' deposit workflows,\nintroducing strong administrative tools for library administrators, and making\nresearch available in open access repositories in a well engineered environment.\nResearch limitations/implications \nThe service produced by the project is relatively new, and the long term benefits or\nfailings cannot yet be enumerated.  The paper looks primarily at the management and\norganisational issues but does not deal with the technical details to any great extent.\n2Practical implications\nA useful source of information for institutions considering heavy integration work and\nthe use of the PRINCE2 methodology for engaging institutional support.\nOriginality/value \nThis paper introduces a heavily integrated repository system within UK higher\neducation.  A lack of literature on this topic suggests that this paper could be\nbeneficial for others considering the same route.\nKeywords:\nImperial College; Institutional repository; Integration; Interoperability; DSpace; Web\nservices\nWord length: 6096\n1. Introduction\nIn July 2007 Imperial College (IC) London made public a fledgling institutional\nrepository (IR), Spir@l, of its academics' research publications\n(http://spiral.imperial.ac.uk), to coincide with its 100th anniversary as an institution\nand its new found independence from the University of London. Imperial College is\none of the highest ranked research institutions in the world, with around 8,000\nstudents and 3,000 research and related staff.  It is exclusively focussed on science\nand technology, with over 50% of the institution being given over to medicine.\nFigure 1 shows the opening page of Spir@l.\nTake in Figure 1.\nFigure 1. Opening page of Spir@l\n3Like many institutions developing repositories, IC faces the challenges of adoption\nand content acquisition that can limit the speedy growth of the repository. The\nexperiences in developing institutional repositories at Southampton University are\ndescribed by Simpson and Hey (2006) and at Edinburgh University by Jones and\nAndrew (2005).  This article looks at the choices that  IC Library made before going\ninto development and then production with this new service, and explains why,\nalthough a comparative latecomer among research institutions into the field, the\ntechniques and technologies employed might be more beneficial in the long run.   The\narticle looks primarily at the management and organisational issues as well as the\ndevelopment process, but does not deal with the technical details to any great extent.\nThe principal objective during the creation of this archive was to ensure adoption by\nembedding the repository so deeply in the institution's working practices, that barriers\nto adoption were significantly lowered, if not removed.  There were several drivers\naffecting this solution:\n• the College already had a central Publications system holding over 130,000\nbibliographic records of various different publication types (journal articles,\nresearch reports, book chapters etc.) from its researchers;\n• there had been a strong and long­standing demand from academics to make full\ntext available in their Professional Web Pages (PWPs) (see section 3.2 for more on\nPWPs).\nIt was therefore an obvious strategy to bring these existing systems together to\nprovide an integrated solution for the repository.  As an additional advantage, the\n4Library had previously taken part in the consortial London E­prints Access Project\n(http://www.sherpa­leap.ac.uk/about.html) in order to identify issues relating to\nsetting up a repository.  This original repository was based on the Eprints software\n(http://www.eprints.org/software/) from Southampton University, but for Spir@l, the\ndecision was taken to use DSpace for its perceived better flexibility in terms of the\ncode itself.\n2. Repository project\n2.1. Project aims\nIn 2005 the Library put forward a case for receiving central College funding for\nestablishing an institutional repository to provide an archival location for all the\nCollege's research output and to make these open access.  This was to include journal\narticles, conference papers, book chapters and all other content types available in the\nPublications system.  The stated aims and objectives of this project were: \n• to implement a quality controlled repository of academic publications;\n• to provide an open access repository (complaint with the standards of the\nOpen Access Initiative ­ Protocol for Metadata Harvesting (OAI­PMH))\nof research publications that will raise the visibility of  IC’s research;\n• to ensure that the content of the repository complies with copyright\nrequirements;\n• to draw on the bibliographic data already available in  IC’s publications\nsystem when creating metadata for repository content;\n• to develop procedures for ongoing collection of research publications for\ndeposit in the repository;\n• to establish links from  IC’s PWPs to the full text of academics’\npublications in the repository, thus giving higher visibility to research\noutput.\nA three year project was funded as a consequence of this proposal and a team formed\nto do the initial analysis, design and development work.  It was to be a collaborative\nwork between the Library and the Information and Communication and Technologies\n(ICT) department. \n52.2 Project management\nIC currently requires new projects to be managed under a PRINCE2\n(http://www.prince2.org.uk/web/site/home/home.asp) project management\nmethodology.  It therefore has a very well defined structure and series of groups of\npeople involved in giving advice to the project team.  Figure 2 gives details of the\ngroups involved.  \nTake in Figure 2\nFigure 2 Schematic view of Project Team structure\nThe Library and the ICT department each provided a Project Manager (PM) who\nworked together to develop either side of the project.  Each PM was jointly\nresponsible for the project team.  The ICT team consisted of a Software Engineer, a\nBusiness Analyst, and some Systems Administration time, while the Library side\nconsisted of a Project Officer, who was in charge of researching requirements, as well\nas several administrators who would aid in testing and who would be the final end­\nuser administrators of the system.  Above the Project Team sat several groups whose\nmembers helped in steering the project, and acted as a point of escalation should\nproblems arise.  The Advisory Board was a group consisting of senior academics and\nfaculty principals, as well as the PMs, the Project Director and representatives from\nICT management.  The User Community were consulted on the actual requirements\n6of the project as practising academics and administrators of varying levels (some very\nsenior) were involved.  They would also offer feedback on the current state of the\nsystem, and propose future work.  The Project Board acted as the prime steering\nmechanism in terms of College policy, and consisted of senior staff members such as\nthe Library Directory (Project Director), ICT management, and the College's Pro­\nRector for Postgraduate Studies.\nThe project obtained a lot of backing from the institution by using the PRINCE2\nframework.  This put the repository team in an excellent position having both top­\ndown and bottom­up support for the endeavour.  This was a significant advantage\nbecause it is often seen that the Library is the 'sole' creator, administrator and policy\nmaker of the repository (Lynch and Lippincott, 2005).  It may be a consequence of\nIC’s relatively recent arrival on the institutional repository scene, as is evident from\nits standing as a research institution, and the rate of IR adoption indicated in Lynch\n2005, that the benefits of institutional repositories are now more apparent, and some\nof the hardest lessons have already been learned.  These are issues such as:\n− how should the idea of the IR be sold both to the institution and to the academics?\n− the lowering of the technical barriers to producing IRs, allowing for more time to\nbe spent on yet more difficult areas;\n− the paradigm of mediated deposit has been formed and tested.\n3. Systems at IC prior to the integrated system\n In order to give a better understanding of the implementation of the integrated\nrepository at IC it is important to know the topology of the original systems prior to\nset up.\n3.1 The Publications system\n IC’s Publications database includes bibliographic details of academic works in\nscholarly journals which are automatically and regularly trawled from online\ndatabases such as PubMed and Web of Science.  It is not just a system for storing\ndata, but also provides several services to the IC management to enable it to assess\nand monitor the level of research performance and activity within departments.  In\naddition to holding information about existing staff publications, as soon as new\n7academics join IC their publications are automatically retrieved and placed in their\nworkspace.  Equally when they leave, their publications are removed from the system.\nAlthough the Publications database is an automated system for gathering material, the\nonus of managing lists of publications themselves is on the authors.  The system sends\ne­mail notifications to academics when it finds new articles for them. This is to:\n• inform them that new information has arrived in the database;\n• ask them to ‘approve’ the article to ensure that it really is theirs.  \nAcademics log in using their IC username and password and find a list of items\nawaiting their approval.  If the article does not belong to them, they can ‘decline’ it.\nThis process of ‘approve’ and ‘decline’ helps the system determine which\npublications may later be available in PWPs, and provides that extra human check on\nwhether the author in the metadata really is the IC staff member.  Figure 3 shows the\noutput an example academic might receive on logging in to the Publications database.\nInsert Figure 3\nFigure 3. Output an academic receives from the Publications database\n8Academics can also manually enter details of publications which have been missed\nout by the automatic trawling, or which have not yet been picked up in the online\ndatabases yet, such as pre­prints.   \nThe Publications database is also a core system for feeding other systems such as the\nPWPs, the Research Assessment Exercise (RAE) database, the grant application\nsystem and now also the repository.  Because of this, academics are encouraged to\nkeep their lists up to date, and to ensure that they have accepted or declined the\nrelevant records in their workspace.\nAnother feature of  the Publications database is its ability to identify, with a\nreasonable degree of accuracy, when two publication citations from different online\ndatabases are actually the same article – as can be seen in Figure 3 the “Red deer\nstags...” article has two entries under one record.  The system uses a variety of fuzzy\nmatching techniques to achieve this, and then marks the two records as ‘joined’ in the\nacademic's workspace.  This means that these records will be treated as effectively a\nsingle unit, unless the academic elects to ‘split’ them into two unique ones.  Likewise,\nif there are two records which are actually the same, they can be ‘joined’ manually if\nthe computer has not spotted the match.\nThis system is in active use by all academics at IC.  If they are too busy to deal with\ntheir publications themselves, they can nominate someone else to manage them on\ntheir behalf.  This nominated person may be a secretary or administrator who then can\n‘impersonate’ one, or several people, in a department.\nAs a whole, the metrics available to management, as discussed at the start of this\nsection show that the system is very popular among IC academics.  They see it as a\nvery valuable tool and essential to their professional activities, and which ensures that\nreferences to their work are always as up to date as possible.  It is clear, therefore,\nwhy integration with this Publications database would be a benefit to the repository.\n3.2 Professional Web Pages (PWPs)\nOne feature of  IC’s Content Management System is the PWPs.  This is a ‘site’ which\ncan be set up  by academics to contain their personal information, research interests,\nlists of publications, any commercial activities, and so on.   PWPs are divided into six\nsub­sections (Home; Personal information; Honours and Awards; Research;\nPublications; Teaching) and provide a standard, uniformly formatted personal website\nfor each academic.  Figure 4 shows part of the publications section of the PWP of one\nIC professor.\n9Take in Figure 4\nFigure 4. Extract of the publications section of one academic’s PWP\nThe PWPs are highly customisable and allow academics to display information for\ndifferent audiences.  For example, they can be configured to show only certain\ninformation to the public, while allowing College users access to a greater amount of\ninformation.  Often, the content of most sub­sections of these pages are populated\nautomatically from other systems.  The publications sub­section, as shown in Figure\n4, is populated directly by a controlled feed from the academic's workspace in the\nPublications database.  These pages have become very important to their owners, as\nthey represent their professional profile, are effectively an online CV, and the most\nlikely point of contact with their name through a search engine.  \nThe PWPs provide a unified ‘look and feel’ to the academics’ websites and conform\nto certain web and accessibility standards.  Because central web servers at IC host\nPWPs, departments do not need to use their own servers to host these nor to provide\nfacilities for backing up and maintenance.  The information on PWPs is always up to\ndate and, because they are managed centrally and looked after by a team of experts,\nlinks to these pages are always live. \n4. Spir@l: the integrated IC repository solution\n4.1 General overview\n10\nSpir@l comprises four distinct components as shown in the schematic diagram in\nFigure 5.\nTake in Figure 5.\nFigure 5. Schematic of Spir@l repository solution\nThe four components include:\n• the Publications system – to provide a unified interface for the IC academics.\nThis also includes the bibliographic details of all publications and the\nacademics’ interface for self deposit.\n• Spir@l Administrative Repository (Spir@lBib) – to deal with the\nadministrative workflows used by the library staff. This stores the file content\nof the full text of the publications as well as the licence information. \n11\n• Spir@l Public Repository (Spir@lPub) –  to provide public access to content\nwhich has passed successfully through the deposit and publication workflow.\n• PWPs – which are populated with links from the Publications database and\nSpir@lPub.\nSpir@lBib and Spir@lPub are both based on the DSpace repository platform\n(http://www.dspace.org).  The work of the project was to implement this set of\nconnections and responsibilities and an explanation of how this was achieved follows.\n4.2 The seamless interface\nAcademics interact with Spir@l via the Publications system, as this enables them to\naccess the repository facilities from within an interface which is popular and familiar\nto them, as well as in regular use.  The intention was to create an environment where\nacademics could not only manage their publications, but also could provide full­text\nfiles and check their copyright status too.  \nThis environment was designed to interact with the repository in a seamless fashion,\nwhere the deposit workflow was simplified, and from the academics' perspective the\ninterface would simply involve uploading their full­text files and granting a licence\nwithout needing to know what went on behind the scenes.  This addresses one of the\nmost significant problems in repository adoption and widespread use, which is that\nfilling in metadata forms is tedious and off­putting; the Spir@l overcame this with its\nautomated approach. \nTo design the ‘seamless interface’, the behaviour of academics was investigated and\nthe kind of functionality they needed for the deposit workflow was identified.  This\nincluded requests like: \n• everything should be available on one page; \n• there should be no delays in interaction; \n• it should be in a familiar style; \n• it should be informative and responsive.  \n12\n The requests were analysed and the processes was designed to provide the ‘seamless\ninterface’.   Throughout the design process the User Community was consulted –\nthese people worked closely with project staff to ensure that the repository met the\nacademics' requirements.  After an academic has logged on to the system and can see\na list of automatically harvested journal articles, a button linking to the repository\nmanagement page is shown.  The following sections describe this page in a little\ndetail, paying particular attention to how this is directly related to the repository\nmanagement.\n4.2.1 Uploading full­text files\nOnce in the ‘seamless interface’, the academic sees a brief description of the\npublication.  Below this, there is a section for uploading as many files as are\nnecessary.  As soon as a file is uploaded, a copy of it goes to the repository and a\nhyperlinked file name appears below the upload box; a copy of the file is at no point\nstored inside the Publications database.  At any point the academic can click on the\nfile name and view the content of the file.  If the file is uploaded in error, or needs to\nbe removed for any other reason, this can be done here too.  There is no specific\nlimitation on the number or type of files uploaded, leaving it to the academic to decide\non whether to include different versions of the work, addenda, corrections and any\nother related material. Figure 6 shows part of the upload interface for academics.\nTake in Figure 6\nFigure 6. File upload screen segment\n13\n4.2.2 Licensing\nThe Library at IC needs to confirm that academics have permission from the relevant\npublisher to make work publicly available.  There is a fairly standard licence available\nin the ‘seamless interface’, which consists of clauses covering the institution’s rights\nto store, disseminate, and preserve the work, as well as providing assurance that\ncopyright is owned by the author or permission has been given by the owner.  This\nlicence is combined with a Creative Commons Attribution, Non­Commercial, Share­\nAlike usage agreement (http://creativecommons.org/licenses/by­nc­sa/2.0/uk).  To\nmitigate against future changes in the length of the licence having an adverse effect on\nthe user interface, the licence is displayed in a scrollable text box, part of which is\nshown in Figure 7. \nTake in Figure 7\nFigure 7. Screenshot showing part of the Licensing process\n  \n14\nSince licensing and copyright issues would normally be addressed by repository staff\nin their advocacy programme, it was necessary to be especially careful with sending\nthe correct message to the academics.  Clear instructions and contact details on this\npage were provided so that academics know who to turn to with any queries.   It was\nunderstood that while preparing a work for deposit, the licence may be removed from\na publication at any time, therefore it was necessary to have this functionality\nembedded in the system.  This is because in the deposit workflow at IC, uploading\nfiles and granting the licence can happen in any order or combination.  Academics can\nassign a licence to their publication and can also withdraw it at a later date if\nnecessary; this is due to the collaborative nature of the environment (see section 4.2.5)\nand thus the need to allow previously performed actions to be undone if taken in\nerror.\n15\n4.2.3 SHERPA RoMEO and journal policies\nThe final section in the seamless interface introduces a link to the SHERPA RoMEO\n(http://www.sherpa.ac.uk/romeo.php) website. This enables academics to check the\ncopyright status of the journal in which their work is published or the publisher of the\nwork.  In discussions it was clear that academics in IC were concerned about\ncopyright issues. The link to the SHERPA RoMEO website enables academics to\ncheck the copyright details of their publications before submitting the full­text file.\nThe link to RoMEO opens a new window in the browser, so that the academic is not\ntaken away from the repository environment.  Irrespective of this, though, library\nadministrators also check the copyright of every item as it appears in their workflow,\nto reduce the risk of publication in error.\n4.2.4 Web services\nIn order to make the seamless interface work using Spir@lBib as its storage provider,\nit was necessary to design a set of web services which could transmit the file and\nlicence content from one server to another, and simultaneously manage all of the\nadditional workflow and status information that was necessary for administrators, but\nhad to be kept obscured from the academic.  It was decided that operations between\nthe two systems would be synchronous (i.e. in real time, with no caching or delayed\neffects), to improve the robustness of the information model.\n16\nThe advantage of using web services is that all communications are performed using\nXML over HTTP, and these common and well understood standards allow very\ndifferent systems, such as the Publications database and DSpace, to communicate in a\ncommon language, and without creating undue dependencies.\n4.2.5 Collaborative environment\nAn interesting consequence of the repository design, and the seamless interface is that\nit provides a collaborative deposit environment for IC academics, so they can see all\nthe actions relating to their own work as well as that of their co­authors.  For example,\nif a publication is produced by three IC academics and all three have approved this\nwork in the Publications system when one of these co­authors uploads the full­text of\nthe publication and then the other two co­authors log in to the system, they can also\nsee that the publication now has a file attached to it.  Equally, if a file has been\nqueried by Library staff, any one of these authors can satisfy the query and deal with\nit.    This functionality is a consequence of the Publications information model, which\noffers a many­to­many mapping between the list of publications and the list of IC\nstaff.\n4.3 Spir@lBib and Spir@lPub\n4.3.1 General overview\nThe repository policy for deposit is both self­deposit and mediated by the Library\nstaff.  Self­deposit would take place via the ‘seamless interface’, as described in the\nprevious section, and the Library would do the mediated deposit via the Spir@lBib\ninterface.   We wanted to take advantage of the quality and consistent metadata held in\nthe Publications system and use that to provide the basic bibliographic information in\nSpir@lBib, and later Spir@lPub.   In this way, the Library staff did not need to\nmanually enter metadata.   It was agreed that staff involved in the Publications system\nwould provide an interface to bulk ingest their records of approved journal articles,\nwhich, when all filtering considerations are taken into account, amounted to 80,000\nbibliographic records at the start of 2007.\nAfter this initial import, the Publications system would regularly feed the repository\nsystem with any updates, additions and deletions of this bibliographic data, and\nSpir@lBib would be free to issue ‘time­boxed’ bulk update requests as a safeguard\nagainst network failure.  In order to deal with ingest at this magnitude,  it was\nnecessary to consider a different approach to the structure of the repository and its\ndeposit workflow, as the default DSpace workflow\n17\n(http://dspace.org/technology/system­docs/functional.html#ingest)  was inadequate\nfor our needs; it is too rigid, and is not designed to scale to such a large degree of\nthroughput.\nOnce the bulk data had arrived, the Library staff needed to differentiate between\nrecords with full text and those without (the majority of records in the system).  To do\nthis, every record would have a series of status codes associated with its current\noverall state.  Upon arrival, every record would be assigned default status codes, and\nthese would be updated as actions were performed on the items.  For example, when a\nfile is added through the seamless interface the general status becomes ‘updated’, and\na field which monitors the state of files is updated to reflect the action.  Where a\npublication has a file and licence associated with it, its overall status would be\n‘Completed’, assembled from the status of all its component parts.  A set of  ‘saved\nsearches’ exist to allow Library staff to navigate the large database of items in their\nvarious conditions, and carry out the daily routines of dealing with incoming deposits.\nThe main activity for Spir@lBib is to deal with these ‘Completed’ publications; the\nLibrary staff check the details of the uploaded file against its bibliographic details,\nthen check its copyright status in RoMEO.  Once happy with all details,  Library staff\nassign the work to a Collection (or set of Collections) in the public repository and\nthen ask the system to deposit the publication in Spir@lPub.  If a RoMEO record does\nnot exist for the publisher, the library administrator must go directly to the publisher's\nwebsite for the self­archiving policy.\nAs soon as a full­text publication arrives in Spir@lPub, details of its URL would\nautomatically be sent to relevant PWPs via Publications, so that the list of\npublications on PWPs would also have a link to the repository record.  To allow for\nmultiple files, and to ensure that any relevant copyright statements are seen by users\nfollowing the links, the URL goes to a repository page which represents the item,\nrather than the content directly.\nThroughout this many staged deposit process, it was felt to be important to keep the\nauthors up to date with the state of their items in the workflow, and also to have a\npoint where they can be informed of any actions that they should take themselves.  To\ndo this an e­mail notification workflow has been built that operates over a web\nservice.  Whenever an identified action takes place, all authors of an item would\nreceive a system generated e­mail, in a digest form, summarising the Institutional\nRepository activities during the day.  These e­mails would include information about\npublications which had successfully been added to Spir@lPub or any which had been\nqueried (see below).  As a consequence of the deposit interface being a collaborative\nenvironment, the e­mails also work towards the same feature.  Not only the author\nwho was responsible for uploading the file or agreeing to the licence is informed, but\n18\nall co­authors would receive this e­mail too.  Effectively any of the co­authors could\ndeal with a query, for example.  Note that in the case of co­authors, we refer explicitly\nonly to authors based at Imperial College; collaborators from external organisations\nare expressly not dealt with by the system, no information other than their name in the\nmetadata is held about them, and the terms of the licensing indicates that IC authors\nhave sought their agreement for deposit of the material in the repository.\nHaving described the ‘basic’ workflow some of the complexities that might arise are\nnow discussed.  For instance, an academic may upload into the repository a file which\nis not acceptable for the public repository for a number of reasons.  The most likely of\nthese is that the publisher's copyright agreement does not permit deposit of that\nparticular version. Another reason for lack of acceptability into the public repository\nis that IC policy might restrict some research outcomes. In these cases, the Library\ncan mark a particular file with a ‘query’, and have that query appear within the\nPublications interface.  Using the notifications mechanism discussed above, all\nauthors are alerted to the query, and upon visiting the supplied link will find all the\ndetails they should need to diagnose, and hopefully fix, the problem.\nIndividual files containing the full text of publications, therefore, take on a role within\ntheir own micro­workflow inside the larger deposit process, which takes full\nadvantage of the power of the web services that manage interactions between these\nsystems.  Further to querying files, they can be marked as ‘deleted’ without physically\nremoving them, or marked as ‘generated’ if there is, for example, a PDF produced\nfrom another format.\n4.3.2 Metadata exchange with MODS\nSpir@lBib receives an initial data feed from the Publications system to form the basic\nmetadata in the repository system.  In order to do this, it was necessary to analyse the\ndatabase data structure at both ends and map the internal metadata onto our desired\nstructure, through an intermediate format.  The Metadata Object Description Schema\n(MODS ­ http://www.loc.gov/standards/mods/) standard was used for this conversion,\nwhere Publications creates the record, and Spir@lBib just needs to know how to\ninterpret it.  Any changes on either side of the system would not result in a problem\nfor the other side, provided that this intermediate format mapping is maintained.\nMODS offers a rich, hierarchic schema into which it is possible to encode the\npotentially complex data that is moved between systems.\nIn addition to the standard bibliographic details, a number of additional administrative\nmetadata fields are received, such as the IC usernames of the authors, the internal\n19\nidentifiers for the records, and the source database of the record imported.  These are\nalso encoded into the MODS record for the items which is sufficiently flexible and\ncomprehensive to support this additional data.\n4.3.3 Communities and collections\nDSpace structures its archive based on the idea of Communities, Collections, Items\nand Bitstreams (files). Institutional repositories developed using DSpace are using\nthese semantics in a variety of ways. At IC, Communities and Collections are used to\nrepresent the College's organisational structure and group publications in their related\nsubject areas.  Faculty staff were consulted to identify the level of detail required for\nthe Communities and Collections.  In some faculties the structure remained at the\ndepartment level, whereas in some others the research groups were also included.\nWith this structure, it is then relatively straightforward to take the incoming\nusernames and match their organisational details with the institutional identity\nmanagement system, and use this to determine the likely deposit location for\nincoming publications.  This is carried out during the metadata ingest process from\nPublications, and allows a single record to appear in as many collections as are\nnecessary for the collaborating authors.\n4.3.4 Management and administration\nWith an integrated system in mind a pool of people with different roles and\nresponsibilities was needed as described earlier in Figure 2.  In the ICT team,\nsomeone who could be responsible for the co­ordination of the server set­up,\nsupporting applications, network and security issues was needed as well as a Software\nEngineer to carry out the bulk of development work on DSpace.  A Business Analyst\nwas engaged to capture requirements from all parties and translate those requirements\nto functional specifications of the system.  During the final stages of the system\ndevelopment, it was also necessary to have a professional tester to examine the system\nthoroughly before its release to production.  \nFrom the Library side of the project, a Project Officer was needed to carry out the\ninitial research and investigation on different aspects of the repository.  These duties\nincluded:\n• investigating copyright issues; \n• defining day­to­day administration processes and general working principles;\n•  performing ongoing testing; \n• recommending new features; \n20\n• identifying metadata requirements;\n• maintaining project web pages.\n  Several repository administrators were later assigned to the project to carry out the\ntasks relating to copyright checking and publishing work to Spir@lPub.\nThe repository administrators are also responsible for the quality of the metadata that\ncomes in from Publications.  Despite coming from authority online sources or added\nmanually by academics, there are still potential problems with the data, such as\nmissing, inaccurate or incomplete details.  In these cases it is sometimes necessary for\nthe source itself to be corrected (for example, the PubMed record), as Spir@lBib is\nnot the authority control for the metadata itself.\nThe system was also required to provide administrative reports to the Library staff.  In\naddition to the complex workflow management, the ability to track exactly what had\nhappened to all items during their life­cycle was required.  This is, at least in part,\nbecause the workflow is about the present, and sometimes there is interest in the past.\nTherefore, audit trails of all actions on significant parts of the item that were\nidentified are kept inside the metadata itself, for easy interaction by the Library\nadministrators.  This includes times, dates and users for events such as file upload,\nmetadata modification, deposit and removal from the public repository and many\nother operations.\nAnother aspect of working in an integrated system is to appreciate the fact that the\nsystem is no longer stand­alone, but is a cluster of systems.  Therefore, any\ndevelopments or enhancements to the system require careful attention to other\nsystems with which it interacts.  If this is overlooked, the consequences of modifying\nor damaging a process which has implications across the distributed network can\nresult in service interruptions, delays, extra demands on resources and even additional\ncosts.\n5. Conclusion\nIt is early days to assess whether this approach will be a genuine success or not, but\nthis does not deter an initial evaluation of it in its own right.  The workflow for\nacademics has been simplified by offering them a seamless interface and allowing\nthem to interact with the repository from within the existing Publications system.\nHowever, this doesn't mean that it was simplified for the Library staff.  Considering\nthat there is still a large gap in what they need and what the technology can offer, the\nprovision of a smooth and flawless interface for them is still some way away.  There\nare a series of complex processes happening within Spir@lBib to cater for the Library\nstaff needs, and it has no comparison to the ease of use supplied for the academics.\n21\nBut as the processes in Spir@lBib are considerably more complex to achieve, a bulk\nof future development will be focussed around perfecting this solution.\nBy having provided an easy to use and attractive system at the front end it is hoped\nthat this will encourage academics to deposit more of their publications and achieve a\nhigher volume of full­text publications in the repository.  By having a critical mass in\nan OAI­PMH compliant repository, such as Spir@lPub, search engines and\naggregators will trawl the content of the repository and IC’s research output will have\na higher visibility.\nAcademics also see that links made to their full text in PWPs are an important feature\nin terms of visibility.  By doing this the direct needs of the academics have been\naddressed and persistent and reliable links to the full text are provided in an open\naccess manner. \n  Designing, developing and implementing an integrated repository has been a\nmassive learning curve for all those involved in the project.  Despite the fact that\nseveral teething problems were encountered, we believe that the benefits brought by\nthis approach outweigh its challenges.  Initial signs show that academics are\nsupportive and see the repository system as a crucial and beneficial feature and,\nencouragingly, find it easy to use.  We hope that by having reduced barriers inhibiting\nacademics from using the system, we have implemented a means whereby they are\nenabled to take an active role and provide more of their work.  The seamless interface\nhas enhanced the Publications system and provided an environment which is\nattractive and already familiar to academics, and which does not require them to fill in\nthe details of their publications.  PWPs are an important part of our academics'\nprofessional activities, and we added value to these pages by enabling full­text links\nto their publications lists.  The integrated repository, Spir@l, is now part of the\nCollege's information architecture and infrastructure. \nWe managed to achieve this because we received full support from the Library and\nICT management.  We had both top­down and grass­roots support from faculties and\nacademics respectively, and we had access to the skill sets required to complete such\nan endeavour.  We have been  fortunate that the College has understood the\nimportance of having an institutional repository, and we hope that in the medium term\nfuture that this system will be a leading example of integration and interoperability at\njust the right level to be truly productive.\nReferences\n22\nJones, R. and Andrew. T. (2005), “Open access, open source and e­theses: the\ndevelopment of the Edinburgh Research Archive”, Program: electronic library and\ninformation system, Vol. 39 No.3, pp. 198­212.\nLynch, C, Lippincott, JK. (2005), “Institutional repository deployment in the United\nStates as of early 2005”, D­Lib Magazine, Vol.11 No. 9.  Available at :\nhttp://www.dlib.org/dlib/september05/lynch/09lynch.html \nSimpson, P. and Hey, J. (2006), “Repositories for research: Southampton’s evolving\nrole in the knowledge cycle”, Program: electronic library and information system,\nVol. 40 No.3, pp.224­231.\n",
            "id": 317840,
            "identifiers": [
                {
                    "identifier": "oai:spiral.imperial.ac.uk:10044/1/493",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "264636850",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "294969",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "1989971116",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "10.1108/00330330710831567",
                    "type": "DOI"
                }
            ],
            "title": "Developing an integrated institutional repository at Imperial College London.",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "1989971116",
            "oaiIds": [
                "oai:spiral.imperial.ac.uk:10044/1/493"
            ],
            "publishedDate": "2007-09-01T00:00:00",
            "publisher": "'Emerald'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [],
            "updatedDate": "2021-10-27T10:24:19",
            "yearPublished": 2007,
            "journals": [
                {
                    "title": "Program electronic library and information systems",
                    "identifiers": [
                        "issn:0033-0337",
                        "0033-0337"
                    ]
                }
            ],
            "links": [
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/317840"
                }
            ]
        },
        {
            "acceptedDate": "2004-11-13T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Comerlato, N M"
                },
                {
                    "name": "Ferreira, G B"
                },
                {
                    "name": "Skakle, Jan"
                },
                {
                    "name": "Wardell, J L"
                }
            ],
            "contributors": [
                "University of Aberdeen.Chemistry",
                "University of Aberdeen.Physics"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/193897676",
                "https://api.core.ac.uk/v3/outputs/281271"
            ],
            "createdDate": "2012-02-15T16:35:38",
            "dataProviders": [
                {
                    "id": 1,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/1",
                    "logo": "https://api.core.ac.uk/data-providers/1/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2004-11-13T00:00:00",
            "abstract": "Peer reviewedPublisher PD",
            "documentType": "research",
            "doi": "10.1107/s1600536804028387",
            "downloadUrl": "https://core.ac.uk/download/281271.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "electronic reprint\nActa Crystallographica Section E\nStructure Reports\nOnline\nISSN 1600-5368\nEditors: W. Clegg and D. G. Watson\n4,5-Bis(benzoylsulfanyl)-1,3-dithiol-2-one\nNadia M. Comerlato, Gla´ucio B. Ferreira, Janet M. S. Skakle and James L. Wardell\nCopyright © International Union of Crystallography\nAuthor(s) of this paper may load this reprint on their own web site provided that this cover page is retained. Republication of this article or its\nstorage in electronic databases or the like is not permitted without prior permission in writing from the IUCr.\nActa Cryst. (2004). E60, o2284–o2286 Nadia M. Comerlato et al. \u000f C17H10O3S4\norganic papers\no2284 Nadia M. Comerlato et al. \u000f C17H10O3S4 doi: 10.1107/S1600536804028387 Acta Cryst. (2004). E60, o2284±o2286\nActa Crystallographica Section E\nStructure Reports\nOnline\nISSN 1600-5368\n4,5-Bis(benzoylsulfanyl)-1,3-dithiol-2-one\nNadia M. Comerlato,a GlaÂucio B.\nFerreira,a Janet M. S. Skakleb*\nand James L. Wardella\naDepartamento de QuõÂmica InorgaÃnica, Instituto\nde QuõÂmica, Universidade Federal do Rio de\nJaneiro, CP 68563, 21945-970 Rio de Janeiro,\nRJ, Brazil, and bDepartment of Chemistry,\nUniversity of Aberdeen, Meston Walk, Aberdeen\nAB24 3UE, Scotland\nCorrespondence e-mail: j.skakle@abdn.ac.uk\nKey indicators\nSingle-crystal X-ray study\nT = 120 K\nMean \u001b(C±C) = 0.002 AÊ\nR factor = 0.025\nwR factor = 0.065\nData-to-parameter ratio = 16.8\nFor details of how these key indicators were\nautomatically derived from the article, see\nhttp://journals.iucr.org/e.\n# 2004 International Union of Crystallography\nPrinted in Great Britain ± all rights reserved\nThe title compound [systematic name: S,S0-2-oxo-1,3-dithiol-\n4,5-diyl bis(thiobenzoate)], C17H10O3S4, obtained from 4,5-\nbis(benzoylsulfanyl)-1,3-dithiole-2-thione and mercury(II)\nacetate in acetic acid/chloroform, exists as isolated molecules\nwith no signi®cant intermolecular S\u0001 \u0001 \u0001S, S\u0001 \u0001 \u0001O or O\u0001 \u0001 \u0001O\ninteractions.\nComment\nThe title compound, 4,5-bis(benzoylsulfanyl)-1,3-dithiol-2-\none, (I), and the zincate salts [Q]2[Zn(dmio)2] (Q is the onium\ncation and dmio is 2-oxo-1,3-dithiole-4,5-dithiolate) are very\nuseful stable sources of the dmio dianion and have found\nextended use as precursors of both organic dmio compounds\nand metal±dmio complexes. Additionally, dmio compounds,\nsuch as the title compound, are good sources of tetra-\nthiafulvalenes on reaction with phosphites (Svenstrup &\nBecher, 1995).\nWhile the crystal structure of a Zn(dmio)2 salt has been\nreported (Candiota et al., 2003), no previous study of the\nstructure of (I) has been reported.\nReceived 2 November 2004\nAccepted 4 November 2004\nOnline 13 November 2004\nFigure 1\nThe molecule of (I), showing the atom-labelling scheme. Displacement\nellipsoids are drawn at the 50% probability level and H atoms are shown\nas circles of arbitrary radii.\nelectronic reprint\nBond lengths and angles within the ®ve-membered dmio\nring in (I) are within the ranges found for other dmio\ncompounds, such as [Q][Sn(dmio)3] and [Q][Zn(dmio)2]\n(Candiota et al., 2003; Chohan et al., 2003; Aupers et al., 2002;\nde Assis et al., 1999).\nThe dmio ring, together with the attached carbonyl O atom,\nis essentially planar, with S1 showing the largest deviation\n[0.0158 (6) AÊ ] from the mean plane. The two phenyl rings are\ninclined at angles of 78.60 (4) (C11±C16) and 6.94 (8)\u000e (C21±\nC26) to the dmio ring. Molecules of (I) show no strong asso-\nciation with each other, the closest intermolecular S\u0001 \u0001 \u0001S,\nS\u0001 \u0001 \u0001O and O\u0001 \u0001 \u0001O separations being 3.6561 (5), 3.4524 (12)\nand 3.1479 (17) AÊ , respectively, all just outside the van der\nWaals radii sum for the appropriate atoms; van der Waals radii\nfor S and O are taken as 1.80 and 1.52 AÊ , respectively\n(PLATON; Spek, 2004).\nThe structure of the analogous 4,5-bis(benzoylsulfanyl)-1,3-\ndithiole-2-thione compound, (II), has been reported at both\n120 (Cox & Doidge-Harrison, 1996) and 288 K (Solans et al.,\n1987). There are weak CÐH\u0001 \u0001 \u0001O and S\u0001 \u0001 \u0001S intermolecular\ninteractions in (II).\nExperimental\nThe title compound was prepared using a modi®cation of a published\nmethod (Hartke et al., 1980). A solution of mercury(II) acetate\n(4.78 g, 15.0 mmol) in glacial acetic acid (120 ml) was added with\nvigorous agitation to a solution of 4,5-bis(benzoylsulfanyl)-1,3-di-\nthiole-2-thione [(II); 6.09 g, 15.0 mmol] (Steimecke, 1979) in chloro-\nform (120 ml). After re¯uxing for 5 h, the reaction mixture was\n®ltered, and the ®ltrate was successively washed with water, saturated\naqueous sodium bicarbonate solution and more water, dried over\nMgSO4 and evaporated to leave a yellow solid, which was recrys-\ntallized from chloroform/methanol (yield 54%, m.p. 388±389 K). IR\n(CsI, cmÿ1): 3083 (\u0017 CÐH), 1701, 1697, 1668, 1621 (\u0017 C O), 1467 (\u0017\nC C), 896 (\u0017 CÐS).\nCrystal data\nC17H10O3S4\nMr = 390.49\nMonoclinic, C2=c\na = 35.6460 (6) AÊ\nb = 5.20360 (10) AÊ\nc = 19.1402 (3) AÊ\n\f = 116.0945 (8)\u000e\nV = 3188.39 (10) AÊ 3\nZ = 8\nDx = 1.627 Mg m\nÿ3\nMo K\u000b radiation\nCell parameters from 4041\nre¯ections\n\u0012 = 2.7±27.5\u000e\n\u0016 = 0.61 mmÿ1\nT = 120 (2) K\nBlock, colourless\n0.60 \u0002 0.30 \u0002 0.20 mm\nData collection\nBruker±Nonius KappaCCD\ndiffractometer with rotating-\nanode source\n’ and ! scans\nAbsorption correction: multi-scan\n(SADABS; Sheldrick, 2003)\nTmin = 0.787, Tmax = 0.883\n25 707 measured re¯ections\n3656 independent re¯ections\n3191 re¯ections with I > 2\u001b(I)\nRint = 0.030\n\u0012max = 27.5\n\u000e\nh = ÿ46 ! 46\nk = ÿ6 ! 6\nl = ÿ24 ! 24\nRefinement\nRe®nement on F 2\nR[F 2 > 2\u001b(F 2)] = 0.025\nwR(F 2) = 0.065\nS = 1.06\n3656 re¯ections\n217 parameters\nH-atom parameters constrained\nw = 1/[\u001b2(Fo\n2) + (0.031P)2\n+ 3.4602P]\nwhere P = (Fo\n2 + 2Fc\n2)/3\n(\u0001/\u001b)max = 0.001\n\u0001\u001amax = 0.26 e AÊ\nÿ3\n\u0001\u001amin = ÿ0.36 e AÊ ÿ3\nTable 1\nHydrogen-bonding geometry (AÊ , \u000e).\nDÐH\u0001 \u0001 \u0001A DÐH H\u0001 \u0001 \u0001A D\u0001 \u0001 \u0001A DÐH\u0001 \u0001 \u0001A\nC16ÐH16\u0001 \u0001 \u0001S3 0.95 2.59 3.0392 (15) 109\nC26ÐH26\u0001 \u0001 \u0001S4 0.95 2.58 3.0146 (14) 108\nAll H atoms were ®rst identi®ed in a difference map and then\nplaced in geometrical positions and re®ned using a riding model with\nC±H distances of 0.95 AÊ . Analysis of molecular interactions was\ncarried out using PLATON (Spek, 2004).\nData collection: COLLECT (Hooft, 1998); cell re®nement:\nDENZO (Otwinowski & Minor, 1997) and COLLECT; data reduc-\ntion: DENZO and COLLECT; program(s) used to solve structure:\nSHELXS97 (Sheldrick, 1997); program(s) used to re®ne structure:\nSHELXL97 (Sheldrick, 1997); molecular graphics: ORTEP-3 for\nWindows (Farrugia, 1997); software used to prepare material for\npublication: SHELXL97.\nWe thank the EPSRC's X-ray Crystallographic Service,\nUniversity of Southampton, for the collection of the data. The\nBrazil-based authors thank CNPq, CAPES and FUJB for\nsupport.\nReferences\nAssis, F. de, Chohan, Z. H., Howie, R. A., Khan, A., Low, J. N., Spencer, G. M.,\nWardell, J. L. & Wardell, S. M. S. V. (1999). Polyhedron, 18, 3533±3544.\nAupers, J. H., Chohan, Z. H., Comerlato, N. M., Howie, R. A., Silvino, A. C.,\nWardell, J. L. & Wardell, S. M. S. V. (2002). Polyhedron, 21, 2107±2116.\nCandiota, R. O., Comerlato, N. M., Howie, R. A. & Wardell, J. L. (2003). Acta\nCryst. E59, m599±m601.\nChohan, Z. H., Comerlato, N. M., Howie, R. A., Skakle, J. M. S. & Wardell, J. L.\n(2003). Acta Cryst. E59, m1006±m1009.\nCox, P. J. & Doidge-Harrison, S. M. S. V. (1996). Acta Cryst. C52, 720±722.\nFarrugia, L. J. (1997). J. Appl. Cryst. 30, 565.\nHartke, K., Kissel, T., Quante, J. & Matusch, R. (1980). Chem. Ber. 113, 1898±\n1906.\nHooft, R. (1998). COLLECT. Nonius BV, Delft, The Netherlands.\nOtwinowski, Z. & Minor, W. (1997). Methods in Enzymology, Vol. 276,\nMacromolecular Crystallography, Part A, edited by C. W. Carter Jr and\nR. M. Sweet, pp. 307±326. New York: Academic Press.\nSheldrick, G. M. (1997). SHELXS97 and SHELXL97. University of\nGoÈ ttingen, Germany.\norganic papers\nActa Cryst. (2004). E60, o2284±o2286 Nadia M. Comerlato et al. \u000f C17H10O3S4 o2285\nFigure 2\nThe unit cell contents of (I), projected on to the (101) plane.\nelectronic reprint\nSheldrick, G. M. (2003). SADABS. Version 2.10. University of GoÈ ttingen,\nGermany.\nSolans, X., Font-Bardia, M., Font-Altaba, M., Vicente, R. & Segum, A. (1987).\nActa Cryst. C43, 1415±1417.\nSpek, A. L. (2004). PLATON. April 2004 version. University of Utrecht, The\nNetherlands.\nSteimecke, G. (1979). Phosphorus Sulfur, 7, 49±55.\nSvenstrup, N. & Becher, J. (1995). Synthesis, pp. 215±235.\norganic papers\no2286 Nadia M. Comerlato et al. \u000f C17H10O3S4 Acta Cryst. (2004). E60, o2284±o2286\nelectronic reprint\n",
            "id": 311277,
            "identifiers": [
                {
                    "identifier": "10.1107/s1600536804028387",
                    "type": "DOI"
                },
                {
                    "identifier": "2098651841",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "281271",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:aura.abdn.ac.uk:2164/2158",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "193897676",
                    "type": "CORE_ID"
                }
            ],
            "title": "4,5-bis(benzoylsulfanyl)-1,3-dithiol-2-one",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2098651841",
            "oaiIds": [
                "oai:aura.abdn.ac.uk:2164/2158"
            ],
            "publishedDate": "2004-12-03T00:00:00",
            "publisher": "'International Union of Crystallography (IUCr)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 3835,
                    "title": "Methods in Enzymology, Vol. 276, Macromolecular Crystallography, Part A, edited by",
                    "authors": [],
                    "date": "1997",
                    "doi": "10.1016/s0076-6879(97)76066-x",
                    "raw": "Otwinowski, Z. & Minor, W. (1997). Methods in Enzymology, Vol. 276, Macromolecular Crystallography, Part A, edited by C. W. Carter Jr and R. M. Sweet, pp. 307±326. New York: Academic Press.",
                    "cites": null
                },
                {
                    "id": 3837,
                    "title": "organic papers Acta Cryst.",
                    "authors": [],
                    "date": "2004",
                    "doi": null,
                    "raw": null,
                    "cites": null
                },
                {
                    "id": 3836,
                    "title": "SHELXS97 and SHELXL97.",
                    "authors": [],
                    "date": "1997",
                    "doi": null,
                    "raw": null,
                    "cites": null
                },
                {
                    "id": 6018892,
                    "title": "SHELXS97 and SHELXL97. University of Go Èttingen, Germany. organic papers Acta Cryst.",
                    "authors": [],
                    "date": "1997",
                    "doi": null,
                    "raw": "Sheldrick, G. M. (1997). SHELXS97 and SHELXL97. University of Go Èttingen, Germany. organic papers Acta Cryst. (2004). E60, o2284–o2286 Nadia M. Comerlato et al.  C17H10O3S4 o2285",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "https://aura.abdn.ac.uk/bitstream/2164/2158/1/e60_o2284.pdf"
            ],
            "updatedDate": "2022-03-16T07:24:35",
            "yearPublished": 2004,
            "journals": [
                {
                    "title": "Acta Crystallographica Section E Structure Reports Online",
                    "identifiers": [
                        "1600-5368",
                        "issn:1600-5368"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/281271.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/281271"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/281271/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/281271/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/311277"
                }
            ]
        },
        {
            "acceptedDate": "2008-03-16T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Achim J. Lilienthal"
                },
                {
                    "name": "Beveridge"
                },
                {
                    "name": "Canny"
                },
                {
                    "name": "Felzenszwalb"
                },
                {
                    "name": "Freund"
                },
                {
                    "name": "Früh"
                },
                {
                    "name": "Gonzales"
                },
                {
                    "name": "Huber"
                },
                {
                    "name": "Martin Persson"
                },
                {
                    "name": "Mayer"
                },
                {
                    "name": "Mueller"
                },
                {
                    "name": "Persson"
                },
                {
                    "name": "Surmann"
                },
                {
                    "name": "Thrun"
                },
                {
                    "name": "Tom Duckett"
                },
                {
                    "name": "Tupin"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/159359665",
                "https://api.core.ac.uk/v3/outputs/159359672",
                "https://api.core.ac.uk/v3/outputs/159460337",
                "https://api.core.ac.uk/v3/outputs/158634297",
                "https://api.core.ac.uk/v3/outputs/159460344",
                "https://api.core.ac.uk/v3/outputs/1644633",
                "https://api.core.ac.uk/v3/outputs/158634351",
                "https://api.core.ac.uk/v3/outputs/191739773"
            ],
            "createdDate": "2012-04-12T18:29:53",
            "dataProviders": [
                {
                    "id": 128,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/128",
                    "logo": "https://api.core.ac.uk/data-providers/128/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 1910,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/1910",
                    "logo": "https://api.core.ac.uk/data-providers/1910/logo"
                },
                {
                    "id": 2808,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/2808",
                    "logo": "https://api.core.ac.uk/data-providers/2808/logo"
                },
                {
                    "id": 905,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/905",
                    "logo": "https://api.core.ac.uk/data-providers/905/logo"
                }
            ],
            "depositedDate": "2008-06-01T00:00:00",
            "abstract": "This work investigates the use of semantic information to link ground level occupancy maps and aerial images. A ground level semantic map, which shows open ground and indicates the probability of cells being occupied by walls of buildings, is obtained by a mobile robot equipped with an omnidirectional camera, GPS and a laser range finder. This semantic information is used for local and global segmentation of an aerial image. The result is a map where the semantic information has been extended beyond the range of the robot sensors and predicts where the mobile robot can find buildings and potentially driveable ground",
            "documentType": "research",
            "doi": "10.1016/j.robot.2008.03.002",
            "downloadUrl": "https://core.ac.uk/download/1644633.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "Fusion of Aerial Images and Sensor Data from a Ground Vehicle for Improved\nSemantic Mapping\nMartin Persson a,∗,1 Tom Duckett b Achim J. Lilienthal a\naCenter for Applied Autonomous Sensor Systems, Department of Technology,\nO¨rebro University, O¨rebro, Sweden\nbDepartment of Computing and Informatics, University of Lincoln, Lincoln, UK\nAbstract\nThis work investigates the use of semantic information to link ground level occupancy maps and aerial images. A ground level\nsemantic map, which shows open ground and indicates the probability of cells being occupied by walls of buildings, is obtained\nby a mobile robot equipped with an omnidirectional camera, GPS and a laser range finder. This semantic information is used for\nlocal and global segmentation of an aerial image. The result is a map where the semantic information has been extended beyond\nthe range of the robot sensors and predicts where the mobile robot can find buildings and potentially driveable ground.\nKey words:\nsemantic mapping, semi-supervised learning, aerial image, mobile robot\n1. Introduction\nA mobile robot has a limited view of its environment.\nMapping of the operational area is one way of enhancing\nthis view for visited locations. In this work we explore the\npossibility of using information extracted from aerial im-\nages to further improve the mapping process. Semantic in-\nformation about buildings is used as the link between the\nground level information and the aerial image. The method\ncan speed up exploration or planning in areas not yet vis-\nited by the robot.\nColour image segmentation is often used to extract infor-\nmation about buildings from aerial images. However, au-\ntomatic detection of buildings in monocular aerial images\nwithout elevation information is hard. Buildings cannot\neasily be separated from other man-made structures such\nas driveways, tennis courts, etc. due to the resemblance in\ncolour and shape. We show that wall estimates found by a\nmobile robot can compensate for the absence of elevation\ndata.\n∗ Corresponding author.\nEmail addresses: martin.persson@tech.oru.se (Martin\nPersson), tduckett@lincoln.ac.uk (Tom Duckett),\nachim@lilienthals.de (Achim J. Lilienthal).\n1 Supported by the Swedish Defence Material Administration\nThis work builds upon previous published work. In [1]\nwe defined a virtual sensor 2 . With an occupancy grid map\nand a virtual sensor learned to separate buildings from non-\nbuildings we have a method to build a probabilistic seman-\ntic map [2]. In [3] we showed how wall estimates extracted\nfrom this probabilistic semantic map could be used for de-\ntection of buildings in aerial images by their roof outlines.\nTo determine potential matches between the wall estimates\nand the roof outlines we used geo-referenced aerial images\nand an absolute positioning system (GPS) on-board the\nmobile robot. The matched lines were then used in region-\nand boundary-based segmentation of the aerial image for\ndetection of buildings.\nIn this work we extend the approach from [3]. The exten-\nsion includes global segmentation of buildings in the aerial\nimage, the introduction of a new class for ground (which\nmay be driveable by the robot) and the establishment of\nthe concept and framework of the predictive map. The pur-\npose is to detect building outlines and driveable paths faster\nthan the mobile robot can explore the area by itself. Using\nthis method, the robot can estimate the outline of found\nbuildings and “see” around one or several corners without\nactually visiting the area. The method does not assume a\n2 A virtual sensor is understood as one or several physical sensors\nwith a dedicated signal processing unit for recognition of real world\nconcepts.\nPreprint submitted to Elsevier 14 January 2008\nperfectly up-to-date aerial image; buildings may be miss-\ning although they are present in the aerial image, and vice\nversa. It is therefore possible to use globally available 3 geo-\nreferenced images.\n1.1. Related Work\nOverhead images have been used in combination with\nground vehicles in a number of applications. Oh et al. [4]\nused map data to bias a robot motion model in a Bayesian\nfilter to areas with higher probability of robot presence. It\nwas assumed that probable paths were known in the map.\nSince mobile robot trajectories are more likely to follow\nthose paths in the map, GPS position errors due to reflec-\ntions from buildings were compensated using the map pri-\nors.\nPictorial information such as aerial photos and city-maps\nhave been used for registration of sub-maps and subsequent\nloop-closing in SLAM [5]. Aerial images were used by Fru¨h\nand Zakhor in Monte Carlo localization of a truck during\nurban 3D modeling [6].\nSilver et al. [7] discuss registration of heterogeneous data\n(e.g. data recorded with different sampling density) from\naerial surveys and the use of these data in classification of\nground surface. Cost maps are produced that can be used\nin long range vehicle navigation. Scrapper et al. [8] used\nheterogeneous data from, e.g., maps and aerial surveys to\nconstruct a world model with semantic labels. This model\nwas compared with vehicle sensor views providing a fast\nscene interpretation.\nFor detection of man-made objects in aerial images, lines\nand edges together with elevation data are the features that\nare used most often. Building detection in single monoc-\nular aerial images is very hard without additional eleva-\ntion data [9]. Mayer’s survey [10] describes some existing\nsystems for building detection and concludes that scale,\ncontext and 3D structure were the three most important\nfeatures to consider for object extraction in aerial images.\nFusion of SAR (Synthetic Aperture Radar) and aerial im-\nages has been employed for detection of building outlines\n[9]. The building location was established in the overhead\nSAR image, where walls from one side of buildings can be\ndetected. The complete building outline was then found\nusing edge detection in the aerial image. Parallel and per-\npendicular edges were considered and the method belongs\nto edge-only segmentation approaches. This work is simi-\nlar to ours in the sense that it uses a partly found building\noutline to segment a building from an aerial image.\nCombination of edge and region information for segmen-\ntation of aerial images has been suggested in several publi-\ncations. Two papers that have influenced our work are [11]\nand [12]. Mueller et al. [11] presented a method to detect\nagricultural fields in satellite images. First, the most rele-\nvant edges were detected. These were then used to guide\n3 E.g. Google Earth, Microsoft Virtual Earth, and satellite images\nfrom IKONOS and its successors.\nboth the smoothing of the image and the following seg-\nmentation in the form of region growing. Freixenet et al.\n[12] investigated different methods for integrating region-\nand boundary-based segmentation, and also claim that this\ncombination is the best approach for image segmentation.\n1.2. Outline and Overview\nThe presentation of our proposed system is divided into\nthree main parts. The first part, Section 2, concerns the es-\ntimation of walls by the mobile robot and edge detection\nin the aerial image. At ground level wall estimates are ex-\ntracted from a probabilistic semantic map. This map is ba-\nsically an occupancy map built from range data and labeled\nusing a virtual sensor for building detection [1] mounted on\nthe mobile robot. The second part describes matching of\nwall estimates from the mobile robot with the edges found\nin the aerial image. This procedure is described in Section\n3. The third part presents the segmentation of an aerial im-\nage based on the matched lines. Section 4 deals with local\nsegmentation to find buildings and Section 5 extends this\nto a global segmentation of the aerial image and also intro-\nduces the class for ground. Details of the mobile robot, the\nexperiments performed and the results obtained are found\nin Section 6. Finally, the paper is concluded and sugges-\ntions for future work are given in Section 7.\n2. Wall Candidates\nA major problem for building detection in aerial images\nis to decide which of the edges in the aerial image corre-\nspond to building outlines. The idea of our approach is to\nmatch wall estimates extracted from two perspectives in\norder to increase the probability that a correct segmenta-\ntion is achieved. In this section we describe the process of\nextracting wall candidates, first from the mobile robot’s\nperspective and then from aerial images.\n2.1. Wall Candidates from Ground Perspective\nThe wall candidates from the ground perspective are ex-\ntracted from a semantic map acquired by a mobile robot.\nThe semantic map we use is a probabilistic occupancy grid\nmap with two classes: buildings and non-buildings [2]. The\nprobabilistic semantic map is produced using an algorithm\nthat fuses different sensor modalities. In this paper, a 2D\nrange sensor is used to build an occupancy map, which is\nconverted into a probabilistic semantic map using the out-\nput of a virtual sensor for building detection based on im-\nages from an omnidirectional camera.\nThe algorithm consists of two parts. First, a local se-\nmantic map is built using the occupancy map and the out-\nput from the virtual sensor. The virtual sensor uses the\nAdaBoost algorithm [13] to train a classifier that classi-\nfies close range monocular gray scale images taken by the\nmobile robot as buildings or non-buildings. This generic\n2\nmethod combines different types of features such as edge\norientation, gray level clustering and corners into a system\nwith high classification rate [1].\nThe classification by the virtual sensor is made for pla-\nnar sub-images, see Section 6.1. However, the image may\nalso contain parts that do not belong to the detected class,\ne.g., an image of a building might also include some vegeta-\ntion, i.e. non-building such as a tree. In order to deal with\nsuch situations, probabilities are assigned to the occupied\ncells that are within a sector, with an opening angle θ, rep-\nresenting the view of the virtual sensor. The size of the cell\nformations within the sector affects the probability values.\nThese sizes are measured by the horizontal covering angles\n{αi} = α1, α2, . . . , αn of the objects within the particular\nview. A sector is illustrated in Figure 1.\nFig. 1. Illustration of a sector with an opening angle θ and length\nLV S representing the view of the virtual sensor. Two objects are\nfound (the grey rectangles) within the sector and their respective\nsizes are represented by α1 and α2.\nThe assigned probabilities Pi(class|VS\nT , αi) for the ob-\njects in view (the grid cells within the sector and seen from\nthe robot) are calculated by the following expression:\nPi(class|VS\nT , αi) =\n1\n2\n+\nαi\nθ\n(P (class|VST )−\n1\n2\n) (1)\nwhere P (class|VST ) is the conditional probability that a\nview is class when the virtual sensor classification at time T\nis class. Thus, higher probabilities are given to larger parts\nof the view, assuming that larger parts are more likely to\nhave caused the view’s classification. In the current imple-\nmentation P (class|VST ) is a constant per class.\nIn the second step the local maps are used to update\na global map, the probabilistic semantic map, utilizing a\nBayesian method. The result is a global semantic map that\ndistinguishes between buildings and non-buildings. An ex-\nample of such a map is given in Figure 2. For more details\non this approach to probabilistic semantic mapping see [2].\nThe lines representing probable building outlines are ex-\ntracted from the probabilistic semantic map. For the line\nextraction an implementation by Peter Kovesi [14] was\nused. The parameter setting for the line extraction is de-\nscribed in Table 1. An example of extracted lines is given\nin Figure 3.\nFig. 2. An example of a probabilistic semantic map created with the\napproach described in the text. White cells denote high probability\nof walls and dark cells show outlines of non-building entities.\nName Value Description\nTOL 2 pixels Maximum deviation from a straight line\nbefore a segment is broken in two\nANGTOL 0.05 rad Angle tolerance used when attempting to\nmerge line segments\nLINKRAD 2 pixels Maximum distance between end points of\nline segments for segments to be eligible\nfor linking\nTable 1\nParameters used for line extraction.\n2.2. Wall Candidates in Aerial Images\nEdges extracted from an aerial image taken from a nadir\nview are used as potential building outlines. The edge image\nis a binary image from which straight lines are extracted to\nbe used as wall candidates for the matching, see Section 3.\nHere, edge detection is performed separately on the three\nRGB-components using Canny’s edge detector [15]. The\nresulting edge image Ie is calculated by fusing the three\nbinary images obtained for the three colour components\nwith a logical OR-function. Finally a thinning operation 4\nis performed to remove points that occur when edges ap-\npear slightly shifted in the different components. For line\nextraction in Ie the same implementation and parameters\nas described in Section 2.1 were used. The lines extracted\nfrom the edges detected in the aerial image in Figure 3 are\nshown in Figure 4.\nWe use the colour edge detection method because it finds\nmore edge points than gray scale edge detection. This is be-\ncause edges on the border between areas that have different\ncolours but similar intensity are not detected in gray scale\nversions of the same image. In a test where the two methods\nhad the same segmentation parameters, the colour version\nproduced 19 % more edge points resulting in 17 % more\ndetected lines for an aerial image of size ca. 800×1300 pix-\n4 The Matlab command bwmorph(im,’thin’,Inf) was used.\n3\nFig. 3. The trajectory of the mobile robot (dashed), the ground\nlevel wall estimates (solid) and the used aerial image ( c©O¨rebro\nCommunity Planning Office). The semantic map in Fig. 2 covers the\nupper left part of this figure.\nFig. 4. The lines extracted from the edge version of the aerial image.\na) b) c)\nFig. 5. Gray scale (b) and colour edge detection (c) in an aerial\nimage (a). In the top the colour version finds edges where light green\nvegetation meets light gray ground, and in the lower part edges are\nfound around the green football field where the grass meets the red\nrunning tracks.\nels (400×650 m). Figure 5 gives a close-up example from\nthat test to show the differences. The calculation time of\nthe colour edge detection is slightly more than three times\nlonger than ordinary gray scale edge detection. This time\nis still small in comparison to the routines we use for de-\ntecting lines in the edge images.\n3. Matching Wall Candidates\nThe purpose of the wall matching step is to relate wall\nestimates, obtained at ground level with the mobile robot,\nFig. 6. Selection of characteristic points for the computation of a\ndistance measure between two lines. The figure shows the line Lg\n(ground level wall candidate) with its midpoint Pg, the line Lia (aerial\nimage wall candidate), and the normal to Lia, en. To the left, Pa = φ\nsince φ is on Lia and to the right, Pa is the endpoint of L\ni\na since φ\nis not on Lia.\nto the edges detected in the aerial image. All wall estimates\nare represented as line segments. We denote a wall esti-\nmate found by the mobile robot as Lg and the N lines rep-\nresenting the edges found in the aerial image by Lia with\ni ∈ {1, . . . , N}. Both line types are geo-referenced in the\nsame Cartesian coordinate system.\nThe lines from both the aerial image and the semantic\nmap may be erroneous, especially concerning the line end-\npoints, due to occlusion, errors in the semantic map, dif-\nferent sensor coverage, etc. We therefore need a measure\nfor line-to-line distances that can handle partially occluded\nlines. Hence, we do not consider the length of the lines and\nrestrict line matching to the line directions and the distance\nbetween two characteristic points, one point on each line.\nThe line matching calculations are performed in two steps:\n1) determine the two characteristic points, and 2) compute\nthe distance measure to find the best matches.\n3.1. Finding the Closest Point\nIn this section we describe how the characteristic points\non the two compared lines are determined. For Lg we use\nthe line midpoint, Pg. To cope with the possible errors de-\nscribed above we select the point Pa on L\ni\na that is closest\nto Pg as the best candidate to be used in our line distance\nmeasure.\nTo calculate Pa, let en be the orthogonal line to L\ni\na that\nintersects Lg in Pg, see Figure 6. We denote the intersection\nbetween en and L\ni\na as φ where φ = en×L\ni\na (using homoge-\nneous coordinates). The intersection φ may be outside the\nline segment Lia, see right part of Figure 6. We therefore\ncheck if φ is within the endpoints and if it is set Pa = φ. If\nφ is not within the endpoints, then Pa is set to the closest\nendpoint on Lia.\n3.2. Distance Measure\nThe calculation of the distance measure is inspired by\n[16], which describes geometric line matching in images for\nstereo matching. We have reduced the complexity in those\ncalculations to have fewer parameters that need to be de-\ntermined and to exclude the line lengths. Matching is per-\nformed using Lg’s midpoint Pg, the closest point Pa on L\ni\na\n4\nand the line directions θg and θa. First, a difference vector\nis calculated as\nr∆ = [Pgx − Pax , Pgy − Pay , θg − θa]\nT . (2)\nSecond, the similarity is measured as the Mahalanobis dis-\ntance\nd = r∆\nTR−1r∆ (3)\nwhere the diagonal covariance matrix R is defined as\nR =\n\n\nσ2Rx 0 0\n0 σ2Ry 0\n0 0 σ2Rθ\n\n (4)\nwith σRx, σRy, and σRθ being the expected standard devi-\nation of the errors between the ground-based and aerial-\nbased wall estimates. Using Mahalanobis distance, it is only\nthe relation between the parameters that influences the line\nmatching. The important relation is σ2Rθ/σ\n2\nRx and usually\nσ2Rx = σ\n2\nRy for symmetry reasons. Note that our distance\nmeasure is not strictly a mathematical metric, due to the\nmethod for selecting characteristic points.\n4. Local Segmentation of Aerial Image\nThis section describes how local segmentation of the\ncolour aerial image is performed. Generally, segmenta-\ntion methods can be divided into two groups; edge- and\nsimilarity-based [17]. In our case we combine these ap-\nproaches by first performing edge based segmentation for\ndetection of closed areas and then colour segmentation\nbased on a small training area to confirm the area’s homo-\ngeneity. The following is a short description of the sequence\nthat is performed for each line Lg:\n(i) Sort the set of lines La based on d from Equation 3\nin increasing order and set i = 0.\n(ii) Set i = i+ 1.\n(iii) Define a start area Astart, 8× 8 pixels square (equiv-\nalent to 4×4 m), on the side of Lia that is opposite to\nthe robot (this will be in or closest to the unknown\npart of the occupancy grid map).\n(iv) Check if Astart includes edge points (parts of edges in\nIe). If yes, return to step 2. This check ensures that\na region has a minimum width and depth.\n(v) Perform edge controlled segmentation, see Section\n4.1.\n(vi) Perform homogeneity test, see Section 4.2.\nThis process is stopped, either when a region has been found\nor when all lines in La that are close enough to the present\nline in Lg to be considered have been checked. The “close\nenough” criterion could be measured by the Euclidean dis-\ntance between the characteristic points Pg and Pa defined\nin Section 3.1. However, in the current implementation this\nwas not activated during the experiment in order to be able\nto study whether other regions were found.\nFig. 7. Illustration of edge controlled segmentation. a) shows a small\npart of Ie and Astart. In b) Ie has been dilated and in c) Asmall\nhas been found. d) shows Afinal as the dilation of Asmall.\n4.1. Edge Controlled Segmentation\nBased on the edge image Ie constructed from the aerial\nimage, we search for a closed area. Since there might be\ngaps in the edges, bottlenecks need to be found [11]. We use\nmorphological operations, with a 3×3 structuring element,\nto first dilate the interesting part of the edge image in order\nto close gaps and then search for a closed area on the side\nof the matched line that is opposite to the mobile robot.\nWhen this area has been found the area is dilated in order\nto compensate for the previous dilation of the edge image.\nThis procedure is illustrated in Figure 7.\n4.2. Homogeneity Test\nWe use the initial starting area Astart as a training sam-\nple and evaluate the rest of the region based on the corre-\nsponding colour model. This means that the colour model\ndoes not gradually adapt to the growing region, but instead\nrequires a homogeneous region on the complete roof part\nthat is under investigation. Regions that gradually change\ncolour or intensity, such as curved roofs, might then be\npartly rejected.\nGaussian Mixture Models, GMM, are popular for colour\nsegmentation. Like Dahlkamp et al. [18] we tested both\nGMM and a model described by the mean and the covari-\nancematrix in RGB colour space.We selected the mean/co-\nvariance model since it is faster and we noted that the\nmean/covariance model performs approximately equally\nwell as the GMM in our case. A limit Olim is calculated for\neach model so that 95% of the training sample pixels (i.e.\npixels in Astart) have a Mahalanobis distance smaller than\nOlim. Olim is then used as the separator limit between pix-\nels belonging to the class and the pixels that do not belong\nto the class.\n4.3. Alternative Methods\nAbove a two step segmentation method to detect homo-\ngeneous regions surrounded by edges was presented. There\nexist a number of segmentation methods that could have\nbeen applied. Two alternative methods are discussed in the\nfollowing. The conclusions are based on preliminary tests\nperformed on the aerial image used in our experiment. For\nthese tests, the parameters used in the respective algo-\nrithms were tuned manually.\nThe first method tested is the graph-based image seg-\nmentation, GBIS, by Felzenszwalb and Huttenlocher [19].\nGBIS can adapt to the texture and can be set to reject\n5\nsmall areas and therefore ignore small-sized disturbances\nsuch as shadows from chimneys. Due to this GBIS produces\nvery homogeneous results. A drawback is that GBIS has a\ntendency to leak and continue to grow outside areas that\nhumans would consider to be closed. Therefore, GBIS does\nnot seem to be an option to replace both steps in our two\nstep method, but it is an alternative to the homogeneity\ntest. In conjunction with the edge controlled segmentation\nit turns out that GBIS produces similar segmentation re-\nsults to the mean/covariance model.\nThe second method tested is a modified flood fill algo-\nrithm. The algorithm takes starting pixels from Astart and\nperforms region growing limited by colour difference to the\nstarting pixels and local gradient information. Let C be\nthe mean value vector (RGB) of the starting pixels, Pi any\npixel that has been selected to be inside the region and Pn\na neighbouring pixel that is 4-connected with Pi. For each\nPn a local value gloc is calculated as\ngloc = e\n−\n∑\nj=r,g,b\n(Pn(j)−C(j))\n2\nσ2\ncol e\n−\n∑\nj=r,g,b\n(Pn(j)−Pi (j))\n2\nσ2\ngrad (5)\nThe value of gloc is then compared to a threshold to see if\nPn should be included in the region or not. Due to the use\nof the local gradient this algorithm performs well both as a\nreplacement for both steps and when it is used only for the\nhomogeneity check. This modified flood fill algorithm can\nalso leak, like GBIS, but only to areas with similar colours\nsince C only depends on the starting pixels.\n5. Global Segmentation of Aerial Images\nIn this section the view of the mobile robot is increased\nfurther. Learned colour models are used in global building\nsegmentation within the entire aerial image. The purpose of\nglobal segmentation is to build a map that predicts different\ntypes of areas, e.g., driveable ground and buildings. We call\nthis the predictive map, PM. When the PM includes both\ndriveable ground and obstacles in the form of buildings it\ncan serve as an input to a path planning algorithm.\nThe global segmentation of an aerial image using colour\nmodels captures all buildings with roofs in similar colours\nas those buildings that were detected by local segmenta-\ntion. However, some colours can be very similar to ground\ncovered by, e.g., asphalt and ground in deep shadow. There-\nfore it may happen that some of the detected building ar-\neas belong to a non-building class. In order to reduce these\nerrors, we introduce an additional class, ground, which will\ncompete with the building class about ambiguous pixels.\nAreas of driveable ground can be extracted in different\nways, e.g., vision has been used in several projects [20–22] to\nfind driveable regions for unmanned vehicles. In this work\nthe free space from the occupancy grid map is interpreted\nas ground. This free space can be considered to be drive-\nable ground assuming that there are no negative obstacles\nor other features, which cannot be sensed with the horizon-\ntally mounted 2D-laser scanner and prevent the robot from\nFig. 8. The combined binary image of free points (reduced using\nmorphological erosion with a square structuring element of size 5×5\npixels) and edges in Ie.\ndriving safely. However, since we cannot guarantee that the\nfree space in the occupancy grid map in fact corresponds\nto a driveable area we call the new class ground.\n5.1. Colour Models\nThe segmentation of the aerial image is based on colour\nmodels. In the example in this article, models are calculated\nfor the two classes: building and ground. To classify pixels\nin the aerial image we use the same procedure as for the\nhomogeneity test in local segmentation, see Section 4.\nTo define the colourmodels for the building class, we sim-\nply use the building estimates found by local segmentation\nas training areas.\nTo extract colour models that represent the different\nground areas we combine the occupancy grid map and the\nedge version of the aerial image, Ie. The free cells in the\noccupancy grid map define the regions in Ie that represent\nground. This combination can be done either under the as-\nsumption that the navigation is precise giving a perfect reg-\nistration or by reduction of the area of free cells with the\nestimated size of the navigation error. We used the latter\napproach and reduced the area of free cells in the occupancy\ngrid map by morphological erosion with a square structur-\ning element of size 5× 5 pixels to compensate for errors up\nto 1 m in all directions. An example of the combination of\nthe occupancy map and Ie is shown in Figure 8. Next, edge\ncontrolled segmentation of that region is performed, as de-\nscribed in Section 4.1, to find the different ground areas.\nThe largest areas 5 point out samples in the aerial image\nthat are used to train mean/covariance models, the same\ntype of colour models as in Section 4.2.\nThe combination of the result from the local building\nsegmentation (an example can be found in Figure 11) and\n5 The limit was set to 50 pixels (12.5 m2) in order to avoid small\nareas that could represent movable objects such as cars and small\ntrucks.\n6\nFig. 9. Flow chart of the process for calculating the predictive map.\nthe ground information from the occupancy grid map are\nreferred to as the local information. Note that both the\nlocal building segmentation and the ground information\nextracted from the occupancy grid map result from direct\nobservation by the mobile robot.\n5.2. The Predictive Map\nThe PM is designed to handle multiclass problems and\nupdating this map can be performed incrementally. The\nPM is a grid map of the same size as the aerial image that\nis segmented. For each of the n classes, a separate layer\nli, with i ∈ {1, . . . , n}, is used to store the accumulated\nsegmentation results. These layers also have the same size\nas the aerial image. The colour models used to segment the\naerial image are two-classmodels (building or non-building,\nground or non-ground, etc.) and the classifiers are therefore\nbinary classifiers.\nTo calculate the predictive map incrementally two main\nsteps are performed; 1) the aerial image is segmented when\na new colour model is available and 2) the predictive map is\nrecalculated using the result from the latest segmentation.\nFigure 9 shows a flow chart of the updating process. This is\nadapted to work also in an on-line situation and is explained\nin the following. When a New sample belonging to class cl\nis available a new colour model CM is calculated. Based\non the quality of CM, a measure p, 0 ≤ p ≤ 1 should be\nestimated 6 . Then the aerial image is segmented using the\nnew model and the result is multiplied with p and stored\nin a temporary layer. The old layer, lcl, is fused with the\ntemporary layer using a max function 7 .\nThe predictive map is based on voting from separate\nlayers li for the n classes, one layer for each class considered.\n6 Estimation of the parameter p is still an unsolved issue for future\nwork. In our experiments we used p = 0.7.\n7 Another possibility to fuse the layers would be to use a Bayesian\nmethod.\nIn this example n = 2; one building layer and one ground\nlayer. The voting is a comparison of the layers cell by cell. In\nthose grid cells where the values are similar, the cells are set\nto unknown. To evaluate the similarity between cells buffer\nzones are introduced in a voting process. The buffer zones\nare collected in the off-diagonal elements of a matrixC. The\noff-diagonal elements, cij ≥ 0, i 6= j, i = {1, 2, . . . , n}, j =\n{1, 2, . . . , n}, are then used for the classification of cells\npmxy in PM, where pmxy denotes cell (x, y). Introducing\nthe buffer zones defined in C in the voting process makes it\npossible to adjust the sensitivity of the voting individually\nfor all classes. The voting is performed using IF-THEN rules\nbiased with cij :\nIF lxyi > l\nxy\nj + cij ∀ j 6= i THEN pm\nxy = classi (6)\nwhere lxyi denotes cell (x, y) in layer i. If the condition\ncannot be fulfilled due to conflicting information, pmxy is\nset to unknown. If cij = 0 the rules in Equation 6 will turn\ninto ordinary voting where the largest value wins and where\nties give unknown.\nDuring the experiments presented in this article C was\nset to\nC =\n\n − 0.1\n0.1 −\n\n (7)\n(the values of the diagonal elements are not used).\nAll in all, the PM contains information about n+ 2 cat-\negories. First there are the n different classes, then the un-\nknown cells due to ambiguous class values and finally the\nunexplored cells that represent the remaining pixels that\ncannot be explained by any of the learned colour models.\n5.3. Combination of Local and Global Segmentation\nThe approach described above results in two sets of in-\nformation. The first is the local information that has been\nconfirmed by the mobile robot and the second is stored in\nthe PM. Where these sets overlap they can be fused into\none final estimate. Since the local information has been\nconfirmed by the mobile robot it is reasonable to let the lo-\ncal information have precedence over the PM by giving it\na higher probability p. Fusion of the PM and the local in-\nformation can use the same method (with the exception of\nsegmentation) as the updates of the PM described in the\nprevious section.\n6. Experiments\n6.1. Data Collection\nThe above algorithms were implemented in Matlab [23]\nfor evaluation and currently work off-line. Data were col-\nlected with a mobile robot, a Pioneer P3-AT from Activ-\nMedia, equipped with differential GPS (NovAtel ProPak-\nG2Plus), a horizontallymounted laser range scanner (SICK\n7\nLMS 200), cameras and odometry. The robot is equipped\nwith two different types of cameras, an ordinary camera\nmounted on a PT-head and an omni-directional camera.\nThe omni-directional camera gives a 360◦ view of the sur-\nroundings in one single shot. The camera itself is a standard\nconsumer-grade SLR digital camera (Canon EOS350D, 8\nmegapixels). On top of the lens, a curved mirror from 0-\n360.com is mounted. From each omni-image 8 (every 45◦)\nplanar views or sub-images, with a horizontal field-of-view\nof 56◦, were computed. These sub-images are the input to\nthe virtual sensor. The images were taken approximately\nevery 1.5 m along the robot trajectory and were stored to-\ngether with the corresponding robot pose. The trajectory\nof the mobile robot is shown in Figure 3. Since the ground\nwhere the robot was driven during the experiment is flat,\ninertial sensors were not needed. This can be confirmed by\nvisual inspection of the resulting occupancy map in Figure\n10.\n6.2. Tests of the Local Segmentation\nThe occupancy map shown in Figure 10 was used for\nthe experiment. This map was built from data measured\nby the laser range scanner (with 180 degrees field of view)\nand positioning data obtained from fusion of odometry and\nDGPS. The grid cell size was 0.5 m, the range of the data\nwas limited to 40 m and the map was built using the known\nposes and a standard Bayes update equation as described\nin [24]. Even though this 2D map works well in our exper-\niments (with exception of the hedge/building mix-up de-\nscribed in Section 6.3), one should note that a fixed hori-\nzontally mounted 2D laser is limited for detection of build-\ning outlines, especially in cases when the terrain is not flat.\nAlternative methods suitable for capturing large objects in\noutdoor environments are 3D laser [25], vertically mounted\nlaser range scanner [6] or (motion) stereo vision [26].\nThe occupied cells in this map (marked in black) were\nlabeled by the virtual sensor giving the semantic map pre-\nsented in Figure 2. The semantic map contains two classes:\nbuildings (values above 0.5) and non-buildings (values be-\nlow 0.5). From this semantic map we extracted the grid\ncells with a high probability of being a building 8 (above\n0.9) and converted them to the lines LMg presented in Fig-\nure 3. Matching these lines with the lines extracted from\nthe aerial image LNa , see Figure 4, was then performed.\nFinally, based on the best line matches segmentation was\nperformed as described in Section 4.\nThe three parameters in R (Equation 4) were set to\nσRx = 1 m, σRy = 1 m, and σRθ = 0.2 rad. The first two\nparameters reflect a possible error of 2 pixels between the\nrobot position and the aerial image, and the third param-\neter allows, for example, each endpoint of a 10 pixel long\n8 The limit 0.9 was chosen with respect to the probabilities used in\nthe process of building the semantic map [2]. With this limit at least\ntwo positive building readings are needed for a single cell to be used\nin LMg .\nFig. 10. Occupancy map used to build the semantic map in Fig. 2.\nline to be shifted one pixel (parallel edges in the aerial im-\nage do not always result in parallel lines, see roof outline in\nFigure 4). In the tests described in the following paragraph\nit will be shown that the matching result is not sensitive to\nsmall changes of these parameters.\nWe have performed two different types of tests. The tests\nare defined in Table 2.Tests 1-3 are the nominal cases when\nthe collected data are used as they are. These tests intend to\nshow the influence of a changed relation between σRx, σRy\nand σRθ by varying σRθ. In Test 2 σRθ is decreased by a\nfactor of 2 and in Test 3 σRθ is increased by a factor of 2.\nIn Tests 4 and 5 additional uncertainty (in addition to the\nuncertainty already present inLMg andL\nN\na ) was introduced.\nThis uncertainty is in the form of Gaussian noise added to\nthe midpoints (σx and σy) and directions (σθ) of L\nM\ng and\nevaluated in Monte Carlo simulation with 20 runs.\nTest σx [m] σy [m] σθ [rad] σRθ [rad]Nrun\n1 0 0 0 0.2 1\n2 0 0 0 0.1 1\n3 0 0 0 0.4 1\n4 1 1 0.1 0.2 20\n5 2 2 0.2 0.2 20\nTable 2\nDefinition of the parameters used in the different tests.\n6.3. Result of Local Segmentation\nThe local segmentation has a limited range and the\nground truth area can grow outside of this range without\naffecting the resulting segmentation, e.g. by including new\nbuildings that are not seen by the robot. A traditional\nquality measure of true positive rate is therefore not suit-\nable for these tests, since a true positive rate depends on\nthe size of the ground truth area. Instead, the positive\npredictive value, PPV or precision, has been used as the\nquality measure. PPV is calculated as\nPPV =\nTP\nTP + FP\n(8)\n8\nab\nc\nFig. 11. The result of the local segmentation of the aerial image using\nthe wall estimates shown in Figure 3. The ground truth building\noutlines are drawn in black.\nwhere TP are the number of true positives and FP are the\nnumber of false positives.\nThe results of Test 1 show a high positive predictive\nvalue of 96.5%, see Table 3. The resulting segmentation\nis presented in Figure 11. Three deviations from an ideal\nresult can be noted. At a and b tree tops were obstructing\nthe wall edges in the aerial image and therefore the area\nopposite to these walls was not detected as a building and a\ngap between two regions appears at c due to a wall visible\nin the aerial image. Finally, a false area, to the left of b,\noriginates from an error in the semantic map where a low\nhedge in front of a building was marked as building because\nthe building was the dominating object in the camera view.\nThe results of Test 1-3 are very similar, indicating that\nthe algorithm in this case was not specifically sensitive to\nthe changes in σRθ. In Test 4 and 5 the scenario of Test\n1 was repeated using a Monte Carlo simulation with in-\ntroduced pose uncertainty. These results are presented in\nTable 3. One can note that the difference between the nom-\ninal case and Test 4 is very small. In Test 5 where the\nadditional uncertainties are higher, the positive predictive\nvalue has decreased slightly.\nTest PPV [%]\n1 96.5\n2 97.0\n3 96.5\n4 96.8 ± 0.2\n5 95.9 ± 1.7\nTable 3\nResults for the tests defined in Table 2. The results of Test 4 and 5\nare presented with the corresponding standard deviation computed\nfrom the 20 Monte Carlo simulation runs.\n6.4. Result of Global Segmentation\nThe result of the global segmentation is shown in Fig.\n12 and 13. Visual inspection of the result shown illustrates\nthe potential of our approach. The PM based on ground\ncolour models from regions in Figure 8 and building colour\nmodels from the regions in Figure 11 is presented in Figures\n12(a) (cells classified as ground and buildings) and 12(b)\n(unexplored and unknown cells).\nCompared with the aerial image in Figure 3 the result\nis promising. One can now follow the outline of the main\n(a) Ground (gray) and building (black) estimates. The white cells\nare unexplored or unknown.\n(b) Ties or unknown cells (black), not classified cells (gray), and\nclassified cells (white).\nFig. 12. The result of the global segmentation of the aerial image\n(see Section 5) using both ground and building models.\nbuilding and most of the paths, including paved paths,\nroads and beaten tracks, have been found. The main prob-\nlem experienced during the work is caused by shadowed\nground areas that look very similar to dark roofs resulting\nin the major part of the unknown cells.\nIf areas representing the unknown cells have already been\nclassified by the mobile robot, as in Figures 10 and 11,\nthat result has precedence over the PM. The final result\nis therefore obtained when the PM is combined with the\nfree areas and the buildings found by local segmentation.\nFor these pixels we set p = 0.9, performed another update\nof the PM (using the second step described in Section 5.2)\nand got the resulting map shown in Figure 13.\nA formal evaluation of the ground class is hard to per-\nform.Ground truth for buildings can bemanually extracted\nfrom the aerial image, but it is hard to specify in detail the\narea that belongs to ground. Based on the ground truth\nof buildings and an approximation of the ground truth of\nground as the non-building cells, statistics of the result are\npresented in Table 4. In the table all values in the right col-\numn, where the results from the combined PM and local\ninformation are shown, are better than those in the middle\ncolumn (only PM).\n9\n(a) Ground (gray) and building (black) estimates. The white cells\nare unexplored or unknown.\n(b) Ties or unknown cells (black), not classified cells (gray), and\nclassified cells (white).\nFig. 13. The PM combined with the local information (see Section\n5.3).\nSince the PPV depends on the actual presence of the\ndifferent classes in the aerial image, normalized values are\nalso presented. The normalized values are calculated as\nPPVnorm =\nTPcl\nTPcl + FPcl\nGTcl\nNGTcl\n(9)\nwhere TPcl and FPcl are the numbers of true and false pos-\nitives of class cl respectively. GTcl is the number of ground\ntruth cells of class cl and NGTcl (non ground truth) is\nthe difference between the total number of cells in PM and\nGTcl. The area covered by buildings is smaller than the\nground area giving an increase in the normalized PPV for\nbuildings and a decrease for ground compared to the nom-\ninal PPV.\n7. Conclusions and Future Work\nThis paper discusses how aerial images can be used to\nextend the observation range of a mobile robot. A virtual\nsensor for building detection on a mobile robot is used to\nbuild a ground level semantic map. This map is used in a\nprocess for building detection in aerial images. The benefit\nDescriptions PM (Fig. 12) [%] PM + local (Fig. 13) [%]\nPPV buildings (norm) 66.6 (88.6) 73.0 (91.3)\nPPV ground (norm) 96.8 (88.6) 97.3 (90.4)\nBuilding cells 12.3 13.8\nGround cells 21.7 25.8\nUnclassified cells 55.5 52.4\nUnknown cells (ties) 10.5 8.1\nTable 4\nResults of the evaluation of the two predictive maps displayed in\nFig. 12 and 13. The last four rows show the actual proportions of\nthe cells in the two predictive maps.\nfrom the extended range of the robot’s view can clearly be\nnoted in the presented example.\nIn the local segmentation step it can be hard to extract\na complete building outline due to factors such as different\nroof materials, different roof inclinations and additions on\nthe roof, specifically when the robot has only seen a small\nportion of the building outline. The global segmentation is\na powerful extension. Even though the roof structure in the\nexample is quite complicated, the outline of a large building\ncould be extracted based on the limited view of the mobile\nrobot, which had only seen a minor part of surrounding\nwalls.\n7.1. Discussion\nOh et al. [4] assumed that probable paths were known in\na map and used this to bias a robot motion model towards\nareas with higher probability of robot presence. Using the\napproach suggested in this article these areas could be au-\ntomatically found from aerial images.\nWith the presented method, changes in the environment\ncompared to an aerial image that is not perfectly up-to-date\nare handled automatically. Assume that a building, present\nin the aerial image, has been removed after the image was\ntaken. It may therefore be classified as a building in the PM\nif it had a roof colour similar to a building already detected\nby the mobile robot. When the robot approaches the area\nwhere the building was situated, the building will not be\ndetected. If the mobile robot classifies the area as ground,\nthe PM will turn into unknown (of course depending on\ncij and p), not only for that specific area but also globally,\nwith the exception of areas where local information exists.\nWhat about the other way around? Assume that a new\nbuilding is erected and this is not yet reflected in the aerial\nimage. If the wall matching indicates an edge as a wall this\ncan of course introduce errors. However, there are several\ncases where it would not be a problem. When the area is\ncluttered, e.g., a forest, several close edges will be found and\nno segmentation is therefore performed. The same result\nis obtained if the building is erected in a smooth area, for\nexample an open field, since there are no edges to be found.\nThe result of these cases is that the building will only be\npresent in the probabilistic semantic map in the form of a\npossible wall.\n10\n7.2. Future Work\nWe believe that the accuracy of the PM could be further\nimproved by using a measure of the colour model quality\nto assign a value to the parameter p for each model. Also\nthe probabilities from the semantic map where the ground\nwall estimates are extracted and the certainty of the virtual\nsensor could be used in the calculation of p.\nWe further expect that shadow detection, which merges\nshadowed areas with corresponding areas in the sun, can\nreduce the number of false positives and decrease unknown\nareas caused by ties.\nExperiments where the PM is used to direct exploration\nof unknown areas should be performed. At the same time it\nshould be investigated whether post-processing of the PM,\ne.g., with filters taking neighbouring cells into account, can\nimprove the results.\nMulti-line matching, in comparison to the single line\nmatching used, can relax the need for accurate localisation\nof the mobile robot. An example of successful matching be-\ntween ground readings and aerial image for localization is\ngiven in [6] and for matching of building outlines in [27].\n8. Acknowledgments\nThe authors gratefully acknowledge the anonymous re-\nviewers for their valuable comments and Christoffer Val-\ngren for the implementation of the modified flood fill algo-\nrithm.\nReferences\n[1] M. Persson, T. Duckett, A. Lilienthal, Virtual sensor for human\nconcepts – building detection by an outdoor mobile robot,\nRobotics and Autonomous Systems 55 (5) (2007) 383–390.\n[2] M. Persson, T. Duckett, C. Valgren, A. Lilienthal, Probabilistic\nsemantic mapping with a virtual sensor for building/nature\ndetection, in: The 7th IEEE International Symposium on\nComputational Intelligence in Robotics and Automation\nCIRA2007, Jacksonville, FL, 2007.\n[3] M. Persson, T. Duckett, A. Lilienthal, Improved mapping and\nimage segmentation by using semantic information to link aerial\nimages and ground-level information, in: The 13th International\nConference on Advanced Robotics, ICAR, Jeju, Korea, 2007,\npp. 924–929.\n[4] S. M. Oh, S. Tariq, B. N. Walker, F. Dellaert, Map-based priors\nfor localization, in: IEEE/RSJ 2004 International Conference\non Intelligent Robotics and Systems, Sendai, Japan, 2004, pp.\n2179–2184.\n[5] C. Chen, H. Wang, Large-scale loop-closing with pictorial\nmatching, in: Proceedings of the 2006 IEEE International\nConference on Robotics and Automation, Orlando, Florida,\n2006, pp. 1194–1199.\n[6] C. Fru¨h, A. Zakhor, An automated mathod for large-scale,\nground-based city model acquisition, International Journal of\nComputer Vision 60 (1) (2004) 5–24.\n[7] D. Silver, B. Sofman, N. Vandapel, J. A. Bagnell, A. Stentz,\nExperimental analysis of overhead data processing to support\nlong range navigation, in: Proceedings of the 2006 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems,\nBeijing, China, 2006, pp. 2443–2450.\n[8] C. Scrapper, A. Takeuchi, T. Chang, T. H. Hong, M. Shneier,\nUsing a priori data for prediction and object recognition in an\nautonomous mobile vehicle, in: G. R. Gerhart, C. M. Shoemaker,\nD. W. Gage (Eds.), Unmanned Ground Vehicle Technology V.\nEdited by Gerhart, Grant R.; Shoemaker, Charles M.; Gage,\nDouglas W. Proceedings of the SPIE, Volume 5083, 2003, pp.\n414–418.\n[9] F. Tupin, M. Roux, Detection of building outlines based on\nthe fusion of SAR and optical features, ISPRS Journal of\nPhotogrammetry & Remote Sensing 58 (2003) 71–82.\n[10] H. Mayer, Automatic object extraction from aerial imagery –\na survey focusing on buildings, Computer vision and image\nunderstanding 74 (2) (1999) 138–149.\n[11] M. Mueller, K. Segl, H. Kaufmann, Edge- and region-based\nsegmentation technique for the extraction of large, man-made\nobjects in high-resolution satellite imagery, Pattern Recognition\n37 (2004) 1621–1628.\n[12] J. Freixenet, X. Munoz, D. Raba, J. Marti, X. Cufi, Yet\nanother survey on image segmentation: Region and boundary\ninformation integration, in: European Conference on Computer\nVision, Vol. III, Copenhagen, Denmark, 2002, pp. 408–422.\n[13] Y. Freund, R. E. Schapire, A decision-theoretic generalization\nof on-line learning and an application to boosting, Journal of\nComputer and System Sciences 55 (1) (1997) 119–139.\n[14] P. D. Kovesi, MATLAB and Octave functions for computer\nvision and image processing, School\nof Computer Science & Software Engineering, The University\nof Western Australia, last checked Nov 2007. Available from:\n<http://www.csse.uwa.edu.au/∼pk/research/matlabfns/>\n(2000).\n[15] J. Canny, A computational approach for edge detection, IEEE\nTransactions on Pattern Analysis and Machine Intelligence 8 (2)\n(1986) 279–98.\n[16] J. Guerrero, C. Sagu¨e´s, Robust line matching and estimate\nof homographies simultaneously, in: Pattern Recognition and\nImage Analysis: First Iberian Conference, IbPRIA 2003, Puerto\nde Andratx, Mallorca, Spain, 2003, pp. 297–307.\n[17] R. C. Gonzales, R. E.Woods, Digital Image Processing, Prentice-\nHall, 2002.\n[18] H. Dahlkamp, A. Kaehler, D. Stavens, S. Thrun, G. Bradski,\nSelf-supervised monocular road detection in desert terrain, in:\nProceedings of Robotics: Science and Systems, Cambridge, USA,\n2006.\n[19] P. F. Felzenszwalb, D. P. Huttenlocher, Efficient graph-based\nimage segmentation, International Journal of Computer Vision\n59 (2) (2004) 167–181.\n[20] Y. Guo, V. Gerasimov, G. Poulton, Vision-based drivable surface\ndetection in autonomous ground vehicles, in: Proceedings of the\n2006 IEEE/RSJ International Conference on Intelligent Robots\nand Systems, Beijing, China, 2006, pp. 3273–3278.\n[21] D. Song, H. N. Lee, J. Yi, A. Levandowski, Vision-based motion\nplanning for an autonomous motorcycle on ill-structured road,\nin: Proceedings of the 2006 IEEE/RSJ International Conference\non Intelligent Robots and Systems, Beijing, China, 2006, pp.\n3279–3286.\n[22] I. Ulrich, I. Nourbakhsh, Appearance-based obstacle detection\nwith monocular color vision, in: AAAI National Conference on\nArtificial Intelligence, Austin, TX, 2000, pp. 866–871.\n[23] The MathWorks, Matlab 7.0, including Image Processing\nToolbox 5.0, <http://www.mathworks.com>.\n[24] S. Thrun, A. Bu¨cken, W. Burgard, D. Fox, T. Fro¨hlinghaus,\nD. Henning, T. Hofmann, M. Krell, T. Schmidt, Map learning\nand high-speed navigation in RHINO, in: D. Kortenkamp, R. P.\nBonasso, R. Murphy (Eds.), Artificial intelligence and mobile\nrobots: case studies of successful robot systems, AAAI Press /\nThe MIT Press, 1998, pp. 21–52.\n[25] H. Surmann, K. Lingemann, A. Nu¨chter, J. Hertzberg, A 3D\nlaser range finder for autonomous mobile robots, in: Proceedings\n11\nof the 32nd ISR (International Symposium on Robotics), 2001,\npp. 153 – 158.\n[26] J. Huber, V. Graefe, Motion stereo for mobile robots, IEEE\nTransactions on Industrial Electronics 41 (4) (1994) 378–383.\n[27] J. R. Beveridge, E. M. Riseman, How easy is matching 2D\nline models using local search?, IEEE Transactions on Pattern\nAnalysis and Machine Intelligence 19 (6) (1997) 564–579.\n12\n",
            "id": 524607,
            "identifiers": [
                {
                    "identifier": "1644633",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:eprints.lincoln.ac.uk:1682",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "2099461434",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "10.1016/j.robot.2008.03.002",
                    "type": "DOI"
                },
                {
                    "identifier": "158634297",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "158634351",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "191739773",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "159460337",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "159359665",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:diva.org:oru-4262",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "159460344",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "159359672",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:diva.org:oru-3274",
                    "type": "OAI_ID"
                }
            ],
            "title": "Fusion of aerial images and sensor data from a ground vehicle for improved semantic mapping",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:diva.org:oru-3274",
                "oai:diva.org:oru-4262",
                "oai:eprints.lincoln.ac.uk:1682"
            ],
            "publishedDate": "2007-01-01T00:00:00",
            "publisher": "'Elsevier BV'",
            "pubmedId": null,
            "references": [
                {
                    "id": 18444566,
                    "title": "A 3D laser range ﬁnder for autonomous mobile robots, in:",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1016/j.robot.2003.09.004",
                    "raw": "H. Surmann, K. Lingemann, A. N¨ uchter, J. Hertzberg, A 3D laser range ﬁnder for autonomous mobile robots, in: Proceedings 11of the 32nd ISR (International Symposium on Robotics), 2001, pp. 153 – 158.",
                    "cites": null
                },
                {
                    "id": 18444557,
                    "title": "A computational approach for edge detection,",
                    "authors": [],
                    "date": "1986",
                    "doi": "10.1016/b978-0-08-051581-6.50024-6",
                    "raw": "J. Canny, A computational approach for edge detection, IEEE Transactions on Pattern Analysis and Machine Intelligence 8 (2) (1986) 279–98.",
                    "cites": null
                },
                {
                    "id": 18444555,
                    "title": "A decision-theoretic generalization of on-line learning and an application to boosting,",
                    "authors": [],
                    "date": "1997",
                    "doi": "10.1006/jcss.1997.1504",
                    "raw": "Y. Freund, R. E. Schapire, A decision-theoretic generalization of on-line learning and an application to boosting, Journal of Computer and System Sciences 55 (1) (1997) 119–139.",
                    "cites": null
                },
                {
                    "id": 18444547,
                    "title": "An automated mathod for large-scale, ground-based city model acquisition,",
                    "authors": [],
                    "date": "2004",
                    "doi": null,
                    "raw": "C. Fr¨ uh, A. Zakhor, An automated mathod for large-scale, ground-based city model acquisition, International Journal of Computer Vision 60 (1) (2004) 5–24.",
                    "cites": null
                },
                {
                    "id": 18444563,
                    "title": "Appearance-based obstacle detection with monocular color vision, in:",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "I. Ulrich, I. Nourbakhsh, Appearance-based obstacle detection with monocular color vision, in: AAAI National Conference on Artiﬁcial Intelligence, Austin, TX, 2000, pp. 866–871.",
                    "cites": null
                },
                {
                    "id": 18444552,
                    "title": "Automatic object extraction from aerial imagery – a survey focusing on buildings, Computer vision and image understanding 74 (2)",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1006/cviu.1999.0750",
                    "raw": "H. Mayer, Automatic object extraction from aerial imagery – a survey focusing on buildings, Computer vision and image understanding 74 (2) (1999) 138–149.",
                    "cites": null
                },
                {
                    "id": 18444551,
                    "title": "Detection of building outlines based on the fusion of SAR and optical features,",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1016/s0924-2716(03)00018-2",
                    "raw": "F. Tupin, M. Roux, Detection of building outlines based on the fusion of SAR and optical features, ISPRS Journal of Photogrammetry & Remote Sensing 58 (2003) 71–82.",
                    "cites": null
                },
                {
                    "id": 18444553,
                    "title": "Edge- and region-based segmentation technique for the extraction of large, man-made objects in high-resolution satellite imagery,",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1016/j.patcog.2004.03.001",
                    "raw": "M. Mueller, K. Segl, H. Kaufmann, Edge- and region-based segmentation technique for the extraction of large, man-made objects in high-resolution satellite imagery, Pattern Recognition 37 (2004) 1621–1628.",
                    "cites": null
                },
                {
                    "id": 18444560,
                    "title": "Eﬃcient graph-based image segmentation,",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1023/b:visi.0000022288.19776.77",
                    "raw": "P. F. Felzenszwalb, D. P. Huttenlocher, Eﬃcient graph-based image segmentation, International Journal of Computer Vision 59 (2) (2004) 167–181.",
                    "cites": null
                },
                {
                    "id": 18444548,
                    "title": "Experimental analysis of overhead data processing to support long range navigation, in:",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/iros.2006.281686",
                    "raw": "D. Silver, B. Sofman, N. Vandapel, J. A. Bagnell, A. Stentz, Experimental analysis of overhead data processing to support long range navigation, in: Proceedings of the 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems, Beijing, China, 2006, pp. 2443–2450.",
                    "cites": null
                },
                {
                    "id": 18444568,
                    "title": "How easy is matching 2D line models using local search?,",
                    "authors": [],
                    "date": "1997",
                    "doi": "10.1109/34.601245",
                    "raw": "J. R. Beveridge, E. M. Riseman, How easy is matching 2D line models using local search?, IEEE Transactions on Pattern Analysis and Machine Intelligence 19 (6) (1997) 564–579.",
                    "cites": null
                },
                {
                    "id": 18444544,
                    "title": "Improved mapping and image segmentation by using semantic information to link aerial images and ground-level information, in:",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1007/978-3-540-76729-9_13",
                    "raw": "M. Persson, T. Duckett, A. Lilienthal, Improved mapping and image segmentation by using semantic information to link aerial images and ground-level information, in: The 13th International Conference on Advanced Robotics, ICAR, Jeju, Korea, 2007, pp. 924–929.",
                    "cites": null
                },
                {
                    "id": 18444546,
                    "title": "Large-scale loop-closing with pictorial matching, in:",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/robot.2006.1641871",
                    "raw": "C. Chen, H. Wang, Large-scale loop-closing with pictorial matching, in: Proceedings of the 2006 IEEE International Conference on Robotics and Automation, Orlando, Florida, 2006, pp. 1194–1199.",
                    "cites": null
                },
                {
                    "id": 18444565,
                    "title": "Map learning and high-speed navigation in RHINO, in:",
                    "authors": [],
                    "date": "1998",
                    "doi": null,
                    "raw": "S. Thrun, A. B¨ ucken, W. Burgard, D. Fox, T. Fr¨ ohlinghaus, D. Henning, T. Hofmann, M. Krell, T. Schmidt, Map learning and high-speed navigation in RHINO, in: D. Kortenkamp, R. P. Bonasso, R. Murphy (Eds.), Artiﬁcial intelligence and mobile robots: case studies of successful robot systems, AAAI Press / The MIT Press, 1998, pp. 21–52.",
                    "cites": null
                },
                {
                    "id": 18444545,
                    "title": "Map-based priors for localization, in:",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1109/iros.2004.1389732",
                    "raw": "S. M. Oh, S. Tariq, B. N. Walker, F. Dellaert, Map-based priors for localization, in: IEEE/RSJ 2004 International Conference on Intelligent Robotics and Systems, Sendai, Japan, 2004, pp. 2179–2184.",
                    "cites": null
                },
                {
                    "id": 18444556,
                    "title": "MATLAB and Octave functions for computer vision and image processing,",
                    "authors": [],
                    "date": "2007",
                    "doi": null,
                    "raw": "P. D. Kovesi, MATLAB and Octave functions for computer vision and image processing, School of Computer Science & Software Engineering, The University of Western Australia, last checked Nov 2007. Available from: <http://www.csse.uwa.edu.au/∼pk/research/matlabfns/> (2000).",
                    "cites": null
                },
                {
                    "id": 18444567,
                    "title": "Motion stereo for mobile robots,",
                    "authors": [],
                    "date": "1994",
                    "doi": "10.1109/41.303787",
                    "raw": "J. Huber, V. Graefe, Motion stereo for mobile robots, IEEE Transactions on Industrial Electronics 41 (4) (1994) 378–383.",
                    "cites": null
                },
                {
                    "id": 18444543,
                    "title": "Probabilistic semantic mapping with a virtual sensor for building/nature detection, in:",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1109/cira.2007.382870",
                    "raw": "M. Persson, T. Duckett, C. Valgren, A. Lilienthal, Probabilistic semantic mapping with a virtual sensor for building/nature detection, in: The 7th IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA2007, Jacksonville, FL, 2007.",
                    "cites": null
                },
                {
                    "id": 18444558,
                    "title": "Sag¨ u´ es, Robust line matching and estimate of homographies simultaneously, in: Pattern Recognition and Image Analysis:",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1007/978-3-540-44871-6_35",
                    "raw": "J. Guerrero, C. Sag¨ u´ es, Robust line matching and estimate of homographies simultaneously, in: Pattern Recognition and Image Analysis: First Iberian Conference, IbPRIA 2003, Puerto de Andratx, Mallorca, Spain, 2003, pp. 297–307.",
                    "cites": null
                },
                {
                    "id": 18444559,
                    "title": "Self-supervised monocular road detection in desert terrain, in:",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "H. Dahlkamp, A. Kaehler, D. Stavens, S. Thrun, G. Bradski, Self-supervised monocular road detection in desert terrain, in: Proceedings of Robotics: Science and Systems, Cambridge, USA, 2006.",
                    "cites": null
                },
                {
                    "id": 18444564,
                    "title": "The MathWorks, Matlab 7.0, including Image Processing Toolbox 5.0,",
                    "authors": [],
                    "date": null,
                    "doi": "10.1007/978-3-319-06820-6_7",
                    "raw": "The MathWorks, Matlab 7.0, including Image Processing Toolbox 5.0, <http://www.mathworks.com>.",
                    "cites": null
                },
                {
                    "id": 18444550,
                    "title": "Using a priori data for prediction and object recognition in an autonomous mobile vehicle, in:",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1117/12.485917",
                    "raw": "C. Scrapper, A. Takeuchi, T. Chang, T. H. Hong, M. Shneier, Using a priori data for prediction and object recognition in an autonomous mobile vehicle, in: G. R. Gerhart, C. M. Shoemaker, D. W. Gage (Eds.), Unmanned Ground Vehicle Technology V. Edited by Gerhart, Grant R.; Shoemaker, Charles M.; Gage, Douglas W. Proceedings of the SPIE, Volume 5083, 2003, pp. 414–418.",
                    "cites": null
                },
                {
                    "id": 18444542,
                    "title": "Virtual sensor for human concepts – building detection by an outdoor mobile robot,",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1016/j.robot.2006.12.002",
                    "raw": "M. Persson, T. Duckett, A. Lilienthal, Virtual sensor for human concepts – building detection by an outdoor mobile robot, Robotics and Autonomous Systems 55 (5) (2007) 383–390.",
                    "cites": null
                },
                {
                    "id": 18444561,
                    "title": "Vision-based drivable surface detection in autonomous ground vehicles, in:",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/iros.2006.282437",
                    "raw": "Y. Guo, V. Gerasimov, G. Poulton, Vision-based drivable surface detection in autonomous ground vehicles, in: Proceedings of the 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems, Beijing, China, 2006, pp. 3273–3278.",
                    "cites": null
                },
                {
                    "id": 18444562,
                    "title": "Vision-based motion planning for an autonomous motorcycle on ill-structured road, in:",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/iros.2006.282438",
                    "raw": "D. Song, H. N. Lee, J. Yi, A. Levandowski, Vision-based motion planning for an autonomous motorcycle on ill-structured road, in: Proceedings of the 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems, Beijing, China, 2006, pp. 3279–3286.",
                    "cites": null
                },
                {
                    "id": 18444554,
                    "title": "Yet another survey on image segmentation: Region and boundary information integration, in:",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1007/3-540-47977-5_27",
                    "raw": "J. Freixenet, X. Munoz, D. Raba, J. Marti, X. Cuﬁ, Yet another survey on image segmentation: Region and boundary information integration, in: European Conference on Computer Vision, Vol. III, Copenhagen, Denmark, 2002, pp. 408–422.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [],
            "updatedDate": "2022-02-25T18:55:59",
            "yearPublished": 2007,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "0921-8890"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/1644633.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/1644633"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/1644633/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/1644633/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/524607"
                }
            ]
        },
        {
            "acceptedDate": "2006-11-20T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Boyer, Marc"
                },
                {
                    "name": "Courtiat, Jean-Pierre"
                },
                {
                    "name": "Sadani, Tarek"
                },
                {
                    "name": "Saqui-Sannes, Pierre de"
                }
            ],
            "contributors": [
                "Centre National de la Recherche Scientifique - CNRS (FRANCE)",
                "Institut Supérieur de l'Aéronautique et de l'Espace - ISAE-SUPAERO (FRANCE)"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/12040536"
            ],
            "createdDate": "2013-07-19T10:13:33",
            "dataProviders": [
                {
                    "id": 437,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/437",
                    "logo": "https://api.core.ac.uk/data-providers/437/logo"
                }
            ],
            "depositedDate": "2006-01-01T00:00:00",
            "abstract": "RT-LOTOS is a timed process algebra which enables compact\n\nand abstract specification of real-time systems. This paper proposes and illustrates a structural translation of RT-LOTOS terms into behaviorally equivalent (timed bisimilar) finite Time Petri nets. It is therefore possible to apply Time Petri nets verification techniques to the profit of RT-LOTOS. Our approach has been implemented in RTL2TPN, a prototype tool which takes as input an RT-LOTOS specification and outputs a TPN. The latter is verified using TINA, a TPN analyzer developed by LAAS-CNRS. The toolkit made of RTL2TPN and TINA has been positively benchmarked against previously developed RT-LOTOS verification tool",
            "documentType": "research",
            "doi": "10.1007/11901433_20",
            "downloadUrl": "https://core.ac.uk/download/12040536.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "Mapping RT-LOTOS specifications into Time\nPetri Nets\nTarek Sadani1,2, Marc Boyer3, Pierre de Saqui-Sannes1,2, and Jean-Pierre\nCourtiat1\ntsadani@ensica.fr, mboyer@enseeiht.fr, desaqui@ensica.fr,\ncourtiat@laas.fr\n1 LAAS-CNRS, 7 av. du colonel Roche, 31077 Toulouse Cedex 04, France\n2 ENSICA, 1 place Emile Blouin, 31056 Toulouse Cedex 05, France\n3 IRIT-CNRS/ENSEEIHT, 2 rue Camichel, 31000 Toulouse, France\nAbstract. RT-LOTOS is a timed process algebra which enables com-\npact and abstract specification of real-time systems. This paper proposes\nand illustrates a structural translation of RT-LOTOS terms into behav-\niorally equivalent (timed bisimilar) finite Time Petri nets. It is therefore\npossible to apply Time Petri nets verification techniques to the profit\nof RT-LOTOS. Our approach has been implemented in RTL2TPN, a\nprototype tool which takes as input an RT-LOTOS specification and\noutputs a TPN. The latter is verified using TINA, a TPN analyzer de-\nveloped by LAAS-CNRS. The toolkit made of RTL2TPN and TINA has\nbeen positively benchmarked against previously developed RT-LOTOS\nverification tool.\n1 Introduction\nThe acknowledged benefits of using Formal Description Techniques (FDTs) in-\nclude the possibility to verify a model of the system under design against user\nrequirements. These benefits are even higher for systems which are both concur-\nrent and submitted to stringent temporal constraints.\nIn this paper, formal verification is addressed in the context of RT-LOTOS,\na timed extension of the ISO-based LOTOS [1] FDT. RT-LOTOS [2] shares\nwith LOTOS and other process algebras its capability to specify systems as a\ncollection of communicating processes. The paper proposes a transformational\napproach from RT-LOTOS to Time Petri Nets (TPNs) which, by contrast, are\ntypical example of non compositional FDT. Therefore, it is proposed to embed\nTPNs into so-called components that can be composed relying on different pat-\nterns. The latters are carefully defined so as they ensure a very tight relation be-\ntween the obtained composite TPN and its corresponding RT-LOTOS behavior.\nThis work can be seen as giving a TPN semantics to RT-LOTOS denotationally.\nA prototype tool implements the translation patterns. It has been interfaced\nwith TINA [3], the Time Petri Net Analyzer developed by LAAS-CNRS. We\nshow that an automatically generated reachability graph of a TPN can be used\nto reason about and check the correctness of RT-LOTOS specifications.\nThe paper is organized as follows. Section 2 introduces the RT-LOTOS lan-\nguage. Section 3 introduces the Time Petri net (TPN) model. Section 4 discusses\nthe theoretical foundations of RT-LOTOS to TPNs mapping. Section 5 addresses\npractical issues, including tool development and verification results obtained for\nwell-established benchmarks. Section 6 surveys related work. Section 7 concludes\nthe paper.\n2 RT-LOTOS\nThe Language of Temporal Ordering Specifications (LOTOS, [1]), is a formal\ndescription technique, based on CCS [4] and extended by a multi-way synchro-\nnization mechanism inherited from CSP [5]. In LOTOS, process definitions are\nexpressed by the specification of behavior expressions which are constructed by\nmeans of a restricted set of powerful operators making it possible to express be-\nhaviors as complex as desired. Among these operators, action prefixing, choice,\nparallel composition and hiding play a fundamental role.\nRT-LOTOS [2] extends LOTOS with three temporal operators: a determinis-\ntic delay (\u0002t), a latency(Ωt) which enables description of temporal indetermin-\nism and a time limited offer g{t}. The main difference between RT-LOTOS and\nother timed extensions of LOTOS lies in the way a non-deterministic delay may\nbe expressed. The solution developed for RT-LOTOS is the latency operator. Its\nusefulness and efficiency have been proved in control command applications and\nhypermedia authoring [6].\nRT-LOTOS formal syntax: Let PV be the set of process variables and X\nrange over PV. Let GV be the set of the user-definable gates. Let g,g′1 . . . g′n ∈\nGV, let also L be any (possibly empty) subset of GV noted L = g′1 . . . g\n′\nn and i\nthe internal action.\nThe formal syntax of RT-LOTOS is recursively given by:\nP ::= stop | exit | X[L] | g;P | g{t};P | i{t};P | \u0002tP | ΩtP | P [] P | P|[L]|P |\nhide L in P | P\u0004 P | P[>P\nThe syntax of a process definition being ”process X[g′1 . . . g\n′\nn]:=PX endproc”.\nTwo alternative syntaxes have been defined for expressing time delays: delay(t)\nwhich is identical to \u0002t, and latencies, namely latency(t) meaning Ωt.\nRT-LOTOS operational semantics in the classical Plotkin’s SOS style can be\nfound in [7].\n3 Time Petri nets\nPetri nets were, to our knowledge, the first theoretical model augmented with\ntime constraints [8, 9], and the support of the first reachability algorithm for\ntimed system [10, 11].\nThe basic idea of time Petri nets (TPN [8, 9]) is to associate an interval\nIs(t) = [a, b] (static interval) with each transition t. The transition can be fired\nif it has continuously been enabled during at least a time units, and it must fire\nif continuous enabling time reaches b time units4.\n[1,2][3,4] [5,5]\nt0 t1 t2\nFig. 1. Priority from ur-\ngency\n[0,2] [0,2][1,1]\nt0 t1\nt2\nFig. 2. Synchronization\n[1,1] [2,2]\nt0 t1\nFig. 3. Continuous\nenabling\nFigure 1 is a first example: in the initial marking, only t0 and t1 are enabled.\nAfter one time unit delay, t1 is firable. Because t1 reaches its upper interval\nalways before t0 becomes enable (3 > 2), then t0 can never be fired. t2 is fired\nfive time units after the firing of t1. Figure 2 illustrates the synchronization\nrule: t0 (resp. t1) is fired at an absolute date θ0 ≤ 2 (resp. θ1 ≤ 2), and t2 is\nfired at max(θ0, θ1)+1. Figure 3 illustrates another important point: continuous\nenabling. In that TPN, transition t1 will never be fired, because, at each time\nunit, t0 is fired, removing the token and putting it back immediately. Then, t1\nis at most 1 time unit continuously enabled, never 2 time units.\n4 Translation definition\nThis section gives the translation from RT-LOTOS terms into TPNs. This map-\nping can also be understood as the definition of an alternative TPNs semantics\nof RT-LOTOS. It is well known that process algebras (e.g. RT-LOTOS) heavily\nrely on the concept of compositionality, whereas Petri nets (and their timed ver-\nsions) lack of compositionality at all. To make the translation possible, a core\nidea behind our approach is to view a TPN as a composition of number of TPN\ncomponents. The following section introduces the concept of TPN Component\nas basic building block.\n4.1 Time Petri net Component\nA Component encapsulates a labeled TPN which describes its behavior. It is\nhandled through its interfaces and interactions points. A component performs an\naction by firing the corresponding transition. A component has two sets of labels:\nAct which is the alphabet of the component and T ime = {tv, delay, latency}.\nThese three labels are introduced to represent the intended temporal behavior of\na component. The tv (for “temporal violation”) label represents the expiration\nof time limited-offering. A delay or latency label represents the expiration of a\ndeterministic delay or a non deterministic delay, respectively.\n4 This urgency when the deadline is reached is called “strong semantics”.\nFig. 4. Component example\nFig. 5. The exit pattern\nA component is graphically represented by a box containing a TPN. The\nblack-filled boxes at the component boundary represent interaction points. For\ninstance, the component CP in the Figure 4 is built from a RT-LOTOS term\nP. During its execution, it may perform the observable action a. The ini (ini-\ntially marked places) represent the component input interface, and the out place\ndenotes its output interface. A token in the out place of a component means\nthat the component has successfully completed its execution. A component is\nactivated by filling its input places. A component is active if at least one of its\ntransitions is enabled. Otherwise, the component is inactive.\nDefinition 1 (Component).\nLet Act = Ao ∪ Ah ∪ {exit} be an alphabet of actions, where Ao is a set of\nobservable actions (with i \u0007∈ Ao, exit \u0007∈ Ao), Ah = {i} ×Ao is the set of hidden\nactions (If a is an observable action, ia denotes a hidden action).\nFormally a component is a tuple C = 〈Σ,Lab, I, O〉 where\n– Σ = 〈P, T, Pre, Post,M0, IS〉 is a TPN.\n– Lab : T → (Act ∪ T ime) is a labeling function which labels each tran-\nsition in Σ with either an action name (Act) or a time-event (T ime =\n{tv, delay, latency}). Let TAct (resp. T Time) be the set of transitions with\nlabels in Act (resp. T ime).\n– I ⊂ P is a non empty set of places defining the input interface.\n– O ⊂ P is the output interface of the component. A component has an output\ninterface if it has at least one transition labeled by exit. If so, O is the\noutgoing place of those transitions. Otherwise, O = ∅.\nThe following invariants apply to all components:\nH1 There is no source transition in a component.\nH2 The encapsulated TPN is 1-bounded (cf. safe nets in [12]). H2 is called the\n”safe marking” property. It is essential for the decidability of reachability\nanalysis procedure applied to TPNs.\nH3 If all the input places are marked, all other places are empty (I ⊂ M ⇒\nM = I).\nH4 If the out place is marked, all other places are empty (O \u0007= ∅ ∧ O ⊂ M ⇒\nM = O).\nH5 For each transition t such that Lab(t) ∈ Act, if the label is an observable\naction (Lab(t) ∈ A0), its time interval is [0,∞), otherwise5, it is [0, 0].\n4.2 Translation patterns\nRT-LOTOS behaviour expressions are inductively defined by means of algebraic\noperators acting on behaviour expressions. Since the translation is meant to be\nsyntax driven we need to endow TPNs with operators corresponding to RT-\nLOTOS ones, so as to allow one to construct a composite TPN. In the following,\nwe first define components corresponding to nullary algebraic operators (stop\nand exit) and, given each RT-LOTOS operator and its operands (i.e. behaviour\nexpressions), we inductively describe how to obtain a new component starting\nfrom the one corresponding to the given RT-LOTOS behaviour expressions. The\nresulting component corresponds to the RT-LOTOS behaviour expression com-\nputed by applying the operator to the given operands.\nDue to lack of space, the formalization of some patterns is skipped. A com-\nplete formal definition can be found in the extended version of this paper [13].\nNotation and definition f ′ = f ∪ (a, b) defines the function f ′ : A ∪ {a} \u0010→\nB ∪ {b} such that f ′(x) = f(x) if x ∈ A and f ′(a) = b otherwise.\nDefinition 2 (First actions set). Let C be a component. The set of first\nactions FA (CP) can be recursively built using the following rules6:\nFA (Cstop) = ∅ FA (Cexit) = {texit}\nFA (Ca;P) = {ta} FA (CµX.(P;X)) = FA (CP)\nFA (Ca{d}P) = {ta} FA (Cdelay(d)P) = FA (CP)\nFA (Clatency(d)P) = FA (CP) FA (CP;Q) = FA (CP)\nFA (CP|[A]|Q) = FA (CP) ∪ FA (CQ) FA (CP>>Q) = FA (CP)\nFA (CP[]Q) = FA (CP) ∪ FA (CQ) FA (CP[>Q) = FA (CP) ∪ FA (CQ)\nFA (Chide a in P) = ha (FA (CP))\nLow level Petri net operations The formal definition of the translation patterns\nuses the following low level Petri nets operators: ∪, \\,unionmulti.\nLet N = 〈P, T, Pre, Post,M0, IS〉 be a TPN.\nAdding a place Let p be a new place (p \u0007∈ P ), Prep and Postp two sets of\ntransitions of T . Then N ′ = N ∪ 〈Prep, p, Postp〉 is the TPN augmented with\nplace p such that •p = Prep and p• = Postp.\nAdding a transition: Let t be a new transition (t \u0007∈ T ), I its time interval,\nPret and Postt two sets of places of P . Then N ′ = N ∪ 〈Pret, (t, I), Postt〉 is\nthe TPN augmented with transitions t such that •t = Pret and t• = Postt.\nSimilarly, \\ is used to remove places or transitions from a net (and all related\narcs), and unionmulti denotes the free merging of two nets.\n5 Lab(t) ∈ Ah ∪ {exit}\n6 where ta is transition labelled by a. ha(α) = α if α \u0004= a and ha(a) = ia\nBasic components The Cstop component is simply the empty net (no place,\nno transition). Cexit is a component which performs a successful termination. It\nhas one input place, one output place, and a single transition labeled with exit\nand a static interval [0, 0] (Fig.5).\nPatterns applying to one component Let us consider the component CP of\nFig. 4. Fig. 6 depicts different patterns applied to CP.\n   \n(a) a;P (b) a{d}P (c) delay(d)P (d) latency(d)P\nFig. 6. Patterns applying to one component\n– Ca;P (Fig. 6(a)) is the component resulting from prefixing CP with action a.\nCa;P executes a then activates CP.\nCa;P = 〈Σa;P, Laba;P, {in}, OP〉 where the TPN Σa;P is obtained by adding a\nplace in and a transition t0 to ΣP, Laba;P associates a to transition t0.\nΣa;P = (ΣP ∪ 〈∅, (t0, [0,∞)), IP〉) ∪ 〈∅, in, t0〉\nLaba;P = LabP ∪ (t0, a)\n– Ca{d};P (Fig. 6(b)) is the component resulting from prefixing CP with a lim-\nited offer of d units of time on action a. If for any reason, a cannot occur\nduring this time interval, the tv transition will be fired (temporal violation\nsituation) and Ca{d};P will transform into an inactive component.\nCa{d};P = 〈Σa{d};P, Laba;P ∪ {(t1, tv)} , {in} , OP〉\nΣa{d};P = Σa;P ∪ 〈{in} , (t1, [d, d]), ∅〉\n– Cdelay(d)P (Fig 6(c)) is the component resulting from delaying the first action\nof P with a deterministic delay of d units of time. This is exactly the same\npattern as Ca;P except that the added transition has a delay label and a\nstatic interval equal to [d, d].\nCdelay(d)P = 〈Σdelay(d)P, LabP ∪ {(t0, delay)} , {in} , OP〉\nΣdelay(d)P = (ΣP ∪ 〈∅, (t0, [d, d])), IP〉) ∪ 〈∅, in, t0〉\n– Clatency(d)P (Fig 6(d)) is the component resulting from delaying the first\nactions of CP with a non deterministic delay of d units of time.\nLike the delay operator, latency is defined by connecting a new transition\nto the input interface of CP. But this time, we add a static interval equal\nto [0, d]. The definition of the latency translation pattern must handle the\n“subtle” case where one (or several) action(s) among CP’s first actions is\n(are) constrained with a limited offer (this set is denoted by FAlo). For\ninstance, in Fig 6(d), action a is offered to the environment during dx units\nof time. The RT-LOTOS semantics states that the latency and the offering of\na start simultaneously, which means that if the latency duration goes beyond\ndx units of time, the offer on a will expire. To obtain the same behavior, we\nadd the input place in0 of a to the input interface of the resulting component\nClatency(d)P. In the definition of the pattern, we denote Ilo the set of these\ninput places (Ilo ⊂ IP). Thus t1 and t are enabled as soon as the component\nis activated (all its input places being marked). Clatency(d)P is able to execute\na (fire t0) if t0 is enabled (i.e if in0 and p are marked) before t1 is fired (at\ndx). Therefore, action a is possibly offered to the environment for no more\nthan dx units of time, hence conforming to the RT-LOTOS semantics.\nLet FA (CP) be the set of transitions associated to the first actions of P7,\nand FAlo (CP) be the set of first actions constrained by a time limited offer:\nFAlo (CP) =\n{\nta ∈ FA (CP) tv ∈ (•ta)•\n}\nIlo = •FAlo (CP)\nClatency(d)P = 〈Σlatency(d)P, LabP ∪ {(t, latency)} , Ilo ∪ {in} , OP〉\nΣlatency(d)P = ΣP ∪\n⋃\nta∈FAlo(CP)\n〈t, pta , ta〉 ∪ 〈∅, in, ∅〉\n∪\n〈\n{in} , (t, [0, d]), (IP\\Ilo) ∪\n⋃\nta∈FAlo(CP)\n{pta}\n〉\n– CµX.(P;X) The recursion operator translation is mainly an untimed problem\n(relying on fixpoint theory). It is not presented in this paper, focused on\ntimed aspects.\n– Chide a in P is the component resulting from hiding action a in CP. Hiding\nallows one to transform observable (external) actions into unobservable (in-\nternal) actions, then making the latter unavailable for synchronization with\nother components. In RT-LOTOS, hiding one or several actions induces a\nnotion of urgency on action occurrence. Consequently, a TPN transition cor-\nresponding to a hidden action will be constrained by a time interval equal\nto [0, 0]. This implies that as soon as a transition is enabled, it is candidate\nfor being fired.\n7 Its formal definition can be found in Def. 2, Sect. 4.2.\nPatterns applying to a set of components Each of the following patterns\ntransforms a set of components into one component.\nC\nP\nC\nQ\n   C\nP\n|[a]|\nQ\nFig. 7. Parallel synchronization pattern\nC\nP\nC\nQ\nC\nP>>Q\nFig. 8. Sequential composition pattern\n– CP|[a]|Q (Fig.7)\nIn Petri nets, a multi-way synchronization is represented by a transition with\nn input places. This transition can fire only if all its input places contain a\ntoken (cf. Fig. 2). At the PN level, the synchronization operation is achieved\nthrough transition merging. While in untimed Petri nets, the operation of\ntransitions merging is straightforward, it turns to be a rather tricky issue in\nTime Petri nets. Indeed, it requires explicit handling of the time intervals\nassigned to the transitions to be merged. Possible incompatibility of these\ntime intervals leads to express global timing constraints as a conjunction of\nintervals whose consistency is not guaranteed. This problem is not solved in\n[14] where each transition is assigned a time interval.\nTo solve this problem and make transitions merging operation always pos-\nsible, we avoid assigning time intervals to actions transitions. Instead, the\ntiming constraints are assigned to conflicting transitions (cf. time limited\noffer pattern). The advantage of this solution is twofold:\n1) To allow an incremental combination of timing constraints. Fig-\nure 7 depicts synchronization between two components CP and CQ on action\na. Our goal is to define a compositional framework, where each component\ninvolved in this synchronization may add timing constraints with respect to\nthe occurrence of action a, such that the global timing constraint on a in\nCP|[a]|Q will be the conjunction of several simpler constraints. This implies\nthat when component CP is ready to execute a, it is forced in the absence of\nalternative actions, to wait for CQ to offer a. This may lead for example to\nthe expiration of a limited temporal offer on a in CP. This goal is achieved\nwithout explicitly handling time intervals, and the synchronization is mod-\neled as a straightforward transition merging as in untimed Petri nets.\n2) Relaxing the TPN’s strong semantics. In TPNs the firing of tran-\nsitions is driven by necessity. Thus an action has to be fired as soon as its\nupper bound is reached (except for a transition in conflict with another one).\nLike process algebras in general, RT-LOTOS favors an interaction point of\nview, where the actual behavior of a system is influenced by its environ-\nment. Thus, an enabled transition may fire within its enabling time window\nbut it cannot be forced to fire. A wide range of real-time systems work on\nthat principle. In particular, soft real-time systems are typical examples of\nsystems that cannot be forced to synchronize with their environment. This\nbehavior would not be possible if the actions transitions were assigned time\nintervals, because their firing would be driven by necessity. To model ne-\ncessity in RT-LOTOS we use the hide operator. Its combination with the\n’temporal limited offering’ and the ’latency’ operators gives an interesting\nflexibility in terms of expressiveness.\nThe synchronization on a of CP and CQ is achieved by merging each a tran-\nsition in CP with each a transition in CQ, thus creating n∗m a transitions in\nCP|[a]|Q (n and m being respectively the number of a transitions in CP and\nCQ).\nLet T aX be the set of transitions labelled with a in CX.\nT aX = {t ∈ TX LabX(t) = a} T AX =\n⋃\na∈A\nT aX\nThe net ΣP|[A]|Q is obtained by replacing each transition tp in CP with label\na ∈ A by a set of transitions (tp, tq) such that tq is also labelled by a, with\n•(tp, tq) = •tp ∪ •tq and (tp, tq)• = tp• ∪ tq•.\nA [0,∞) temporal interval is associated with the newly created transition\n(cf. H5).\nNote that the two components have to synchronize on exit transition to\nconform to RT-LOTOS semantics. The two output interfaces are merged:\nOut = {out} if OP \u0007= ∅ ∧OQ \u0007= ∅, Out = ∅ otherwise.\nLet us denote merge(tp, tq) = 〈•tp ∪ •tq, ((tp, tq), IS(tp)), tp• ∪ tq•〉; A′ =\nA ∪ {exit}; PreOut = T exitP × T exitQ if OP \u0007= ∅ ∧ OQ \u0007= ∅, PreOut = ∅\notherwise.\nCP|[A]|Q = 〈ΣP|[A]|Q, LabP|[A]|Q, IP ∪ IQ, Out〉\nΣP|[A]|Q =\n(\nΣP\\T A’P \\OP\n) unionmulti (ΣQ\\T A’Q \\OQ) ∪ ⋃\ntp∈T A’P ,tq∈T A’Q\nmerge(tp, tq) ∪ 〈PreOut,Out, ∅〉\nLabP|[A]|Q(t) =\n{\nLabX(t) if t ∈ TX, X ∈ {P, Q}\na if t = (tp, tq) ∧ tp ∈ T aP\n– CP>>Q (Fig. 8) depicts a sequential composition of CP and CQ which means\nthat if CP successfully completes its execution then it activates CQ. This kind\nof composition is possible only if CP has an output interface. The resulting\ncomponent CP>>Q is obtained by merging the output interface of CP and the\ninput interface of CQ, and by hiding the exit interaction point of CP.\nCP>>Q = 〈ΣP>>Q, Labhide exit in P ∪ LabQ, IP, 0Q〉\nΣP>>Q = 〈PP\\OP ∪ PQ, Thide exit in P ∪ TQ, P reP ∪ PreQ, PostP>>Q, ISP ∪ ISQ〉\nPostP>>Q = (PostP\\ {(t, OP) t ∈ •OP}) ∪ {(t, inQ) inQ ∈ IQ ∧ t ∈ •OP} ∪ PostQ\n– CP[]Q (Fig. 9) is the component which behaves either as CP or CQ.\nWe do not specify whether the choice between the alternatives is made by\nthe component CP[]Q itself, or by the environment. Anyway, it should be\nmade at the level of the first actions in the component. In other words, the\noccurrence of one of the first actions in either component determines which\ncomponent will continue its execution and which one must be deactivated.\nThe problem can be viewed as a competition between CP and CQ. These\ntwo components compete to execute their first action. As long as the that\naction has not yet occurred, CP and CQ age similarly, which means that T ime\ntransitions (labeled by tv, delay or latency) may occur in both components\nwithout any consequence on the choice of the wining component. Once one\nfirst action has occurred, the control is irreversibly transferred to the winning\ncomponent, the other one being deactivated, in the sense that it no longer\ncontains enabled transitions. The choice operator is known to cause trouble\nin presence of initial parallelism. [15] defines a choice operator where each\nalternative has just one initial place. Therefore, none of the alternative allows\nany initial parallelism. We think that it is a strong restriction. We do not\nimpose any constraint on the choice alternatives.\nThe solution we propose to define a choice between two components is as\nfollows: to obtain the intended behavior, we introduce a set of special places,\ncalled lock places. Those places belong to the input interface of component\nCP[]Q. Their function is to undertake control transfer between the two compo-\nnents. For each first action of CP we introduce one lock place per concurrent\nfirst action in CQ (for instance a has one concurrent action in CQ: c, while\nc has two concurrent actions in CP: a and b) and vice versa. A lock place\ninteracts only with those transitions representing the set of initial actions\nand the time labeled transitions they are related with (delay for a and tv\nfor b). T ime transitions restore the token in the lock place, since they do\nnot represent an action occurrence, but a time progression which has not\nto interfere with the execution of the other component (as long as the first\naction has not occurred, the two components age similarly). The occurrence\nof an initial action of CP (respectively CQ) locks the execution of CQ (re-\nspectively CP) by stealing the token from the lock places related to all CQ’s\n(respectively CP’s) first actions.\nA unique out place is created by merging the out places of CP and CQ.\n– CP[>Q (Fig. 10) is the component representing the behavior where component\nCP can be interrupted by CQ at any time during its execution. It means that\nat any point during the execution of CP, there is a choice between executing\nFig. 9. Choice between CP and CQ\nC\nP [>Q\nC\nP\nC\nQ\nFig. 10. The disrupt pattern\none of the next actions from CP or one of the first actions from CQ. For\nthis purpose, CQ steals the token from the shared place named disrupt\n(which belongs to the input interface of CP[>Q), thus the control is irreversibly\ntransferred from CP to CQ (disrupt is an input place for CQ first action\nand exit transition of CP, it is also an input/output place for all the others\ntransitions of CP). Once an action from CQ is chosen, CQ continues executing,\nand transitions of CP are no longer enabled.\n4.3 Formal proof of the translation consistency\nWe prove that the translation preserves the RT-LOTOS semantics and that\nthe defined compositional framework preserves the good properties (H1–H5) of\nthe components. This is done by induction: assuming that some components\nC1, ..., Cn are respectively equivalent to some RT-LOTOS terms T1, ..., Tn, and\ngiven a RT-LOTOS operator Op, we prove that the pattern applied to C1, ..., Cn\ngives a component which is equivalent to the term Op(T1, ..., Tn) (the behavior\nover time must be accounted for).\nThe proof is carried out in two steps:\n– we first define a more informative RT-LOTOS semantics, which does not\nintroduce any new operation, but explicitly acquaints the occurrence of time-\nevents. A time-event represents the expiration of an RT-LOTOS temporal\noperator. As an illustration, let us consider the rule of the limited offering\nas it appears in the original semantics of RT-LOTOS. In the following rule,\nany delay d′ > d will silently transform a process a{d};P into stop.\na{d};P d′−→ stop (1)\nIn the augmented RT-LOTOS semantics, a ”tv” transition is introduced to\ndenote the limited offer expiration (cf 2).\na{d};P d−→ a{0};P tv−→ stop d′−d−−−→ stop (2)\nA delay d′ > d is of course still allowed from a{d};P, but it is splited into\nthree steps: a delay d, a “temporal violation” (tv), and a delay d′ − d.\nit is easy to define a branching bisimulation8 which abstracts from the oc-\ncurence of the newly added time-event transitions and show that the new\nsemantics of RT-LOTOS is branching bisimilar to the original semantics of\nRT-LOTOS.\n– We then prove that the semantic model of the components is strongly timed\nbisimilar to this more informative RT-LOTOS semantics. Intuitively an RT-\nLOTOS term and a component are timed bisimilar [16] iff they perform the\nsame action at the same time and reach bisimilar states. For each opera-\ntor, we prove that, from each reachable state, if the occurrence of a time\nprogression (respectively an action) is possible in an RT-LOTOS term, it\nis also possible in its associated component, and conversely. Therefore, we\nensure that the translation preserves the sequences of possible actions but\nalso the occurrence dates of these actions. It is worth to mention that dur-\ning the execution of a component the structure of the encapsulated TPN\nremains the same; only the markings are different, while the structure of an\nRT-LOTOS term may change through its execution. As a result, the TPN\nencapsulated in a component may not directly correspond to an RT-LOTOS\nbehavior translation but is strongly bisimilar with a TPN which does corre-\nspond to an RT-LOTOS expression translation (The same TPN and current\nstate without the unrechable structure).\nLet us illustrate the template of the proof on the parallel synchronization.\nNotation A paragraph starting with R−→ proves that each time progression which\napplies to the RT-LOTOS term is acceptable in its associated component. Con-\nversely, R←− denotes the opposite proof. Similarly a−→ and a←− are used for the proof\non actions occurrences.\nProof of the synchronization pattern (Fig. 7)\nR−→: A time progression is acceptable in CP|[A]|Q iff it is acceptable for each\nenabled transition.\nBy construction, TP|[A]|Q = (TP\\T A’P )∪ (TQ\\T A’Q )∪E, with E the set of newly\ncreated transitions: E =\n⋃\ntp∈T A’P ,tq∈T A’Q merge(tp, tq).\n8 Our temporal branching bisimulation looks like the weak bisimulation of CCS which\nabstracts the internal actions. The difference with ours is that the time-event tran-\nsitions do not resolve the choice and the disabling contrary to the internal actions\nof CCS.\nLet t be an enabled transition of CP|[A]|Q.\nIf t is in (TP\\T A’P ), by construction this time progression is acceptable for t\nsince its initial timing constraint in CP has not been changed, and it is not\ninvolved in the synchronization. The same applies if t is in (TQ\\T A’Q ).\nIf t is in E, we show that Lab(t) \u0007= exit. Let us assume that Lab(t) = exit.\nBy construction, it exists tp and tq such that t = (tp, tq) and Lab(tp) =\nLab(tq) = exit. exit is a special urgent action (cf. H5), its execution is\nenforced as soon as it is enabled. By construction •t = •tp ∪ •tq: if t is\nenabled, then tp is enabled in CP and tq is enabled in CQ. The enabling of\ntp and tq precludes any time progression. By induction, it precludes time\nprogression in P and Q and then in P|[A]|Q (\u0007 R−→).\nFrom H5, we have that all non exit transitions in E are associated with a\ntime interval equal to [0,∞). Therefore, time progression is also acceptable\nin CP|[A]|Q.\nR←−: The proof is similar to R−→.\na−→: Assuming P|[A]|Q a−→, is this action acceptable for CP|[A]|Q?\nTwo cases must be discussed: either action a is a synchronization action\nbetween P and Q (a ∈ A’), or it is not.\nCase a∈A’: A synchronization action a is possible in P|[A]|Q iff it is pos-\nsible in P and in Q. By induction, it exists transitions tp and tq labelled\nby a firable in CP and in CQ.\nThat is to say, marking •pt ⊆ MP, and the same for Q. By construction,\nwe have •(tp, tq) = •tp ∪ •tq, then the marking MP|[A]|Q enables (tp, tq).\nAfter the firing, the marking of CP|[A]|Q can be seen as the union of the\none of CP’ and CQ’ because (tp, tq)\n• = tp• ∪ tq•.\nCase a/∈A’: If action a occurs in P|[A]|Q it either is an action of P or an\naction of Q. By induction hypothesis, it is either a firable transition in\nCP or in CQ. Since this transition is not modified in CP|[A]|Q, it is firable\nin CP|[A]|Q.\na←−: Similarly to a−→.\n5 Tools and experiments\n5.1 Tools\nRTL. The Real-Time LOTOS Laboratory developed by LAAS-CNRS [17], takes\nas input an RT-LOTOS specification and generates either a simulation trace or\na reachability graph. RTL generates a minimal reachability graph preserving the\nCTL9 properties.\nTINA. (TIme petri Net Analyzer [3]) is a software environment to edit and\nanalyze Petri nets and Time Petri nets.\nTINA offers various abstract state space constructions that preserve specific\nclasses of properties of the concrete state space of the nets. Those classes of\n9 Computational Tree Logic\nproperties include general properties (reachability properties, deadlock freeness,\nliveness), and specific properties.\nRTL2TPN. is a translator prototype, which implements the translation pattern\nof Sect. 4. It takes as an input an RT-LOTOS specification and outputs a TPN in\nthe format accepted by TINA. RTL2TPN reuses RTL’s parser and type-checker.\n5.2 Case studies\nFig. 11. RT-LOTOS specification of the Multimedia scenario\nMultimedia scenario. The author of this multimedia scenario wants to present\n3 medias named A, B and C, respectively. The duration of these medias are\nrespectively defined by the following time intervals [3,6], [3,5] and [3,8]. This\nmeans that the presentation of the media A will last at least 3 sec and at most\n6 sec. From the author’s point of view, any duration of the media presentation\nis acceptable, as long as it belongs to the specified time interval. Besides, the\nauthor expresses the following global synchronization constraints:\n1) The presentation of medias A and B must end simultaneously.\n2) The presentation of medias B and C must start simultaneously .\n3) The beginning of the multimedia scenario is determined by the beginning\nof A and its termination is determined by the end of A and B, or by the end of\nC (cf Fig. 12(a)).\nFig. 12(b) depicts the associated TPN generated by RTL2TPN. For this\nexample TINA builds a graph of 230 classes and 598 transitions.\nThe scenario is potentially consistent because it exists a path starting by\naction sA (sA characterizes the beginning of the scenario) and leading to the\nend of scenario presentation (occurrence of either eAB or eC).\nDinning philosophers. We use one well known multi-process synchronization\nproblem: the Dinning philosophers to check the robustness of the solutions pro-\nposed in the paper while facing a state explosion situation. We propose a timed\nextension of the problem which goes like this: A certain number of philosophers,\n(a) The MM constraints .44\n(b) The component\nsitting around a round table spend their lives thinking and eating. Each of them\nhas in front of him a bowl of rice. Between each philosopher and his neighbor is\na chopstick. Each philosopher thinks for a while, then gets hungry and wants to\neat. In order to eat, he has to get two chopsticks. Once a philosopher picks his\nleft chopstick, it will take him between 0 and 10 seconds to take the right one.\nOnce he possesses two chopsticks, he can begin eating and it will take him 10 to\n1000 seconds10 to finish eating, then he puts back down his left chopstick and\nthe right one in a delay between 0 to 10 seconds.\nFig. 12. RT-LOTOS specification of the dinning philosophers\nThe RT-LOTOS specification of the problem consists in the parallel syn-\nchronization of different instances of processes Philosopher and Chopstick11 (cf\nFig 12).\n10 delay and latency may be expressed together by a single syntactic construct de-\nlay(dmin, dmax)meaning delay(dmin)latency(dmax-dmin)\n11 All the experiments described in this paper have been performed on a PC with 512\nMo memory and a processor at 3.2 GHz.\nTimed Milner scheduler. This is a temporal extension of Milner’s scheduler prob-\nlem [18]. The interesting point about this example is that the timing constraints\nas introduced in the system, do not increase the state space (compared to the\nuntimed version of the problem). However they complicate the computation of\nthe system’s state space.\nFig. 13. RT-LOTOS specification of the timed Milner scheduler\nLet us consider a ring of n process called Cyclers. A Cycler should cycle\nendlessly as follows: (i) Be enabled by predecessor at gi, (ii) after a non deter-\nministic delay between 0 and 10 units of time, receive initiation request at ai,\n(iii) after a certain amount of time between 10 and 100 units of time, it receives\na termination signal at bi and enables its successor at gi+1(in either order).\nTable 1. Performance comparison of RTL2TPN+TINA vs RTL\nThe reachability algorithm implemented in RTL is exponential in number of\nclocks. As the number of philosophers (respectively Cyclers) grows RTL does\nnot challenge TINA’s runtime performances. However the size of state space\ngenerated by RTL for the RT-LOTOS specifications is more compact than the\none generated by TINA for the associated TPNs issued by RTL2TPN. This is due\nto a useful but however expensive minimization procedure carried out in RTL.\nThis minimization adapted from [19] permits to consider regions larger than the\nones required from a strict reachability point of view, thereby minimizing the\nnumber of regions.\n6 Related work\nMuch work has been done on translating process algebras into Petri Nets, by\ngiving a Petri net semantics to process terms [20, 15, 21]. [21] suggests that a\ngood net semantics should satisfy the retrievability principle, meaning that no\nnew ”auxiliary” transitions should be introduced in the reachability graph of\nthe Petri net. [20, 15] do not satisfy this criterion. In this paper, we define a\none-to-one mapping which is compliant with this strong recommendation.\nUntimed models A survey of the literature indicates that proposals for LOTOS\nto Petri net translations essentially deal with the untimed version of LOTOS\n[22–27]. The opposite translation has been discussed by [26] where only a subset\nof LOTOS is considered, and by [28] where the authors addressed the transla-\ntion of Petri nets with inhibitor arcs into basic LOTOS by mapping places and\ntransitions into LOTOS expressions. [25] demonstrated the possibility to verify\nLOTOS specifications using verification techniques developed for Petri nets by\nimplementing a Karp and Miller procedure in the LOTOS world.\nAmong all these approaches, [22, 27] is the only one operating a complete\ntranslation of LOTOS (it handles both the control and data parts of LOTOS).\nMoreover, it just considers regular LOTOS terms, and so do we. The LOTOS\nto PN translation algorithms of [22, 27] were implemented in the CAESAR tool.\nBesides the temporal aspects addressed in this paper, a technical difference with\n[22, 27] lies in the way we structure TPNs. Our solution is based on TPNs com-\nponents. In our approach, a component may contain several tokens. Conversely,\n[22, 27] structures Petri nets into units, each of them containing one token at\nmost. This invariant limits the size of markings, and permits optimizations on\nmemory consumption. The counterpart is that [22, 27] use \u0005-transitions. The\nlatter introduces non determinism. They are eliminated when the underlying\nautomaton is generated (by transitive closure). The use of \u0005-transitions may be\ninefficient in some particular cases, such as the example provided in [29].\nThe major theoretical study on taking advantage of both Petri nets and\nprocess algebras is presented in [12]. The proposed solution is Petri Box Calculus\n(PBC), a generic model that embodies both process algebra and Petri nets. The\nauthors start from Petri nets to come up with a CCS-like process algebra whose\noperators may straightforwardly be expressed by means of Petri nets.\nTimed models [30] pioneered work on timed enhancements of the control part of\nLOTOS inspired by timed Petri nets models. [31] defined a mapping from TPNs\nto TE-LOTOS which makes it possible to incorporate basic blocks specified as\n1-bounded TPNs into TE-LOTOS specifications. However, because of the strong\ntime semantics of TPNs (a transition is fired as soon as the upper bound of its\ntime interval is reached unless it conflicts with another one) a direct mapping\nwas not always possible.\nA Timed extension of PBC has been proposed in [14]. Although the compo-\nnent model proposed in this paper is not a specification model but an intermedi-\nate model used as gateway between RT-LOTOS and TPNs, we find it important\nto compare our work with [14].\nOf prime interest to us is the way [14] introduces temporal constraints in\nhis framework by providing each action with two time bounds representing the\nearliest firing time and latest firing time. This approach is directly inspired by\nTPNs, where the firing of actions is driven by necessity. However, a well known\nissue with this strategy is that it is badly compatible with a compositional and\nincremental building of specifications. The main difficulty is to compose time\nintervals when dealing with actions synchronization. The operational semantics\nof [14] relies on intervals intersection to calculate a unique time interval for a\nsynchronized transition. However, this approach is not always satisfactory (see\n[13]).\n7 Conclusion\nSearch for efficiency in RT-LOTOS specification verification is the main moti-\nvation behind the work presented in this paper. We propose a transformational\napproach between RT-LOTOS, which is a compositional FDT, and Time Petri\nNets, which are not. The semantics of the two FDTS are compared. In order\nto bridge the gap between RT-LOTOS and TPNs, the latter are embedded into\ncomponents that may be composed. RT-LOTOS-to-TPN translation patterns\nare defined in order to match the RT-LOTOS composition operators. The trans-\nlation has been formally proved to be semantics preserving. The patterns have\nbeen implemented in a prototype tool which takes as input an RT-LOTOS spec-\nification and outputs a TPN in a format that may be processed by TINA [3].\nThe benchmarks provided in Section 6 demonstrate the interest of the proposed\napproach.\nOne major contribution of the paper is to give RT-LOTOS an underlying\nsemantics expressed in terms of TPNS and to clarify the use of RT-LOTOS\noperators, in particular the latency operator. Discussion in this paper is never-\ntheless limited to the control part of the RT-LOTOS FDT defined in [2]. We\nhave recently extended our work to the data part of RT-LOTOS. RT-LOTOS\nspecifications will be translated into the new format supported by TINA: Pred-\nicates/Actions Time Petri nets. The latter enhance the modelling capabilities\nof TPNs with global variables associated with the nets together with predicates\nand actions associated with transitions.\nThe verification approach developed for RT-LOTOS is being adapted to\nTURTLE, a real-time UML profile based on RT-LOTOS. We thus expect to\ndevelop an interface between the TURTLE toolkit [32] and TINA.\nReferences\n1. ISO - Information processing systems - Open Systems Interconnection: LOTOS\n- a formal description technique based on the temporal ordering of observational\nbehaviour. ISO International Standard 8807:1989, ISO (1989)\n2. Courtiat, J.P., Santos, C., Lohr, C., Outtaj, B.: Experience with RT-LOTOS, a\ntemporal extension of the LOTOS formal description technique. Computer Com-\nmunications 23(12) (2000)\n3. Berthomieu, B., Ribet, P., Vernadat, F.: The TINA tool: Construction of abstract\nstate space for Petri nets and time Petri nets. Int. Journal of Production Research\n42(14) (2004)\n4. Milner, R.: Communications and Concurrency. Prentice Hall (1989)\n5. Hoare, C.: Communicating Sequential Processes. Prentice-Hall (1985)\n6. Courtiat, J.P.: Formal design of interactive multimedia documents. In H.Konig,\nM.Heiner, A., ed.: Proc. of 23rd IFIP WG 6.1 Int Conf on Formal Techniques for\nNetworked and distributed systems (FORTE’2003). Volume 2767 of LNCS. (2003)\n7. Courtiat, J.P., de Oliveira, R.: On RT-LOTOS and its application to the formal\ndesign of multimedia protocols. Annals of Telecommunications 50(11–12) (1995)\n888–906\n8. Merlin, P.: A study of the recoverability of computer system. PhD thesis, Dep.\nComput. Sci., Univ. California, Irvine (1974)\n9. Merlin, P., Faber, D.J.: Recoverability of communication protocols. IEEE Trans-\nactions on Communications COM-24(9) (1976)\n10. Berthomieu, B., Menasche, M.: Une approche par e´nume´ration pour l’analyse des\nre´seaux de Petri temporels. In: Actes de la confe´rence IFIP’83. (1983) 71–77\n11. Berthomieu, B., Diaz, M.: Modeling and verification of time dependant systems\nusing Time Petri Nets. IEEE Transactions on Software Engineering 17(3) (1991)\n12. Best, E., Devillers, R., Koutny, M.: Petri Net Algebra. Monographs in Theoretical\nComputer Science: An EATCS Series. Springer-Verlag (2001) ISBN: 3-540-67398-9.\n13. Sadani, T., Boyer, M., de Saqui-Sannes, P., Courtiat, J.P.: Effective representation\nof regular RT-LOTOS terms by finite time petri nets. Technical Report 05605,\nLAAS/CNRS (2006)\n14. Koutny, M.: A compositional model of time Petri nets. In: Proc. of the 21st Int.\nConf. on Application and Theory of Petri Nets (ICATPN 2000). Number 1825 in\nLNCS, Aarhus, Denmark, Springer-Verlag (2000) 303–322\n15. Taubner, D.: Finite Representations of CCS and TCSP Programs by Automata\nand Petri Nets. Number 369 in LNCS. Springer-Verlag (1989)\n16. Yi, W.: Real-time behaviour of asynchronous agents. In: Proc. of Int. Conf on\nTheories of Concurrency: Unification and Extension (CONCUR). Volume 458 of\nLNCS. (1990)\n17. RT-LOTOS: Real-time LOTOS home page. (http://www.laas.fr/RT-LOTOS/)\n18. Milner, R.: A calculus of communication systems. Volume 92 of LNCS. (1980)\n19. Yannakakis, M., Lee, D.: An efficient algorithm for minimizing real-time transition\nsystem. In: Proc. of f the Conf. on Computer-Aided Verification (CAV). Volume\n697 of LNCS., Berlin (1993)\n20. Goltz, U.: On representing CCS programs by finite Petri nets. In: Proc. of Int.\nConf. on Math. Foundations of Computer Science. Volume 324 of LNCS. (1988)\n21. Olderog, E.R.: Nets, Terms, and formulas. Cambridge University Press (1991)\n22. Garavel, H., Sifakis, J.: Compilation and verification of LOTOS specifications.\nIn Logrippo, L., et al., eds.: Protocol Specification, Testing and Verification, X.\nProceedings of the IFIP WG 6.1 Tenth International Symposium, 1990, Ottawa,\nOnt., Canada, Amsterdam, Netherlands, North-Holland (1990) 379–394\n23. Barbeau, M., von Bochmann, G.: Verification of LOTOS specifications: A Petri\nnet based approach. In: Proc. of Canadian Conf. on Electrical and Computer\nEngineering. (1990)\n24. Larrabeiti, D., Quelmada, J., Pavo´n, S.: From LOTOS to Petri nets through expan-\nsion. In Gotzhein, R., Bredereke, J., eds.: Proc. of Int. Conf. on Formal Description\nTechniques and Theory, application and tools (FORTE/PSV’96). (1996)\n25. Barbeau, M., von Bochmann, G.: Extension of the Karp and Miller procedure to\nLOTOS specifications. Discrete Mathematics and Theoretical Computer Science\n3 (1991) 103–119\n26. Barbeau, M., von Bochmann, G.: A subset of LOTOS with the computational\npower of place/transition-nets. In: Proc. of the 14th Int. Conf. on Application and\nTheory of Petri Nets (ICATPN). Volume 691 of LNCS. (1993)\n27. Garavel, H., Lang, F., Mateescu, R.: An overview of cadp 2001. European Asso-\nciation for software science and technology (EASST) Newsletter 4 (2002)\n28. Sisto, R., Valenzano, A.: Mapping Petri nets with inhibitor arcs onto basic LOTOS\nbehavior expressions. IEEE Transactions on computers 44(12) (1995) 1361–1370\n29. Sadani, T., Courtiat, J., de Saqui-Sannes, P.: From RT-LOTOS to time Petri nets.\nnew foundations for a verification platform. In: Proc. of 3rd IEEE Int Conf on\nSoftware Engineering and Formal Methods (SEFM). (2005)\n30. Bolognesi, T., Lucidi, F., Trigila, S.: From timed Petri nets to timed LOTOS.\nIn: Protocol Specification, Testing and Verification X (PSTV), Proceedings of the\nIFIP WG6.1 Tenth International Symposium on Protocol. (1990) 395–408\n31. Durante, L., Sisto, R., Valenzano, A.: Integration of time Petri net and TE-LOTOS\nin the design and evaluation of factory communication systems. In: Proc. of the\n2nd IEEE Workshop on Factory Communications Systems (WFCS’97). (1997)\n32. Apvrille, L., Courtiat, J.P., Lohr, C., de Saqui-Sannes, P.: TURTLE : A real-\ntime UML profile supported by a formal validation toolkit. IEEE Transactions on\nSoftware Engineering 30(4) (2004)\n",
            "id": 5185874,
            "identifiers": [
                {
                    "identifier": "10.1007/11901433_20",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:oatao.univ-toulouse.fr:2151",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "1735957012",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "12040536",
                    "type": "CORE_ID"
                }
            ],
            "title": "Mapping RT-LOTOS specifications into Time Petri Nets",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "1735957012",
            "oaiIds": [
                "oai:oatao.univ-toulouse.fr:2151"
            ],
            "publishedDate": "2006-01-01T00:00:00",
            "publisher": "'Springer Science and Business Media LLC'",
            "pubmedId": null,
            "references": [
                {
                    "id": 10427248,
                    "title": "A calculus of communication systems.",
                    "authors": [],
                    "date": "1980",
                    "doi": "10.1007/3-540-10235-3",
                    "raw": "Milner, R.: A calculus of communication systems. Volume 92 of LNCS. (1980)",
                    "cites": null
                },
                {
                    "id": 10427245,
                    "title": "A compositional model of time Petri nets. In:",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1007/3-540-44988-4_18",
                    "raw": "Koutny, M.: A compositional model of time Petri nets. In: Proc. of the 21st Int. Conf. on Application and Theory of Petri Nets (ICATPN 2000). Number 1825 in LNCS, Aarhus, Denmark, Springer-Verlag (2000) 303–322",
                    "cites": null
                },
                {
                    "id": 10427267,
                    "title": "A realtime UML proﬁle supported by a formal validation toolkit.",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1109/tse.2004.34",
                    "raw": "Apvrille, L., Courtiat, J.P., Lohr, C., de Saqui-Sannes, P.: TURTLE : A realtime UML proﬁle supported by a formal validation toolkit. IEEE Transactions on Software Engineering 30(4) (2004)",
                    "cites": null
                },
                {
                    "id": 10427239,
                    "title": "A study of the recoverability of computer system.",
                    "authors": [],
                    "date": "1974",
                    "doi": null,
                    "raw": "Merlin, P.: A study of the recoverability of computer system. PhD thesis, Dep. Comput. Sci., Univ. California, Irvine (1974)",
                    "cites": null
                },
                {
                    "id": 10427255,
                    "title": "A subset of LOTOS with the computational power of place/transition-nets. In:",
                    "authors": [],
                    "date": "1993",
                    "doi": "10.1007/3-540-56863-8_40",
                    "raw": "Barbeau, M., von Bochmann, G.: A subset of LOTOS with the computational power of place/transition-nets. In: Proc. of the 14th Int. Conf. on Application and Theory of Petri Nets (ICATPN). Volume 691 of LNCS. (1993)",
                    "cites": null
                },
                {
                    "id": 10427265,
                    "title": "A.: Integration of time Petri net and TE-LOTOS in the design and evaluation of factory communication systems. In:",
                    "authors": [],
                    "date": "1997",
                    "doi": "10.1109/wfcs.1997.634358",
                    "raw": "Durante, L., Sisto, R., Valenzano, A.: Integration of time Petri net and TE-LOTOS in the design and evaluation of factory communication systems. In: Proc. of the 2nd IEEE Workshop on Factory Communications Systems (WFCS’97). (1997)",
                    "cites": null
                },
                {
                    "id": 10427259,
                    "title": "A.: Mapping Petri nets with inhibitor arcs onto basic LOTOS behavior expressions.",
                    "authors": [],
                    "date": "1995",
                    "doi": "10.1109/12.477242",
                    "raw": "Sisto, R., Valenzano, A.: Mapping Petri nets with inhibitor arcs onto basic LOTOS behavior expressions. IEEE Transactions on computers 44(12) (1995) 1361–1370",
                    "cites": null
                },
                {
                    "id": 10427249,
                    "title": "An eﬃcient algorithm for minimizing real-time transition system. In:",
                    "authors": [],
                    "date": "1993",
                    "doi": "10.1007/3-540-56922-7_18",
                    "raw": "Yannakakis, M., Lee, D.: An eﬃcient algorithm for minimizing real-time transition system. In: Proc. of f the Conf. on Computer-Aided Veriﬁcation (CAV). Volume 697 of LNCS., Berlin (1993)20. Goltz, U.: On representing CCS programs by ﬁnite Petri nets. In: Proc. of Int. Conf. on Math. Foundations of Computer Science. Volume 324 of LNCS. (1988)",
                    "cites": null
                },
                {
                    "id": 10427257,
                    "title": "An overview of cadp 2001. European Association for software science and technology (EASST)",
                    "authors": [],
                    "date": "2002",
                    "doi": null,
                    "raw": "Garavel, H., Lang, F., Mateescu, R.: An overview of cadp 2001. European Association for software science and technology (EASST) Newsletter 4 (2002)",
                    "cites": null
                },
                {
                    "id": 10427236,
                    "title": "Communicating Sequential Processes.",
                    "authors": [],
                    "date": "1985",
                    "doi": "10.1016/0167-6423(87)90028-1",
                    "raw": "Hoare, C.: Communicating Sequential Processes. Prentice-Hall (1985)",
                    "cites": null
                },
                {
                    "id": 10427251,
                    "title": "Compilation and veriﬁcation of LOTOS speciﬁcations.",
                    "authors": [],
                    "date": "1990",
                    "doi": "10.1109/icse.1992.753504",
                    "raw": "Garavel, H., Sifakis, J.: Compilation and veriﬁcation of LOTOS speciﬁcations. In Logrippo, L., et al., eds.: Protocol Speciﬁcation, Testing and Veriﬁcation, X. Proceedings of the IFIP WG 6.1 Tenth International Symposium, 1990, Ottawa, Ont., Canada, Amsterdam, Netherlands, North-Holland (1990) 379–394",
                    "cites": null
                },
                {
                    "id": 10427240,
                    "title": "D.J.: Recoverability of communication protocols.",
                    "authors": [],
                    "date": "1976",
                    "doi": "10.1109/tcom.1976.1093424",
                    "raw": "Merlin, P., Faber, D.J.: Recoverability of communication protocols. IEEE Transactions on Communications COM-24(9) (1976)",
                    "cites": null
                },
                {
                    "id": 10427234,
                    "title": "Experience with RT-LOTOS, a temporal extension of the LOTOS formal description technique.",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1016/s0140-3664(99)00240-6",
                    "raw": "Courtiat, J.P., Santos, C., Lohr, C., Outtaj, B.: Experience with RT-LOTOS, a temporal extension of the LOTOS formal description technique. Computer Communications 23(12) (2000)",
                    "cites": null
                },
                {
                    "id": 10427254,
                    "title": "Extension of the Karp and Miller procedure to LOTOS speciﬁcations.",
                    "authors": [],
                    "date": "1991",
                    "doi": "10.1007/bfb0023747",
                    "raw": "Barbeau, M., von Bochmann, G.: Extension of the Karp and Miller procedure to LOTOS speciﬁcations. Discrete Mathematics and Theoretical Computer Science 3 (1991) 103–119",
                    "cites": null
                },
                {
                    "id": 10427246,
                    "title": "Finite Representations of CCS and TCSP Programs by Automata and Petri Nets. Number 369 in LNCS.",
                    "authors": [],
                    "date": "1989",
                    "doi": "10.1007/3-540-51525-9",
                    "raw": "Taubner, D.: Finite Representations of CCS and TCSP Programs by Automata and Petri Nets. Number 369 in LNCS. Springer-Verlag (1989)",
                    "cites": null
                },
                {
                    "id": 10427237,
                    "title": "Formal design of interactive multimedia documents. In",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1007/978-3-540-39979-7_23",
                    "raw": "Courtiat, J.P.: Formal design of interactive multimedia documents. In H.Konig, M.Heiner, A., ed.: Proc. of 23rd IFIP WG 6.1 Int Conf on Formal Techniques for Networked and distributed systems (FORTE’2003). Volume 2767 of LNCS. (2003)",
                    "cites": null
                },
                {
                    "id": 10427261,
                    "title": "From RT-LOTOS to time Petri nets. new foundations for a veriﬁcation platform. In:",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1109/sefm.2005.22",
                    "raw": "Sadani, T., Courtiat, J., de Saqui-Sannes, P.: From RT-LOTOS to time Petri nets. new foundations for a veriﬁcation platform. In: Proc. of 3rd IEEE Int Conf on Software Engineering and Formal Methods (SEFM). (2005)",
                    "cites": null
                },
                {
                    "id": 10427263,
                    "title": "From timed Petri nets to timed LOTOS. In:",
                    "authors": [],
                    "date": "1990",
                    "doi": "10.1016/0920-5489(94)90002-7",
                    "raw": "Bolognesi, T., Lucidi, F., Trigila, S.: From timed Petri nets to timed LOTOS. In: Protocol Speciﬁcation, Testing and Veriﬁcation X (PSTV), Proceedings of the IFIP WG6.1 Tenth International Symposium on Protocol. (1990) 395–408",
                    "cites": null
                },
                {
                    "id": 10427232,
                    "title": "Information processing systems - Open Systems Interconnection: LOTOS - a formal description technique based on the temporal ordering of observational behaviour.",
                    "authors": [],
                    "date": "1989",
                    "doi": "10.3403/00230466",
                    "raw": "ISO - Information processing systems - Open Systems Interconnection: LOTOS - a formal description technique based on the temporal ordering of observational behaviour. ISO International Standard 8807:1989, ISO (1989)",
                    "cites": null
                },
                {
                    "id": 10427244,
                    "title": "J.P.: Eﬀective representation of regular RT-LOTOS terms by ﬁnite time petri nets.",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1007/11888116_29",
                    "raw": "Sadani, T., Boyer, M., de Saqui-Sannes, P., Courtiat, J.P.: Eﬀective representation of regular RT-LOTOS terms by ﬁnite time petri nets. Technical Report 05605, LAAS/CNRS (2006)",
                    "cites": null
                },
                {
                    "id": 10427242,
                    "title": "Modeling and veriﬁcation of time dependant systems using Time Petri Nets.",
                    "authors": [],
                    "date": "1991",
                    "doi": "10.1109/32.75415",
                    "raw": "Berthomieu, B., Diaz, M.: Modeling and veriﬁcation of time dependant systems using Time Petri Nets. IEEE Transactions on Software Engineering 17(3) (1991)",
                    "cites": null
                },
                {
                    "id": 10427250,
                    "title": "Nets, Terms, and formulas.",
                    "authors": [],
                    "date": "1991",
                    "doi": "10.1017/cbo9780511526589",
                    "raw": "Olderog, E.R.: Nets, Terms, and formulas. Cambridge University Press (1991)",
                    "cites": null
                },
                {
                    "id": 10427238,
                    "title": "On RT-LOTOS and its application to the formal design of multimedia protocols.",
                    "authors": [],
                    "date": "1995",
                    "doi": "10.1109/ftdcs.1995.525004",
                    "raw": "Courtiat, J.P., de Oliveira, R.: On RT-LOTOS and its application to the formal design of multimedia protocols. Annals of Telecommunications 50(11–12) (1995) 888–906",
                    "cites": null
                },
                {
                    "id": 10427243,
                    "title": "Petri Net Algebra. Monographs in Theoretical Computer Science: An EATCS Series. Springer-Verlag",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1007/978-3-662-04457-5",
                    "raw": "Best, E., Devillers, R., Koutny, M.: Petri Net Algebra. Monographs in Theoretical Computer Science: An EATCS Series. Springer-Verlag (2001) ISBN: 3-540-67398-9.",
                    "cites": null
                },
                {
                    "id": 10427247,
                    "title": "Real-time behaviour of asynchronous agents. In:",
                    "authors": [],
                    "date": "1990",
                    "doi": "10.1007/bfb0039080",
                    "raw": "Yi, W.: Real-time behaviour of asynchronous agents. In: Proc. of Int. Conf on Theories of Concurrency: Uniﬁcation and Extension (CONCUR). Volume 458 of LNCS. (1990)",
                    "cites": null
                },
                {
                    "id": 10427253,
                    "title": "S.: From LOTOS to Petri nets through expansion.",
                    "authors": [],
                    "date": "1996",
                    "doi": null,
                    "raw": "Larrabeiti, D., Quelmada, J., Pav´ on, S.: From LOTOS to Petri nets through expansion. In Gotzhein, R., Bredereke, J., eds.: Proc. of Int. Conf. on Formal Description Techniques and Theory, application and tools (FORTE/PSV’96). (1996)",
                    "cites": null
                },
                {
                    "id": 10427235,
                    "title": "The TINA tool: Construction of abstract state space for Petri nets and time Petri nets.",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1080/00207540412331312688",
                    "raw": "Berthomieu, B., Ribet, P., Vernadat, F.: The TINA tool: Construction of abstract state space for Petri nets and time Petri nets. Int. Journal of Production Research 42(14) (2004)",
                    "cites": null
                },
                {
                    "id": 10427241,
                    "title": "Une approche par ´ enum´ eration pour l’analyse des r´ eseaux de Petri temporels. In: Actes de la conf´ erence IFIP’83.",
                    "authors": [],
                    "date": "1983",
                    "doi": null,
                    "raw": "Berthomieu, B., Menasche, M.: Une approche par ´ enum´ eration pour l’analyse des r´ eseaux de Petri temporels. In: Actes de la conf´ erence IFIP’83. (1983) 71–77",
                    "cites": null
                },
                {
                    "id": 10427252,
                    "title": "Veriﬁcation of LOTOS speciﬁcations: A Petri net based approach. In:",
                    "authors": [],
                    "date": "1990",
                    "doi": "10.1007/3-540-55179-4_11",
                    "raw": "Barbeau, M., von Bochmann, G.: Veriﬁcation of LOTOS speciﬁcations: A Petri net based approach. In: Proc. of Canadian Conf. on Electrical and Computer Engineering. (1990)",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "http://oatao.univ-toulouse.fr/2151/1/De_Saqui_Sannes_2151.pdf"
            ],
            "updatedDate": "2022-02-28T02:21:22",
            "yearPublished": 2006,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "0302-9743"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/12040536.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/12040536"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/12040536/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/12040536/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/5185874"
                }
            ]
        },
        {
            "acceptedDate": "2004-09-18T00:00:00",
            "arxivId": "q-bio/0405007",
            "authors": [
                {
                    "name": "A. Pagnani"
                },
                {
                    "name": "Gavin"
                },
                {
                    "name": "Hishigaki"
                },
                {
                    "name": "Ho"
                },
                {
                    "name": "Hodgman"
                },
                {
                    "name": "Jansen"
                },
                {
                    "name": "Lappe"
                },
                {
                    "name": "Letovsky"
                },
                {
                    "name": "M. Leone"
                },
                {
                    "name": "Marcotte"
                },
                {
                    "name": "Marcotte"
                },
                {
                    "name": "Pearson"
                },
                {
                    "name": "Schwikowski"
                },
                {
                    "name": "Uetz"
                },
                {
                    "name": "Vazquez"
                },
                {
                    "name": "Xenarios"
                },
                {
                    "name": "Yook"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/76526762",
                "https://api.core.ac.uk/v3/outputs/186865790",
                "https://api.core.ac.uk/v3/outputs/234905638"
            ],
            "createdDate": "2012-04-13T14:24:44",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 12601,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/12601",
                    "logo": "https://api.core.ac.uk/data-providers/12601/logo"
                },
                {
                    "id": 351,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/351",
                    "logo": "https://api.core.ac.uk/data-providers/351/logo"
                }
            ],
            "depositedDate": "2004-09-17T00:00:00",
            "abstract": "Motivation: In the last few years a growing interest in biology has been\nshifting towards the problem of optimal information extraction from the huge\namount of data generated via large scale and high-throughput techniques. One of\nthe most relevant issues has recently become that of correctly and reliably\npredicting the functions of observed but still functionally undetermined\nproteins starting from information coming from the network of co-observed\nproteins of known functions.\n  Method: The method proposed in this article is based on a message passing\nalgorithm known as Belief Propagation, which takes as input the network of\nproteins physical interactions and a catalog of known proteins functions, and\nreturns the probabilities for each unclassified protein of having one chosen\nfunction. The implementation of the algorithm allows for fast on-line analysis,\nand can be easily generalized to more complex graph topologies taking into\naccount hyper-graphs, {\\em i.e.} complexes of more than two interacting\nproteins.Comment: 12 pages, 9 eps figures, 1 additional html tabl",
            "documentType": "research",
            "doi": "10.1093/bioinformatics/bth491",
            "downloadUrl": "http://arxiv.org/abs/q-bio/0405007",
            "fieldOfStudy": "computer science",
            "fullText": "ar\nX\niv\n:q\n-b\nio\n/0\n40\n50\n07\nv1\n  [\nq-\nbio\n.Q\nM\n]  7\n M\nay\n 20\n04\nPredicting protein functions with message passing algorithms\nM. Leone1, A. Pagnani1\n1 Institute for Scientific Interchange (ISI), Viale Settimio Severo 65, I-10133 , Turin, Italy\nMotivation: In the last few years a growing interest in biology has been shifting towards the\nproblem of optimal information extraction from the huge amount of data generated via large scale\nand high-throughput techniques. One of the most relevant issues has recently become that of cor-\nrectly and reliably predicting the functions of observed but still functionally undetermined proteins\nstarting from information coming from the network of co-observed proteins of known functions 1.\nMethod: The method proposed in this article is based on a message passing algorithm known\nas Belief Propagation 2, which takes as input the network of proteins physical interactions and a\ncatalog of known proteins functions, and returns the probabilities for each unclassified protein of\nhaving one chosen function. The implementation of the algorithm allows for fast on-line analysis,\nand can be easily generalized to more complex graph topologies taking into account hyper-graphs,\ni.e. complexes of more than two interacting proteins.\nResults: The benchmark of our method is the Saccaromices Cerevisiæ protein-protein interaction\nnetwork (PPI)3,4 and the validity of our approach is successfully tested against other available\ntechniques5,6,7.\nContact: leone@isiosf.isi.it, andrea.pagnani@roma1.infn.it\nKeywords: protein-protein interaction, protein function prediction, message passing algorithms,\nbelief propagation.\nI. INTRODUCTION\nThe most classical protein function prediction methods are those inferring similarity in function from sequence\nhomologies between proteins listed in databases using programs such as FASTA8 and BLAST9; via comparison with\nknown proteins interactions in similar genomes (the so called Rosetta Stone Method10); or by phylogenetic analysis11.\nMore recently, a new class of methods has been proposed that relies on the available data on the global structure of the\nPPI networks for a growing number of organisms of completely sequenced genome3,12,13. The most complete available\non-line data are structured in a graph-like format, with graph sites indexed with protein names and links representing\na physical experimentally tested interaction among two proteins. More limited databases on larger protein complexes\nare also available14,15. From the side of functional classification, databases are now available (MIPS24 and Gene\nOntology among others25), that provide a classification of a continuously growing number of proteins, listing them\nin different functional categories classes with a hierarchical-like organization. Among the presently available methods\nthat try to exploit the global PPI network structure to infer yet unknown functions for unclassified proteins whose\ninteractions with the rest of the graph are at least partially known, there are the so called Neighboring Counting\nMethod16, the χ2 Method17, the Bayesian approaches5,6, the Redundancy Method18 and a more recent Monte Carlo\nSimulated Annealing (SA) approach7.\nII. METHODS\nLet us name G a PPI graph, with set of vertexes V = {1, · · · , N} representing the observed proteins, each protein\nname being assigned a numerical value form 1 to N . Let us also define a mapping between the set of all observed\nfunctions and the numbered set F = {1, · · · , F}. Each protein i belonging to V can then be characterized via a\ndiscrete variable Xi that can take values f ∈ F . One would like to compute the probability Pi(f) = Pr(Xi = f) for\neach protein to have a given function f given the functions assigned to the proteins in the rest of the graph. The\nmethod is based on the definition of a score function E on the PPI graph (see eq. (1)), that counts the number of\nall common predicted functions among neighboring proteins of the graph over all interactions. In addition to this, a\ncertain fraction of the proteins is already classified, which means that there exists a subset A ⊂ V of vertexes with at\nleast one function belonging to F attached to it (see fig.(1) (a) for an example of a graph portion). The effect of the\nalready classified proteins with a given function in the neighborhood of protein i on the PPI network is taken into\naccount as an external field acting on i and proportional to the number of the neighbors belonging to A with that\ngiven function. From this score function a variational potential (called Gibbs potential) can be defined that measures\nthe distance between the true unknown function probabilities and a trial estimation of them. The values of the best\nestimated probabilities are found extremizing the Gibbs potential2,19. The Gibbs potential extremizing equations\nused in this work are commonly known under the name of Belief Propagation (BP) equations and can be easily found\nvia a procedure called Cavity Method20. We have solved the BP equations both for the probabilities of completely\n2unclassified proteins belonging to V \\A and for the more complete model where we let a protein belonging to A the\npossibility of having other yet unknown functions. The modifications to be applied to the method are technical minor\nso that they will not be described here. Given a choice of initial conditions on probability functions {Pi(Xi)}i=1,...,N\nand a choice of the score function E, the algorithm calculates the stationary probabilities whose values extremize\nthe resulting Gibbs potential. The potential in general depends on one free real parameter β that plays the role of\nan inverse temperature and weights the possibility of allowing functional assignments that do not exactly maximize\nthe score, but could still be possible due to their large degeneracy: at low enough values of β (high temperature)\nalmost any function assignation to proteins in V \\A gives and equivalent value of the potential. In this region the\nsystem is said to be in a “paramagnetic phase”. Every functional assignment is therefore accepted and the algorithm\nis not predictive. After a certain critical value βc the shape of the Gibbs potential changes: only some values of the\nprobability functions extremize it. Augmenting β, the algorithm tends to weight more and more those functional\nassignments that exactly maximize the score. Strictly at zero temperature (β → ∞) only the score maximizing\nfunctional assignments survive with non zero probability. Given sets V , A and V \\A, the PPI graph G, the graph of\nunclassified proteins U ⊂ G and the set of observed function F , a score function can be defined following Vazquez et\nal.7 as\nE[{Xi}\nN\ni=1] = −\n∑\nij\nJijδ(Xi;Xj)−\n∑\ni\nhi(Xi) (1)\nwhere Jij is the adjacency matrix of U (Jij = 1 if i and j ∈ V \\A and they interact with each other). δ(σ; τ) is the\nKronecker delta function measured between functions σ and τ assigned to the neighboring proteins and hi(τ) is an\nexternal field that counts the number of classified neighbors of protein i in the original graph G that have at least\nfunction τ . The Gibbs potential can than be calculated as a variational way to compute the quantity\nF = −\n1\nβN\nlog\n\n ∑\n{Xi}i=1,...,N\ne−βE[{Xi}i=1,...,N ]\n\n (2)\ncalled free-energy of the system, a fundamental quantity than in statistical physics counts the logarithm of the sum of\nall the weights of the probabilities each configuration of the variables in the systems appear with. Configurations with\na largest statistical weight can then be calculated as those maximizing this potential function. Using the message\npassing approach2,20 under the assumption that correlations are low enough in the graph so that one can write\nPij(Xi, Xj) ∝ Pi(Xi)Pj(Xj) if proteins i and j are chosen at random, one can calculate each Pj(Xj) as product of\nconditional probabilities contributions Mi→j(Xj) incoming to j from all neighbors of protein j, conditional to the fact\nthat j has function Xj:\nPj(Xj) ∝\n∏\ni∈I(j)\nMi→j(Xj) = e\nβ\n∑\ni∈I(j)\nui→j(Xj)\n(3)\nwhere I(j) ⊂ V \\A denotes the set of unclassified neighbors of j and ui→j(σ) is a “message” that represents the field\nin direction σ ∈ F acting on protein j due to the presence of protein i when protein j has function σ. Equations for\nthe message functions can be solved iteratively as fixed points of the system of equations\nMi→j(σ) =\nF∑\nτ=1\n\n ∏\nl∈I(i)\\j\nMl→i(τ)\n\n eβJijδ(σ;τ)+βhi(τ) (4)\none for each link of U , for both directions in the graph. Self consistent BP equations can be rewritten in terms of\nmessages u’s. The ones explicitly used in our algorithm are shown in the following:\nui→j(σ) =\n1\nβ\nlog\n(\nA(β; ~T 1i→j(σ))\nA(β; ~T 2i→j)\n)\n(5)\nwhere \n\nA(β; ~Ti→j) =\n∑F\nτ=1 e\nβT τi→j\n~T\n1,τ\ni→j(σ) = hi(τ) +\n∑\nl∈I(i)\\j ul→i(τ)\nif σ 6= τ\nor hi(τ) +\n∑\nl∈I(i)\\j ul→i(τ) + 1\nif σ = τ\n~T\n2,τ\ni→j = hi(τ) +\n∑\nl∈I(i)\\j ul→i(τ)\n3115\n115\n449  472  473  829  \n829\n829\n829\n829\n115\n1   \n115  282  285  288  829   \n1   1   1   1   1   \n29  115  131  246  271  472  473  474  827  828  829\n2   2   1   1   1   1   1   1   1   3h:\nF:\nI(i) \\j\nh i \\ k\nk iu\nk iu\nui j\n378\n246\n29  131\n29  115  473  474\n271  472  827  828  829\n288\n282  285\nF:\nh:\nh:\nF:\nu\nu\nu\nu\nu\nu\nu\nu\nu\nu\nu\nu\nk\nk\ni j\n(b)\n(c)\n(a)\n652\n582\n269\n488\n690\n693\n694\n259\n652\n423\n3\n1820\n691\n528\n692\n362\n683\n688\n282\n429\n433\n689\n2 433\n689\n688\n694\n693\n690\nFIG. 1: From G to the BP equations: fig.(a) shows a small fraction of U network G. Circles represent proteins with their\nnumerical ID used by the algorithm. Classified proteins are filled, while unclassified ones are left white. Each classified protein\nhas a series of functions whose numerical values ∈ F are written in boxes. In (b) only the corresponding part of the U subgraph\nhas been drawn. Dotted arrows represent external fields acting on the unclassified proteins and are vectors whose non zero\ncomponents are defined in the Lower boxes. For each protein in U , they count the number of classified neighbors having a\ngiven function. Upper boxes are sets of all functions of all classified proteins neighboring a given unclassified one. Thick arrows\nrepresent “messages” among unclassified proteins according to eq. (4). Notice how U is significantly less connected than G and\noften divides in smaller connected components. (c) Is a more detailed representation of the message passing between proteins\ni and j, in direction i→ j.\nThe BP algorithm has been written in terms of that equation and solved at any β with a population dynamic\ntechnique20. In general, all previously described quantities depend on the inverse temperature β. Eq. (3) turns out\nto be a good approximation of the solution of the problem of finding the probabilities for configurations maximizing\n(2). A pictorial view of the iteration procedure is shown in fig (1)(c).\n4G U\ncs noc cs noc\n2 114 1 248\n3 30 2 40\n4 23 3 8\n5 6 4 7\n6 4 5 1\n7 1 6 1\n8 4 7 1\n11 1 14 1\n13 1 17 1\n1299 1 27 1\nTABLE I: Cluster size (cs) and number of clusters (noc) for both the original graph G (the two leftmost columns) and the\ngraph of the unknow proteins U (the two rightmost columns). Note the giant componenent of 1299 sites for the G graph.\nIII. DATA AND GRAPH ANALYSIS\nAs benchmarks for the method, we have used two yeast Saccaromices Cerevisiæ PPI graphs3,4, referred in the\nfollowing as U and D respectively. The functional categories set F was extracted form the MIPS database. The U\nnetwork contains N = 1826 proteins out of which 1370 belong to A, while the remaining 456 are unclassified or have\nan unclear MIPS classification; and M = 2238 pairwise interactions. The D network contains N = 4713 proteins out\nof which 3303 belong to A and 1410 are unclassified; and M = 14846 interactions. Different choices of functional\ncategories sets F are possible in the MIPS database, depending on the level of the coarse-grained specification of the\nhierarchical classification scheme. We used the latest publicly available finest classification scheme retrieving F = 165\nfunctional categories present U and F = 176 in D, but experiments where run also on the most coarse-grained\nclassification scheme. Results are available upon request.\nThe PPI graph consists of a giant component of 1299 sites (990 classified), and the rest of the sites are grouped into\n184 smaller isolated components of at most 13 sites. We have also analyzed the structure of the V \\A graph which\nturns of 456 sites, grouped in 309 clusters of size at most 27. Each cluster in V \\A can be considered as an isolated\nfunctional island of the graph surrounded by external fields as displayed in the last picture of the main body of the\npaper. More details on the cluster composition of both G and U for the U PPI network are shown in Table I. One\nmay wonder if these clusters are more then a topological feature of our model, but reflect also a more interesting\nfunctional segregation. In other words one is interested to understand in quantitative terms how different clusters in\nV \\A label different functional areas in our graphs. To this aim we measured inter-cluster and intra-cluster functional\noverlaps as in eqs. (6) and (7) Both observables take value in the interval (0, 1) and give a measure of the functional\nsimilarity of clusters (higher values indicate higher similarity). The emerging scenario shows clear signs of segregation\nsince the intra-cluster overlap distributions has support in the interval (0, 0.1) while the inter-cluster distribution has\nsupport in the whole interval (0, 1). This test can be interpreted as a coherence test on the graph itself, and also on\nthe working hypothesis of our method, since segregation is tacitly assumed in the functional form of score function\nwhere only first neighbors interations on the graph are taken into account. Let us define the notion of intra and inter\ncluster functional overlap as\nOi =\n1\nN2i\n∑\nl,k∈Ci\nφ(sl, sl)\nΦ(Ci)\n(6)\noij =\n1\nNiNj\n∑\nl∈Ci\n∑\nl∈Cj\nφ(sl, sk)\nΦ(Ci\n⋃\nCj)\ni 6= j (7)\nwhere index i labels the different clusters Ci and run between 1 and the total number of clusters C, Ni is the number\nof site in cluster Ci, φ(sl, sk) counts the number of function that site l and j have in common, and Φ(Ci) is the number\nof different functions acting onto cluster Ci, while Φ(Ci\n⋃\nCj) is the number of different functions acting onto the union\nset Ci\n⋃\nCj . It is interesting to note that according to Eqs. 6, 7 both oi and Oij have take real values in the interval\n5 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0  0.2  0.4  0.6  0.8  1\nΠ\n(in\nter\n,in\ntra\n)\noverlap\ninter-cluster\nintra-cluster\nFIG. 2: Cumulative probability distribution of intra/inter-cluster funtional overlap as defined in Eqs. 6, 7 for the U U graph\n. Since the intra-cluster overlap turns out to be alwais lower than 0.085, the intra-cluster cumulative probability distribution\n(solid line) saturates to 1 above this value. The inter-cluster overlap instead shows clear sign of segregation. Note that the\nsudden jump at 1 for the dashed line is due to a significative fraction of clusters (84 out of 309) with functional overlap strictly\nequal to one.\n(0, 1). We can consider probability densities of the two variables Oi and oij as\nP (intra)(O) =\n1\nN\nC∑\ni=1\nδ(O,Oi) (8)\nP (inter)(o) =\n1\nN(N − 1)\n∑\n1≤i<j≤C\nδ(o, oij) (9)\nwhere δ(a, b) is the Kronecker delta function equal to 1 when a = b and zero otherwise. It is interesting a this point\nto compare the average intracluster overlap 〈O〉 = 0.440673 with the average entercluster overlap 〈o〉 = 0.0294147\nwhich is a factor 15 smaller and that can be taken as a quantitative measure of the functional segregation on the PPI\ngraph.\nWe define then the cumulative distribution functions as\nΠ(intra)(O) =\n∫ O\n0\nP (intra)(x) dx (10)\nΠ(inter)(o) =\n∫ o\n0\nP (inter)(x) dx . (11)\nThe two cumulative functions are displayed in Fig.2. The algorithm can be run separately and in parallel on all\nconnected components of U , because there no exchange of information between them. Equivalently speaking, the\nscore function can be written as a sum over all components c of separated scores: E =\n∑\ncEc({Xi}i∈c).\n6 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 2  3  4  5  6  7  8  9  10  11\nF 1\nProtein degree\n(a)\nSA\nNCM\nd4 alld5 alld7 all\nd4 2\nnd\nd5 2\nnd\nd7 2\nnd\nd4 1\nst\nd5 1\nst\nd7 1\nst\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 2  3  4  5  6  7  8  9  10  11\nF 2\nProtein degree\n(b)\nd4 alld5 alld7 all\nd4 2\nnd\nd5 2\nnd\nd7 2\nnd\nd4 1\nst\nd5 1\nst\nd7 1\nst\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 2  3  4  5  6  7  8  9  10  11\nS\nProtein degree\n(c)\nd4 alld5 alld7 all\nd4 2\nnd\nd5 2\nnd\nd7 2\nnd\nd4 1\nst\nd5 1\nst\nd7 1\nst\nFIG. 3: (a) F1, (b) F2 and (c) Sharpness versus protein degree for different U dilutions, as described in the text. Results\nare displayed for three dilution levels d4 = 0.4, d5 = 0.5, d7 = 0.7. Dotted lines are results considering only functions of higher\nprobabilities (1st best rank). Dotted-dashed lines are results considering both best and second best ranks. Thick lines consider\nall non background noise ranks. SA and NCM are the Simulated Annealing and the Neighboring Counting Method results\nfor dilution d = 0.4. Notice that a low value of Sharpness does not necessarily indicate a poor performance of the algorithm. It\ncould also be due to the fact that indeed many functions have not been observed also in already classified proteins and therefore\nthe catalogs are incomplete not only for proteins in U , but on all G.\nIV. RESULTS\nWe have run our algorithm solving eqs. (3) and (4) at several values of β > βc and for different choices of initial\nconditions of populations {Pi(Xi)}i=1,...,N . Results are always very stable with respect to initial conditions. Instead\nof maximizing the Gibbs potential directly at zero temperature we have worked at finite β because we were interested\nalso in predicting functional assignments that could be biologically allowed although not strictly maximizing the\nscore function (1). Above βc, the function probabilities for each protein converge on a set of values organized in\nhierarchies. The probability values are β-dependent, but not so the hierarchical structure (see fig (5) for an example).\nAll results presented in the following are therefore taken at one given high value of β (β = 2 for the DIP PPI graph\nand β = 10 for the U PPI graph. For any protein i and the connected component c i belongs to, we have filtered out\nall background noise probability values for functions that are not present in c and still have a non zero contributions\ndue to the form of eq. (3). We have then collected and ranked the remaining functional probabilities, following their\nemerging hierarchical structure. A list of predicted functions for all the unclassified proteins in the U PPI network\nusing MIPS 2003 functional categories catalog is presented in the Supplementary Table. The rank division is\nexplained in fig.(5). In order to probe the reliability of our algorithm, we have followed the standard procedures of\nVazquez et al.7: starting from G and a corresponding MIPS functional annotation, we disregarded the functions of\na given fraction d of classified proteins and considered them as unclassified. We have called d “dilution” of G. If a\npreviously classified protein is considered unclassified we say it has been “whitened”. With this procedure one obtains\na new larger graph Ud of unclassified proteins, where the algorithm can be run and its ability of finding again the\nerased functions can be tested. This testing procedure is very similar but more stringent than the Leave One-Out\nMethod5, because it assumes as unclassified an extensive fraction of proteins in the graph instead of only one each\ntime. We repeated the procedure for both PPI networks. Results for a set of performance parameters are presented\n7 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8\nF 1\nDilution\n(d)\nD all\nD 1st+2nd\nD 1st\nU all\nU 1st+2nd\nU 1st  0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8\nF 2\nDilution\n(e)\nD all\nD 1st+2nd\nD 1st\nU all\nU 1st+2nd\nU 1st\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8\nS\nDilution\n(f)\nD all\nD 1st+2nd\nD 1st\nU all\nU 1st+2nd\nU 1st\nFIG. 4: (d) F1, (e) F2 and (f) Sharpness versus dilution, averaged over all the PPI network and over n = 10 random dilution\nrealizations. Thick lines are results for the D network, dotted for U. For each network we have again considered 1st best, 1st\nand 2nd best and all non noise ranks results. The different spacing between lines comparing the two networks reflect their\ndifferent topological structure. Proteins to be whitened were chosen randomly in A. The procedure was repeated n = 10 times\n(Larger n datasets can be easily produced, but data are already very stable for n = 10) for each d, and the results averaged.\nWe disregarded as statistically non significant the few observed proteins with k > 8.\nin fig. (3) as a function of protein degrees for some fixed dilution values: Fig.(a):Reliability. We have defined as a\nfirst reliability parameter F1 the fraction of whitened proteins for which the algorithm predicts correctly at least one\nfunction. Fig.(b) measures a second reliability parameter F2, defined as the fraction of correctly predicted functions\nout of all functions a whitened protein has on the original graph G. This test is more stringent because it checks the\nability of the algorithm of predicting not only one function, but as many as it can. It is worth noticing that under\nthe F2 test the method still performs very well when all non background noise ranks are considered. The legenda is\nthe same as in picture (a). (Fig.c): Sharpness. S measures the precision of the method and it is defined as the\nfraction of the number of correctly predicted functions over the number of all predicted ones. It is intuitive that the\nsharpness decreases with the number probability levels (ranks) one accepts as significant. For whitened proteins of\ndegree 5, for instance, on average only 31% of predicted functions belong to the set of already known erased ones,\nwhile in the case of best ranks only, this percentage raises to 65%-70%. In case one allows the algorithm to predict\nstill experimentally unobserved functions also on the classified proteins in G, the sharpness still decreases. Results\nas a function of network dilution. are presented in fig. (4) When (see fig. (3).(a)) a direct comparison with other\nmethods on the same G and MIPS catalog was possible, results of our method were systematically better than both\nthe Neighboring Counting Method and the SA. Performance further improves if we consider non only highest rank\npredictions, but all significant non background noise probabilities. Together with the other available methods, BP\nperforms worse in predicting functions on leaves of the PPI graph, i.e. on whitened proteins with only one neighbor.\nNevertheless, even in this case we observed better reliability results.\n8 0  20  40  60  80  100  120  140\nRa\nnk\nFunction  Id\n1st best\n2nd best\n3rd best\n4th bests\nYDR386W\nFIG. 5: Example of predicted probabilities ranks for protein YDR386W in the MIPS catalog and for the U PPI network. In\nthis example, out of all possible 140 functions only the ones with a vertical bar have non background noise probabilities. Bars\nheights (ranks) are proportional to the logarithm of the probability of having a given function for all the functions ordered on\nthe horizontal axis.\nV. DISCUSSION\nHierarchical probabilities structure: Let consider as a simple example a protein i surrounded by 3 classified\nneighbors, two having function σ and one having function τ . according to (3), in the zero temperature (β →∞) limit\none has Pi(σ) = 1 and Pi(τ) = 0 together with all other function probabilities. However, if the interaction between\nprotein i and the one neighbor with function τ is correct, a biologically more sound functional assignment would be\nthat of giving to protein i both functions. Working at finite β one can see again from (3) that a non zero value of\nPi(τ) is also found. The numerical value will continuously depend of β. The hierarchy of the values of the predicted\nprobabilities turns out to be nevertheless very stable after crossing the critical point βc. One example of probabilities\nat convergence at a given β and for a randomly chosen protein is given in fig. (5).\nExtension to the algorithm from the unclassified proteins graph U only to all proteins in G: Looking\nat four subsequent versions MIPS databases releases (2001, 2002, 2003 and 2004 respectively, one can see that new\nfunctions are progressively assigned to already classified proteins too, so that an inference procedure that allows for\nthis possibility is in principle more complete. However, this procedure can lead to a spreading in the values of inferred\nprobabilities, loosing in Sharpness. Indeed, we tested the performance of our algorithms in both the general and\nthe restricted case, without noticing any significant difference in performance. In fact, since G is significantly more\nconnected than U the algorithm is significantly slower in reaching convergence of the probability values (It still requires\none single run, being therefore faster than the SA approach). Nevertheless, the possibility that the more general case\nwould work significantly better under the definition of a more refined score function, or with a more complete and\nreliable PPI network, or with the extension of the method to multi-body interactions taking into consideration larger\nprotein complexes cannot be ruled out. Results shown in the body of the text have been limited for clarity to the\nrestricted case where inference is measured only on proteins ∈ V \\A and for the 2002 and 2003 MIPS catalogs, in\norder to compare them with results already present in the literature. The same algorithm could be run on the latest\n2004 MIPS catalog release with no effort.\nComparison with other available methods. Differently from SA7, BP algorithm allows to compute directly and\nin a single run all probabilities Pi(f) for a given protein i to be assigned a function f . This is an advantage with\nrespect to the SA approach, where the output of a single run is one configuration only out of a mutually exclusive set,\nand in order to obtain trustful probabilities one should average over a large number of SA runs. Moreover, provided\none can trust the numerical BP results hierarchies at convergence, some non ground state configurations that could\nhave a biological sound interpretation (see Methods for details) are captured in the BP approach in a hierarchical way,\nwhile are missed by SA unless one had time to run a number of cooling experiments of the order of 106 (Compare with\nthe 102 runs reported in Vazquez et al.7). Differently from Kasif et al.6, our version of BP naturally converges and\ndoes not therefore need iterations truncation. The connection of computed probability values with the real unknown\nones can be made only at convergence of the BP iteration equations, and it is not clear how to interpret the probability\nvalues after only a limited number (two in Kasif et al.6) of iteration steps, when one could still be in the middle of\na transient still heavily dependent on initial conditions. Moreover, truncating the iteration after a small number of\nsteps means disregarding propagation of information coming from distant regions of the network, which is the spirit of\nany message passing algorithm like BP. The method could still in principle work if the most distant message passing\n9nodes of any chosen node i ∈ V \\A were a few neighboring steps away. This turns out to be almost the case for the\nconsidered PPI networks, due to high clusterization and function segregation of V \\A, as described in Methods, but\nit is not generally true in inference problems. In a second Bayesian approach5, a large number of external parameters\n(one set for each function) has to be estimated before running the Bayesian inference algorithm. Still, the Gibbs\npotential5 could in principle be of a more complete form, allowing for the presence of a chemical potential-like terms\n(one for each function) proportional to the overall number of times one function is present in the graph. However, it\nis not clear what the reliability of the biological significance of a term of this type, since influence from the classified\nfunctions of distant proteins should already be taken into account in the structure of the message massing procedure.\nMoreover, if the property of functional segregation was true also on the complete (still unknown) PPI network (See\nBarabasi et al.21 for some indications that this might be the case), it is not clear why a protein should have a high\nprobability of being classified with a certain function only because a large group of proteins with a very frequent\nfunction existed, even if not interacting with the protein under consideration. In addition, our BP method does not\nrequire keeping track of single configurations of functions under the iterations, but only directly of probability weights.\nThe algorithm converges to a stable fixed point and does not need the definition of a measuring time window period5.\nTogether with the Monte Carlo approach, our algorithm does not need previous estimation of external parameters\ndefining the Gibbs potential, except for the overall tuning inverse temperature β.\nLimitations. Our method has got of course many limitations. (1) The uncertainty over the graph structure, due to\nthe presence of false positive and false negative interactions. The network topology could vary greatly and the network\ninstead of being divided into connected components could be made of essentially only one giant component. The degree\ndistribution of the network could vary, even though some authors suggest there is evidence for a stabilization towards\na scale-free like form21. Attempts of healing PPI networks errors or missing links are described in the literature22,23,\ntogether with a general description of a message passing approach to network reconstruction19. Our algorithm could\nbe generalized to partially deal in parallel with these problems, considering two sets of dynamical variables {Xi}\nand {Jij} instead of {Xi} only. Each Jij could then take values in a discrete set measuring the likelihood of the\ninteraction between proteins i and j to be present as a function of reliability of the experimental data and of the\npredicted functions assigned to the proteins under consideration. The extrapolated set {Jij} could then be taken as a\nstarting point to calculate new function probabilities over the whole graph using again the BP procedure. Extensions\nof the method in this direction are under study but are not presented in this paper. (2) Pairwise interactions in the\nobserved PPI graph could hide a more complex hyper-graph like structure, with more than two proteins interacting\ntrough protein complexes. Our algorithm is readily generalizable to these cases, but we have not tried to test it on\nactual data yet. (3) The way the BP algorithm predicts functions on classified proteins is intrinsically different from\nthe way new annotations are listed in the growing catalogs: to the authors understanding, experiments are typically\nrun concentrating on one or a limited number of interesting function, while other functions could be disregarded. The\ninference algorithms predictions treat all functions in the same way, so frequency of predicted functions could differ\nsignificantly from the experimentally observed ones. (4) The numerical values of the probabilities can be proved to\nbe correct (for a given score function) only in the case the PPI graph is strictly a tree. This is not the case for the\nexperimental data, where cycles are present. However, we believe that the order of magnitude of the Pi(Xi) values\nshould be trustful. This approximation is one of the sources of error in the results of fig. (3). (5) The PPI graph is\nusually built as a time and space average of all processes going all within the cell: a given protein classified for instance\nwith functions 1 and 2 could in principle interact with two other proteins at times in the cell cycle and/or in different\nplaces. One of the neighboring protein could then take common function 1, while the other could take common\nfunction 2, in a perfectly sound configuration. Running the BP algorithm on the averaged graph would however lead\nto the prediction of both functions on both neighbors. In this way the algorithm would loose predictive power and\nsharpness, however it would still predict the correct functions with a certain probability whose exact numerical values\nshould again be taken “cum grano salis”, as already said in point (4). (6) Different databases information should\nbe merged in a proper way: for instance, the U and D PPI graphs are significantly different both in overlap and in\ntopological structure (this point is strictly connected with (1)). We have decided to apply the function prediction\nmethod to both graphs separately, but in principle it could be used on a merging of different networks that properly\nweights relative reliability of interaction links. (7) The Kronecker delta function defines a binary distance between\nfunctions: one link in U contributes 1 to the total score only if the interacting proteins have exactly the same function;\n0 otherwise. However, The MIPS classification scheme is organized hierarchically: some proteins have very specific\nfunctions, while others can be classified only in a more coarse-grained functional categories. The choice of a binary\ndistance is probably appropriate if one considers only functional categories at a given hierarchical level but it seems\nunsatisfactory for the total classification, where a more complete notion of hierarchical distance between different\nfunctional categories would be needed. In particular one would like to have a distance that recognizes as possibly\nclose two neighboring proteins of functions σ and τ in the case σ belongs to a very specific functional category, while τ\nbelongs only to a broader one that includes the first, but with no further specification. In this paper we have limited\nthe method to the binary distance score function (1), considering only functions at a chosen hierarchical level in the\n10\nMIPS catalogs and disregarding all the others. In this way some information on partial knowledge of the functions\nassigned to a given protein is lost. This limitation becomes particularly dramatic in the case of the use of catalogs\nthat are not organized hierarchically, but in a more complex way such as Gene Ontology. Extensions of this method\nare under study.\nVI. ACKNOWLEDGEMENTS\nThe authors would like to thank Paolo Provero for having given us the starting input for this work, together with\nRiccardo Zecchina for fruitful discussions and critical reading of the manuscript; Alexei Vazquez, Alessandro Flammini\nand Vittoria Colizza for data and discussions.\n1 Hodgman, T. C. A historical perspective on gene/protein functional assignment. Bioinformatics 16, 10-15 (2000).\n2 Yedidia, J.S., Freeman, W.T. and Weiss, Y. Understanding Belief Propagation and Its Generalizations, Exploring Artificial\nIntelligence in the New Millennium, ISBN 1558608117, Chap. 8, pp. 239-236, (2003).\n3 Uetz, P. et al. A comprehensive analysis of protein-protein interactions in Saccharomyces cerevisiae. Nature 403, 623-627\n(2000).\n4 The Database of Interacting Proteins, http://dip.doe-mbi.ucla.edu/\n5 Deng, M. et al. Prediction of protein function using protein-protein interaction data, Proceeding of IEEE Computer Society\nBioinformatics Conference (2002).\n6 Letovsky, S. & Kasif, S. Predicting protein function from protein/protein interaction data: a probabilistic approach, Bioin-\nformatics 19 Suppl 1:i197-204 (2003).\n7 Vazquez, A, Flammini, A., Maritan, A. & Vespignani, A Global protein function prediction in protein-protein interaction\nnetworks. Nature Biotech. 21, 697-700 (2003).\n8 Pearson, W. R.& Lipman, D. J. Improved Tools for Biological Sequence Comparison, PNAS 85 2444-2448 (1988).\n9 Altschul, S. F. et al. Gapped BLAST and PSI-BLAST: a new generation of protein database search programs, Nucleic Acids\nResearch 25 3389-3402, (1997).\n10 Marcotte, E. M. et al. Detecting protein functions and protein-protein interactions from genime sequences, Science 285\n751-753, (1999).\n11 Marcotte, E. M. et al. A combined algorithm for genome-wide prediction of protein function, Nature 402 83-86, (1999).\n12 Ito, T. et al. Toward a protein-protein interaction map of the budding yeast: A comprehensive system to examine two-hybrid\ninteractions in all possible combinations between the yeast proteins. Proc. Nat. Acad. Sci. 98, 4569-1147 (2001).\n13 Giot, L. et al. A protein interaction map of Drosophila melanogaster, Science. 302(5651) 1727-36 (2003).\n14 Gavin A. C. et al. Functional organization of the yeast proteome by systematic analysis of protein complexes, Nature 415,\n141-147, (2002).\n15 Ho, Y. et. al. Systematic identification of protein complexes in Saccharomyces Cerevisiæby mass spectrometry, Nature 415,\n180-183, (2002).\n16 Schwikowski, B., Uetz, P. and Fields, S. A network of protein-protein interactions in yeast. Nature Biotech. 18, 1257-1261\n(2000).\n17 Hishigaki, H., Nakai, K., Ono, T., Tanigami, A., & Tagaki, T. Assessment of prediction accuracy of protein function from\nprotein-protein interaction data. Yeast 18, 523-531 (2001).\n18 Samanta, M. P.& Liang, S. Predicting protein functions from redundancies in large-scale protein interaction networks, Proc.\nNat. Acad. Sci. 100, 12579-12583, (2003).\n19 Yeang, C.-H. & Jaakkola, T. Physical network models and multi-source data integration. In The Seventh Annual International\nConference on Research in Computational Molecular Biology, (2003).\n20 Me`zard, M. & Parisi G., Eur. Phys.J. B 20:217 (2001).\n21 Yook, S. Y., Oltvai, Z. N. & Barabsi, A.-L. Functional and topological characterization of protein interaction networks to\nbe publisehd in Proteomics (2003). Also available at URL: http://www.nd.edu/∼networks/papers.htm\n22 Lappe, M. & Holm, L. Unraveling protein interaction networks with near-optimal efficiency, Nature Biotech. 22 1, 98-103\n(2004).\n23 Jansen, R. et al. A Bayesian Networks Approach for Predicting Protein-Protein Interactions from Genomic Data, Science\n302 449-453 (2003).\n24 The MIPS Comprehensive Yeast Genome Database (CYGD), http://mips.gsf.de/proj/yeast/CYGD/db/.\n25 The Gene Ontology Consortium, http://www.geneontology.org/\n",
            "id": 1134462,
            "identifiers": [
                {
                    "identifier": "234905638",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "q-bio/0405007",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "oai:arxiv.org:q-bio/0405007",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "76526762",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2146529782",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "10.1093/bioinformatics/bth491",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:iris.polito.it:11583/2605764",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "oai:porto.polito.it:2605764",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "186865790",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2647537",
                    "type": "CORE_ID"
                }
            ],
            "title": "Predicting protein functions with message passing algorithms",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2146529782",
            "oaiIds": [
                "oai:arxiv.org:q-bio/0405007",
                "oai:porto.polito.it:2605764",
                "oai:iris.polito.it:11583/2605764"
            ],
            "publishedDate": "2004-01-01T00:00:00",
            "publisher": "'Oxford University Press (OUP)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/q-bio/0405007"
            ],
            "updatedDate": "2021-04-22T15:13:04",
            "yearPublished": 2004,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "1367-4803"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/q-bio/0405007"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/1134462"
                }
            ]
        },
        {
            "acceptedDate": "2002-11-07T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Muresan, Valentin"
                },
                {
                    "name": "Muresan, Valentina"
                },
                {
                    "name": "Vladutiu, M."
                },
                {
                    "name": "Wang, Xiaojun"
                }
            ],
            "contributors": [
                "Valentin"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/11309685",
                "https://api.core.ac.uk/v3/outputs/147599585",
                "https://api.core.ac.uk/v3/outputs/192442000",
                "https://api.core.ac.uk/v3/outputs/143908988"
            ],
            "createdDate": "2013-07-10T11:53:34",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 3365,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/3365",
                    "logo": "https://api.core.ac.uk/data-providers/3365/logo"
                },
                {
                    "id": 2921,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/2921",
                    "logo": "https://api.core.ac.uk/data-providers/2921/logo"
                },
                {
                    "id": 346,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/346",
                    "logo": "https://api.core.ac.uk/data-providers/346/logo"
                }
            ],
            "depositedDate": "2000-01-01T00:00:00",
            "abstract": "A list scheduling approach is proposed in this paper to overcome the problem of unequal-length block-test scheduling under power dissipation constraints. An extended tree growing technique is also used in combination with the list scheduling algorithm in order to improve the test concurrency, having assigned power dissipation limits. Moreover, the algorithm features a power dissipation balancing provision. Test scheduling examples are discussed, highlighting further research steps towards an efficient system-level test scheduling algorith",
            "documentType": "research",
            "doi": "10.1109/iwrsp.2000.855220",
            "downloadUrl": "https://core.ac.uk/download/11309685.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "Power-Constrained Block-Test List Scheduling\nValentin Mures¸an, Xiaojun Wang\nDublin City University, Ireland\nmuresanv@eeng.dcu.ie\nValentina Mures¸an, Mircea Vla˘dut¸iu\n”Politehnica” University of Timis¸oara, Romaˆnia\nvmuresan@cs.utt.ro\nAbstract\nA list scheduling approach is proposed in this paper to\novercome the problem of unequal-length block-test schedul-\ning under power dissipation constraints. An extended tree\ngrowing technique is also used in combination with the list\nscheduling algorithm in order to improve the test concur-\nrency having assigned power dissipation limits. Moreover,\nthe algorithm features a power dissipation balancing pro-\nvision. Test scheduling examples are discussed highlighting\nfurther research steps towards an efficient system-level test\nscheduling algorithm.\n1 INTRODUCTION\nVLSI devices running in test mode can consume 100 -\n200% higher power than when running in normal mode [1].\nThe heat dissipated during test application is lately one of\nthe major considerations in test scheduling. Test schedul-\ning is strongly related to test concurrency. Test concurrency\nis a design property which strongly impacts testability and\npower dissipation. To satisfy high fault coverage goals with\nreduced test application time under certain power dissipa-\ntion constraints, the testing of all components on the system\nshould be performed in parallel to the greatest extent possi-\nble.\nThe current paper brings under focus the high-level\npower-constrained block-test scheduling problem which\nlacks of practical solutions. An efficient scheme for over-\nlaying the block-tests, called extended tree growing tech-\nnique is employed together with the list scheduling, to\nsearch for power-constrained block-test schedule profiles in\na polynomial time. The algorithm fully exploits test paral-\nlelism under power dissipation constraints. This is achieved\nby overlaying the block-test intervals of compatible subcir-\ncuits to test as many of them as possible concurrently so\nthat the maximum accumulated power dissipation does not\ngo over the given limit. A constant additive model is em-\nployed for the power dissipation estimation throughout the\nalgorithm.\n2 TEST SCHEDULING PROBLEM\nThe components which are required to perform a test\n(test control logic, test pattern generators, signature analy-\nzers, test buses) are known as test resources and they may\nbe shared among the blocks under test (BUT). Each activity\nor the ensemble of activities requiring a clock period during\nthe test mode and occurring in the same clock period, can\nbe considered as a test step. A block test is the sequence\nof test steps that correspond to a specific part of hardware\n(block). The testing of a VLSI system can be viewed as the\nexecution of a collection of block tests. The steps in a step\nsequence belonging to the same block test can be pipelined\nand steps from different block tests can be executed concur-\nrently, obviously if there are no resource conflicts between\nthe steps. Two major types of test parallelism approaches\nhave been identified in the literature thus far:\n\u000f block-test scheduling, which deals with tests for blocks\nof logic [2]. These potentially consist of many test\nvectors and are regarded as indivisible entities for test\nscheduling. It deals with more abstract descriptions\n(from register transfer (RT) to system levels);\n\u000f test pipelining, which deals with test steps that need\nto be applied and resources to be utilized in a specific\ntemporal order [3]. It is applied at lower levels of ab-\nstraction, where the structure of the datapath is known\nin detail (logic or RT level);\nBlock tests and test steps have their resource sets used\nto build up their test plans. Depending on the test design\nmethodology selected, once a resource set is compiled for\neach test t\ni\n, then it is possible to determine whether they\ncould run in parallel without any resource conflict. A pair\nof tests that cannot be run concurrently is said to be incom-\npatible. Each application of time compatible tests is called a\ntest session, and the time required for a test session is often\nreferred to as test length. From this point of view, circuits\nfall, in general, into two classes:\n\u000f circuits in which all tests are equal in length;\n\u000f circuits in which the tests are unequal in length.\n0-7695-0668-2/00 $10.00 \u0001 2000 IEEE \nAuthorized licensed use limited to: DUBLIN CITY UNIVERSITY. Downloaded on July 20,2010 at 12:44:20 UTC from IEEE Xplore.  Restrictions apply. \n3 POWER-CONSTRAINED BLOCK-TEST\nSCHEDULING\n3.1 PROBLEM FORMULATION\nIn practical circuits (e.g., MCMs) only a few blocks or\nmodules are activated at a certain moment, under normal\nsystem operation, while other blocks are in the power-down\nmode to minimize the power dissipation. Under testing en-\nvironment, however, in order to test the system in the short-\nest possible time, it is desirable to concurrently activate as\nmany blocks as possible provided that the power dissipation\nlimit of the system is not exceeded.\nThe instantaneous power, p(t), is the power dissipation\nat any time instant t: p(t) = i(t)\u0002v(t), where i(t) and v(t)\nare the instantaneous current and voltage in the circuit. If\np(t\ni\n) is the power dissipation during test t\ni\nand p(t\nj\n) is the\npower dissipation during test t\nj\n, then the power dissipation\nof a test session consisting of just these two tests is the sum\nof the instantaneous powers of test t\ni\nand t\nj\n. Usually this\ninstantaneous power is constrained to not exceed the max-\nimum power dissipation limit, PD\nmax\n, if they were meant\nto be executed in the same test session. However, in reality\nthe instantaneous power for each test vector is hard to ob-\ntain at high-levels since it depends, e.g., in a CMOS circuit\non the number of zero-to-one and one-to-zero transitions,\nwhich in turn could be dependent on the order of execution\nof test vectors. Consequently, different test schedules will\nresult in different instantaneous power dissipation profiles\nfor the same test.\nA constant additive model is employed here. For high-\nlevel approaches the power dissipation P (t\ni\n) of a test t\ni\ncould be estimated in three ways. Firstly, P (t\ni\n) can be de-\nfined as the average power dissipation over all test vectors\nin t\ni\n. This definition might be overly optimistic in the analy-\nsis of power dissipation when many test vectors are applied\nsimultaneously, since the average value cannot reflect the\ninstantaneous power dissipation of each test vector. Hence,\nit might lead to an undesirable test schedule which exceeds\nthe power dissipation allowance of the device at some time\ninstants. Secondly, P (t\ni\n) can be defined as the maximum\npower dissipation over all test vectors in t\ni\n. This is the up-\nper bound power dissipation in t\ni\nand its definition is pes-\nsimistic in this case since it disallows two tests t\ni\nand t\nj\n,\nwhose peak powers occur at different time instants, from\nbeing scheduled in the same test session. However, the test\nschedule obtained with this definition guarantees the max-\nimum power dissipation allowance of the device to be ob-\nserved at all time instants. Thirdly, an RMS power dissipa-\ntion can be used when the instantaneous power dissipation\nis prone to power spikes and a more accurate estimation is\nsought. The engineer has the freedom to choose any of the\nabove ways to be run with the algorithm proposed here.\n3.2 PREVIOUS WORK VS CURRENT WORK\nPower dissipation during test scheduling was seldom un-\nder research so far. [4] proposes for the first time only a\ntheoretical analysis of this problem at IC level. It is, basi-\ncally, a compatible test clustering, where the compatibility\namong tests is given by test resource and power dissipation\nconflicts at the same time. Unfortunately, from an imple-\nmentation point of view the identification of all cliques in\nthe graph of compatible block-tests belongs to the class of\nNP-hard problems. Instead, a greedy approach is proposed\nin this paper. It has a polynomial complexity, which is very\nimportant for the success of a fast system-level test sche-\nduling solution. A list scheduling together with a tree grow-\ning technique are employed here to generate the block-test\nschedule profile at the node level, within a test hierarchy.\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nGAP1\nt2 t3\nt1\nt2 t3\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nGAP1 - t4\nt4\nt1\nt2\nxxxxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxxxxx\nxxxxxxxx\nt3\nt1\nGAP1\nxxxxxxxx\nt2 t3 t4\nxxxxxxxxx\nxxxxxxxxx\nxxxxxxxxx\nxxxxxxxxx\nxxxxxxxxx\nxxxxxxxxx\nxxxxxxxxx\nxxxxxxxxx\nt1\nGAP1 - t4\n(a) (b)\nFigure 1. Merging Step Example\nThe proposed algorithm is an unequal-length block-test\nscheduling one [2] because it deals with tests for blocks of\nlogic, which do not have equal test lengths. It is meant to be\npart of a system-level block-test approach to be applied on\na modular view of a test hierarchy. The modular elements\nof this hierarchy could be given at any of the high-level\nsynthesis (HLS) domains, between the system and RT lev-\nels: subsystems, backplanes, boards, MCM’s, IC’s (dies),\nmacro blocks and RTL transfer blocks. The lowest level\nblock the test hierarchy accepts is the RTL one, but at this\nlevel it is assumed that a test-step level scheduling has al-\nready been taken into consideration and applied. Generally\nspeaking any node in hierarchy (apart from leaves) has dif-\nferent subnodes as children. Every test node t\ni\nis character-\nized by a few parameters, which it was previously assigned\nwith, after the test scheduling optimization has been applied\non it. These features are the following: test application time\nT\ni\n, power dissipation P\ni\n, and test resource set RES:SET\ni\n.\nThis approach assumes a bottom-up traversing of the hier-\narchical test model within a divide et impera optimization\nstyle. Thus, at a certain moment the subnodes of a certain\nnode are considered for optimization in order to get an opti-\nmal or near optimal sequencing or overlaying of them com-\nplying with the power dissipation constraints.\n0-7695-0668-2/00 $10.00 \u0001 2000 IEEE \nAuthorized licensed use limited to: DUBLIN CITY UNIVERSITY. Downloaded on July 20,2010 at 12:44:20 UTC from IEEE Xplore.  Restrictions apply. \n4 TREE GROWING TECHNIQUE\nSince the block-test set in a complex VLSI circuit is\nhuge, it is possible to schedule some short tests to begin, if\nthey are resource compatible, when subcircuits with shorter\ntesting time have finished testing, while other subcircuits\nwith longer testing time have not. The tree growing tech-\nnique given in [5] is very productive from this point of\nview. That is because it is used to exploit the potential of\ntest parallelism by merging and constructing the concurrent\ntestable sets (CTS). This was achieved by means of a binary\ntree structure (not necessarily complete), called compati-\nbility tree, which was based on the compatibility relations\namong the tests.\nNevertheless, a big drawback in [5] is that the compati-\nbility tree is a binary one. This limits the number of children\ntest nodes that could be overlapped to the parent test node to\nonly two. In reality the number of children test nodes can be\nmuch bigger, as in the examples depicted in figures 1 and 2.\nTherefore, an extended compatibility tree (ECT), given by\nmeans of a generalized tree, is proposed here to overcome\nthis problem. Figure 2 gives the test schedule chart and the\nECT for the power-test scheduling chart example depicted\nin figure 3(a) and presented in section 6. The sequence of\nnodes contained in the same tree path represents an expan-\nsion of the CTS. Given a partial schedule chart of a CTS,\na test t can be merged in this CTS if and only if there is\nat least one tree path P in the corresponding compatibility\ntree of CTS, such that every test contained in the nodes of P\nis compatible to t. The compatibility relation here has three\ncomponents. Firstly, tests have to be compatible from a con-\nflicting resources point of view. Secondly, the test length of\nthe nodes in a tree path have to be monotonously growing\nfrom leaf to root. Thirdly, the power dissipation accumu-\nlated on the above tree path is less than or equal to PD\nmax\n.\nA merging step example is given in figure 1. Partial test\nschedule charts are given at the top, while partially grown\ncompatibility trees are given at the bottom. Suppose tests\nt\n2\n, t\n3\nand t\n4\nare compatible to t\n1\n, while they are not com-\npatible to each other. Suppose T\n1\n, T\n2\n, T\n3\nand T\n4\nare, re-\nspectively, the test lengths of tests t\n1\n, t\n2\n, t\n3\nand t\n4\n, and say\nT\n2\n+ T\n3\n< T\n1\n. Suppose now, a new test t\n4\nhas to be sched-\nuled in parallel to the partial test schedule depicted in figure\n1(a). As can be seen, there is a gap GAP\n1\ngiven by the\nfollowing test length difference: GAP\n1\n= T\n1\n\u0000 (T\n2\n+ T\n3\n).\nThus a merging step can be achieved, if T\n4\n\u0014 GAP\n1\n, by in-\nserting t\n4\nin the partial test schedule and its associated ECT\nin figure 1(b).\nThe process of constructing CTSs can be implemented\nby expanding (growing) the ECT from the roots to their leaf\nnodes. The root nodes are considered test sessions, while\nthe expanded tree paths are considered their test subses-\nsions. When a new test has to be merged with the CTS,\nt9\nt2\nt17\nt16 t14\nt8\nt15\nt19\nt5\nt6\nt7\nt9\nt12 t3 t10\nt20\nt2\nt4\nt1\nt11\nt13\nt18\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxxxx\nxxxxxx\nxxxxxx\nxxxxxx\nxxxxxx\nxxxxxx\nxxxxxx\nxxxxxx\nxxxxxx\nxxxxxx\nxxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxxxx\nxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxx\nxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxx xxx\nxxx\nxxx\nxxx\nxxx\nxxx\nxxx\nxxx\nxxx\nxxx\nxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nxxxxxxxxxxx\nt1 t6\nt3\nt8\nt4 t7\nt5 t11\nt10 t18\nt11\nt14\nt20t12\nt15 t19\nt17\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nxxxxxxxx\nt16\nRest (Hatched) Gap\nTwin Gap\nt i\nt i\ntwiShaded Gap\nNormal Node\nFigure 2. Tree Growing Example\nthe algorithm should avail of all possible paths in the ECT.\nIn order to keep track of the available tree paths and to avoid\nthe complexity of the generalized tree travel problem, a list\nof potentially expandable tree paths (ETP) is kept. This\nlist is kept by means of special nodes that are inserted as\nleaf nodes within each ETP of ECT. These leaf nodes are\ncalled gaps and are depicted as hatched or shaded nodes\nin figures 1 and 2. There are two types of gaps. The first\nset of gaps (hatched) are those ”rest gaps” left behind each\nmerging step, like it was the case of GAP\n1\nand GAP\n1\n\u0000 t\n4\nin the above example. They are similar to the uncomplete\nbranches of the binary tree from [5]. The second set of gaps\n(shaded), are actually bogus gaps generated as the superpo-\nsition of the leaf nodes and their twins as in the bottom-right\nequivalence given in figure 2. The twin gaps are generated\nin order to keep track of ”non-saturated” tree paths, which\nare also potential ETP’s. By ”non-saturated” tree path is\nmeant any ETP who’s accumulated power dissipation is still\nunder the given power dissipation limit. The root nodes (test\nsessions) are considered by default ”shaded” gaps before\nany test subsession is generated below them. Finally, the\ntest scheduling chart (figure 2) can easily be expanded to a\npower-test scheduling chart (figures 3) to asses the power\ndissipation distribution over the test schedule.\n5 POWER-CONSTRAINED BLOCK-TEST\nLIST SCHEDULING\nThe biggest achievement of the tree growing technique\nis that HLS algorithms proved to be efficient can be eas-\nily applied on the power-constrained test scheduling (PTS)\n0-7695-0668-2/00 $10.00 \u0001 2000 IEEE \nAuthorized licensed use limited to: DUBLIN CITY UNIVERSITY. Downloaded on July 20,2010 at 12:44:20 UTC from IEEE Xplore.  Restrictions apply. \nPOWER\nDISSIPATION\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n28\n30\n32\n34\n2 TIME\n10 20 30 40 50\n16\n1\n4\n9\n17 19\n15\n12\n14\n5\n3\n2\n20\n7\n6\n11\n10 18\n13\n8\nTOTAL TIME =  41TEST APPLICATION\nMAXIMAL POWERNO DISSIPATION CONSTRAINT\nPOWER DISSIPATION max =  29\nt t t\nt t\nt t\nt\nt\nt\nt\nt t\nt\nt\nt\nt\nt\nt\nt\n(a) Without Power Dissipation Constraints\n4\n6\n8\n10\n12\n14\n16\n18\n20\n2\nPOWER\nDISSIPATION\nTIME\n10 20 30 40 50\n1\n22\n24\n26\n28\n30\n32\n34\n4 9\n15\n1712\n2\n3\n5\n20\n6\n7\n14\n8 10\n11 16\n13\n18t\nMAXIMAL POWER =  15DISSIPATION CONSTRAINT\nTOTAL TIME =  49TEST APPLICATION\nt\nt\nt t\nt\nt\nt\nt\nt\nt\nt\nt\nt\nt\nt\nt\nt t\n19t\n(b) With Maximal Power Dissipation Constraint = 15\nFigure 3. Power-Test Scheduling Charts Of The List Scheduling Approach\nproblem modeled as an extended tree growing process.\nThis is due to the high degree of similarities between the\nHLS tasks, e.g. HLS scheduling, and power-constrained\nblock-test scheduling modeled as a growing tree prob-\nlem. Classical HLS approaches like the left-edge algo-\nrithm for HLS register allocation have already been success-\nfully applied on power-constrained block-test scheduling in\n[7]. In this paper the efficiency of the HLS list schedul-\ning algorithm (HLS-LS) in test scheduling is presented\nand is named power-constrained block-test list scheduling\n(PTS-LS). Moreover, the same tree growing technique is\ncurrently experienced in combination with other classical\nHLS scheduling algorithms like the force-directed based\nones : force-directed scheduling, improved force-directed\nscheduling, and force-directed list scheduling.\nIn the HLS-LS approach [6], operations were sorted in\ntopological order by using control and data dependencies.\nThe set of operations that could be placed in a control step\n(c-step) were then evaluated. These operations were called\nready operations. If the number of ready operations of a sin-\ngle type exceeded the number of hardware modules avail-\nable to perform them, then one or more operations had to\nbe deferred. In previous list scheduling algorithms, the se-\nlection of the deferred operations was determined by a local\npriority function such as mobility or urgency.\nSince every block-test t\ni\nin our approach has a test length\nT\ni\nand a power dissipation P\ni\n, a local priority function\ncalled test mobility TM\ni\ncan also be defined for t\ni\nas the in-\nverse of the product between its test length T\ni\nand its power\ndissipation P\ni\n: TM\ni\n=\n1\nT\ni\n\u0003P\ni\n. Intuitively, the probability\nof scheduling a block-test into a test subsession is higher\nthe higher block-test’s mobility TM\ni\nis. The mobility of\na block-test t\ni\nis inverse proportional with its dimensions.\nThe dimension of a block-test t\ni\nis given by the rectangle\narea having its test length T\ni\nand its power dissipation P\ni\nas\nsides. That is, bigger is the area of the rectangles associated\nwith the block-tests, smaller mobility they get. Though, in\nthe tree growing approach the block-tests in an ETP are mo-\nnotonously growing in terms of test length. A block-test\ncannot be scheduled in an ETP, where the leaf’s test length\nis shorter than the test length of the block-test to be sched-\nuled. This is due to the fact that the block-tests have to be\nscheduled in an ETP in the order of their test lengths. There-\nfore the mobility in the PTS-LS approach is split into two,\nits test length component and its power dissipation compo-\nnent. Thus, in PTS-LS, the block-tests are sorted in topo-\nlogical order by using the test length as primary key to order\nin descending order, and the power dissipation as secondary\nkey, to order the block-tests having the same test length in\ndescending order as well.\nA clear parallel between the HLS scheduling problem\nand the PTS problem is given by the similarities between\nthe c-steps from HLS and the test sessions (test subses-\nsions) in PTS, between operations (HLS) and block-tests\n(PTS), and between hardware resource constraints (HLS)\nand power dissipation constraints (PTS). Therefore, there\nis an obvious coincidence between the process of assigning\noperations to c-steps (HLS scheduling) and the process of\nassigning block-tests to test (sub)sessions (PTS). In the cur-\nrent PTS-LS algorithm the block-tests are initially ordered\n(as described above) before being scheduled. The sorted\nblock-tests are then iteratively scheduled into the available\ntest (sub)sessions (ETPs). When the power dissipation is\nexceeded the block-tests to be currently scheduled are de-\nferred for the other test (sub)sessions (ETP) left for further\nexpansion. In terms of test scheduling, the list scheduling\napproach given here (PTS-LS) resembles the first algorithm\nof the PTS approach given in [7]. An iterative tree growing\ntechnique was also proposed there to minimize the total test\napplication time by searching for every block-test an ETP to\n0-7695-0668-2/00 $10.00 \u0001 2000 IEEE \nAuthorized licensed use limited to: DUBLIN CITY UNIVERSITY. Downloaded on July 20,2010 at 12:44:20 UTC from IEEE Xplore.  Restrictions apply. \nbe assigned to. The assignment was achieved if and only if\nthe accumulated power dissipation in the just expanded tree\npath was not exceeding the given power dissipation con-\nstraint.\nThe PSEUDOCODE of the PTS-LS ALGORITHM:\n-sort all the block-tests by their mobility in two steps (test length,\npower consumption);\n-initialize the GrowingTree, the BlockTestList and the GapsList;\n-initialize the time constraint to the initial number of test subsessions;\n-GapsList is ordered by its gaps’ power consumption;\n-while (there are unscheduled block-tests) do:\n/*BlockTestList is not empty*/ f\n\u000f if (GapsList is empty) then f\n– CurTest = head of BlockTestList;\n– insert CurTest as the tail of GrowingTree roots (new\ntest section);\n– make CurTest ”used”;\n– remove CurTest from BlockTestList;\n– generate a TwinGap gap as the twin of CurTest;\n– insert TwinGap into GapsList g;\n\u000f else f\n– CurGap = the head of GapsList;\n– CurTest = the head of Comp:List\nCurGap\n;\n– while (T\nCurTest\n> T\nCurGap\nOR\nPD\nCurGap\n+ PD\nCurTest\n> PD\nmax\nOR\nCurTest ”used”) do: f\n\u0003 CurTest = CurTest \u0000!next\n(next in the Comp:List\nCurGap\n);g /*while*/\n– if (T\nCurTest\n\u0014 T\nCurGap\nAND\nPD\nCurGap\n+ PD\nCurTest\n\u0014 PD\nmax\nAND\nCurTest NOT ”used”) then\n\u0003 SCHEDULE(CurTest,CurGap,\nGrowingTree,GapsList,BlockTestList);\n– else remove CurGap from GapsList;\n-g/*while*/\nThe complexity of this approach is O(n2). The\ndata structures used in it are: the Growing Tree to\nmodel the ECT, GapsList to model the ordered list of\npotentially expandable gaps (shaded and hatched gaps),\nBlockTestList to keep the ordered but not yet merged\nblock-tests. CurTest is the block-test to be merged at a\ncertain iteration. CurGap is the gap under focus at a cer-\ntain iteration to see whether it is expandable (compatible)\nwith theCurTest. In the pseudocode ”used” means that the\nblock-test has already been merged in the ECT. TwinGap\nis the newly generated shaded gap at every iteration and it\nwill not be inserted in the GapsList anymore after its gen-\neration, if its resulting compatibility list is null, i.e. it will\nnot be an ETP. RestGap is meant to keep the hatched gap\ngenerated at every iteration if it is not null, i.e. CurTest\ncovers completely CurGap.\nThe algorithm is iterative. Every iteration looks for the\nblock-test with biggest test length and lowest mobility to\nbe scheduled in the test subsession (ETP) having the low-\nest accumulated power dissipation. Thus, CurGap (cur-\nrent gap) is assigned all the time with the first gap from\nthe GapsList, which is the one with the lowest accumu-\nlated power dissipation. Next, CurGap0s compatibility\nlist is crossed from the block-test with the lowest mobil-\nity to the one with the highest mobility. The first block-\ntest (CurTest) which is found assignable (compatible from\nall points of view) to the CurGap is scheduled in the cur-\nrent gap. Another pair of gaps (twin and rest) are gener-\nated and they have to be inserted in the GapsList so that\nthe list is still ordered by gaps’ accumulated power dissi-\npation. If no block-test is found for further scheduling in\nthe CurGap0s compatibility list, then CurGap is removed\nfrom the GapsList and the algorithm continues with the\nnew head of the list. Otherwise, the new head of GapsList\nis considered for expansion. When all the gaps from the\nGapsList are removed a new test session is generated ex-\nactly like at the beginning of the algorithm during the ini-\ntialization of the GapsList. Namely, the GapsList is set\nto the first (biggest test length) block-test left unused in\nthe sorted BlockTestList. The SCHEDULE procedure\ncarries out the insertion of block-test to be merged into\nthe GrowingTree and subsequently its removal from the\nBlockTestList. For space reasons it is not detailed in this\npaper. As can be seen in figure 1, the merging step implies\nthe generation of shaded and hatched gaps. The SCHED-\nULE procedure in the above pseudocode is similar to the\nSCHEDULE procedures of the other tree growing based ap-\nproaches [7], but the current one features a power dissipa-\ntion balancing provision. It keeps the gaps in the GapsList\nin a increased order of their accumulated power dissipation.\nThis is implemented in order to select at every iteration for\nfurther expansion the test subsessions with lower accumu-\nlated power dissipation. Thus, there is a higher probability\nof decreasing the power dissipation difference between the\ntest subsessions (ETPs) of the growing tree. Consequently,\nthe power dissipation would be more balanced.\n6 EXPERIMENTAL RESULTS\nThe following example should provide a deeper insight\ninto how this algorithm works and its results. Suppose the\nfollowing block-tests are to be scheduled and their parame-\nters specified in the order: power dissipation, test length and\ntheir compatibility list. For simplicity reasons, the block\ntests are already ordered by test length and test mobility as\ndescribed in section 5.\nt\n1\n( 3; 12; ft\n4\n; t\n5\n; t\n8\n; t\n9\n; t\n10\n; t\n12\n; t\n15\n; t\n16\n; t\n17\n; t\n19\n; t\n20\ng)\nt\n2\n( 5; 11; ft\n3\n; t\n4\n; t\n5\n; t\n9\n; t\n12\n; t\n13\n; t\n14\n; t\n17\n; t\n19\n; t\n20\ng)\nt\n3\n( 9; 9; ft\n2\n; t\n5\n; t\n7\n; t\n10\n; t\n11\n; t\n12\n; t\n13\n; t\n14\n; t\n17\n; t\n18\ng)\nt\n4\n(12; 8; ft\n1\n; t\n2\n; t\n7\n; t\n9\n; t\n11\n; t\n14\n; t\n15\n; t\n17\n; t\n19\ng)\nt\n5\n( 4; 8; ft\n1\n; t\n2\n; t\n3\n; t\n6\n; t\n7\n; t\n8\n; t\n12\n; t\n15\n; t\n17\n; t\n18\n; t\n20\ng)\n0-7695-0668-2/00 $10.00 \u0001 2000 IEEE \nAuthorized licensed use limited to: DUBLIN CITY UNIVERSITY. Downloaded on July 20,2010 at 12:44:20 UTC from IEEE Xplore.  Restrictions apply. \nt6\n( 2; 8; ft\n5\n; t\n7\n; t\n9\n; t\n11\n; t\n14\n; t\n17\n; t\n20\ng)\nt\n7\n( 1; 8; ft\n3\n; t\n4\n; t\n5\n; t\n6\n; t\n9\n; t\n12\n; t\n14\n; t\n15\n; t\n16\n; t\n18\n; t\n19\n; t\n20\ng)\nt\n8\n( 7; 6; ft\n1\n; t\n5\n; t\n9\n; t\n10\n; t\n11\n; t\n14\n; t\n16\n; t\n17\n; t\n19\n; t\n20\ng)\nt\n9\n( 6; 6; ft\n1\n; t\n2\n; t\n4\n; t\n6\n; t\n7\n; t\n8\n; t\n11\n; t\n12\n; t\n15\n; t\n17\n; t\n19\ng)\nt\n10\n( 7; 5; ft\n1\n; t\n3\n; t\n8\n; t\n11\n; t\n15\n; t\n16\n; t\n17\n; t\n18\ng)\nt\n11\n( 5; 5; ft\n3\n; t\n4\n; t\n6\n; t\n8\n; t\n9\n; t\n10\n; t\n14\n; t\n16\n; t\n18\n; t\n20\ng)\nt\n12\n(11; 4; ft\n1\n; t\n2\n; t\n3\n; t\n5\n; t\n7\n; t\n9\n; t\n13\n; t\n14\n; t\n16\n; t\n19\ng)\nt\n13\n( 2; 4; ft\n2\n; t\n3\n; t\n12\n; t\n15\n; t\n16\n; t\n17\n; t\n18\n; t\n19\ng)\nt\n14\n( 3; 3; ft\n2\n; t\n3\n; t\n4\n; t\n6\n; t\n7\n; t\n8\n; t\n11\n; t\n12\n; t\n16\n; t\n18\n; t\n20\ng)\nt\n15\n( 1; 3; ft\n1\n; t\n4\n; t\n5\n; t\n7\n; t\n9\n; t\n10\n; t\n13\n; t\n16\n; t\n17\n; t\n18\ng)\nt\n16\n( 5; 2; ft\n1\n; t\n7\n; t\n8\n; t\n10\n; t\n11\n; t\n12\n; t\n13\n; t\n14\n; t\n15\n; t\n17\n; t\n19\n; t\n20\ng)\nt\n17\n( 4; 2; ft\n1\n; t\n2\n; t\n3\n; t\n4\n; t\n5\n; t\n6\n; t\n8\n; t\n9\n; t\n10\n; t\n13\n; t\n15\n; t\n16\n; t\n18\n; t\n19\n; t\n20\ng)\nt\n18\n(12; 1; ft\n3\n; t\n5\n; t\n7\n; t\n10\n; t\n11\n; t\n13\n; t\n14\n; t\n15\n; t\n17\n; t\n19\n; t\n20\ng)\nt\n19\n( 8; 1; ft\n1\n; t\n2\n; t\n4\n; t\n7\n; t\n8\n; t\n9\n; t\n12\n; t\n13\n; t\n16\n; t\n17\n; t\n18\n; t\n20\ng)\nt\n20\n( 7; 1; ft\n1\n; t\n2\n; t\n5\n; t\n6\n; t\n7\n; t\n8\n; t\n11\n; t\n14\n; t\n16\n; t\n17\n; t\n18\n; t\n19\ng)\nIn figure 3 the results of the PTS-LS algorithm are given\nboth with (figure 3(a)) and without (figure 3(b)) power dis-\nsipation constraints (PD\nmax\n= 15). Table 1 gives the re-\nsults of the PTS-LS algorithm for a randomly generated 50\nblock-tests set. Their degree of resource compatibility has\nbeen increased within a range from low to high: low (L)\n10%, average-low (AL) 30%, average (AV) 50%, average-\nhigh (AH) 70% and high (H) 90%. The following abbrevia-\ntions have been used in the table: test length (TL), maximal\naccumulated power dissipation (MPD), average power dis-\nsipation (APD) and the total power dissipation dispersion\n(PDD). TL represents the total test application time of the\ntest scheduling solutions. MPD is the maximal power dissi-\npation over the power dissipations accumulated in all ETPs\nof the final ECT (growing tree). APD is considered the ideal\nMPD when all the ETP’s would exhibit the same accumu-\nlated power dissipation, that is the power dissipation would\nbe fully balanced over the power-test scheduling chart. It is\ncalculated as the ratio between the power-test area, taken up\nby the chart (see figure 3) of the ECT, and the TL. The rect-\nangle given by APD and TL would be the ideal power-test\nscheduling chart and, therefore, the ideal test schedule pro-\nfile. PDD is direct proportional to the accumulated power\ndissipation dispersion over the power-test scheduling chart,\nwhich is considered here to be given by the power-test area\nleft unused inside the power-test rectangle given by MPD\nand TL. PDD is calculated as the difference between MPD\nand APD. The smaller the PDD values of the PTS-LS solu-\ntions are the better representativeness the MPD value gets.\nIt can be seen in figure 3(b) and table 1 that a tighter\npower dissipation constraint forces the test scheduling to a\nmore balanced power dissipation throughout the test appli-\ncation time. At the same time obvious power dissipation\nspikes could be seen in figure 3(a) due to the lack of power\ndissipation constraints. That means the power dissipation is\nless balanced when it is loosely constrained. On the other\nhand when there are tighter power dissipation constraints\n(see table 1), the total test application time increases. Thus,\nthe PTS-LS problem turned out to be a trade-off problem to\nbe solved with more complex algorithms.\nPD\nmax\nLIMIT = 200 PD\nmax\nLIMIT = 50\nTL MPD APD PDD TL MPD APD PDD\nL 401 33 15.9 17.1 401 33 15.9 17.1\nAL 301 41 21.3 19.7 301 41 21.3 19.7\nAV 204 79 31.4 47.6 224 50 28.6 21.4\nAH 154 94 41.6 52.4 167 50 38.4 11.6\nH 99 197 62.3 134.7 151 50 42.5 7.5\nTable 1. Power-Test List Scheduling Results\n7 CONCLUSIONS\nThis novel greedy unequal-length block-test scheduling\napproach is based on the classical list scheduling algo-\nrithm applied to an extended tree growing technique and\nits polynomial complexity is beneficial to the system-level\ntest scheduling problem. Thus, this fast algorithm is very\nsuitable to be part of a HLS methodology meant to obtain\nrapid system prototyping solutions. Even though it does not\nguarantee optimal block-test scheduling solutions, its final\nresult can be used as a starting point by near-optimal block-\ntest scheduling approaches (i.e., simulated annealing, ge-\nnetic algorithms, tabu search) to get an improved solution.\nReferences\n[1] Y. ZORIAN: A Distributed BIST Control Scheme for\nComplex VLSI Devices - Proceedings of The 11th IEEE\nVLSI Test Symposium, pp. 4-9, Apr, 1993.\n[2] G.L. CRAIG, C.R. KIME, K.K. SALUJA: Test Schedul-\ning and Control for VLSI Built-In Self-Test - IEEE Trans-\nactions on Computer, Vol. 37, No. 9, pp. 1099–1109, Sep,\n1988.\n[3] M. ABADIR, M. BREUER: Test Schedules for VLSI Cir-\ncuits Having Built-in Test Hardware - IEEE Transactions\non Computer, Vol. C-35, No. 4, pp. 361–368, Apr, 1986.\n[4] R.M. CHOU, K.K. SALUJA, V.D. AGRAWAL: Scheduling\nTests for VLSI Systems Under Power Constraints - IEEE\nTransactions on Very Large Scale Integration (VLSI) Sys-\ntems, Vol. 5, No. 2, pp. 175–185, Jun, 1997.\n[5] W.B. JONE, C. PAPACHRISTOU, M. PEREIRA: A\nScheme for Overlaying Concurrent Testing of VLSI Cir-\ncuits - Proceedings of the 26th Desing Automation Confer-\nence, pp. 531-536, 1989.\n[6] S. DAVIDSON et al: Some Experiments in Local Mi-\ncrocode Compaction for Horizontal Machines - IEEE\nTransactions on Computers, Vol. C-30, No. 7, pp. 460–477,\nJul, 1981.\n[7] V. MURESAN, X. WANG, V. MURESAN, M. VLADU-\nTIU: The Left Edge Algorithm and the Tree Growing\nTechnique in Power-Constrainted Block-Test Scheduling\n- Proceedings of the 18th IEEE VLSI TEST SYMPOSIUM,\n2000, accepted paper, Montreal, May, 2000.\n0-7695-0668-2/00 $10.00 \u0001 2000 IEEE \nAuthorized licensed use limited to: DUBLIN CITY UNIVERSITY. Downloaded on July 20,2010 at 12:44:20 UTC from IEEE Xplore.  Restrictions apply. \n",
            "id": 4763176,
            "identifiers": [
                {
                    "identifier": "2140040123",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "10.1109/iwrsp.2000.855220",
                    "type": "DOI"
                },
                {
                    "identifier": "11309685",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "147599585",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:doras.dcu.ie:15536",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "143908988",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "192442000",
                    "type": "CORE_ID"
                }
            ],
            "title": "Power-constrained block-test list scheduling",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2140040123",
            "oaiIds": [
                "oai:doras.dcu.ie:15536"
            ],
            "publishedDate": "2000-01-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 16539318,
                    "title": "A Distributed BIST Control Scheme for Complex VLSI Devices -",
                    "authors": [],
                    "date": "1993",
                    "doi": "10.1109/vtest.1993.313316",
                    "raw": "Y. ZORIAN: A Distributed BIST Control Scheme for Complex VLSI Devices - Proceedings of The 11th IEEE VLSI Test Symposium, pp. 4-9, Apr, 1993.",
                    "cites": null
                },
                {
                    "id": 16539339,
                    "title": "A Scheme for Overlaying Concurrent Testing of VLSI Circuits -",
                    "authors": [],
                    "date": "1989",
                    "doi": "10.1145/74382.74471",
                    "raw": "W.B. JONE, C. PAPACHRISTOU, M. PEREIRA: A Scheme for Overlaying Concurrent Testing of VLSI Circuits - Proceedings of the 26th Desing Automation Conference, pp. 531-536, 1989.",
                    "cites": null
                },
                {
                    "id": 16539347,
                    "title": "al: Some Experiments in Local Microcode Compaction for Horizontal Machines -",
                    "authors": [],
                    "date": "1981",
                    "doi": "10.1109/tc.1981.1675826",
                    "raw": "S. DAVIDSON et al: Some Experiments in Local Microcode Compaction for Horizontal Machines - IEEE Transactions on Computers, Vol. C-30, No. 7, pp. 460–477, Jul, 1981.",
                    "cites": null
                },
                {
                    "id": 16539333,
                    "title": "Scheduling Tests for VLSI Systems Under Power Constraints -",
                    "authors": [],
                    "date": "1997",
                    "doi": "10.1109/92.585217",
                    "raw": "R.M. CHOU, K.K. SALUJA,V.D. AGRAWAL: Scheduling Tests for VLSI Systems Under Power Constraints - IEEE Transactions on Very Large Scale Integration (VLSI) Systems, Vol. 5, No. 2, pp. 175–185, Jun, 1997.",
                    "cites": null
                },
                {
                    "id": 16539327,
                    "title": "Test Schedules for VLSI Circuits Having Built-in Test Hardware -",
                    "authors": [],
                    "date": "1986",
                    "doi": "10.1109/tc.1986.1676771",
                    "raw": "M. ABADIR, M. BREUER: Test Schedules for VLSI Circuits Having Built-in Test Hardware - IEEE Transactions on Computer, Vol. C-35, No. 4, pp. 361–368, Apr, 1986.",
                    "cites": null
                },
                {
                    "id": 16539325,
                    "title": "Test Scheduling and Control for VLSI Built-In Self-Test -",
                    "authors": [],
                    "date": "1988",
                    "doi": "10.1109/12.2260",
                    "raw": "G.L. CRAIG, C.R. KIME, K.K. SALUJA: Test Scheduling and Control for VLSI Built-In Self-Test - IEEE Transactions on Computer, Vol. 37, No. 9, pp. 1099–1109, Sep, 1988.",
                    "cites": null
                },
                {
                    "id": 16539354,
                    "title": "VLADUTIU: The Left Edge Algorithm and the Tree Growing Techniquein Power-Constrainted Block-Test Scheduling -",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1109/vtest.2000.843873",
                    "raw": "V. MURESAN, X. WANG, V. MURESAN, M. VLADUTIU: The Left Edge Algorithm and the Tree Growing Techniquein Power-Constrainted Block-Test Scheduling - Proceedings of the 18th IEEE VLSI TEST SYMPOSIUM, 2000, accepted paper, Montreal, May, 2000. 0-7695-0668-2/00 $10.00  2000 IEEE Authorized licensed use limited to: DUBLIN CITY UNIVERSITY. Downloaded on July 20,2010 at 12:44:20 UTC from IEEE Xplore.  Restrictions apply.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "http://doras.dcu.ie/15536/1/wang14.pdf"
            ],
            "updatedDate": "2021-12-13T06:11:24",
            "yearPublished": 2000,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/11309685.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/11309685"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/11309685/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/11309685/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/4763176"
                }
            ]
        },
        {
            "acceptedDate": "2008-09-17T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "A. Grunwald"
                },
                {
                    "name": "A.G. Olabi"
                },
                {
                    "name": "Olabi"
                },
                {
                    "name": "Olabi"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/143910606",
                "https://api.core.ac.uk/v3/outputs/11310076",
                "https://api.core.ac.uk/v3/outputs/147600580",
                "https://api.core.ac.uk/v3/outputs/190506719"
            ],
            "createdDate": "2013-07-10T11:53:34",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 3365,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/3365",
                    "logo": "https://api.core.ac.uk/data-providers/3365/logo"
                },
                {
                    "id": 2921,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/2921",
                    "logo": "https://api.core.ac.uk/data-providers/2921/logo"
                },
                {
                    "id": 346,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/346",
                    "logo": "https://api.core.ac.uk/data-providers/346/logo"
                }
            ],
            "depositedDate": "2008-11-01T00:00:00",
            "abstract": "Design and optimization of an actuators based on magnetostrictive technology requires computation of the magnetic field. The “MS”-technology offers an attractive controllability with high power density. The magnetostriction is a reversible feature which can be used in various actuator layouts. The actuator performance depends on driving magnetic field and the particular magnetic  properties of used materials. Good understanding of specific design constrains is required to define and to optimized a magnetostrictive actuator. The non-linear computation of the magnetic field using FEM software is vital for the finale experimental design of a low-frequency actuator. This paper presents results of magnetic field simulation with FEMM software package and experimental measurements of the magnetic flux density. Good correlation between the simulation results and experimental measurements has been achieved",
            "documentType": "research",
            "doi": "10.1016/j.simpat.2008.08.014",
            "downloadUrl": "https://core.ac.uk/download/11310076.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "                                                                     1 / 11\nComputation of magnetic field in an actuator \n \nA. G. Olabi and A. Grunwald \n \nDublin City University, School of Mechanical and Manufacturing Engineering, Glasnevin, \nDublin 9, Ireland, Email: abdul.olabi@dcu.ie, artur.grunwald@dcu.ie \n \n \n \nAbstract: \nDesign and optimization of an actuators based on magnetostrictive technology requires \ncomputation of the magnetic field. The “MS”-technology offers an attractive controllability \nwith high power density. The magnetostriction is a reversible feature which can be used in \nvarious actuator layouts. The actuator performance depends on driving magnetic field and the \nparticular magnetic  properties of used materials. Good understanding of specific design \nconstrains is required to define and to optimized a magnetostrictive actuator. The non-linear \ncomputation of the magnetic field using FEM software is vital for the finale experimental \ndesign of a low-frequency actuator. This paper presents results of magnetic field simulation \nwith FEMM software package and experimental measurements of the magnetic flux density. \nGood correlation between the simulation results and experimental measurements has been \nachieved.  \n \nKeywords: FEM, FEMM, Magnetostriction, Actuator, Terfenol-D \n                                                                     2 / 11\n1. Introduction of the magnetostrictive actuator \nMagnetostriction (“MS”) is the change in shape of materials under the influence of an \nexternal magnetic field. The magnetostrictive effect was first described in the 19th century \n(1842) by an English physicist James Joule. Several applications based on magnetostrictive \ntechnology have been introduced in various industries. A summary of literature survey can be \nfound in article [1]. The article [2] captures the optimization of the strain performance using \nmechanical pre-stress. The maximum useful magnetoelastic strain is one of the key \nparameters defining the resulting mechanical output in the case of a magnetostrictive actuator. \nIn case a shaft is made of magnetostrictive material, i.e. Terfenol-D, magnetic field along the \nshaft axle will cause axial elongation. The applied magnetic field leads to relative strain in the \nmagnetostrictive material. A higher magnetic field causes higher strain and leads to larger \nelongation. Without the magnetic field the shape of the magnetostrictive material reverse to \nthe original.  Fig. 1 depicts a cross section of the actuation hardware based on \nmagnetostrictive technology.  \n \n     \nFig. 1: Cross-section of the actuator used in the study \n \n2. The Magnetic circuitry for the magnetostrictive actuator \nA coil in an appropriate ferromagnetic housing is defined as the source of the magnetic field. \nThe chosen diameter of the Terfenol-D shaft is 8 mm and the length is about 68 mm [2]. The \nmagnetic field is generated by electric current I[A] through the actuator coil. The coil is \nwound around the Terfenol-D shaft and the magnetic field is therefore parallel to the axis of \nthe rod. Fig. 2 depicts the generally the coil layout. \n \n                                                                     3 / 11\n \nFig. 2: Layout of the actuator coil \n \nSince not only the coil is involved in the magnetic circuit other ferromagnetic components \nlike housing, Terfenol-D shaft and inserts have to be considered.  Fig. 3 presents the main \nmagnetic path through the magnetostrictive actuator. \n \n \nFig. 3: Magnetic path through the actuator \n \nIn Fig. 3 there are six designated sections of component through which the magnetic flux \npasses. It is important to include all six terms in order to estimate the required total \nmagnetomotive force, and so the return path of the magnetic flux through steel components \nmust also be considered. For each section there is a length of the magnetic path, l, and a value \nfor the magnetic field strength, H. The length is fixed by the geometry of the system, but the \nvalue of H must be determined by making use of the magnetic properties of the material.  \nFig. 4 presents the coil specification which has been used for the magnetic field simulation \nand for the experimental evaluation.  \n \n                                                                     4 / 11\n \n \nFig. 4: Electric coil for the actuator \n \nIn order to verify the simulated model of actuator, the experimental rig assembly has been \nprepared for measurements of flux density with Tesla Meter. The magnetic properties of \nTerfenol-D at various pre-stress levels have been provided by the Terfenol-D supplier [3]. Fig. \n5 depicts the B-H characteristic of the Terfenol-D shaft at pre-stress about 7.2MPa.  \nThe pre-stress in the assembly has been achieved by using appropriate preload-spring (part  \nnumber 11 in Fig. 2). A force sensor based on piezo technology (part number 15 in Fig. 2) has \nbeen used to measure the pre-load in the experimental rig assembly.  \n                                                                     5 / 11\n  \nFig. 5: B-H diagram of Terfenol-D with measurement data [3] \n \nFor the housing of the actuator the low carbon steel, Ck15, has been used. The magnetic \nproperties of Ck15 are shown in Fig. 6. \n \n \nFig. 6: B-H diagram of Ck15 with measurement data \n \n \n                                                                     6 / 11\n4. Magnetic Field Simulation \nIn order to optimize the design of the actuator, a simulation of magnetic field, including the \nreal magnetic properties of used material, is required. The nonlinear simulation method was \nused to determine and optimize the actuator and control performance. Due to nonlinear B-H \nfunction the system has to be solved by an iterative way.  Finite element modelling methods \nof the magneto-mechanical phenomena have been proposed in several publications. Terfenol-\nD is a smart material in that the magnetic properties are coupled with mechanical state and \nvice versa. For the optimization of the proposed structure the free available software package \nFEMM have been used. The simple user interface and efficiency has been found in using the \nfree available FEMM software packaging developed by David Meeker, Senior Engineer at \nForster-Miller Inc. [4].  Results from the magnetic field simulation from FEMM have been \nevaluated and positively verified with measurements. For low-frequency (<500Hz)  \nevaluations only a part of the complete Maxwell’s equations is considered. Fig. 7 shows the \nreference plot of the cross-section from the actuator. \n  \n \nFig. 7: Reference figure of the actuator plots of B (T) and H (A/m) \n \nDue to proposed measuring method with the Tesla-Meter, minor modifications of the actuator \nstructure were required. Following simulation model has been used to verify the simulated \nflux density with the measured flux density. Fig. 8 presents the actuator model without (left) \nand with (right) Tesla-Meter probe. \n \n                                                                     7 / 11\n     \nFig. 8: Actuator model without (left) and with (right) Tesla-Meter probe \n \nFig. 9 presents the simulation results of flux density B (T) obtained with FEMM at nominal \nelectrical current of about 9A. On the left side: the original actuator assembly; on the right \nside: the modified assembly for Tesla-Meter probe. Top-to-bottom reference centre line has \nbeen used as reference. \n \n \nFig. 9: Flux density without (left) and with (right) Tesla-Meter probe \n \nThe difference between the two above showed results has been found as acceptable for \nverification of the magnetic field simulation results. Figures 10 and 11 show some simulation \nresults of flux density B(T) and field intensity H (kA/m) at various current levels. \n                                                                     8 / 11\n \nFig. 10: FEMM flux density B (left) and field intensity H (right) at 1 A \n \n  \nFig. 11: FEMM flux density B (left) and field intensity H (right) at 10 A \n \n3. Experimental evaluation \nA power supply unit (up to 10A, DC), a Multi-Meter for current measurements and the Tesla-\nMeter Model 5080 have been used for experimental evaluation. The actuator assembly has \nbeen completed and used to verify the simulation results with measured magnetic flux density. \nFig. 12 depicts the test bench layout for magnetic flux density measurements. \n \nFig. 12: Picture from “MS”-actuator assembly \n                                                                     9 / 11\nThe flat Tesla-probe has been adapted in to the actuator assembly according Fig. 13.  \n \n \nFig. 13: Actuator with Tesla-Meter probe (Model 5080) \n \nThe Fig. 14 presents the Tesla-Meter with the transverse probe. Hall effect element at the top \nof the flat probe has been used to measure the magnetic flux density B in tesla.  \n \nFig. 14: Tesla-Meter Model 5080 [5] \n                                                                     10 / 11\n \nTable 1 presents the results from FEMM magnetic field simulation and the measurement \nresults of flux density at the top of the Terfenol-D shaft \nTable 1: Simulation and measurement result for comparison \n \n Fig. 15 depicts the measured flux density at the top of the Terfenol-D shaft with simulated \nresults, obtained with FEMM. \n \nFig. 15: FEMM flux density B (T) versus electrical current \n \n4. Conclusions  \nSummarizing the results from simulation and the flux density measurement with the Tesla-\nMeter Model 5080 can be stated, that the results are within the specified range and in good \nconsistence. The FEMM software for non-linear magnetic field simulation can be used to \nfinalize and to optimize the product design. The difference in the simulated results and the \nmeasured flux density is predictable and was caused by accepted variations of the housing \nmaterial and measurement tolerances. The presented simulation results have been used for \ndesign freeze and for further experimental evaluations of the magnetostrictive actuator. \n                                                                     11 / 11\n \nReference list \n \n[1] A. G. Olabi, A. Grunwald, Design and application of magnetostrictive materials, \nMaterials And Design, 29, 2, pp469-483 \n[2] A. G. Olabi, A. Grunwald, Design of magnetostrictive (“MS”) actuator, Sensors and \nActuators A: Physical, Volume 144, Issue 1, 28 May 2008, Pages 161-175. \n [3] ETREMA Products Inc., Terfenol-D Magnetostrictive Actuator Information, \nSpecifications, Public domain information, www.terfenoltruth.com, www.etrema.com  \n[4] D. Meeker, FEMM-Software Manual, Finite Elements Method Magnetics User's \nManual and Tutorial, 2004,  http://femm.foster-miller.net/wiki/FAQ \n[5] Tesla-Meter Instruction Manual, F.W. Bell, Tesla-Meter Model 5080 \n",
            "id": 4764175,
            "identifiers": [
                {
                    "identifier": "oai:doras.dcu.ie:16025",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "147600580",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "11310076",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "190506719",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1016/j.simpat.2008.08.014",
                    "type": "DOI"
                },
                {
                    "identifier": "143910606",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2036040619",
                    "type": "MAG_ID"
                }
            ],
            "title": "Computation of magnetic field in an actuator",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:doras.dcu.ie:16025"
            ],
            "publishedDate": "2008-11-01T00:00:00",
            "publisher": "'Elsevier BV'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://doras.dcu.ie/16025/1/Olabi-1-04-08.pdf"
            ],
            "updatedDate": "2021-12-13T06:11:36",
            "yearPublished": 2008,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "1569-190X"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/11310076.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/11310076"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/11310076/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/11310076/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/4764175"
                }
            ]
        },
        {
            "acceptedDate": "2007-08-10T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "E. Izquierdo"
                },
                {
                    "name": "I. Kompatsiaris"
                },
                {
                    "name": "J. R. Casas"
                },
                {
                    "name": "M. G. Strintzis"
                },
                {
                    "name": "Noel E"
                },
                {
                    "name": "Noel E. O&apos"
                },
                {
                    "name": "P. Migliorati"
                },
                {
                    "name": "R. Leonardi"
                }
            ],
            "contributors": [
                "The Pennsylvania State University CiteSeerX Archives"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/53585903",
                "https://api.core.ac.uk/v3/outputs/377231665",
                "https://api.core.ac.uk/v3/outputs/11308305",
                "https://api.core.ac.uk/v3/outputs/147596566"
            ],
            "createdDate": "2013-07-10T11:53:31",
            "dataProviders": [
                {
                    "id": 145,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/145",
                    "logo": "https://api.core.ac.uk/data-providers/145/logo"
                },
                {
                    "id": 17714,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/17714",
                    "logo": "https://api.core.ac.uk/data-providers/17714/logo"
                },
                {
                    "id": 1095,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/1095",
                    "logo": "https://api.core.ac.uk/data-providers/1095/logo"
                },
                {
                    "id": 2921,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/2921",
                    "logo": "https://api.core.ac.uk/data-providers/2921/logo"
                },
                {
                    "id": 346,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/346",
                    "logo": "https://api.core.ac.uk/data-providers/346/logo"
                }
            ],
            "depositedDate": "2003-03-01T00:00:00",
            "abstract": "The aim of the SCHEMA Network of Excellence is to bring together a critical mass of universities, research centers, industrial partners and end users, in order to design a reference system for content-based semantic scene analysis, interpretation and understanding. Relevant research areas include: content-based multimedia analysis and automatic annotation of semantic multimedia content, combined textual and multimedia information retrieval, semantic -web, MPEG-7 and MPEG-21 standards, user interfaces and human factors. In this paper, recent advances in content-based analysis, indexing and retrieval of digital media within the SCHEMA Network are presented. These advances will be integrated in the SCHEMA module-based, expandable reference system",
            "documentType": "research",
            "doi": "10.1142/9789812704337_0094",
            "downloadUrl": "https://core.ac.uk/download/11308305.pdf",
            "fieldOfStudy": "computer science",
            "fullText": " 1 \n \nADVANCED CONTENT-BASED SEMANTIC SCENE ANALYSIS AND \nINFORMATION RETRIEVAL: THE SCHEMA PROJECT \nE. IZQUIERDO, J.R. CASAS, R. LEONARDI, P. MIGLIORATI, NOEL E. \nO’CONNOR, I. KOMPATSIARIS AND M. G. STRINTZIS† \nEC  project IST-2000-32795 SCHEMA,  \nhttp://www.iti.gr/SCHEMA \nE-mail: schema@iti.gr \n \nThe aim of the SCHEMA Network of Excellence is to bring together a critical \nmass of universities, research centers, industrial partners and end users, in order \nto design a reference system for content-based semantic scene analysis, \ninterpretation and understanding. Relevant research areas include: content-based \nmultimedia analysis and automatic annotation of semantic multimedia content, \ncombined textual and multimedia information retrieval, semantic-web,  MPEG-7 \nand MPEG-21 standards, user interfaces and human factors. In this paper, recent \nadvances in content-based analysis, indexing and retrieval of digital media within \nthe SCHEMA Network are presented. These advances will be integrated in the \nSCHEMA module-based, expandable reference system.  \n1. Introduction  \nThe rapid development of innovative tools to create user friendly and effective \nmultimedia libraries, services and environments requires novel concepts to \nsupport storage and fast retrieval of huge amounts of digital visual data. \nFurthermore, the World Wide Web has evolved to a vast distributed system of \ninter-networked, interactive databases containing text, audio and video stored in \na digital form. As a result of almost daily improvements in encoding and \ntransmission schemes, the items of these databases are easily accessible by \nanyone on the planet. In order to facilitate the rapid retrieval and efficient \nmanagement of useful information from such multimedia databases, research \nactivities related to content-based multimedia analysis and automatic \nannotation of semantic multimedia content, combined textual and multimedia \ninformation retrieval and semantic-web have evolved along with standards such \nas MPEG-7 and MPEG-21, user interfaces and human factors. \n                                                             \n† E. Izquierdo is with the Queen Mary, University of London, J.R. Casas is with the UPC – \nTechnical University of Catalonia, Spain, R. Leonardi and P. Migliorati are with the DEA \nUniversity of Brescia, Italy, Noel E. O’Connor is with the Dublin City University, Ireland, I. \nKompatsiaris and M. G. Strintzis are with the Informatics and Telematics Institute, Greece. \n  \n2\nSuccessful resolution of these matters will allow more efficient and user-\nfriendlier access to all forms of data and will improve data accessibility for all. \nThe diversity and complexity of all these topics requires experts from fields \nsuch as signal processing (image-video-audio processing), computer vision, \ninformation technology (database design and implementation, multimedia \ninformation retrieval), computer networks, human factors engineering and \nartificial intelligence. Therefore, the aim of the SCHEMA Network of \nExcellence is to bring together a critical mass of universities, research centres \nindustrial partners and end users, in order to improve the systematic exchange \nof information and design a reference system for content-based semantic scene \nanalysis, interpretation and understanding. \nIn this paper, recent advances in content-based analysis, indexing and \nretrieval of digital media within the SCHEMA network of excellence are \npresented that are going to be integrated in the project’s module-based and \nexpandable reference system. More specifically, after an overview of the \ncurrent state of the art, two algorithms for knowledge-based inference, one for \nstill images and one for video (soccer game video sequences) are presented. \nAdditionally, two World Wide Web based community-access digital video and \nimage systems are described that could be used as suitable client-server \narchitectures to allow SCHEMA applications to be able to run interactively \nover the Internet.   \n2. State of the Art     \nMany content based indexing and retrieval systems have been developed \nover the last decade. However, no system or technology has yet become widely \npervasive. Most of these systems are currently available for general and domain \nspecific use. These systems fall broadly under four categories: query by \ncontent [1, 2], iconic query [3], SQL query [4], and mixed queries [5]. The query \nby content is based on images, tabular form, similarity retrieval (rough \nsketches) or by component features (shape, color, texture). The iconic query \nrepresents data with ‘look alike’ icons and specifies a query by the selection of \nicons. SQL queries are based on keywords, with the keywords being conjoined \nwith the relationship (AND, OR) between them, thus forming compound \nstrings. The mixed queries can be specified by text and as well as icons. All of \nthese systems are based on different indexing structures. In his excellent \nreview of current content based recognition and retrieval systems, Paul Hill [6] \ndescribes the most relevant systems in terms of both commercial and academic \navailability, classified according to the used database population, query \ntechniques and indexing features. Here database population refers to actual \nprocess of populating a database. The most popular open available systems are: \n  \n \n3\nQBIC, Photobook, Netra & Netra-V, Virage, Webseek, Islip/Infomedia and \nViBE [7].  \nCognitively, a predominant feature in video is its higher-level temporal \nstructure. People are unable to perceive millions of individual frames, but they \ncan perceive episodes, scenes, and moving objects. A scene in a video is a \nsequence of frames that are considered to be semantically consistent. Scene \nchanges therefore demarcate changes in semantic context. Segmenting a video \ninto its constituent scenes permits it to be accessed in terms of meaningful \nunits. A video is physically formed by shots and semantically described by \nscenes. A shot is a sequence of frames representing continuous action in time \nand space. A scene is a story unit and consists of a sequence of connected or \nunconnected shots. Most of the current research is devoted to shot-based video \nsegmentation [8]. Differences between frames can be quantified by pairwise \npixel comparisons, or with schemes based on intensity or colour histograms. \nMotion and dynamic scene analysis can also provide cues for temporal \nsegmentation. A good review of these scene detection schemes is found in [9]. \nAnother approach is proposed by Corridoni and Del Bimbo to detect gradual \ntransitions [10]. They introduce a metric based on the chromatic properties. \nArdizzone et al. [11] proposed a neural network approach for scene detection in \nthe video retrieval system JACOB [12]. The approach reported in [13, 14] uses \na priori knowledge to identify scenes in a video sequence.  \nIn [15], image annotation or indexing is defined as the process of \nextracting from the video data the temporal location of a feature and its value. \nThe work by Davis [16] is an example of high level indexing. This approach uses \na set of predefined index terms for annotating video. The index terms are \norganized based on a high level ontological categories like action, time, space, \netc. The high level indexing techniques are primarily designed from the \nperspective of manual indexing or annotation. This approach is suitable for \nsmall quantities of new video and for accessing previously annotated databases. \nTo deal with large databases low-level features are needed. Low-level indexing \ntechniques provide access to video based on properties like color, texture etc. \nOne of the pioneering works in this area is by Swanberg et al. [17]. They have \npresented work on finite state data models for content based parsing and \nretrieval of news video. Smoliar el al [18] have also proposed a method for \nparsing news video. Underpinning all indexing techniques in the spatial domain \nare different processing tasks and methodologies ranging from data base \nmanagement to low-level image understanding.  \n3. Knowledge-based inference: from visual features to objects \nSegmentation often results insufficient for complex image analysis operations. \nThe paradigm is segmentation applied to object extraction [19]. Objects are \n  \n4\nsemantic entities by definition, often composed of visually distinguishable \nparts. This compromises the performance of segmentation algorithms. A \nnumber of visual features can be considered to better assess segmentation \ncriteria:  \n§ Homogeneity: objects of interest tend to be homogeneous, hopefully with \ntransitions at outer boundaries allowing the definition of contours. Either \ntrivial spatial/temporal or more complex forms of homogeneity such as \ncontour or shape homogeneity can be used as segmentation criteria [20]. \n§ Compactness (adjacency of parts): objects tend to be connected. When \nobjects are composed of different parts, the parts are often spatially \nlinkedc \n§ Regularity (low complexity): object shapes and contours usually show \nsome regularity. Most objects of interest present piecewise straight or \nrounded contour boundaries. Their shape complexityd tends to be rather \nlow. \n§ Inclusion: objects may present holes but, most often, smaller parts are \nincluded in larger areas. Sometimes inclusion is not complete: larger parts \nmay partially cover smaller parts. In the latter case, smaller parts may still \nbe partially included in the convex hull of larger parts (partial inclusion). \n§ Symmetry: Symmetry abounds in most natural and artificial objects [21, \n22]. Segmentation algorithms tend to disregard symmetry for the \ncomplexity of analysis. We put forward symmetry as a strong feature for \nobject extraction. \nThe features listed above define the visual structure of objects and, as such, \nare related to their physical structure, showing up directly in the visual signal. \nWe call these features \"syntactic\" features, as opposed to semantic. Like spatial \nhomogeneity, syntactic features are not specific to a particular kind of targeted \nor modeled objects and, thus, their use for object extraction remains generic, \nnot narrowing the scope of the application domain for segmentation \nalgorithms. \nSyntactic features can be found by structure (or syntax) analysis. Structure \nanalysis is based on shapes and spatial configuration of spatially homogeneous \nregions in the image. Shape and structure are difficult to assess for \nsegmentation criteria at the level of the pixels, before any segmentation has \nbeen carried out. Therefore, structure analysis is more conveniently carried out \nstarting from a simple initial segmentation (possibly an over-segmented \nimage). This allows assessing shape and position criteria for the initial regions \nin order to detect structures formed by sets of simple regions that might be \nproposed as object candidates. \n                                                             \nc Object compactness is one reason why segmentation is applied to object extraction. \nSegmentation algorithms produce partitions, i.e. sets of connected (compact) regions. \nd Shape complexity can be defined as the squared contour length divided by the area. \n  \n \n5\nAn example is shown in Figure 1. The proposed features have been \nanalyzed over the initial partition. A merging algorithm progressively structures \nthe regions into sets of quasi-symmetric or partially included regions until the \nfinal partition. Observe, how inclusion allows the merging of regions 1 and 2 \ninto the background in Figure 1.d). Partial symmetry with respect to a central \nelement allows the merging of regions 4 and 5 with 3 (the central element). \nTexture information and transition strength between regions is also considered \nat each merging. This explains why the last two lower regions are merged \nwithin the background, and not with the central element (former regions \n3+4+5) what might happen if only symmetry was considered. \n \na)  b)  c)  d)  e)  \nFigure 1. Structure analysis using symmetry, compactness, regularity and inclusion: a) initial \npartition (57 regions), b) 24 regions, c) 8 regions, d) 3 regions e) 2 regions. \n4. Sport Content Characterization by Controlled Markov Chains \nIn this section, a semantic indexing algorithm based on the controlled \nMarkov chain-modelling framework is proposed. The proposed algorithm has \nbeen conceived for soccer game video sequences. The problem of automatic \ndetection of semantic events in sport games has been studied by many \nresearchers. In general the objective is to identify certain spatio-temporal \nsegments that correspond to semantically significant events.  \nIn [23], for example, presents a method that tries to detect the complete \nset of semantic events, which may happen in a soccer game. This method uses \nthe position information of the player and of the ball during the game as input, \nand therefore needs a quite complex and accurate tracking system to obtain this \ninformation. In [24] and [25] the correlation between low-level descriptors and \nthe semantic events in a soccer game have been studied. In particular, in [24], it \nis shown that the low-level descriptors are not sufficient, individually, to obtain \nsatisfactory results. In [26] the temporal evolution of the low-level descriptors \nin correspondence with semantic events has been exploited, by proposing an \nalgorithm based on a finite-state machine. This algorithm gives good results in \nterms of accuracy in the detection of the relevant events, whereas the number \nof false detections remains still quite large. \nIn this work, a semantic video indexing algorithm using controlled Markov \nchains to model the temporal evolution of low-level descriptors was studied. \nCertain low-level descriptors were chosen, which represent the following \n  \n6\ncharacteristics: (i) lack of motion, (ii) camera operations (pan and zoom \nparameters), and (iii) presence of shot-cuts. It is supposed that each semantic \nevent takes place over a two-shot block and that it can be modelled by a \ncontrolled Markov chain.  \nSpecifically, 6 models were considered denoted by A, B, C, D, E, and F, \nwhere model A is associated to goals, model B to corner kicks, and models C, \nD, E, F describe other situations of interest that occur in soccer games, such as \nfree kicks, plain actions, and so on. On the basis of the derived six Markov \nmodels, one can classify each pair of shots in a soccer game video sequence by \nusing the maximum likelihood criterion.  \nThe performance of the proposed algorithm have been tested considering \nabout 2 hours of MPEG2 sequences containing more than 800 shot-cuts, \ndetermined using the algorithm described in Section 2. The sequences contain \n9 goals and 16 corner kicks. The obtained results are the following: 8 goals out \nof 9, and 10 corner kicks out of 16 are detected. The number of false \ndetections could seem quite relevant. However, these results are obtained using \nmotion information only, so using other type of media information could \nprobably reduce these false detections. Therefore, further work needs to be \ndone in order to improve its performance by considering other descriptors, \nrelated for example to audio information. \n5.  Físchlár: a suite of online multi-media information retrieval \nsystems \nThe Centre for Digital Video Processing (CDVP) in Dublin City University has \ndeveloped a Web-based community-access digital video system called Físchlár \n[26]. The system allows recording, analysis, browsing, searching and playback \nof content from 8 terrestrial television channels. It stores over 300 hours of \nMPEG-1 encoded content at any time, supports over 200 simultaneous video \nstreams and has over 1400 users on campus.  The system is used as an \nexperimentation platform for testing and evaluating automatic audio-visual \nindexing tools and structured design methods for user interfaces. A version of \nthe system designed specifically for broadcast news has been deployed for \neducational use by journalism students, whereas a version providing medical \ncontent is used by nursing students. \nThe most recent version of the system was developed as part of the CDVP’s \nwork in the video track of the Text Retrieval Conference (TREC) – a U.S. \ninitiative to benchmark IR [27]. TREC provided a test corpus, selected a set of \naudio-visual features to be extracted and specified a set of queries. Features \nincluded indoor scenes, outdoor scenes, presence of a face, presence of a \ngroup of people, cityscape, landscape, text overlay, speech, instrumental sound, \nmonologue. Targeting three of these, the CDVP developed automatic \n  \n \n7\napproaches for speech/music discrimination and rhythm detection [28] and face \ndetection. The results of participants’ different feature extraction tools were \nmade available to all participants in MPEG-7 format. A Físchlár interface for \nsearching the test corpus using all available TREC features was developed and \nis illustrated in Figure 2. It allows formulation of a query based on selecting \nfeatures from the available set and browsing and ranking of results based on \ndifferent combinations of features. \n \n \nFigure 2 - The Fischlar-TREC search and retrieval interface \n6. A World Wide Web Region-Based Search Engine \nISTORAMAe is a region-based color indexing and retrieval system for the \nInternet. As a basis for the indexing, a novel K-Means segmentation algorithm \nis used, modified so as to take into account the assumed region coherence [29], \n[30]. Based on the extracted regions, characteristic features are estimated using \ncolor, texture and shape information. This algorithm is integrated into a \n                                                             \ne htp://uranus.ee.auth.gr/Istorama \n  \n8\ncomplete content-based search engine system using web and database \ntechnologies [31]. An important and unique aspect of the system is that, in the \ncontext of similarity-based querying, the user is allowed to view the internal \nrepresentation of the submitted image and the query results. More specifically, \nin a querying task, the user can access the regions directly in order to see the \nsegmentation of the query image and specify which aspects of the image are \ncentral to the query.  \nThe overall system is split into two parts: (i) the on-line part and (ii) the \non-line or user part. In the on-line part, Information Crawlers, implemented \nentirely in Java, continuously traverse the WWW, collect images and transfer \nthem to the central Server for further processing. Then the image indexing \nalgorithms process the image in order to extract descriptive features. Based on \nthe extracted by the modified K-means algorithm [29], [30] regions, \ncharacteristic features are estimated using color, texture and shape/region \nboundary information. The characteristic features along with information \nregarding the images such as the URL, date of transaction, size and a thumbnail \nare then stored in the database. For the database access and management, the \nMySQL database management system was used.  \nIn the on-line part, a user connects to the system through a common Web \nBrowser using the HTTP protocol. The user can then submit queries either by \nexample images or by simple image information (size, date, initial location, \netc). The query is processed by the server and the retrieval phase begins; the \nindexing procedure is repeated again for the submitted image and then the \nextracted features are matched against those stored in the database using an \nSQL query. The results containing the URL as well as the thumbnail of the \nsimilar images are transmitted to the user by creating an HTML page with use \nof PHP (recursive acronym for PHP: “Hypertext Preprocessor\"). The results \nare ranked according to their similarity to the submitted image. \nIn order to evaluate the performance of the system a variety of queries \nusing a set of 3,500 images were performed. In most cases, the retrieved \nimages match the selection criteria of the user. A comparison to global \nhistogram was made and it was seen that the global histogram matching \nperforms much worse than that of the described system, with average lower \nprecisions in all image categories.  \nReferences \n1.   S.K. Chang and T. Kunii, “Pictorial Database Systems,” IEEE Computer, \nEd. S.K. Chang, November 1981, pp. 13-21.  \n2.   M. Flickner, H. Sawhney, W. Niblack, J. Ashley, Q. Huang, B. Dom, M. \nGorkhani, J. Hafner, D. Lee, D. Petkovic, D. Steele, and P. Yanker, ``Query \n  \n \n9\nby Image and Video Content: The QBIC System,'' IEEE Computer, Vol. 28, \nNo. 9, September 1995, pp. 23-32.  \n3.   A.D. Bimbo, M. Campanai, and P. Nesi, ``A Three-Dimensional Iconic \nEnvironment for Image Database Querying,'' IEEE Trans. on Software \nEngineering, Vol. 19, No. 10, October 1993, pp. 997-1011.  \n4.   J.A. Orenstein, and F.A. Manola, ``PROBE Spatial Data Modeling and \nQuery Processing in an Image Database Application,'' IEEE Trans. on \nSoftware Engineering, Vol. 14, No. 5, pp. 661-629, May 1988.  \n5.   G. Ahanger, D. Benson, and T.D.C. Little, ``Video Query Formulation,'' \nProc. IS&T/SPIE, Conference on Storage and Retrieval for Image and \nVideo Databases, Vol. 2420, February 1995, pp. 280-291.  \n6.   P. Hill, ‘Review of current content based recognition and retrieval \nsystems’, Technical report 05/1, Virtual DCE. \n7.   J-Y Chen, C.A. Bouman, and John Dalton. “Similarity pyramids for \nbrowsing and organization of large image databases”, Proc. SPIE, vol. \n3656, pp 144-154, 1999.  \n8.  F. Beaver. Dictionary of Films Terms. Twayne Publishing, New-York, \n1994. \n9.   G. Ahanger and T.D.C. Little. A survey of technologies for parsing and \nindexing digital videos. Journal of Visual Communication and Image \nRepresentation, 7:28—43, March 1996. \n10.   J.M. Corridoni and A. Del Bimbo. Structured digital video indexing. In \nICPR ‘96, pages 125—129, 1996. \n11.   E. Ardizzone, G.A.M. Gioiello, M. La Cascia, and D. Molinelli. A real-\ntime neural approach to scene cut detection. In SPIE Storage and Retrieval \nfor Image and Video Databases IV, 1996. \n12.   M. La Cascia and E. Ardizzone. Jacob: Just a content-based query system \nfor video databases. In ICASSP’96, 1996. \n13.   D. Swanberg, C.-F. Shu, and R. Jam. Knowledge guided parsing in video \ndatabases. In SPIE vol.1908, pages 13—21, 1993. \n14.   H. Zhang, Y. Gong, S.W. Smoliar, and S.Y. Tan. Automatic parsing of news \nvideo. In International Conference on Multimedia Computing and Systems, \npages 45—54, 1994. \n15.   A Survey of Technologies for Parsing and Indexing Digital Video, Boston \nUniversity, http://hulk.bu.edu/pubs/papers/1995/ahanger-jvcir95/TR-11-\n01-95.html \n16.   Deborah Swanberg, Chiao-Fe Shu, and Remesh Jain. \"Knowledge guided \nparsing in v ideo databases.\" Electronic Imaging: Science and Technology, \nSan J ose, California, February 1993. IST/SPIE.  \n17.  Stephen W Smoliar, HongJiang Zhang, and Jian Hua Wu. \"Using frame \ntechnology to manage video.\" In Proc. of the Workshop on Indexing and \n  \n10\nReuse in Multimed ia Systems. American Association of Artificial \nIntelligence, August 1994  \n18.   Arun Hampapur, Ramesh Jain and Terry E Weymouth, \"Feature Based \nDigital Video Indexing\"  \n19.   S-F Chang, et al. Semantic Visual Templates: Linking Visual Features to \nSemantics. International Conference on Image Processing (1998) \n20.   B. Johanson. Multiscale Curvature Detection in Computer Vision. Thesis \nNo. 877, Department Electrical Eng., Linköping University, Sweden \n(2001) \n21.   P. J. van Otterloo, A Contour-Oriented Approach to Shape Analysis. \nPrentice Hall, London (1991). \n22.   C. Sun and D. Si, \"Fast Reflectional Symmetry Detection Using Fourier \nTransform\", Journal of Real-Time Imaging, vol.5, no.1, p.63-74 (Feb \n1999). \n23.  V. Tovinkere, R. J. Qian, “Detecting Semantic Events in Soccer Games: \nToward a Complete Solution”, Proc. ICME'2001, pp. 1040-1043, August \n2001, Tokyo, Japan. \n24.  A. Bonzanini, R. Leonardi, P. Migliorati, “Semantic Video Indexing Using \nMPEG Motion Vectors\", Proc. EUSIPCO'2000, pp. 147-150,  4-8 Sept. \n2000, Tampere, Finland \n25.  A. Bonzanini, R. Leonardi, P. Migliorati, “Event Recognition in Sport \nPrograms Using Low-Level Motion Indices”, Proc. ICME'2001, pp. 920-\n923, August 2001, Tokyo, Japan.  \n26.  N. O'Connor, et al, “Físchlár: An On-line System for Indexing and \nBrowsing of Broadcast Television Content”, Proc ICASSP 2001, Salt Lake \nCity, UT, 7-11 May 2001. \n27.  A. Smeaton et al, “The TREC2001 Video Track: Information Retrieval on \nDigital Video Information”,  ECDL 2002 - European Conference on \nResearch and Advanced Technology for Digital Libraries. Rome, Italy, 16-\n18 September 2002.   \n28.   R. Jarina et al, “ Rhythm Detection for Speech-Music Discrimination in \nMPEG Compressed Domain”, Proc.  DSP 2002 - 14th International \nConference on Digital Signal Processing, Santorini, Greece, 1-3 July \n2002. \n29.  I. Kompatsiaris and M.G. Strintzis, \"Spatiotemporal Segmentation and \nTracking of Objects for Visualization of Videoconference Image \nSequences\", IEEE Trans. on Circuits and System for Video Technology, \nvol. 10, no. 8, December 2000. \n30.  N. V. Boulgouris, I. Kompatsiaris, V. Mezaris, D. Simitopoulos, M. G. \nStrintzis, \"Segmentation and Content-based Watermarking for Color Image \nand Image Region Indexing and Retrieval\", EURASIP Journal on Applied \nSignal Processing, April 2002. \n  \n \n11\n31.  I. Kompatsiaris, E. Triantafillou and M. G. Strintzis, \"Region-Based Color \nImage Indexing and Retrieval\", 2001 International Conference on Image \nProcessing (ICIP2001), Thessaloniki, Greece, October 7-10, 2001. \n",
            "id": 4762299,
            "identifiers": [
                {
                    "identifier": "53585903",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "147596566",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:doras.dcu.ie:420",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "20853687",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:citeseerx.psu:10.1.1.12.6648",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "2100084515",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "10.1142/9789812704337_0094",
                    "type": "DOI"
                },
                {
                    "identifier": "11308305",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:iris.unibs.it:11379/10244",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "377231665",
                    "type": "CORE_ID"
                }
            ],
            "title": "Advanced content-based semantic scene analysis and information retrieval: the SCHEMA project",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:citeseerx.psu:10.1.1.12.6648",
                "oai:iris.unibs.it:11379/10244",
                "oai:doras.dcu.ie:420"
            ],
            "publishedDate": "2003-01-01T00:00:00",
            "publisher": "'World Scientific Pub Co Pte Lt'",
            "pubmedId": null,
            "references": [
                {
                    "id": 16520571,
                    "title": "A Contour-Oriented Approach to Shape Analysis.",
                    "authors": [],
                    "date": "1991",
                    "doi": null,
                    "raw": "P. J . van Otterloo, A Contour-Oriented Approach to Shape Analysis. Prentice Hall, London (1991).",
                    "cites": null
                },
                {
                    "id": 16520515,
                    "title": "A realtime neural approach to scene cut detection.",
                    "authors": [],
                    "date": "1996",
                    "doi": null,
                    "raw": "E. Ardizzone, G.A.M. Gioiello, M. La Cascia, and D. Molinelli. A realtime neural approach to scene cut detection. In SPIE Storage and Retrieval for Image and Video Databases IV, 1996.",
                    "cites": null
                },
                {
                    "id": 16520535,
                    "title": "A Survey of Technologies for Parsing and Indexing Digital Video,",
                    "authors": [],
                    "date": null,
                    "doi": "10.1006/jvci.1996.0004",
                    "raw": "A Survey of Technologies for Parsing and Indexing Digital Video, Boston University, http://hulk.bu.edu/pubs/papers/1995/ahanger-jvcir95/TR-11-01-95.html",
                    "cites": null
                },
                {
                    "id": 16520511,
                    "title": "A survey of technologies for parsing and indexing digital videos.",
                    "authors": [],
                    "date": "1996",
                    "doi": "10.1006/jvci.1996.0004",
                    "raw": "G. Ahanger and T.D.C. Little. A survey of technologies for parsing and indexing digital videos. Journal of Visual Communication and Image Representation, 7:28—43, March 1996.",
                    "cites": null
                },
                {
                    "id": 16520497,
                    "title": "A Three-Dimensional Iconic Environment for Image Database Querying,''",
                    "authors": [],
                    "date": "1993",
                    "doi": "10.1109/32.245741",
                    "raw": "A.D. Bimbo, M. Campanai, and P. Nesi, ``A Three-Dimensional Iconic Environment for Image Database Querying,'' IEEE Trans. on Software Engineering, Vol. 19, No. 10, October 1993, pp. 997-1011.",
                    "cites": null
                },
                {
                    "id": 16520530,
                    "title": "Automatic parsing of news video.",
                    "authors": [],
                    "date": "1994",
                    "doi": "10.1109/mmcs.1994.292432",
                    "raw": "H. Zhang, Y. Gong, S.W. Smoliar, and S.Y. Tan. Automatic parsing of news video. In International Conference on Multimedia Computing and Systems, pages 45—54, 1994.",
                    "cites": null
                },
                {
                    "id": 16520580,
                    "title": "Detecting Semantic Events in Soccer Games: Toward a Complete Solution”,",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1109/icme.2001.1237851",
                    "raw": "V. Tovinkere, R. J. Qian, “Detecting Semantic Events in Soccer Games: Toward a Complete Solution”, Proc. ICME'2001, pp. 1040-1043, August 2001, Tokyo, Japan.",
                    "cites": null
                },
                {
                    "id": 16520509,
                    "title": "Dictionary of Films Terms.",
                    "authors": [],
                    "date": null,
                    "doi": null,
                    "raw": "F. Beaver. Dictionary of Films Terms. Twayne Publishing, New-York,",
                    "cites": null
                },
                {
                    "id": 16520589,
                    "title": "Event Recognition in Sport Programs Using Low-Level Motion Indices”,",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1109/icme.2001.1237894",
                    "raw": "A. Bonzanini, R. Leonardi, P. Migliorati, “Event Recognition in Sport Programs Using Low-Level Motion Indices”, Proc. ICME'2001, pp. 920-923, August 2001, Tokyo, Japan.",
                    "cites": null
                },
                {
                    "id": 16520576,
                    "title": "Fast Reflectional Symmetry Detection Using Fourier Transform&quot;,",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1006/rtim.1998.0135",
                    "raw": "C. Sun and D. Si, &quot;Fast Reflectional Symmetry Detection Using Fourier Transform&quot;, Journal of Real-Time Imaging, vol.5, no.1, p.63-74 (Feb 1999).",
                    "cites": null
                },
                {
                    "id": 16520552,
                    "title": "Feature Based Digital Video Indexing&quot;",
                    "authors": [],
                    "date": null,
                    "doi": "10.1007/978-0-387-34905-3_8",
                    "raw": "Arun Hampapur, Ramesh Jain and Terry E Weymouth, &quot;Feature Based Digital Video Indexing&quot;",
                    "cites": null
                },
                {
                    "id": 16520594,
                    "title": "Físchlár: An On-line System for Indexing and Browsing of Broadcast Television Content”,",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1109/icassp.2001.941249",
                    "raw": "N. O'Connor, et al, “Físchlár: An On-line System for Indexing and Browsing of Broadcast Television Content”, Proc ICASSP 2001, Salt Lake City, UT, 7-11 May 2001.",
                    "cites": null
                },
                {
                    "id": 16520524,
                    "title": "Just a content-based query system for video databases.",
                    "authors": [],
                    "date": "1996",
                    "doi": "10.1109/icassp.1996.543585",
                    "raw": "M. La Cascia and E. Ardizzone. Jacob: Just a content-based query system for video databases. In ICASSP’96, 1996.",
                    "cites": null
                },
                {
                    "id": 16520540,
                    "title": "Knowledge guided parsing in v ideo databases.&quot;",
                    "authors": [],
                    "date": "1993",
                    "doi": "10.1117/12.143647",
                    "raw": "Deborah Swanberg, Chiao-Fe Shu, and Remesh Jain. &quot;Knowledge guided parsing in v ideo databases.&quot; Electronic Imaging: Science and Technology, San J ose, California, February 1993. IST/SPIE.",
                    "cites": null
                },
                {
                    "id": 16520527,
                    "title": "Knowledge guided parsing in video databases.",
                    "authors": [],
                    "date": "1993",
                    "doi": "10.1117/12.143647",
                    "raw": "D. Swanberg, C.-F. Shu, and R. Jam. Knowledge guided parsing in video databases. In SPIE vol.1908, pages 13—21, 1993.",
                    "cites": null
                },
                {
                    "id": 16520566,
                    "title": "Multiscale Curvature Detection in Computer Vision.",
                    "authors": [],
                    "date": "2001",
                    "doi": null,
                    "raw": "B. Johanson. Multiscale Curvature Detection in Computer Vision. Thesis No. 877, Department Electrical Eng., Linköping University, Sweden (2001)",
                    "cites": null
                },
                {
                    "id": 16520491,
                    "title": "Pictorial Database Systems,”",
                    "authors": [],
                    "date": "1981",
                    "doi": null,
                    "raw": "S.K. Chang and T. Kunii, “Pictorial Database Systems,” IEEE Computer, Ed. S.K. Chang, November 1981, pp. 13-21.",
                    "cites": null
                },
                {
                    "id": 16520500,
                    "title": "PROBE Spatial Data Modeling and Query Processing in an Image Database Application,''",
                    "authors": [],
                    "date": "1988",
                    "doi": "10.1109/32.6139",
                    "raw": "J.A. Orenstein, and F.A. Manola, ``PROBE Spatial Data Modeling and Query Processing in an Image Database Application,'' IEEE Trans. on Software Engineering, Vol. 14, No. 5, pp. 661-629, May 1988.",
                    "cites": null
                },
                {
                    "id": 16520494,
                    "title": "Query by Image and Video Content: The QBIC System,''",
                    "authors": [],
                    "date": "1995",
                    "doi": "10.1109/2.410146",
                    "raw": "M. Flickner, H. Sawhney, W. Niblack, J. Ashley, Q. Huang, B. Dom, M. Gorkhani, J. Hafner, D. Lee, D. Petkovic, D. Steele, and P. Yanker, ``Query by Image and Video Content: The QBIC System,'' IEEE Computer, Vol. 28, No. 9, September 1995, pp. 23-32.",
                    "cites": null
                },
                {
                    "id": 16520621,
                    "title": "Region-Based Color Image Indexing and Retrieval&quot;,",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1109/icip.2001.959131",
                    "raw": "I. Kompatsiaris, E. Triantafillou and M. G. Strintzis, &quot;Region-Based Color Image Indexing and Retrieval&quot;, 2001 International Conference on Image Processing (ICIP2001), Thessaloniki, Greece, October 7-10, 2001.",
                    "cites": null
                },
                {
                    "id": 16520505,
                    "title": "Review of current content based recognition and retrieval systems’,",
                    "authors": [],
                    "date": null,
                    "doi": null,
                    "raw": "P. Hill, ‘Review of current content based recognition and retrieval systems’, Technical report 05/1, Virtual DCE.",
                    "cites": null
                },
                {
                    "id": 16520607,
                    "title": "Rhythm Detection for Speech-Music Discrimination in MPEG Compressed Domain”,",
                    "authors": [],
                    "date": null,
                    "doi": "10.1109/icdsp.2002.1027851",
                    "raw": "R. Jarina et al, “ Rhythm Detection for Speech-Music Discrimination in MPEG Compressed Domain”, Proc.  DSP 2002  - 14th International Conference on Digital Signal Processing, Santorini, Greece, 1 -3 July",
                    "cites": null
                },
                {
                    "id": 16520614,
                    "title": "Segmentation and Content-based Watermarking for Color Image and Image Region Indexing and Retrieval&quot;,",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1155/s1110865702000768",
                    "raw": "N. V. Boulgouris, I. Kompatsiaris, V. Mezaris, D. Simitopoulos, M. G. Strintzis, &quot;Segmentation and Content-based Watermarking for Color Image and Image Region Indexing and Retrieval&quot;, EURASIP Journal on Applied Signal Processing, April 2002.",
                    "cites": null
                },
                {
                    "id": 16520584,
                    "title": "Semantic Video Indexing Using MPEG Motion Vectors&quot;,",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "A. Bonzanini, R. Leonardi, P. Migliorati, “Semantic Video Indexing Using MPEG Motion Vectors&quot;, Proc. EUSIPCO'2000, pp. 147-150,  4-8 Sept. 2000, Tampere, Finland",
                    "cites": null
                },
                {
                    "id": 16520558,
                    "title": "Semantic Visual Templates: Linking Visual Features to Semantics.",
                    "authors": [],
                    "date": "1998",
                    "doi": "10.1109/icip.1998.727321",
                    "raw": "S-F Chang, et al. Semantic Visual Templates: Linking Visual Features to Semantics. International Conference on Image Processing (1998)",
                    "cites": null
                },
                {
                    "id": 16520507,
                    "title": "Similarity pyramids for browsing and organization of large image databases”,",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1117/12.320147",
                    "raw": "J-Y Chen, C.A. Bouman, and John Dalton. “Similarity pyramids for browsing and organization of large image databases”, Proc.  SPIE, vol. 3656, pp 144-154, 1999.",
                    "cites": null
                },
                {
                    "id": 16520612,
                    "title": "Spatiotemporal Segmentation and Tracking of Objects for Visualization of Videoconference Image Sequences&quot;,",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1109/mmcs.1999.779286",
                    "raw": "I. Kompatsiaris and M.G. Strintzis, &quot;Spatiotemporal Segmentation and Tracking of Objects for Visualization of Videoconference Image Sequences&quot;, IEEE Trans. on Circuits and System for Video Technology, vol. 10, no. 8, December 2000.",
                    "cites": null
                },
                {
                    "id": 16520513,
                    "title": "Structured digital video indexing.",
                    "authors": [],
                    "date": "1996",
                    "doi": "10.1109/icpr.1996.546807",
                    "raw": "J.M. Corridoni and A. Del Bimbo. Structured digital video indexing. In ICPR ‘96, pages 125—129, 1996.",
                    "cites": null
                },
                {
                    "id": 16520600,
                    "title": "The TREC2001 Video Track: Information Retrieval",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1007/3-540-45747-x_20",
                    "raw": "A. Smeaton et al, “The TREC2001 Video Track: Information Retrieval on Digital Video Information”,  ECDL 2002  - European Conference on Research and Advanced Technology for Digital Libraries. Rome, Italy, 16-18 September 2002.",
                    "cites": null
                },
                {
                    "id": 16520546,
                    "title": "Using frame technology to manage video.&quot;",
                    "authors": [],
                    "date": "1994",
                    "doi": null,
                    "raw": "Stephen W Smoliar, HongJiang Zhang, and Jian Hua Wu. &quot;Using frame technology to manage video.&quot; In Proc. of the Workshop on Indexing and Reuse in Multimed ia Systems. American Association of Artificial Intelligence, August 1994",
                    "cites": null
                },
                {
                    "id": 16520502,
                    "title": "Video Query Formulation,''",
                    "authors": [],
                    "date": "1995",
                    "doi": "10.1117/12.205295",
                    "raw": "G. Ahanger, D. Benson, and T.D.C. Little, ``Video Query Formulation,'' Proc. IS&T/SPIE, Conference on Storage and Retrieval for Image and Video Databases, Vol. 2420, February 1995, pp. 280-291.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "http://doras.dcu.ie/420/1/wiamis_2003_3.pdf",
                "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.12.6648"
            ],
            "updatedDate": "2022-02-24T16:55:52",
            "yearPublished": 2003,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/11308305.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/11308305"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/11308305/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/11308305/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/4762299"
                }
            ]
        },
        {
            "acceptedDate": "2014-06-17T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Casetti, Claudio Ettore"
                },
                {
                    "name": "Chiasserini, Carla Fabiana"
                },
                {
                    "name": "Scopigno, Riccardo"
                },
                {
                    "name": "Vesco, Andrea Guido Antonio"
                }
            ],
            "contributors": [
                "Andrea"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/16467317",
                "https://api.core.ac.uk/v3/outputs/457976653",
                "https://api.core.ac.uk/v3/outputs/234897804"
            ],
            "createdDate": "2013-09-22T06:02:05",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 12601,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/12601",
                    "logo": "https://api.core.ac.uk/data-providers/12601/logo"
                },
                {
                    "id": 351,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/351",
                    "logo": "https://api.core.ac.uk/data-providers/351/logo"
                }
            ],
            "depositedDate": "2013-12-01T00:00:00",
            "abstract": null,
            "documentType": "research",
            "doi": "10.1109/glocomw.2013.6825176",
            "downloadUrl": "https://core.ac.uk/download/pdf/16467317.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "1Investigating the Effectiveness of Decentralized\nCongestion Control in Vehicular Networks\nAndrea Vesco∗, Riccardo Scopigno∗, Claudio Casetti†, and Carla-Fabiana Chiasserini†\n∗Istituto Superiore Mario Boella, Torino, Italy\nE-mail: vesco@ismb.it, scopigno@ismb.it\n†Politecnico di Torino, Torino, Italy\nE-mail: casetti@polito.it, chiasserini@polito.it\nAbstract—Vehicular ad hoc networks are expected to suffer\nfrom channel congestion, due to the high number of vehicles\nmoving on the roads, the limited available bandwidth and the\nmany applications that they will support. In order to mitigate\nsuch problem, ETSI has recently specified the Decentralized Con-\ngestion Control (DCC) mechanisms for Intelligent Transportation\nSystems (ITS) operating in the 5 GHz range. Although they\nare already part of the standard, very few results exist on the\nDCC performance. In this work, we aim at filling this gap by\ninvestigating the impact on the system performance of the single\nDCC mechanisms, as well as their joint effect when they are all\nimplemented at the DCC access layer. Surprisingly, we find that\nDCC has little impact, and, in certain scenarios, it may even lead\nto performance degradation with respect to the case where the\nlegacy IEEE 802.11p MAC protocol is implemented.\nI. INTRODUCTION\nVehicular ad hoc networks (VANETs) will enhance traffic\nsafety, for both in-vehicle passengers and other road-users,\nthrough a diverse set of applications. Most of such applications\nrequire real time communication, with strict constraints, not\nonly on the message delivery delay, but also on the message\ntransfer reliability. In addition, they often relay on broadcast\nmessage transmissions, for which it is important to ensure a\ngood level of scalability.\nAccording to the ongoing standardisation, traffic safety\napplications in VANETs rely on the IEEE 802.11p specifi-\ncations, which introduce a distributed channel access scheme\nbased on the CSMA/CA technique. It is well-known that the\nCSMA/CA mechanism provides a fair channel access to the\ncontending nodes, i.e., on average, it lets nodes access the\nchannel the same number of times in a given time period.\nHowever, in the short run, it is an inherently unpredictable\nprotocol: the random exponential backoff procedure may lead\nto unbounded channel access delays, interference between con-\ncurrent transmissions may end up into transmissions failures,\nand messages, whose transmission fails repeatedly, have to be\ndropped. Such problems arise especially when there are several\nnodes contending for the wireless medium, thus highlighting\nthe poor scalability of the mechanism.\nIn order to mitigate the above problems, ETSI has recently\nstandardised the so called Decentralized Congestion Control\n(DCC) [1] whose goal is to let the network work as efficiently\nas possible, achieving high throughput while maintaining low\npacket loss and delay. For the sake of precision, DCC is\ncurrently being amended at access layer and extended to\nother layers1, leading to the definition of a cross-layer DCC\nfunction. Concerning the upcoming amendments, it seems that\nthey will deeply reduce the mechanisms at access layer and\nborrow some of the ideas proposed in [4], with the aim of\ncounteracting the poor effectiveness of current algorithms,\naccordingly to what will presented here.\nHowever, to this end, DCC leverages the local (ego-\nvehicle’s) knowledge about the channel status so as to trig-\nger adjustments to the parameters characterising the node\ntransmissions, thus reducing channel congestion. The channel\nstate information is acquired using channel probing, and\nthe obtained measurements are used to enable the following\nmechanisms:\n• Transmit Power Control (TPC), controlling the average\ntransmit power per packet;\n• Transmit Rate Control (TRC), varying the node transmis-\nsion duty cycle, i.e., the fraction of time during which a\nnode is in “transmit” state;\n• Transmit Data-rate Control (TDC), determining the data\nrate used by the node to transmit its packet;\n• DCC Sensitivity Control (DSC), adapting the clear chan-\nnel assessment to resolve local channel congestion;\n• Transmit Access Control (TAC), introducing a transmit\nqueueing concept to handle packet priority.\nCurrently, few works have investigated how much effective\nthe DCC mechanisms are mitigating traffic congestion and\nwhether they are stable. For instance, in [5] the authors focus\ntheir analysis of DCC on the TPC mechanism only, which is\nshown to lead to poor performance due to the coarse settings\ndefined in the standard [1]. This indeed highlights the need\nfor more extensive analyses so as to delve into the underlying\nphenomena of the overall DCC – not just TPC – and to\npinpoint its potential weaknesses. Despite DCC will be shortly\nredefined, the proposed analysis intends to draw conclusions\non the current DCC, hopefully supporting the analysis of\nfuture embodiments.\n1The worry about congestion is such that different layer-specific DCC\nentities have been defined to prevent and counteract the effects of congestion.\nThe DCC Access [1], being amended by the Specialist Task Force STF420,\nacts on parameters at physical and MAC layer to lower the load; the DCC Net\n[2], at network and transport layer is being studied by STF447 to map the\ntraffic classes of CAM messages, generated by the Facility Layer, to DCC\nprofiles acting on power and rate; finally DCC Facility [3] which acts on\nCAM / DENM generation.\n2In our work, we therefore implement the current DCC\nmechanisms in the widely-used network simulator ns-2 and\nstudy, separately, the impact of each of the aforementioned\nmechanisms as well their joint effect when they are all\ntriggered at the same time. We show the system behaviour in\nterms of packet delivery ratio (PDR) as well as channel access\ndelay. Our results suggest that, as it is currently envisioned, the\nDCC mechanism has low impact on the system performance\nand, in many cases, it is unable to reduce channel congestion.\nThe analysis of the single mechanisms also highlights two\nimportant facts. Firstly, some of the DCC mechanisms may be\nparticularly effective, but their advantage vanishes when they\nare all triggered at the same time, as they often favour opposite\nsystem behaviors. Secondly, other DCC mechanisms do not\nhave any significant impact when implemented with the default\nsettings, i.e., as suggested by the standard specifications;\nhowever, they could play a major role if different setting could\nbe used.\nThe rest of the paper is organised as follows. Section II\nsummarises the DCC mechanisms, specifying their goal and\nthe way they should be implemented. Such mechanisms are\nthen investigated through simulation in the scenarios described\nin Section III. Their performance, when they are separately and\njointly activated, is presented in Section IV. Finally, we draw\nour conclusions in Section V.\nII. THE DCC MECHANISMS\nAs mentioned, the current DCC acts at several layers of the\nprotocol stack, through mechanisms that are jointly activated\nto (i) achieve fair allocation of resources among all nodes\noperating in the same geographical area, (ii) keep the channel\nload low, (iii) provide fast adaptation to the surrounding,\nhighly-dynamic environment.\nTwo important DCC components are located at the manage-\nment and access layers. The former is responsible for setting\nthe parameter values that are used for configuring the access\ncomponent, specifically, their minimum, maximum, default\nand target values, as well as signal level thresholds and time\nconstants. The latter enhances the 802.11p MAC architecture,\nby adding four blocks:\n1) Transmit Queuing, which enhances the standard 802.11\nqueues by DCC mechanisms;\n2) Channel Probing, collecting statistics on the communi-\ncation channel and assessing the channel load. Mea-\nsurements are taken based on the power detected on\nthe channel and on the physical header of the packets\nover the medium, which depend on the DCC Sensitivity\n(NDL defDCCSensitivity);\n3) Transmit Statistics, which takes into account all trans-\nmitted packets, including retransmissions and control\npackets. For each access priority, it yields statistics such\nas the packet arrival rate at the transmit queue, and the\naverage transmission duration and power level.\n4) Control Loop, which adapts the behavior of\nthe ITS node to the estimated channel load,\nby properly setting the reference value of\ntransmit power (NDL refTxPower), packet interval\n(NDL refPacketInterval), data rate (NDL refDataRate),\nCCA sensitivity (NDL refCarrierSense), and queue\nstatus; such parameters are then used by the DCC\nmechanisms (i.e., TPC, TRC, TDC, DSC and TAC).\nThe DCC access blocks act as follows. Once the measure-\nments on channel load are made available by the Channel\nProbing block, the Control Loop sets the channel state to\nRelaxed if the channel load is below a minimum threshold\n(NDL minChannelLoad) specified by the DCC management\nlayer, to Restricted if it is above a maximum threshold\n(NDL maxChannelLoad), and to Active otherwise. While in\nRelaxed state, the reference transmit power level is set to the\nmaximum value while the reference transmit interval, data rate\nand CCA sensitivity are set to their minimum. On the contrary,\nwhen in Restricted state, the reference transmit power level\nis set to the minimum, while the reference transmit interval,\ndata rate and CCA sensitivity to their maximum. Indeed, the\nhigher the CCA sensitivity value that is used, the higher\nthe probability to detect the medium as idle and then that a\npacket is transmitted. When instead the channel is congested,\na lower CCA sensitivity makes an ITS node to refrain from\ntransmitting. Similarly, a higher data rate is advisable under\nhigh load, as it leads to a shorter packet air time.\nUpon the arrival of a data packet at the MAC layer, the\nTransmit Queueing block reads the values of access priority,\nCCA threshold, transmit power and data rate, which have been\npreset in the packet header at the network layer. Based on\nthe access priority value, this block inserts the packet in the\nappropriate transmit queue at the MAC layer – an event that\nshould not occur at a rate greater than the reference value. The\nTRC mechanism is in charge of enforcing such requirement.\nSpecifically, TRC delays a packet if necessary, and it may drop\na packet if its transmission duration exceeds the maximum\nvalue corresponding to its access priority queue. Note that the\nlatter is estimated thanks to the Transmit Statistics block.\nOnce a packet has been enqueued, the other values that have\nbeen preset at the network layer are compared by the DSC,\nthe TPC and the TDC mechanisms against, respectively, the\nreference CCA sensitivity, the reference transmit power and\nthe reference data rate. The preset values are lowered if they\nresult to be above the reference values.\nFinally, the TAC mechanism has been introduced with the\naim of enforcing fairness among the transmit queues within\na node as well as among different ITS nodes. In particular,\nthe TAC defines the number of transmit queues that can be\nimplemented. Furthermore, if the Transmit Statistics indicate\nthat too many packets with a given priority index have been\ntransmitted, the TAC “closes” the corresponding queue, which\nthus cannot send any more packets till it is reopened. In our\nwork, we do not activate the TAC mechanism as it cannot be\nsupported through the current standard MAC architecture.\nIII. SIMULATION SCENARIOS\nIn order to evaluate the DCC performance, we developed a\nnew module in ns-2, which cooperates with 802.11 PHY and\nMAC modules. The implementation is fully compliant to the\nETSI standard [1].\n3TABLE I\nSIMULATION PARAMETERS\nLayer Parameter Value\nPHY Frequency/Channel bandwidth 5.9 GHz/10 MHz\nPropagation Nakagami (m=3)\nPower monitor threshold -102 dBm\nNoise floor -99 dBm\nMAC Slot time 13 µs\nSIFS 32 µs\nDIFS 58 µs\nHeader length 40 µs\naCWmin 15\nTABLE II\nDCC PARAMETERS\nParameter Min Default Max\nNDL defDCCSensitivity - -85 dBm -\nNDL ChannelLoad 0.2 - 0.5\nNDL refTxPower -10 dBm 23 dBm 33 dBm\nNDL refPacketInterval 0.04 s 0.5 sec 2.0 sec\nNDL refDataRate 6 Mb/s 12 Mb/s 24 Mb/s\nNDL refCarrierSense -95 dBm -85 dBm -65 dBm\nAs a reference topology, we use a 6×6 double-lane road\ngrid in a 750×750 m2-wide area. Roads are 150 m apart from\neach other, and vehicles are uniformly placed on the grid.\nSince we focus on the dynamics of the MAC protocol, we\nconsider a snapshot of the topology where vehicles do not\nmove; in this way, we can control exactly the number of\nneighbours as well as of interferers for each node. Vehicles are\nassumed to operate on the IEEE 802.11p SCH and to broadcast\na packet each every 100 ms. Packets are tagged as belonging\nto the same access category at MAC Layer. The offered traffic\nload is a varying system parameter in our simulations, and it\nis varied by changing the data packet size.\nIn the environment outlined above, we consider two sce-\nnarios: in the former 600 vehicles are placed in the grid in\nLine-of-Sight (LOS) condition, whereas in the latter the signal\nattenuation due to buildings is taken into account through\nthe Realistic Urban Grid (RUG) propagation model [6]. We\nwill refer to such scenarios as LOS and Urban, respectively.\nResults obtained with a different number of vehicles have\nbeen obtained but they are omitted since they exhibited a\nsimilar behaviour. The main PHY and MAC-layer simulation\nparameters are set according to standard specifications and are\nsummarised in Table I. Also, all the DCC parameters are set\nin accordance to the ETSI specifications [1], as reported Table\nII.\nIV. PERFORMANCE EVALUATION\nA. LOS Scenario\nIn the following, we show the impact that, separately, each\nof the DCC mechanisms has on the system performance, as\nwell their joint effect when they are all triggered at the same\ntime, both in the LOS and in the Urban scenarios. The legacy\n802.11p CSMA/CA (labelled as “no DCC” in the plots and\ntables) is used as a benchmarking solution.\nFig. 1 shows the PDR as a function of the distance between\ncommunicating vehicles, and as the data packet size varies\nbetween 100 and 600 bytes. Recall that the packet generation\nrate is constant, thus a larger packet size implies a higher\noffered load; namely, as the packet size grows from 100 to\n600 bytes, the offered load varies from 24% to 72%.\nLooking at the plots, we can see that, as expectedx, the\nPDR decreases with the distance due to the greater attenuation,\nand it decreases as the packet size, i.e., the traffic load,\nincreases. What it is surprising is the fact that, when separately\nactivated, the single DCC mechanisms either bring a minor\nbenefit in case of small packet size (TRC for 100 bytes and\nDSC for 200 bytes), no significative changes (e.g., TDC),\nor even a performance degradation (TPC for low-medium\npacket size, TRC for large packet size, and TDC). Overall,\nthe joint effect of such mechanisms, depicted in Fig. 1(a), is\na slight improvement and a small degradation, respectively,\nfor short and long packets, with respect to the legacy 802.11p\nCSMA/CA scheme.\nTable IV shows the packet access delay as a function of\nthe packet size. The access delay is computed from the packet\ngeneration time instant at application layer, to its transmission\non the channel. The delay increases with the packet size, as the\nincrease in offered load leads to a higher collision probability.\nHowever, we notice a dramatic performance degradation when\nthe full DCC scheme is enabled: the access delay increases\nfrom 504µs under the legacy 802.11p CSMA/CA to 3.05 ms\nunder the DCC. As shown by the values reported in the table,\nthis behavior is mainly due to the TRC, which acts as a\nleaky bucket mechanism: it delays packets so as to maintain\nthe reference packet interval. The other mechanisms, instead,\nslightly decrease the access delay. Indeed, the TPC reduces\nthe power level as the channel load increases, thus making the\nchannel appear as idle more often. As noted above, however, a\nlower power level degrades the PDR performance. The TDC\nincreases the data rate, hence it reduces the packet air time\nand speeds up the transmission rate, although it leads to a\nhigher error rate. Finally, as the channel load increases, the\nDSC lets the vehicles sense the channel with a higher carrier\nsense threshold of -65 dBm; it follows that the access delay\ndecreases, at the cost of an increased collision probability\n(hence PDR).\nFig. 2 portrays the state taken by the nodes as a function\nof time, when either only the single mechanisms or the full\nDCC are activated. For brevity, only the results for 100 and\n600 bytes are shown. Looking at the plots when TPC only\nis implemented, we note that this mechanism operates even\nunder very low traffic load, as most of the nodes are in Active\nstate for a packet size of 100 bytes. For a larger packet size,\nalmost all nodes are in Restrictive state, suggesting that TPC\nalone is unable to keep the traffic congestion limited. A similar\nbehaviour can be observed in the case of TDC and DSC.\nTRC, instead, is active in most cases and, even with 600-\nbyte packet, it can reduce channel congestion, although at\nthe price of a greatly increased delay. When all mechanisms\nare jointly implemented (full DCC), essentially the scheme\nbehaves as TRC, which is therefore confirmed to be the\ndominant action. Another important observation, which holds\n4 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0  20  40  60  80  100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400\nPD\nR\nDistance [m]\npkt 100\npkt 200\npkt 400\npkt 600\n(a) no DCC\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0  20  40  60  80  100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400\nPD\nR\nDistance [m]\npkt 100\npkt 200\npkt 400\npkt 600\n(b) TPC\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0  20  40  60  80  100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400\nPD\nR\nDistance [m]\npkt 100\npkt 200\npkt 400\npkt 600\n(c) TRC\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0  20  40  60  80  100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400\nPD\nR\nDistance [m]\npkt 100\npkt 200\npkt 400\npkt 600\n(d) TDC\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0  20  40  60  80  100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400\nPD\nR\nDistance [m]\npkt 100\npkt 200\npkt 400\npkt 600\n(e) DSC\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0  20  40  60  80  100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400\nPD\nR\nDistance [m]\npkt 100\npkt 200\npkt 400\npkt 600\n(f) full DCC\nFig. 1. PDR in LOS scenario as function of distance.\nfor all mechanisms, is that the node state exhibits an oscillatory\nbehaviour, e.g., all nodes pass from active to relax and vice\nversa at the same time. This suggests that when the channel\nload, e.g., decreases, the DCC mechanism at all nodes reacts\nleading to an increased number, or duration, of transmissions.\nThen, upon observing a higher load, all nodes become less\naggressive and the load decreases again. Such a behaviour\nis of scarce use and contributes to determining the poor\nperformance of DCC.\nB. Urban Scenario\nThe PDR versus the receiver distance in the Urban scenario\nis depicted in Fig. 3, for different values of the packet size. The\ncomparison between the case where DCC is not implemented\nand the full DCC highlights that the same performance is\nachieved in both cases. This is due to the fact that in the Urban\nscenario the attenuation introduced by the buildings greatly\nlimits the signal propagation, thus reducing the number of\nvehicles interfering with each other. It follows that the channel\nload is always below the minimum threshold and the DCC is\nrarely active.\nThe only mechanism that is triggered for a significant\namount of time is the TRC when the packet size is 600 bytes,\nas confirmed by the values of access delay reported in Ta-\nble III. Interestingly, the delay obtained through DCC for 600-\nbyte packets is much greater than with the legacy 802.11p.\nThis suggests that TRC becomes active unnecessarily, i.e., it\ndelays packets at the MAC layer even if a higher traffic load\n5 0\n 100\n 200\n 300\n 400\n 500\n 600\n 700\n 0  10  20  30  40  50  60\nN\nod\nes\n [#\n]\nSimulation Time [s]\nRELAXED\nACTIVE\nRESTRICTIVE\n(a) TPC: Packet size 100 Byte.\n 0\n 100\n 200\n 300\n 400\n 500\n 600\n 700\n 0  10  20  30  40  50  60\nN\nod\nes\n [#\n]\nSimulation Time [s]\nRELAXED\nACTIVE\nRESTRICTIVE\n(b) TPC: Packet size 600 Byte.\n 0\n 100\n 200\n 300\n 400\n 500\n 600\n 700\n 0  10  20  30  40  50  60\nN\nod\nes\n [#\n]\nSimulation Time [s]\nRELAXED\nACTIVE\nRESTRICTIVE\n(c) TRC: Packet size 100 Byte.\n 0\n 100\n 200\n 300\n 400\n 500\n 600\n 700\n 0  10  20  30  40  50  60\nN\nod\nes\n [#\n]\nSimulation Time [s]\nRELAXED\nACTIVE\nRESTRICTIVE\n(d) TRC: Packet size 600 Byte.\n 0\n 100\n 200\n 300\n 400\n 500\n 600\n 700\n 0  10  20  30  40  50  60\nN\nod\nes\n [#\n]\nSimulation Time [s]\nRELAXED\nACTIVE\nRESTRICTIVE\n(e) TDC: Packet size 100 Byte.\n 0\n 100\n 200\n 300\n 400\n 500\n 600\n 700\n 0  10  20  30  40  50  60\nN\nod\nes\n [#\n]\nSimulation Time [s]\nRELAXED\nACTIVE\nRESTRICTIVE\n(f) TDC: Packet size 600 Byte.\n 0\n 100\n 200\n 300\n 400\n 500\n 600\n 700\n 0  10  20  30  40  50  60\nN\nod\nes\n [#\n]\nSimulation Time [s]\nRELAXED\nACTIVE\nRESTRICTIVE\n(g) DSC: Packet size 100 Byte.\n 0\n 100\n 200\n 300\n 400\n 500\n 600\n 700\n 0  10  20  30  40  50  60\nN\nod\nes\n [#\n]\nSimulation Time [s]\nRELAXED\nACTIVE\nRESTRICTIVE\n(h) DSC: Packet size 600 Byte.\n 0\n 100\n 200\n 300\n 400\n 500\n 600\n 700\n 0  10  20  30  40  50  60\nN\nod\nes\n [#\n]\nSimulation Time [s]\nRELAXED\nACTIVE\nRESTRICTIVE\n(i) full DCC: Packet size 100 Byte.\n 0\n 100\n 200\n 300\n 400\n 500\n 600\n 700\n 0  10  20  30  40  50  60\nN\nod\nes\n [#\n]\nSimulation Time [s]\nRELAXED\nACTIVE\nRESTRICTIVE\n(j) full DCC: Packet size 600 Byte.\nFig. 2. Histogram of the operational states with the TPC (a,b), TRC (c,d), TDC (e,f), DSC (g,h) mechanisms and full DCC (i,j), as function of time, in the\nLOS scenario.\ncould be supported on the channel.\nFig. 4 further clarifies the system behaviour when the packet\nsize is set to 600 bytes. The plots present the state taken by the\nTPC and the TRC as a function of the vehicle distance from the\nnearest intersection (as farther than that the signal propagation\nis blocked by the presence of buildings). The histograms for\nthe TDC, the DSC and the full DCC have been omitted, as\nthe first two mechanisms exhibit the same behaviour as in the\ncase of TPC, while the plot for the full DCC is the same as for\nthe TRC. The results clearly highlight that most of the time\nvehicles are in Active state with TRC, while they operate in\nRelaxed state with the other mechanism, hence, as remarked\nabove, the DCC performance is determined by the TRC only.\nV. CONCLUSIONS\nWe investigated through extensive simulations the perfor-\nmance of the DCC scheme, as currently specified by ETSI.\nIn particular, we studied the impact of each of the DCC\nmechanisms on the system performance, as well their joint\neffect when they are all triggered at the same time. The\nanalysis has been carried out in a vehicular scenario where\nnodes are in radio visibility as well as in a urban scenario\naccounting for the presence of buildings. In both cases, our\nresults highlight that DCC has little effect. Furthermore, the\nDCC behaviour is mainly determined by the transmission\nrate control mechanism, which may even degrade the system\nperformance with respect to the case where the legacy 802.11p\nMAC protocol is implemented. Our future works will certainly\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0  20  40  60  80  100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400\nPD\nR\nDistance [m]\npkt 100\npkt 200\npkt 400\npkt 600\n(a) No DCC\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0  20  40  60  80  100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400\nPD\nR\nDistance [m]\npkt 100\npkt 200\npkt 400\npkt 600\n(b) full DCC\nFig. 3. PDR in Urban scenario as a function of distance.\n 0\n 20\n 40\n 60\n 80\n 100\n 120\n 140\n10 20 30 40 50 60 70 80\nN\nod\nes\n [#\n]\nDistance from corner [m]\nRELAXED\nACTIVE\nRESTRICTIVE\n(a) TPC\n 0\n 20\n 40\n 60\n 80\n 100\n 120\n 140\n10 20 30 40 50 60 70 80\nN\nod\nes\n [#\n]\nDistance from corner [m]\nRELAXED\nACTIVE\nRESTRICTIVE\n(b) TRC\nFig. 4. Histogram of the operational states with TPC and TRC as functions\nof the minimum distance from the nearest intersection, in the Urban scenario\nand with packet size of 600 bytes.\ninvestigate the upcoming DCC solution but also variations in\nthe settings of current DCC, slightly different algorithms [7]\nand different non-local channel measurements as proposed in\n[4].\n6TABLE III\nPACKET ACCESS DELAY IN THE URBAN SCENARIO\nPacket Size [byte] No DCC [s] TPC [s] TRC [s] TDC [s] DSC [s] Full DCC [s]\n100 0.000054 0.000054 0.000121 0.000054 0.000059 0.000126\n200 0.000070 0.000070 0.000137 0.000070 0.000078 0.000145\n400 0.000110 0.000110 0.001880 0.000110 0.000128 0.001790\n600 0.000168 0.000167 7.21 0.000168 0.000186 7.85\nTABLE IV\nPACKET ACCESS DELAY IN THE LOS SCENARIO\nPacket Size [byte] No DCC [s] TPC [s] TRC [s] TDC [s] DSC [s] Full DCC [s]\n100 0.000504 0.000482 15.31 0.000504 0.000594 14.98\n200 0.000809 0.000508 19.14 0.000769 0.000783 15.55\n400 0.001510 0.000404 19.99 0.001090 0.000850 20.57\n600 0.003050 0.000427 19.97 0.001440 0.001070 20.50\nREFERENCES\n[1] ETSI TS 102 687, 2011, Intelligent Transport Systems (ITS); Decentral-\nized Congestion Control Mechanisms for Intelligent Transport Systems\noperating in the 5 GHz range; Access layer part.\n[2] DRAFT ETSI TS 102 636, 2013, Intelligent Transportation Systems\n(ITS); Vehicular Communications; Part 4: Geographical Addressing and\nForwarding for Point-to-Point and Point-to-Multipoint Communications;\nSub-part 2: Media-Dependent Functionalities for ITS-G5.\n[3] DRAFT ETSI TS 103 141, 2013, Intelligent Transport Systems; Facilities\nlayer; Communication congestion control.\n[4] S. B. J. Kenney and V. Rai, “Comparing Communication Performance of\nDSRC OBEs from Multiple Suppliers.” in ITS World Congress, 2012.\n[5] S. Subramanian, M. Werner, S. Liu, J. Jose, R. Lupoaie, and X. Wu,\n“Congestion control for vehicular safety: synchronous and asynchronous\nMAC algorithms,” in ACM International Workshop on Vehicular inter-\nnetworking, systems, and applications, 2012, pp. 63–72.\n[6] H. Cozzetti, C. Campolo, R. Scopigno, and A. Molinaro, “Urban VANETs\nand Hidden Terminals: Evaluation through a Realistic Urban Grid propa-\ngation model,” in IEEE International Conference on Vehicular Electronics\nand Safety, Jul., pp. 93–98.\n[7] J. H. B. Kloiber and T. Strang, “Dice the TX power - Improving\nawareness quality in VANETs by random transmit power selection.” in\nIEEE Vehicular Networking Conference, 2012.\n",
            "id": 6814509,
            "identifiers": [
                {
                    "identifier": "16467317",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:iris.polito.it:11583/2514322",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "457976653",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1109/glocomw.2013.6825176",
                    "type": "DOI"
                },
                {
                    "identifier": "234897804",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:porto.polito.it:2514322",
                    "type": "OAI_ID"
                }
            ],
            "title": "Investigating the Effectiveness of Decentralized Congestion Control in Vehicular Networks",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:porto.polito.it:2514322",
                "oai:iris.polito.it:11583/2514322"
            ],
            "publishedDate": "2013-01-01T00:00:00",
            "publisher": "IEEE",
            "pubmedId": null,
            "references": [
                {
                    "id": 6454727,
                    "title": "Comparing Communication Performance of DSRC OBEs from Multiple Suppliers.” in ITS World Congress,",
                    "authors": [],
                    "date": "2012",
                    "doi": null,
                    "raw": "S. B. J. Kenney and V. Rai, “Comparing Communication Performance of DSRC OBEs from Multiple Suppliers.” in ITS World Congress, 2012.",
                    "cites": null
                },
                {
                    "id": 6454731,
                    "title": "Congestion control for vehicular safety: synchronous and asynchronous MAC algorithms,”",
                    "authors": [],
                    "date": "2012",
                    "doi": "10.1145/2307888.2307900",
                    "raw": "S. Subramanian, M. Werner, S. Liu, J. Jose, R. Lupoaie, and X. Wu, “Congestion control for vehicular safety: synchronous and asynchronous MAC algorithms,” in ACM International Workshop on Vehicular internetworking, systems, and applications, 2012, pp. 63–72.",
                    "cites": null
                },
                {
                    "id": 6454735,
                    "title": "Dice the TX power - Improving awareness quality in VANETs by random transmit power selection.”",
                    "authors": [],
                    "date": "2012",
                    "doi": "10.1109/vnc.2012.6407445",
                    "raw": "J. H. B. Kloiber and T. Strang, “Dice the TX power - Improving awareness quality in VANETs by random transmit power selection.” in IEEE Vehicular Networking Conference, 2012.",
                    "cites": null
                },
                {
                    "id": 6454723,
                    "title": "Intelligent Transport Systems (ITS); Decentralized Congestion Control Mechanisms for Intelligent Transport Systems operating in the 5 GHz range; Access layer part.",
                    "authors": [],
                    "date": "2011",
                    "doi": null,
                    "raw": "ETSI TS 102 687, 2011, Intelligent Transport Systems (ITS); Decentralized Congestion Control Mechanisms for Intelligent Transport Systems operating in the 5 GHz range; Access layer part.",
                    "cites": null
                },
                {
                    "id": 6454724,
                    "title": "TS 102 636, 2013, Intelligent Transportation Systems (ITS); Vehicular Communications; Part 4: Geographical Addressing and Forwarding for Point-to-Point and Point-to-Multipoint Communications; Sub-part 2: Media-Dependent Functionalities for ITS-G5.",
                    "authors": [],
                    "date": null,
                    "doi": null,
                    "raw": "DRAFT ETSI TS 102 636, 2013, Intelligent Transportation Systems (ITS); Vehicular Communications; Part 4: Geographical Addressing and Forwarding for Point-to-Point and Point-to-Multipoint Communications; Sub-part 2: Media-Dependent Functionalities for ITS-G5.",
                    "cites": null
                },
                {
                    "id": 6454725,
                    "title": "TS 103 141, 2013, Intelligent Transport Systems; Facilities layer; Communication congestion control.",
                    "authors": [],
                    "date": null,
                    "doi": null,
                    "raw": "DRAFT ETSI TS 103 141, 2013, Intelligent Transport Systems; Facilities layer; Communication congestion control.",
                    "cites": null
                },
                {
                    "id": 6454733,
                    "title": "Urban VANETs and Hidden Terminals: Evaluation through a Realistic Urban Grid propagation model,”",
                    "authors": [],
                    "date": null,
                    "doi": "10.1109/icves.2012.6294332",
                    "raw": "H. Cozzetti, C. Campolo, R. Scopigno, and A. Molinaro, “Urban VANETs and Hidden Terminals: Evaluation through a Realistic Urban Grid propagation model,” in IEEE International Conference on Vehicular Electronics and Safety, Jul., pp. 93–98.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [],
            "updatedDate": "2021-07-22T16:59:30",
            "yearPublished": 2013,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/pdf/16467317.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/16467317"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/16467317/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/16467317/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/6814509"
                }
            ]
        },
        {
            "acceptedDate": "2012-09-18T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "C. Carmona"
                },
                {
                    "name": "J. Jovanović"
                },
                {
                    "name": "N. Henze"
                },
                {
                    "name": "P. Brusilovsky"
                },
                {
                    "name": "P. Brusilovsky"
                },
                {
                    "name": "T. Apted"
                }
            ],
            "contributors": [
                "Andrew",
                "David",
                "The Pennsylvania State University CiteSeerX Archives",
                "Sergey"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/71491901",
                "https://api.core.ac.uk/v3/outputs/73889266",
                "https://api.core.ac.uk/v3/outputs/16509396",
                "https://api.core.ac.uk/v3/outputs/186973449"
            ],
            "createdDate": "2013-09-23T20:00:12",
            "dataProviders": [
                {
                    "id": 145,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/145",
                    "logo": "https://api.core.ac.uk/data-providers/145/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 1254,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/1254",
                    "logo": "https://api.core.ac.uk/data-providers/1254/logo"
                },
                {
                    "id": 2407,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/2407",
                    "logo": "https://api.core.ac.uk/data-providers/2407/logo"
                },
                {
                    "id": 457,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/457",
                    "logo": "https://api.core.ac.uk/data-providers/457/logo"
                }
            ],
            "depositedDate": "2012-01-01T00:00:00",
            "abstract": "Abstract. Teacher and students can use WWW as a limitless source of learning material for nearly any subject. Yet, such abundance of content comes with the problem of finding the right piece at the right time. Conventional adaptive educational systems cannot support personalized access to open-corpus learning material as they rely on manually constructed content models. This paper presents an approach to this problem that does not require intervention from a human expert. The approach has been implemented in an adaptive system that recommends students supplementary reading material and adaptively annotates it. The results of the evaluation experiment have demonstrated several significant effects of using the system on students ’ learning",
            "documentType": "research",
            "doi": "10.1007/978-3-642-33263-0_38",
            "downloadUrl": "https://core.ac.uk/download/16509396.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "Adaptation “in the Wild”: Ontology-based \nPersonalization of Open-Corpus Learning Material \nSergey Sosnovsky1, I-Han Hsiao2, Peter Brusilovsky2 \n1Center for e-Learning Technology, DFKI, \nCampus D3.2, D-66123 Saarbrücken, Germany \nsosnovsky@gmail.com \n2University of Pittsburgh, School of Information Sciences, \n135, N. Bellefield ave., Pittsburgh, PA, 15260, USA \n{peterb, hyl12}@pitt.edu \nAbstract. Teacher and students can use WWW as a limitless source of learning \nmaterial for nearly any subject. Yet, such abundance of content comes with the \nproblem of finding the right piece at the right time. Conventional adaptive \neducational systems cannot support personalized access to open-corpus learning \nmaterial as they rely on manually constructed content models. This paper \npresents an approach to this problem that does not require intervention from a \nhuman expert. The approach has been implemented in an adaptive system that \nrecommends students supplementary reading material and adaptively annotates \nit. The results of the evaluation experiment have demonstrated several \nsignificant effects of using the system on students’ learning. \nKeywords. Open-Corpus Personalization, Adaptive Educational System \n1   Introduction \nFrom the educational perspective, the WWW can be viewed as a very large collection \nof learning material. For many subjects, one can find online tutorials, textbooks, \nexamples, problems, lectures slides, etc. Nowadays, teachers often do not have to \ncreate most of course materials themselves, instead they can reuse the best content \navailable online. For example, a teacher developing a course on Java programming \nmight decide to use a web-based Java tutorial, an electronic version of the course \nbook, an existing Web-based assessment system, and online code examples. \nAlthough, all these resources are useful, students might get lost in this large volume \nof content without additional guidance. Organizing adaptive access to the course \nmaterials would help solving the problem. Appropriate tutorial pages can be \nrecommended to students based on their progress; an adaptive navigation technique \ncan be implemented to facilitate the choice of the most relevant example; an \nintelligent tutoring system can adaptively sequence learning problems. A teacher \nmight be able to find a system implementing one of these technologies and providing \nadaptive access to one of the collections of learning material. However this system \nwill not be aware of the rest of the available content, unless it supports Open-Corpus \nPersonalization (OCP). \nOCP is one of the classic problems of adaptive information systems, in general, \nand adaptive educational systems (AES), in particular. Many research projects tried to \npropose a solution for it with different degrees of completion (e.g. [1], [2], [3], [4]). \nBrusilovsky and Henze in [5] presented a comprehensive overview of the problem \nand draw the evolution of research addressing it. This paper focuses on the OCP \nbased on semantic content models, as the dominant personalization approaches in the \nfield of e-Learning rely on representation of student knowledge and learning activities \nin terms of domain semantics. Therefore automatic extraction of domain knowledge \nfrom Web-content becomes an important component of the problem we address here. \nWe propose a novel approach towards a fully automated OCP in the context of e-\nLearning. It is based on harvesting coarse-grained models from semi-structured digital \ncollections of open-corpus education material (such as tutorials and textbooks) and \nmapping them into the pre-defined domain ontology serving as the main domain \nmodel and the reference point for multiple open-corpus collections. Once the mapping \nis established, the content from the processed open-corpus collection can be presented \nto students in adaptive way, according to their student models computed in terms of \nthe central ontology. The rest of this paper describes the details of the approach, the \nadaptive e-learning service implementing it, and the results of the evaluation \nexperiment demonstrating several learning effects of the developed service. \n2.   Ontology-based OCP Approach \nInformation on the Web is not without structure. Authors of many online resources \ncreate them as a reflection of their own internal organization of related knowledge. \nThey encode this organization by formatting the text with lists and headings, breaking \ndocuments into sections and pages, linking pages together, creating tables of contents, \netc. The approach proposed in this paper attempts to utilize this hidden semantic layer \nof well-formatted content collections to achieve fully automated OCP. The entire \nprocedure consists of the three steps presented below. \nStep 1: Modeling of Open-Corpus Content in Terms of its Structure. An author \ncreating an instructional resource tries to make it more readable and understandable \nby structuring it into chapters and sections. Every section is intended to represent a \ncoherent topic. It is given a title conveying the meaning of the topic and contains the \ntext explaining it. Their main purpose is to structure content, but they inescapably \nstructure the knowledge, as well. A topic-based structure of such a resource can be \nparsed automatically and represented formally, e.g. as an RDF model. This model will \nhave some drawbacks: (1) subjectivity; (2) poor granularity; (3) undefined semantics \nof topics and relations between them; (4) incompleteness. Yet, such model provides \nmeans to access the material of the collection in terms of topics, reason about the \nmaterial in terms of topics and adapt the material in terms of topics. \nStep 2: Mapping Extracted Model into the Central Domain Ontology. Extraction \nof the hidden semantic layer is not enough for two reasons. First, coarse-grained \ndomain and content models can be effective when delivering the adaptation to \nstudents, but cannot maintain student modeling of good quality [6]. Second, a model \nextracted from a single collection can be used to adaptively present only the content \nof this collection. The learning material from different collections will be isolated. \nThe solution is to use the central domain ontology as a reference model. It will help to \nmodel students’ knowledge and to translate between the topic-based structures of \nindividual open-corpus collections. The connection between the harvested models and \nthe central ontology is established based on the automatic mapping of these models. \nStep 3: Mediated Personalization of Open-Corpus Learning Material. Once the \ntwo models are mapped the systems can reason across them. The mapping bridge \nbetween the central ontology and the tutorial model enables two principle procedures: \n(1) tracing student’s actions with the tutorial’s topics, representing these actions in \nterms of the ontology concepts and updating the ontology-based student model; (2) \nrequesting the current state of student model expressed in terms of ontology concepts, \ntranslating it into the open-corpus topics, and adapting students’ access to the open-\ncorpus material. Fig. 1 summarizes the principle relation between the components of \nthe central ontology and the open-corpus material, as well as the information flow \nacross these relations. \n \nFig. 1. Meditated personalization of open-corpus learning material \n3.   The Ontology-Based Open-Corpus Personalization Service \nThe proposed approach has been implemented in the Ontology-based Open-corpus \nPersonalization Service (OOPS). It has been developed as a value-added service used \nin parallel with a central exercise system and augmenting it with adaptive access to \nsupplementary reading material. As a central system we used QuizJET – the system \nserving parameterized exercises for Java programming language [7]. Both OOPS and \nQuizJET have been integrated with the CUMULATE user modeling server [8]. As an \nopen-corpus collection of instructional material we used the electronic version of an \nintroductory Java textbook. QuizJET is responsible for objective assessment of \nstudents’ knowledge. Its exercises are indexed in terms of the central ontology and it \nreport students’ activity to the central user modeling component – CUMULATE, \nwhich models students’ knowledge in terms of the central domain ontology and \nreports it to OOPS. As a result, student practicing with QuizJET exercises and \nstruggling with a difficult topic receives recommendations of relevant open-corpus \nreading material from OOPS. \nThe student interface of OOPS has two interaction phases: recommendation (when \na student is presented with a list of recommended pages) and reading (when a student \nis studying recommended material). Left part of Fig. 2 presents a screenshot of the \nrecommendation phase. Area “B” is the interface of the central system – QuizJET. \nArea “A” presents a list of recommendations produced by OOPS for the current \nexercise of QuizJET. Every item in the list is a topic label from the harvested open-\ncorpus content collection. The order of an item in the list is determined by its \nrelevance to the current QuizJET exercise computed based on the aggregated \nsimilarity of the topic and the concepts indexing the exercise. The similarity values \nare calculated by the ontology mapping algorithm. The recommended items are \nprovided with adaptive annotation in form of human-shaped colored icons. The \ncoloring of an icon annotating a topic represents the amount of knowledge a student \nhas demonstrated for the learning material behind this topic measured in terms of \ncentral ontology concepts mapped to the topic and provided by the central student \nmodel. The annotation level is computed as a weighted aggregate of knowledge levels \nfor all concepts mapped into the topic. Once a student decides to accept a \nrecommendation by clicking on a topic link, s/he goes into the reading phase of the \nOOPS interface (right part of Fig. 2). In this phase, OOPS provides a student with an \nopportunity to read the actual material behind the topic link. OOPS widget expands, \nand its interface changes. The expanded interface contains three main areas. Area “A” \nis the content area, where the content of the selected recommendation is presented. \nArea “B” is the navigation area, where the links to the previous and the next topics are \npresented, should the student choose browsing the structure of the open-corpus \ncollection. Area “C” contains two buttons that allow the student to exit the reading \nphase and to report whether s/he has found the recommendation useful for the current \nlearning task or not. Once the student leaves the reading phase, OOPS interface \nswitches to the recommendation phase again. \n \n \nFig. 2. Interface: Left: recommendation phase; Right: reading phase \n4. The Evaluation \nThis section presents the results and the procedure of the OOPS service evaluation. It \nwas organized as a controlled balanced experiment comparing the developed system \nagainst two control conditions. The experimental system (open-corpus) provided \nstudents solving QuizJET exercises with open-corpus recommendation of reading \nmaterial. Another version of the system (closed-corpus) had the identical interface \nand generated recommendations from the same pool of reading material, but used \ntraditional closed-corpus adaptation approach based on the manual indexing of \nrecommended pages. The last configuration of the system (textbook) did not \nrecommend any reading material. Instead, students using this version had a hard copy \nof the textbook, which was the source of reading material for the first two versions. \nThe experiment consisted of two sessions corresponding to two sets of introductory \nJava topics. First set included simpler topics: from basics of variable and object \nhandling to conditional statement and Boolean expressions. The second set covered \nmore advanced topics: from loops to arrays and ArrayLists. Each session started with \na pretest, continued with the 30 minutes work with the system and ended with the \nposttest. Forty subjects with limited Java programming experience participated in the \nexperiments. Subjects were randomly assigned to one of the four groups: \nA. Session 1easy topics – open-corpus; Session 2complex topics – closed-corpus; \nB. Session 1easy topics – open-corpus; Session 2complex topics – textbook; \nC. Session 1easy topics – closed-corpus; Session 2complex topics – open-corpus; \nD. Session 1easy topics – textbook; Session 2complex topics – open-corpus; \nGeneral Learning Effect. In order to verify that work with the system actually leads \nto learning, pair-wise comparisons of scores on the pre-test and the post-test have \nbeen made (Table 1). Significant learning has been registered for all groups and \nconditions during Session 1. For Session 2, the open-corpus condition and the closed-\ncorpus condition resulted in significant (or bordering on significance) learning. At the \nsame time, the textbook-condition led to no learning. \nTable 1. General learning effect statistics (Scorepost-test VS. Scorepre-test ) \nGroup Session 1 Session 2 \nt(9) p-value t(9) p-value \nA 3.787 0.004 1.941 0.084 \nB 4.409 0.002 0.0 1.0 \nC 8.213 <0.001 2.250 0.051 \nD 4.077 0.03 3.361 0.008 \nEffect on Learning Complex Material. The main difference between Session 1 and \nSession 2 is material complexity. The analysis of the general learning effect suggests \nthat the recommendation of supplementary reading material can have a positive \ninfluence on learning the complex learning material. During Session 1 (easy topics), \nnone of the comparisons resulted in significant difference in Knowledge Gain. \nHowever, once the learning material became more complex (Session 2), the open-\ncorpus system significantly outperformed the textbook: Knowledge Gain for the open-\ncorpus condition (M=1.55; SD=1.23) is significantly higher than Knowledge Gain for \nthe textbook condition (M=0.60; SD=0.97): t(28) = 2.124; p = 0.043 during Session 2 \n(complex topics). At the same time, no difference was observed between the closed-\ncorpus and the open-corpus system when students were learning complex material. \nThis is an important effect with a reasonable explanation. When learning easy \nmaterial, students need less support from the system. They learn just by practicing \nwith QuizJET exercises. And if they need extra reading, it is easier for them to find a \nrelevant chapter in the textbook. On the other hand, when the material becomes \ncomplex, students can benefit from the recommendations and the adaptive annotations \nguiding them to the most important piece of reading material. Thus, personalized \nlearning support results in better learning when support is needed. The comparison of \nopen-corpus and closed-corpus conditions show that they are equally effective, which \nindicates that OCP produced by OOPS has similar quality to the traditional closed-\ncorpus personalization. \nEffect on Learning Conceptual Material. The personalization implemented by \nOOPS is aimed at achieving two instructional goals: (1) Support students solving self-\nassessment exercises by bringing them the most relevant reading material; (2) Balance \nstudents’ learning by giving them the opportunity to read instructional texts in \naddition to practicing. The second means that OOPS should contribute better to the \nknowledge of important concepts and fact in the domain. The pre- and post-tests of \nboth sessions contained two kinds of questions: those evaluating students’ practical \nskills in code understanding and manipulation and those checking their factual and \ntheoretical knowledge. In order to measure the conceptual knowledge gain, only the \nsecond kind of questions was taken into account. The comparison of conceptual \nknowledge gain between the open-corpus and the textbook conditions shows that the \nhypothesis is partially confirmed. During Session 1, the conceptual knowledge gain \nfor the open-corpus condition (M=2.61; SD=1.75) was higher, than for the textbook \n(M=1.75; SD=1.15), but not significantly: t(28)=1.762; p=0.089. However, during \nSession 2 (complex topics), the conceptual knowledge gain for the open-corpus \ncondition (M=0.73; SD=0.47) was significantly higher than the one for the textbook \ncondition (M=0.30; SD=0.42): t(28)=2.403; p=0.023. No significant effect was \nobserved when comparing the open-corpus and the closed-corpus conditions. \n5.   Conclusion \nIn this paper, we have addressed the problem of OCP in the context of e-Learning \nand proposed a solution for it. As a proof-of-concept the adaptive e-Learning service \nOOPS has been implemented. It adaptively recommends and annotates pages for \nsupplementary reading to students solving self-assessment exercises. The evaluation \nof OOPS has shown that students were able to achieve significant learning while \nusing the open-corpus version of the system. OOPS significantly improved students’ \nknowledge gain when they work with more challenging learning material. In \ncomparison, students using the textbook demonstrated no significant learning while \nworking with complex topics. OOPS helped to maintain a more balanced learning by \nsignificantly improving gain in conceptual knowledge, no such effect was observed \nfor the textbook. At the same time, on no tests, we could statistically distinguish \nbetween the results of the proposed fully-automated open-corpus approach and a \nconventional closed-corpus technique based on a carefully handcrafted content model. \nReferences \n1. Henze, N. and W. Nejdl, Adaptation in open corpus  hypermedia. Int. J. of Artificial \nIntelligence in Education, 2001. 12(4): p. 325-350. \n2. Carmona, C., et al., SIGUE: Making Web Courses Adaptive, in AH'2006, Springer Berlin / \nHeidelberg. p. 376-379. \n3. Jovanović, J., D. Gasevic, and V. Devedzic, Ontology-based automatic annotation of \nlearning content. Int. J. on Semantic Web and Information Systems, 2006. 2(2): p. 91-119. \n4. Apted, T. and J. Kay, MECUREO Ontology and Modelling Tools. Int. J. of Continuing \nEngineering Education and Lifelong Learning, 2004. 14(3): p. 191-211. \n5. Brusilovsky, P. and N. Henze, Open Corpus Adaptive Educational Hypermedia, in The \nAdaptive Web: Methods and Strategies of Web Personalization, 2007, Springer Verlag: \nHeidelberg, Germany. p. 671-696. \n6. Sosnovsky, S. and P. Brusilovsky, Layered Evaluation of Topic-Based Adaptation to \nStudent Knowledge, in Workshop on Evaluation of Adaptive Systems at UM'2005 \nEduburgh, UK. p. 47-56. \n7. Hsiao, S., P. Brusilovsky, and S. Sosnovsky, Web-based Parameterized Questions for \nObject-Oriented Programming, in E-Learn 2008, Las Vegas, NV, USA. p. 3728-3735. \n8. Brusilovsky, P., S. Sosnovsky, and O. Shcherbinina, User Modeling in a Distributed E-\nLearning Architecture, in UM'2005, Edinburgh, UK. p. 387-391. \n \n",
            "id": 6829637,
            "identifiers": [
                {
                    "identifier": "23542751",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "73889266",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "16509396",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1007/978-3-642-33263-0_38",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:d-scholarship.pitt.edu:19534",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "2710505",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "186973449",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:citeseerx.psu:10.1.1.386.4682",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "71491901",
                    "type": "CORE_ID"
                }
            ],
            "title": "Adaptation “in the Wild”: Ontology-Based Personalization of Open-Corpus Learning Material",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:d-scholarship.pitt.edu:19534",
                "oai:citeseerx.psu:10.1.1.386.4682"
            ],
            "publishedDate": "2012-01-01T00:00:00",
            "publisher": "'Springer Science and Business Media LLC'",
            "pubmedId": null,
            "references": [
                {
                    "id": 6675491,
                    "title": "Adaptation in open corpus hypermedia.",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1007/3-540-47952-x_19",
                    "raw": "Henze,  N.  and  W.  Nejdl,  Adaptation in open corpus  hypermedia.  Int.  J.  of  Artificial Intelligence in Education, 2001. 12(4): p. 325-350.",
                    "cites": null
                },
                {
                    "id": 6675496,
                    "title": "Layered Evaluation of Topic-Based Adaptation to Student Knowledge,",
                    "authors": [],
                    "date": null,
                    "doi": null,
                    "raw": "Sosnovsky,  S.  and  P.  Brusilovsky,  Layered  Evaluation  of  Topic-Based  Adaptation  to Student  Knowledge,  in  Workshop  on  Evaluation  of  Adaptive  Systems  at  UM'2005 Eduburgh, UK. p. 47-56.",
                    "cites": null
                },
                {
                    "id": 6675494,
                    "title": "MECUREO Ontology and Modelling Tools.",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1504/ijceell.2004.004969",
                    "raw": "Apted, T. and J. Kay, MECUREO Ontology and Modelling Tools. Int. J. of Continuing Engineering Education and Lifelong Learning, 2004. 14(3): p. 191-211.",
                    "cites": null
                },
                {
                    "id": 6675493,
                    "title": "Ontology-based automatic annotation of learning content.",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.4018/jswis.2006040103",
                    "raw": "Jovanović,  J.,  D.  Gasevic,  and  V.  Devedzic,  Ontology-based  automatic  annotation  of learning content. Int. J. on Semantic Web and Information Systems, 2006. 2(2): p. 91-119.",
                    "cites": null
                },
                {
                    "id": 6675495,
                    "title": "Open Corpus Adaptive Educational Hypermedia, in The Adaptive Web: Methods and Strategies of Web Personalization,",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1007/978-3-540-72079-9_22",
                    "raw": "Brusilovsky, P. and N. Henze, Open Corpus Adaptive Educational Hypermedia,  in  The Adaptive Web: Methods and Strategies of Web Personalization,  2007,  Springer  Verlag: Heidelberg, Germany. p. 671-696.",
                    "cites": null
                },
                {
                    "id": 6675492,
                    "title": "SIGUE: Making Web Courses Adaptive,",
                    "authors": [],
                    "date": null,
                    "doi": "10.1007/3-540-47952-x_41",
                    "raw": "Carmona, C., et al., SIGUE: Making Web Courses Adaptive, in AH'2006, Springer Berlin / Heidelberg. p. 376-379.",
                    "cites": null
                },
                {
                    "id": 6675498,
                    "title": "User Modeling in a Distributed ELearning Architecture,",
                    "authors": [],
                    "date": null,
                    "doi": "10.1007/11527886_50",
                    "raw": "Brusilovsky, P., S. Sosnovsky, and O. Shcherbinina, User Modeling in a Distributed ELearning Architecture, in UM'2005, Edinburgh, UK. p. 387-391.",
                    "cites": null
                },
                {
                    "id": 6675497,
                    "title": "Web-based Parameterized Questions for Object-Oriented Programming, in E-Learn",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1007/978-3-642-04636-0_10",
                    "raw": "Hsiao,  S.,  P.  Brusilovsky,  and  S.  Sosnovsky,  Web-based  Parameterized  Questions  for Object-Oriented Programming, in E-Learn 2008, Las Vegas, NV, USA. p. 3728-3735.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "http://d-scholarship.pitt.edu/19534/1/ectel2012_sosnovsky_final.pdf",
                "http://dx.doi.org/10.1007/978-3-642-33263-0_38",
                "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.386.4682"
            ],
            "updatedDate": "2022-02-28T03:49:45",
            "yearPublished": 2012,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "0302-9743"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/16509396.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/16509396"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/16509396/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/16509396/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/6829637"
                }
            ]
        },
        {
            "acceptedDate": "2010-02-17T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "LAU, Hoong Chuin"
                },
                {
                    "name": "Pan, Jie"
                },
                {
                    "name": "SONG, Huawei"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/190688638",
                "https://api.core.ac.uk/v3/outputs/13242125"
            ],
            "createdDate": "2013-08-19T16:52:07",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 517,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/517",
                    "logo": "https://api.core.ac.uk/data-providers/517/logo"
                }
            ],
            "depositedDate": "2010-07-01T00:00:00",
            "abstract": null,
            "documentType": "research",
            "doi": "10.1109/tase.2010.2040827",
            "downloadUrl": "https://core.ac.uk/download/13242125.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "Singapore Management University\nInstitutional Knowledge at Singapore Management University\nResearch Collection School of Information Systems\n(Open Access) School of Information Systems\n1-2010\nPeriodic Resource Reallocation in Two-Echelon\nRepairable Item Inventory Systems\nHoong Chuin LAU\nSingapore Management University, hclau@smu.edu.sg\nJie Pan\nHuawei Song\nFollow this and additional works at:http://ink.library.smu.edu.sg/sis_research\nPart of theDatabases and Information Systems Commons, and theGraphics and Human\nComputer Interfaces Commons\nThis Journal Article is brought to you for free and open access by the School of Information Systems at Institutional Knowledge at Singapore\nManagement University. It has been accepted for inclusion in Research Collection School of Information Systems (Open Access) by an authorized\nadministrator of Institutional Knowledge at Singapore Management University. For more information, please emaillibIR@smu.edu.sg.\nCitation\nLAU, Hoong Chuin; Pan, Jie; and Song, Huawei, \"Periodic Resource Reallocation in Two-Echelon Repairable Item Inventory\nSystems\" (2010).Research Collection School of Information Systems (Open Access).Paper 213.\nhttp://ink.library.smu.edu.sg/sis_research/213T-ASE-2009-070  \n \n1\n  \nAbstract—Given an existing stock allocation in an inventory \nsystem, it is often necessary to perform reallocation over multiple \ntime points to address inventory imbalance and maximize \navailability. In this paper, we focus on the situation where there \nare two opportunities to perform reallocation within a \nreplenishment cycle. We derive a mathematical model to \ndetermine when and how to perform reallocation. Furthermore, \nwe consider the extension of this model to the situation allowing \nan arbitrary number of reallocations. Experimental results show \nthat the two-reallocation approach achieves better performance \ncompared with the single-reallocation approach found in the \nliterature. We also illustrate how to apply the proposed model to \ndesign cost-optimal periodic resupply policies. \n \nIndex Terms—Two-Echelon Inventory, Periodic Resupply, \nReallocation, Repairable Item, Military Logistics \n \nI.  INTRODUCTION \nWe consider an arboreal inventory system in which a central \ndepot serves n bases. Military systems such as aircrafts and \ntanks are deployed at the bases. These systems break down \nbecause the underlying components, which are called LRU \n(line replaceable units), are either worn out over time and/or \ndamaged during usage. Stocks are allocated at the depot and \nbases to insure continuity of operations. When an LRU fails at \na base, a spare replaces it if one is available; otherwise a \nbackorder is incurred. In military practice, due to the limited \nspace at the bases, the failed LRUs are usually sent back to the \ndepot for repair. In the mean time, an order is placed by the \nbase to the depot to send a spare. A spare will be sent to the \nbase if one is available; otherwise there is a backorder at the \ndepot. After the failure is repaired, it will be sent to the depot \ninventory to fulfill future demands. \nAs demands are stochastic, inventory imbalance will occur \nand tends to grow with time. This imbalance ultimately \n \nManuscript received October 20, 2007.  \n This work was supported in part by the Singapore Ministry of Defense \nunder the JAGUAR research grant. \nHoong Chuin Lau is with School of Information Systems, Singapore \nManagement University, 80 Stamford Road, Singapore 178902 (phone: 65-\n6828-0229; e-mail: hclau@smu.edu.sg). \nJie Pan is with School of Information Systems, Singapore Management \nUniversity, 80 Stamford Road, Singapore 178902 (e-mail: \njiepan@smu.edu.sg). \nHuawei Song is with Decision Science and Engineering Systems, \nRensselaer Polytechnic Institute, 110 8\nth Street, Troy, NY 12180 USA (e-mail: \nsongh2@rpi.edu). \nreaches a situation where some bases hold excess inventories, \nwhile others face critical shortage. To correct the imbalance, \nstocks need to be reallocated. To increase the efficiency and \nreduce unavailability, we also allow excess inventories at \nsome bases to be reallocated laterally to others with shortage. \nIn this paper, we are concerned with a two-instant reallocation \nscheme within a system replenishment cycle. This work is in \nresponse to the open challenge post by Cao and Silver [2] to \nconsider two or more possible reallocations within a cycle. \nIn practice, this problem is also faced by planners who need \ndetermine the time between periodic reallocations. Although \nthe (S-1,S) replenishment policy is generally assumed in the \nliterature (i.e. depot will send a spare to the base once a failure \noccurs), this is a stylized situation since it is almost impossible \nto supply continuously in practice, especially in a naval \nenvironment. The depot has to send spares to offshore bases \nand bring the failures back for repair periodically. Hence, it is \nimportant to determine when and how to distribute stocks to \nthe bases periodically in the cycle. \nGiven that we have two reallocation instants, it is important \nto determine when each reallocation should occur. If we \nperform the first reallocation too early, it may prevent early \nbackorders but could lead to growth of backorders before the \nnext reallocation or during the remaining time in the cycle. \nConversely, performing reallocation later presents two \nproblems: First, it may cause high levels of early backorders. \nSecond, late reallocation may leave no time to perform the \nsecond reallocation. Furthermore, the time interval between \nthe first and the second reallocations is also important: if it is \ntoo short, failures brought back to depot for repair may not \nhave been completed and consequently the depot has too few \nspares to perform the second reallocation; if the time interval \nis too long, it causes higher levels of backorders at bases \nbetween reallocations. Therefore, the key issue is how best to \nsynchronize the two reallocations. \nMany papers have analyzed resource reallocation, periodic \nresupply and risk pooling effect. One example is the classic \npaper by Eppen and Schrage [4] which analyzed a multi-\nechelon inventory system considering external lead times and \nrandom demands, where the optimal allocation of stocks \namong multiple sites may not be feasible due to stock \nimbalance. In their model, the depot will order enough stocks \nfrom an outside supplier to ensure a certain level of system-\nwide inventory position, and then perform complete allocation \nof the received stock to the sites according to the demands \nduring the external lead time.  Jönsson and Silver [7] saw that \nPeriodic Resource Reallocation in Two-Echelon \nRepairable Item Inventory Systems \nHoong Chuin LAU, Jie PAN, Huawei SONG T-ASE-2009-070  \n \n2\nthis scheme has no reallocation possibility, and proposed a \nscheme that performs complete transshipment of all site \ninventories at a fixed instant, which is one period before the \nend of the order cycle – since their rationale is that stockouts \nprimarily occur during the last periods of an order cycle.   \nJackson and Muckstadt [6] also considered a single, \npredetermined reallocation time and derive both exact and \napproximate optimality conditions that do not ignore the \npossibility of imbalance at the time of reallocation. One \nlimitation of this work however is that, since they do not \npermit lateral resupply between sites, they encountered \ndifficulties in trying to ascertain how much stock to allocate to \neach site.  Another avenue of extension of the Eppen and \nSchrage [4] work, which also improves the situation given in \nJackson and Muckstadt [6], is found in Jackson [5], which \nallows the central warehouse to hold stock and make \nallocations to the retailers in every period of the cycle. The \nproposed allocation policy is a \"ship-up-to-S\" policy: the \nwarehouse makes shipments to restore the inventory position \nof each retailer to some predetermined value, S, in every \nperiod so long as the warehouse has sufficient stock.  The \nconcept of “pooled-risk period” was introduced, which refers \nto the latest period of allocation.  Tsao and Enkawa [13] \nproposed a “two-phase push control policy” for considering \nthe optimal reallocation instant in a two-echelon inventory \nsystem. Their method predetermines a fixed reallocation \ninstant in all replenishment cycles, independent of the \ndynamic behavior of the inventories at the retailers. This \nsituation was improved by Cao and Silver [2] recently, who \nproposed a heuristic method to dynamically determine the \noptimal reallocation instant in each replenishment cycle and \nperform reallocation at that instant. \nThe abovementioned papers deal mostly with consumable \nitems. In a military context, it is important to perform \nreallocations of spare parts periodically because they are often \nvery expensive and affect system availability greatly ([3], [9]). \nSystem availability is usually measured in terms of Expected \nBackorders (EBO) (e.g. [1], [8], [11]). To our knowledge, \nthere are few works on redistribution of spare parts in multi-\nechelon systems, except a brief mention of the problem in [11] \nand a feature within a proprietary commercial tool OPUS [10]. \nThis paper makes technical contributions in the following \nways. First and foremost, we consider how to perform more \nthan one reallocation within a replenishment cycle, and \ninstead of fixing reallocation instants to certain time points, \nwe propose how to determine the time point instants for \nreallocation. This is a response to the challenge post by Cao \nand Silver [2], which issued an open question for the problem \nof multiple reallocations within a cycle.  Furthermore,  we \nrelax the classical assumptions in the following manner. The \nreplenishment of stocks from the depot to bases is periodic, \ni.e. stocks and failures can be transported only at certain time \npoints in a batch. The internal lead (or transport) time between \nthe depot and bases is nonzero. Transshipments among bases \nare allowed. \nThe remainder of this paper is organized as follows. Section \n2 gives the problem definition, assumptions and notations. \nOur mathematical model and approach are then presented in \nSection 3. Section 4 presents extensive experimental results. \nSection 5 shows how our model can be extended to design \ncost-optimal periodic resupply policy. Finally, conclusions \nand future work are provided in Section 6. \n \nII.  PROBLEM DEFINITION \nOur resource reallocation problem is based on two \nimportant and simplifying assumptions. First we assume the \ninternal lead (or transport) times for moving items from the \ncentral depot to each base is not negligible, but the lead time \nlaterally between bases is negligible. In practice, the transport \ntime between echelons is more important to military planners \nsuch that it usually cannot be ignored while the assumption of \nnegligible lateral lead time is consistent with [2], [5], [13] \nwhich assume reallocation can be achieved instantly. Thus the \nspares should be transported from the depot ahead of internal \nlead time for the destination so that it can arrive on time for \nreallocation. Second, to simplify the problem, we assume all \nfailures are only repairable at the depot which has infinite \nrepair capacities. The repair time is exponentially distributed \nwith mean T. Because of transport time, repair at the depot can \nonly take place in lead time after reallocation. \nThe system replenishment cycle is H base periods, i.e. \nevery H base periods, the central depot places orders to an \noutsider supplier. Therefore our reallocation decision time \nhorizon is within this replenishment cycle. Demands over time \nat the bases are assumed to be independent, Poisson variables \nwith mean λi at base i during each period. \nThe main notations used in this paper are as follows. \ni: index of site (i = 0 for the depot) \nn: number of bases \nSi: initial stock level of LRU at site i \nH: system replenishment cycle, in base periods \nL: internal lead time, i.e. transport time between depot and \nbase \nT: mean repair time of LRU \nyi(t): (Poisson random variable) demands in a single period t \nat base i  \nλi: mean value of yi(t) \nτ: index of the period at the end of which reallocation takes \nplace \nIi(τ): stock level at site i instantly before reallocation at the \nend of period τ \nUi(τ): stock level at site i instantly after reallocation at the end \nof period τ \nt1: time point at which the first reallocation is performed \nt2: time point at which the second reallocation is performed \nEBOi(t): expected backorder at the end of period t at base i (If \nreallocation takes place at t, it denotes the EBO instantly \nbefore reallocation) \nEBO(t): expected sum of backorders over all bases at the end \nof period t T-ASE-2009-070  \n \n3\nTEi(t1, t2): total (i.e. aggregate) EBO at base i at time points t1, \nt2 and H \nTE (t1, t2): total EBO across all bases at time points t1, t2 and H. \nGiven the initial number of stocks at each site, we need to \ndecide the variables t1, t2 at which reallocation takes place, as \nwell as the number of stocks at each site after reallocation so \nas minimize the total EBO over all bases at three time points \n(at the end of period t1 and t2 just before the reallocations \nrespectively and at the end of the cycle). We like to clarify at \nthis stage that we use the term ‘reallocation’ to refer to three \nseparate reallocation activities of different types of inventories \nat different time points within a cycle: a) Reallocation of spare \nitems from the depot to bases at time points t1 – L and t2 – L; b) \nReallocation of spare items among bases at time points t1 and \nt2; and c) Sending failed items from bases to the depot at time \npoint t t1. \nNote that the value of the objective function depends on the \ntimes at which the reallocations are carried out and how the \nreallocations are done. Notationally therefore, our aim is to \nfind  t1,  t2,  Ui(t1),  Ui(t2) such that the function   \n) , ( ) , ( 2 1 1 2 1 t t TE t t TE\nn\ni i ∑ = =  is minimized, where \n) ( ) ( ) ( ) , ( 2 1 2 1 H EBO t EBO t EBO t t TE i i i i + + =       ( 1 )  \n \nIII.  MATHEMATICAL MODEL \nA.  Before the First Reallocation \nGiven the initial stock allocations at all sites, the expected \nbackorders over all bases at time t1, EBO(t1), instantly before \nreallocation can be calculated according to the standard \ndefinition of EBO as follows: \n∑∑ ∫\n==\n∞\n− = =\nn\ni\nn\ni\nS i i i i i\ni\ndx x f S x t EBO t EBO\n11\n1 1 ) ( ) ( ) ( ) (       ( 2 )  \nwhere  xi is a realization of random variable of \nDi(t1).\n1\n1 1 () ( )\nt\nii t Dt yt\n= =∑ denotes the demands of LRUs at base \ni during time interval [0, t1], which is a Poisson random \nvariable with mean t1λi and f(⋅) is the probability density \nfunction of Di(t1). In this paper, we approximate \n1 () i Dt by a \nnormally distributed random variable with \nmean\n11 (( ) ) ii E Dt t λ = , and variance\n11 (( ) ) ii Var D t t λ =  and hence \nthe probability density function is \n⎭\n⎬\n⎫\n⎩\n⎨\n⎧ −\n− =\ni\ni i\ni\ni t\nt x\nt\nx f\nλ\nλ\nλ π 1\n2\n1\n1 2\n) (\nexp\n2\n1\n) ( . We justify this \napproximation as follows. Standard statistics have shown that \nthis approximation is good when the mean value of the \nPoisson random variable is no smaller than 10. Furthermore, it \nturns out from our detailed numerical investigation that the \napproximation is still satisfactory for our purpose even with \nmean values no smaller than 4 (see Appendix A). In the \nmilitary context, we witness a prolonged replenishment cycle, \nwhere the sum of demands arising in the interval between the \nstart and the first allocation or between the two allocations \nexhibit relatively large mean values. This phenomenon is also \nseen in a variety of commercial settings reviewed in [14], such \nas copying machines and transportation equipment, which \nhave relatively long product lifecycle, or electronics, which \nrequire a relatively large number of repairable items.  \nHence, after standardization and computation, EBO(t1) can \nbe expressed by: \n1\n11\n1 1\n()\nn\nii\ni\ni i\nSt\nEBO t t G\nt\nλ\nλ\nλ =\n⎛⎞ −\n= ⎜⎟ ⎜⎟\n⎝⎠\n∑             ( 3 )  \nwhere  ∫\n∞\n− =\nk dz z k z k G ) ( ) ( ) ( φ  is the unit normal loss \nfunction and φ(z) is the probability density function of the \nstandard normal distribution. \nB.  The First Reallocation \nAt time t1, we will perform the first reallocation. The spares \nwill be distributed from the depot to bases and among \ndifferent bases while failures at all bases will be sent back to \ndepot for repair. From our assumption, the spares at the depot \nwill commence transportation for the bases at time t1 – L and \narrive at time t1 for reallocation, while transshipment among \nbases will occur instantly. On the other hand, repair for failed \nitems will start at the depot at t1 + L because of the transport \ntime. Given the inventory levels at all sites before reallocation \nIi(t1) (i = 0, 1, …, n), our goal is to find the inventory levels at \nall sites after reallocation Ui(t1) (i = 0, 1, …, n) such that the \nEBO over all bases by t2 just before the second reallocation \nwill be minimized. That is, the reallocated spares will be used \nto last until the next reallocation. Mathematically, EBO over \nall bases by t2 can be expressed as: \n1\n1\n22 () '\n1\n12 1\n21 () '\n1 21\n() m i n ()\n() ( )\nmin ( )\n()\ni\ni\nn\ni Ut s\ni\nn\nii\ni Ut s\ni i\nEBO t EBO t\nUt t t\nttG\ntt\nλ\nλ\nλ\n=\n=\n=\n⎛⎞ −−\n=− ⎜⎟ ⎜⎟ − ⎝⎠\n∑\n∑\n      ( 4 )  \nAnd we have the constraints \n) ,..., 0 ( , 0 ) (\n) ( ) ( ) ( ) (\n1\n11\n1 0 1 1 0 1\nn i t U\nt I t I t U t U\ni\nn\ni\nn\ni\ni i\n= ≥\n+ = + ∑∑\n==\n         ( 5 )  \nWithout transshipments, the inequality in constraints (5) \nshould be changed into Ui(t1)  ≥ I i(t1). We know that those \nspares in the transport pipelines from the depot have no effect \non the EBO at bases until they arrive at bases instantly before \nthe reallocation. Therefore, we constrain I0(τ) = I0(τ-L) and \nU0(τ) = U0(τ-L) where reallocation takes place at the end of \nperiod τ. So we have I0(t1) = I0(t1–L) = S0 and we know Ii(t1) = \nSi – Di(t1) for i = 1, …, n where  ∑ = =\n1\n1 1 ) ( ) (\nt\nt i i t y t D  and t1 > L. \nHence, Equation (5) can be changed into: \n∑∑ ∑\n== =\n− + = +\nn\ni\nn\ni\ni i\nn\ni\ni t D S S t U t U\n11\n1 0\n1\n1 0 1 ) ( ) ( ) (       ( 6 )  \nUsing a Lagrange multiplier λ, the optimization can be \nrepresented as T-ASE-2009-070  \n \n4\n1\n12 1\n21 () '\n1 21\n10 1 1 0 1\n11\n() ( )\nmin ( )\n()\n(( ) ( ) ( ) ( ) )\ni\nn\nii\ni Ut s\ni i\nnn\nii\nii\nUt t t\ntt G\ntt\nUt Ut It It\nλ\nλ\nλ\nλ\n=\n==\n⎛⎞ −−\n− ⎜⎟ ⎜⎟ − ⎝⎠\n++ − −\n∑\n∑∑\n      ( 7 )  \nDifferentiating with respect to Ui(t1) (i = 1, …, n) and \nsetting the result to zero, we obtain \n12 1\n21\n() ( )\n0\n()\nii\ni\nUt t t\ntt\nλ\nλ\nλ\n⎛⎞ −−\n−Ψ + = ⎜⎟ ⎜⎟ − ⎝⎠\n   (i = 1, …, n)     (8) \nwhere  ∫\n∞\n= Ψ\nk dz z k ) ( ) ( φ  is the right-hand tail area of the \nstandard normal distribution. So according to the property of \nstandard normal distribution, \n12 1\n21\n() ( )\n()\nii\ni\nUt t t\nc\ntt\nλ\nλ\n−−\n=\n−\n   (i = 1, …, n)        ( 9 )  \nwhere c is a constant, independent of i. \nUsing (9) to sum over all bases and using (6) leads to \n12 1\n1\n01 0 1 2 1\n11 1\n() ( )\n[( ) ( ) ( ) ]\ni\nii n\nj j\nnn n\nii j\nii j\nUt t t\nSSD t U t t t\nλ\nλ\nλ\nλ\n=\n== =\n=− +\n×+ − − −−\n∑\n∑∑ ∑\n      ( 1 0 )  \nC.  The Second Reallocation \nUsing the same method as above, for the second \nreallocation, our purpose is to find Ui(t2) (i = 0, 1, …, n) such \nthat the EBO over all bases at the end of the cycle will be \nminimized. However, due to the transport time L between the \ndepot and bases, the repair cannot start until failures arrive at \nthe depot at time t1 + L and similarly spares must be sent out to \nbases at t2  – L. In order to have more spares for the second \nreallocation, we constrain t2 > t1 + 2L. Those failures coming \nout of the repair pipeline after t2 – L can only be used for \nreallocation next time if there are subsequent reallocation \ninstants, but since there is no more opportunity to perform a \nthird reallocation, we will completely redistribute all stocks on \nhand at the depot to bases by t2  –  L. Assuming complete \nredistribution at the second reallocation instant, we have U0(t2) \n= U0(t2 – L) = 0. \nOur objective function is: \n2\n2\n() '\n1\n22\n2 () '\n1 2\n() m i n ()\n()( )\nmin ( )\n()\ni\ni\nn\ni Ut s\ni\nn\nii\ni Ut s\ni i\nEBO H EBO H\nUt H t\nHt G\nHt\nλ\nλ\nλ\n=\n=\n=\n⎛⎞ −−\n=− ⎜⎟ ⎜⎟ − ⎝⎠\n∑\n∑\n      ( 1 1 )  \nsubject to the constraints \n∑∑\n==\n+ =\nn\ni\nn\ni\ni i t I t I t U\n11\n2 0 2 2 ) ( ) ( ) (              ( 1 2 )  \nWe know Ii(t2) = Ui(t1) – Di(t2) for i = 1, …, n where \n∑ + = =\n2\n1 1 2 ) ( ) (\nt\nt t i i t y t D . The number of available spares at the \ndepot just before the second reallocation equals to the number \nof spares left at the depot after the first reallocation U0(t1), \nplus those failed items (sent to depot during the first \nreallocation) which have finished repair and sent to depot \ninventory by time t2 – L. Let R be the random variable that \nrepresents the number of such items.  Hence, we have I0(t2) = \nI0(t2–L) = U0(t1) + R.  Assuming the repair time at the depot \nfollows an exponential distribution with mean T, the \nprobability that a failed item has finished repair and sent to \ninventory by t2 – L is given by 1 – exp[-(t2 – t1 – 2L)/T]. Since \nwe know \n11 11 (( ) )\nnn\nii ii ED t tλ\n== = ∑ ∑  and \n11 11 (( ) )\nnn\nii ii Var D t t λ\n== = ∑ ∑ , this implies \n21 (2 ) /\n1 1 () ( 1 )\nn tt L T\ni i ER t e λ\n−− −\n= =− ∑  and \n21 (2 ) / 2\n1 1 () ( 1 )\nn tt L T\ni i Var R t e λ\n−− −\n= =− ∑ . Therefore, Equation \n(12) can be rewritten as: \nR t U t D t U t U\nn\ni\nn\ni\ni i\nn\ni\ni + + − =∑∑ ∑\n== = 11\n1 0 2 1\n1\n2 ) ( ) ( ) ( ) (             (13) \nUsing (6), Equation (13) can be further changed into: \n∑∑ ∑ ∑\n== = =\n+ − − + =\nn\ni\nn\ni\nn\ni\nn\ni\ni i i i R t D t D S S t U\n11 1 1\n2 1 0 2 ) ( ) ( ) (     (14) \nAs the above method, using a Lagrange multiplier and \ndifferentiating with respect to Ui(t2) (i = 1, …, n) and setting \nthe result to zero, we obtain: \n22\n1\n02\n11\n() ( )\n[( ) ]\ni\nii n\nj j\nnn\nij\nij\nUt H t\nSS Y H t\nλ\nλ\nλ\nλ\n=\n==\n= −+ ×\n+− − −\n∑\n∑∑\n            ( 1 5 )  \nwhere Y = Y1 + Y2 – R,  ∑ = =\nn\ni i t D Y\n1 1 1 ) ( ,  ∑ = =\nn\ni i t D Y\n1 2 2 ) ( . \nD.  Compute Optimal EBO \nUsing (10) and (15), we can compute the optimal spare \nallocations of LRU after each reallocation. However, we have \nnot specified how to compute EBO(t2) by (4) and EBO(H) by \n(11). In addition, U0(t1) is still included in (10), i.e. the \ninventory level at each base after the first reallocation depends \non different inventory levels left at the depot. \n  From (15), we know that no matter how many spares are \nleft at the depot after the first reallocation, under the complete \nredistribution assumption for the second reallocation, the term \nU0(t1) will disappear, i.e. the inventory level is independent of \nU0(t1). Therefore, in order to reduce EBO just before the \nsecond reallocation at t2, U0(t1) should be set to zero according \nto (10). That is, we also adopt complete redistribution for the \nfirst reallocation. Thus, \n12 1\n1\n01 2 1\n11\n() ( )\n[( ) ]\ni\nii n\nj j\nnn\nij\nij\nUt t t\nSS Y t t\nλ\nλ\nλ\nλ\n=\n==\n= −+ ×\n+− − −\n∑\n∑∑\n            ( 1 6 )  \nwhere  ∑ ∑ ∑ == = = =\n1\n11 1 1 1 ) ( ) (\nt\nt\nn\ni i\nn\ni i t y t D Y . \nSubstituting  Ui(t1) in (4) by (16), we can compute the T-ASE-2009-070  \n \n5\nEBO(t2) just before the second reallocation for a given value \nof Y1. However, Y1 is a normally distributed random variable \nwith mean  \n111 ()\nn\ni i EY t λ\n= = ∑  and variance \n111 ()\nn\ni i Var Y t λ\n= = ∑ . \nThus, weighting EBO(t2) for a given value of Y1 by the density \nof Y1 and integrating over Y1, we have \n12 1\n22 1 1 1\n1 21\n() ( )\n() ( ) ()\n()\nn\nii\ni\ni i\nUt t t\nEBO t t t G f y dy\ntt\nλ\nλ\nλ\n∞\n−∞\n=\n⎛⎞ −−\n=− ⎜⎟ ⎜⎟ − ⎝⎠\n∑ ∫\n  (17) \nBy substituting 11 1\n1 1\nn\ni i\nn\ni i\nYt\nt\nλ\nξ\nλ\n=\n=\n−\n= ∑\n∑\n and using (16), Equation \n(17) can be rewritten as \n22 1\n1\n() ( ) ( )()\nn\ni\ni\nEBO t t t G a b d λ ξϕ ξ ξ\n∞\n−∞\n=\n=− + ∑ ∫    (18) \nwhere  1 1\n21 1\nn\ni i\nn\ni i\nt\na\ntt\nλ\nλ\n=\n=\n−\n=\n−\n∑\n∑\n and  02 11\n21 1\nnn\nii ii\nn\ni i\nSS t\nb\ntt\nλ\nλ\n==\n=\n+−\n=\n−\n∑∑\n∑\n \nUsing the result in [2] and [12] \nwhere∫\n∞\n∞ − ⎟ ⎟\n⎠\n⎞\n⎜ ⎜\n⎝\n⎛\n+\n+ = +\n2\n2\n1\n1 ) ( ) (\na\nb\nG a dx x b ax G φ , we obtain \n2\n22 1 1\n11\n02 11\n2\n21 1 11\n() ( ) ( )\n() ( )\nnn\nii\nii\nnn\nii ii\nnn\nii ii\nEBO t t t t\nSS t\nG\ntt t\nλ λ\nλ\nλ λ\n==\n==\n==\n=− +\n⎛⎞ +− ⎜⎟ × ⎜⎟ ⎜⎟ −+ ⎝⎠\n∑∑\n∑∑\n∑∑\n         ( 1 9 )  \n  Respectively, using (11) and (15), we can also construct \nthe formula of EBO(H) for a given value of Y, in which Y is a \nnormally distributed random variable. Y=Y1+Y2–\nR, ∑∑ ∑ == = = =\n1\n11 1 1 1 ) ( ) (\nt\nt\nn\ni i\nn\ni i t y t D Y , and \n∑∑ ∑ + == = = =\n2\n1 11 1 2 2 ) ( ) (\nt\nt t\nn\ni i\nn\ni i t y t D Y . Thus, it has mean \n21\n21\n12\n(2 ) /\n12 11 11 1\n(2 ) /\n21 11\n() () ( ) ()\n() [ 1 ]\n(1 )\nnn n tt L T\nii i ii i\nnntt L T\nii ii\nEY EY EY ER\ntt tt e\ntt e\nλλ λ\nλλ\n−− −\n== =\n−− −\n==\n=+−\n=+ − − −\n=− −\n∑∑ ∑\n∑∑\n \nand variance \n21\n21\n12\n(2 ) / 2\n12 11 11 1\n(2 ) / 2\n21 11\n() () ( ) ()\n() ( 1 )\n(1 )\nnn n tt L T\nii i ii i\nnntt L T\nii ii\nVar Y Var Y Var Y Var R\ntt tt e\ntt e\nλλ λ\nλλ\n−− −\n== =\n−− −\n==\n=++\n=+ − + −\n=+ −\n∑∑ ∑\n∑∑\n \n Weighting  EBO(H) for a given value of Y by the density \nof Y and integrating over Y, we have \n22\n2\n1 2\n()( )\n() ( ) ( )\n()\nn\nii\ni\ni i\nUt H t\nEBO H H t G f y dy\nHt\nλ\nλ\nλ\n∞\n−∞\n=\n⎛⎞ −−\n=− ⎜⎟ ⎜⎟ − ⎝⎠\n∑ ∫ (20) \nand using the same method as EBO(t2), we obtain \n21\n21\n21\n(2 ) / 22\n22 1 1\n11\n(2 ) /\n01 11 1\n(2 ) / 22\n22 1 11 1\n() ( ) ( ) ( 1 )\n(1 )\n() ( ) ( 1 )\nnn\nn tt L T\nii i i\nii\nnn n tt L T\nii i ii i\nnn n tt L T\nii i ii i\nEBO H H t t t e\nSS H t e\nG\nHt t t e\nλλλ\nλλ\nλλ λ\n−− −\n=\n==\n−− −\n== =\n−− −\n== =\n=− + + −\n⎛⎞ +− + − ⎜⎟ × ⎜⎟ ⎜⎟ −+ + − ⎝⎠\n∑∑ ∑\n∑∑ ∑\n∑∑ ∑\n(21) \nE.  Two-Allocation Approach \nUsing (3), (19) and (21), we can calculate our objective \nfunction TE, the total expected backorders over all bases at \nthree time points for a given reallocation instant pair t1 and t2: \nTE(t1, t2) = EBO(t1) + EBO(t2) + EBO(H)         ( 2 2 )  \nOur purpose is to find such t1 and t2 (0 < t1 < t2 < H) that TE \nis minimized. Due to the transport time L from the depot to \nbases, the first reallocation (t1) cannot take place earlier than \nthe period L. In addition, the second reallocation (t2) has to \ntake place before the period H. The two reallocation times are \nconstrained by t2 > t1  + 2L as mentioned in Section 3.3. \nTherefore, the first reallocation (t1) can take place at the \ninterval [L, H-2L-2] whereas the second reallocation (t2) can \ntake place at the interval [t1+2L+1,  H-1]. Hence, we can \ncompute  TE for possible pairs of (t1,  t2) to determine the \nminimal TE and the corresponding (t1, t2).  \n  However, if the second reallocation instant is the end of \nthe cycle, we do not make good use of two reallocation \nopportunities. TE will be the same as that in [2] with only one \nreallocation because our objective function is the expected \nbackorders just before reallocation. [2] proved that the TE \nvalue will decrease first and then increase as t2 increases for a \ngiven t1. Hence our heuristic strategy can be stated as a simple \nsearch procedure as follows: \nfor (t1 = L; t1 < H – 2L – 1; t1++)  \n   for (t2 = t1 + 2L + 1; t2 < H; t2++) { \n    //  perform  2\nnd reallocation later     \n    i f   ( TE(t1, t2) > TE(t1, t2+1)) continue; \n    //  perform  2\nnd reallocation at this t2            \n       else store this value of TE(t1, t2) and break;      \n } \nCompare the stored TE(t1, t2) values for different values \nof t1 and choose the minimum with corresponding t1 and \nt2.  \n  \nComputationally speaking, the worst case number of \niterations is bounded by (H-3L)*(H-3L-1)/2, and hence the \ncomputational time complexity is\n2 () On H ∗ , since each “if” \nstatement requires O(n) computation. We present the \ncomputation performance for various replenishment horizons \nH in Appendix B. These results show that the computation \ntime for our approach is reasonably acceptable. \nF.  Extension to Multiple Reallocations \nBased on the result of two-reallocation for the repairable \nitem inventory system, the extension to M-reallocation is now \npresented. The EBO over all bases at (M + 1) time points for a \ngiven set of reallocation instants {t1, t2, …, tM} are given as \nTE(t1, t2, …, tM) = EBO(t1) + EBO(t2) + ...  \n+ EBO(tM) + EBO(H)          ( 2 3 )  T-ASE-2009-070  \n \n6\nWith the assumption about complete distribution of items at \ncentral depot at each reallocation instant, the order-up-to level \nat each reallocation instant and the corresponding expected \nbackorder from this reallocaton to the next reallocation can be \nsimilarly calculated as Two-reallocaton problem stated before. \nHere, we only present the formula on the order-up-level at \nfinal reallocation instant tM and the corresponding expected \nbackorder from tM till the end of replenishment cycle as \nfollows: \n0\n11\n1\n()( )\n[( ) ]\niM M i\nnn\ni\niM j n\nij j j\nUt H t\nSS Y H t\nλ\nλ\nλ\nλ ==\n=\n=− +\n+− − − ∑∑\n∑\n        ( 2 4 )  \nwhere \n11 2 ()\nM tn M\nii ti i Yy t R\n== = =− ∑∑ ∑ and  Ri ( i = 2, 3, …, \nM) denotes the arriving repaired items at reallocation instant ti \nfrom the depot to bases. We use Xk to denote the number of \nfailures generated during period [tk-1,  tk] and we know \n1 1 () ( )\nn\nkk k i i EX t t λ − = =− ∑ and \n1 1 () ( )\nn\nkk k i i Var X t t λ − = =− ∑  for all k \n(k=1,…, M) if we assume t0 = 0. Thus, we can calculate Ri as \nfollows: \n1\n1\n1\n1\n1\n1\n[( ) 2 ]/ 2 () /\n1\n(2 ) /\n1\n[( ) 2 ]/ 2 () /\n1 11\n(2 ) /\n12 1\n2[(\n1\n(1 )\n[1 ]\n()( 1 ) ( )\n()[ 1 ]\n() ( )\nik\nii\nii\nik\nii\nii\ntt L T i tt T\nik k\ntt LT\ni\ntt L T in tt T\nik k j kj\nn tt LT\nii j j\nt\nik k j\nRX e e\nXe\nER e t t e\ntt e\nVar R t t e\nλ\nλ\nλ\n−\n−\n−\n−\n−\n−\n−− − − −−\n=\n−−−\n−\n−− − − −−\n− ==\n−−−\n−− =\n−\n−\n=−\n+−\n=− −\n+− −\n=−\n∑\n∑∑\n∑\n11\n1\n2 )2] / ( ) / 2\n11\n(2 ) / 2\n12 1\n(1 )\n()[ 1 ]\nik i i\nii\nin tL T t t T\nkj\nn tt LT\nii j j\ne\ntt e λ\n−−\n−\n− −− −−\n==\n−−−\n−− =\n−\n+− −\n∑∑\n∑\n (25) \nFurthermore, the mean and variance of Y can be calculated as \nfollows: \n11 2\n12\n() [( ) ] ( )\n()\nM tn M\nii ti i\nnM\nMi i ii\nEY Ey t ER\ntE R λ\n== =\n==\n=−\n=−\n∑∑ ∑\n∑∑\n \nand  \n11 2\n12\n() (( ) ) ( )\n() .\nM tn M\nii ti i\nnM\nMi i ii\nVar Y Var y t Var R\ntV a r R λ\n== =\n==\n=+\n=+\n∑∑ ∑\n∑∑\n \nWeighting EBO(H) for a given value of Y by the density of Y \nand integrating over Y, we have \n1\n()( )\n() ( ) ( )\n()\nn\niM M i\nMi\ni Mi\nUt H t\nEBO H H t G f y dy\nHt\nλ\nλ\nλ\n∞\n−∞\n=\n⎛⎞ −−\n=− ⎜⎟ ⎜⎟ − ⎝⎠\n∑ ∫\n (26) \nand using the same method as the two-reallocation problem, \nwe obtain \n2\n11 2\n0 11 2\n2\n11 2\n() ( ) ( ) ()\n()\n() ( ) ( )\nnn M\nMi M i i\nii i\nnn M\nii i ii i\nnn M\nMi M i i ii i\nEBO H H t t Var R\nSS H E R\nG\nHt t V a r R\nλλ\nλ\nλλ\n== =\n== =\n== =\n=− + +\n⎛⎞ +− + ⎜⎟ × ⎜⎟ ⎜⎟ −+ + ⎝⎠\n∑∑ ∑\n∑∑ ∑\n∑∑ ∑\n (27) \nAgain, our purpose is to find such set of reallocation \ninstants {t1,  t2, …, tM} (0 < t1  < … < tM  <  H) that TE is \nminimized. Due to the transport time L from the depot to \nbases, the first reallocation (t1) cannot take place earlier than \nthe period L. In addition, final reallocation (tM) has to take \nplace before the period H. Two consecutive reallocation \ninstants are constrained by ti > ti-1 + 2L (2 ≤ i ≤ M) for the \nsame reason as the two-reallocation problem (stated in Section \n3.3). Due to this constraint, it is also easy to see that Normal \ndistribution approximates Poisson distribution well. \nTherefore, the first reallocation t1 can take place at the interval \n[L, H-(M-1)*(2L+1)-1] whereas the reallocation instant ti (2 ≤ \ni ≤ M) can take place at the interval     [ti-1+2L+1, H-(M-\ni)*(2L+1)-1].  The worst-case total iterations is thus \n2 HL M L\nM\n− + ⎛⎞\n⎜⎟\n⎝⎠\n, and hence the computational time \ncomplexity can be measured as ()\n()\n!\nM nHM\nO\nM\n∗− . \n \nIV.  EXPERIMENTAL RESULT AND SENSITIVITY ANALYSIS \nOur experimental results are presented in this section. In \nSection 4.1, we use test cases to compare the results under two \nreallocation instants with those having single instant in [2]. In \nSection 4.2, we perform extensive sensitivity analysis to show \nthe effects of each independent parameter on the values of \ntotal EBO and reallocation instants. \nA.  Comparison \nFirst, we use test case to determine when and how to \nreallocate for two reallocation instants, and compare the \nresults with those allowing single reallocation, as seen in [2]. \nIn our experiment, we have one depot supports 5 bases (n=5). \nThe length of the replenishment cycle is 30 periods (H=30). In \norder to make meaningful comparison between the two-\nreallocation and the single-reallocation scheme proposed in \n[2], we set both the internal lead time and the mean repair time \nto be zero (L=0,  T=0). First we focus on the identical \nindependent demand distributions at all bases. We set λi=4 for \nall i (i = 1, …, n). The stock level at each base i is set to \nSi=H*λi =120. We determine the stock level at the depot to \nbe\n0 1 2.33 57.07\nn\ni i SH λ\n= == ∑ , where 2.33 represents the \nprobability of 1% probability that the total system demands in \na cycle H exceeds the total stocks at the depot and all bases. \nWe implement both our method and that of [2] so that we \ncan compare them on the effect of two reallocations during a \ncycle. The results are shown in Fig. 1, together with Fig. 2 \n(which provides an enlarged view on the comparison among \ntwo-reallocations over different first-reallocation time \ninstants). From Fig. 2, we can see firstly that given the first \nreallocation instant, the total EBO TE decreases and then \nincreases as the second reallocation instant t2 increases. This is \nconsistent with what we mentioned in the above section. \nTherefore, the time interval between reallocations can be \nneither too short because of fewer repaired failures at the \ndepot nor too long because of more failures at bases. T-ASE-2009-070  \n \n7\nSecondly, we can see from Fig. 1 that TE decreases and then \nincreases as the first reallocation instant t1 increases. This is \nindicated by comparing TE(0, t2), TE(4, t2) and TE(27, t2). The \ncurve of TE(4, t2) is below that of TE(0, t2), indicating it better \nto perform the first reallocation at t1=4 than t1=0 while the \ncurve of TE(27,  t2) is above that of TE(4,  t2), indicating it \nworse to perform the first reallocation at t1=27 than t1=4. \nThirdly, Fig. 2 shows that the curve of TE(27, t2) intersects \nwith that of TE(0, t2). This indicates that it should not always \ndelay the first reallocation and perform the second one in a \nhurry. In fact, there is also an intersection between the curve \nof TE(4, t2) and the curve of TE(27, t2) although not obvious. \nComparing all combinations of (t1, t2), we find the optimal \nreallocation instants pair is (14, 20) with TE = 3.7e-14. \nFourthly, we can see from Fig. 1 that multiple reallocations \ncan reduce the total EBO compared with single reallocation. \nFrom Fig. 1, the optimal reallocation instant in [2] is at t=24 \nwith TE = 0.6670. We can also see that our first reallocation \ninstant should be before t=24. This is because if we reallocate \nat later than t=24, there will be a large number of backorders \nat bases. More interestingly, we compare [2]’s result with \nthose whose first reallocation takes place at t1=24. The results \nare shown in Table I. From Table I, we can see that when the \nfirst reallocation instant is t=24, if we perform the second \nreallocation immediately after the first one, TE can also be \nimproved because more failures can be repaired as more \nfailures are brought back the depot under the assumption of \ninfinite repair capacities. However, if we perform the second \nreallocation at the end of the cycle, it is equivalent to \nreallocate only once (recall our objective function is that just \nbefore reallocation). Hence, TE should be the same as that of \nthat presented in [2], which is proved to be 0.6670 in Table I. \nNext, we set lower stock levels at bases to investigate what \nwill happen under a higher level of EBO. The results are \nshown in Fig. 3, where we multiply the stock level at each \nbase in the previous case by 0.8. Fig. 4 is to highlight the \ncomparison within two-reallocations when the first \nreallocation takes place at different time instant. Cao and \nSilver [2] claim that because generally it is more costly to \ndelay allocation beyond the best time than to perform it \nsomewhat early, it tends to hedge against the higher penalties \nby committing to an earlier allocation time. Fig. 3 shows \nfirstly that the optimal reallocation instant for [2] is t=18 with \nTE=63.9580. From Fig. 4, our optimal reallocation instant pair \nis (14, 19) with TE=3.68e-5 correspondingly, the second one \nbeing brought forward. This is consistent with the claim in [2]. \nFurthermore, Fig. 4 shows secondly that under the same \nreallocation instant for the first time, the second reallocation \ninstant is also brought forward. In the previous case, when \nt1=0, the optimal t2 is t2=26 while in the current case the \noptimal t2 is t2=21.  Similarly when t1=4, t2=19 in the previous \ncase while t2=22 in the current one. \nNext, we run a test case by relaxing the assumption of \nidentical demand distributions at bases while demand is still \nassumed to be independent. For non-identical demand \ndistributions at bases, we use the same method as in [2], \nsetting the mean demand of each base i by λi = 2i*λ/ (n+1) (i \n= 1, …, n,  λ=4). The results are shown in Table II. From \nTable II, we observe that this change does not bring about \nconsistent effect on the results at high stock level and low \nstock level with the method in [2]. However, using our \nmethod, either at high stock level or at low stock level, the \nnon-identical demand will incurs higher expected backorder \nthan identical demand. It is probably due to the higher CV at \nsome bases with non-identical demand. \nFig. 1. Comparison of EBO vs. time between single- and two-reallocation \n \nFig. 2. Comparison of EBO vs. time among two-reallocations \n  \nTABLE I \nTOTAL EBO WHEN THE FIRST REALLOCATION INSTANT IS 24 (H=30) \n (t1, t2) TE \n(24, 25)  0.1147 \n(24, 26)  0.1147 \n(24, 27)  0.1164 \n(24, 28)  0.1190 \n(24, 29)  0.1857 \n(24, 30)  0.6670 \nTABLE II \nTE FOR IDENTICAL DEMAND DISTRIBUTIONS VS. NON-IDENTICAL DEMAND \nDISTRIBUTIONS \nCase [2]-1  Ours-1  [2]-0.8  Ours-0.8 \nIdentical  0.6670 3.7e-14  63.9580 3.68e-5 \nNon-identical  0.7624 2.02e-8  55.3050 1.63e-3 \n T-ASE-2009-070  \n \n8\n \nB.  Sensitivity Analysis \nAs presented in [2], in this subsection we will show the \neffects of each independent parameter on the values of total \nEBO (i.e. TE) and reallocation instants for two reallocation \ninstants cases. We use the parameters in Section 4.1, plus \nrepair time and internal lead time set by us (see Table III) and \nfocus on the situation of low stock level as stated in Section \n4.1. We run the test cases for both identical and non-identical \ndemand distributions while demand is still assumed to be \nindependent. For non-identical demand distributions at bases, \nwe use the same method as in [2], setting the mean demand of \neach base i as λi = 2i*λ / (n+1) (i = 1, …, n). \nInstead of using graphics as in [2], which seems to give a \nvisual illusion that the relationship is linear, we use tables to \nshow the changes of TE with different independent \nparameters and the changes of reallocation instants with \nrepair time and internal lead time. Tables IV to VI illustrate \nthat TE decreases as k (the safety factor) increases, that TE \ndecrease as λ (the average demand level) increases and that \nTE decreases with H (the system cycle length), all of which \nare consistent with corresponding ones in [2] for single \nreallocation cases. \nMore interestingly, we show the effects of repair time and \ninternal lead time on the total EBO TE and reallocation \ninstants (t1, t2), which is not in the model of [2]. It is not \nsurprising that TE increases as T (repair time) increases \n(Table VII) since it takes more time to repair a failure so that \nfewer items come out given a certain time interval. From \nTable VII, it is interesting for us to find that as T increases, \nthe time interval between two reallocations increases (8 \nwhen T=10, 9 when T=20, 10 when T=30 for identical case \nand 8 when T=10, 10 when T=20, 11 when T=30 for non-\nidentical case). However, when T=100, it recommends that \nthe first reallocation should be shifted earlier (t1=13) and \nthen increases the time interval between two reallocations \n(11 for identical case). In Table IX, it is also not surprising \nthat TE increases as L (internal lead time) increases since it \ntakes more time to transport items back and forth except that \nthe effect is not obvious when L is small (TE remains same for \nL = 0 and L = 2.). In Table X, we show the interesting effect \nof L on reallocation instants especially t1. From Table X, we \nknow that the first optimal reallocation instant is 14 if \ntransport time (L) is zero and repair will take place at the \ndepot instantly after the first instant. But if L=2, repair can \nonly take place 2 time periods after the first instant. When \nmean repair time is assumed to be zero, this change on \ntransport time will not affect reallocation instants. However, \nwhen transport time becomes larger like L=5, the first instant \nhas to be put earlier so as to let the second reallocation not so \nhurry. In addition, transport must take place from the depot to \nthe bases at the beginning of the cycle (t=0) for reallocation in \nsuch a way that the first reallocation will not be done earlier \nthan t1=8 as shown in Table X. \n \n \nTABLE III: PARAMETERS FOR SENSITIVITY ANALYSIS \nParameter Values \nk  1.645, 2.33 \nλ   4, 6, 10 \nH  30, 50, 100 \nT  0, 10, 20, 30, 100 \nL  0, 2, 5, 8 \n \nFig. 3. Comparison of EBO vs. time between single- and two-reallocation \nwith fewer stocks \n \nFig. 4. Comparison of EBO vs. time among two-reallocations with fewer\nstocks \nTABLE IV: EFFECTS OF k ON TE \nk  1.645 2.33 \nIdentical 2.4e-4  3.68e-5 \nNon-identical 3.3e-3  1.63e-3 \n \nTABLE V: EFFECTS OF λ ON TE \nλ 4 6 10 \nIdentical 3.68e-5  3.13e-7  3.98e-11 \nNon-identical 1.63e-3  8.47e-5  4.53e-7 \n \nTABLE VI: EFFECTS OF H ON TE \nH  30 50  100 \nIdentical  3.68e-5 3.11e-8 5.2e-16 \nNon-identical  1.63e-3 1.37e-4 1.43e-8 T-ASE-2009-070  \n \n9\n \n \nV.  APPLICATION: PERIODIC RESUPPLY POLICY \nIn most studies on periodic resupply policies, one usually \ndetermines the optimal allocation of inventories under a given \nfixed time interval between periodic allocations, for example, \nevery h time units. The question we need to ask is whether \nthis h time unit is cost-optimal. Note that without considering \ncosts, it would be optimal to reallocate as frequently as \npossible, i.e. let the time interval between reallocations tend to \nbe infinitely small so that it approximates continuous resupply \nas in [1], [8]-[11]). Given that reallocation is costly, frequent \nreallocation will incur high operating cost, while shortage of \nstocks will also incur penalty costs. It is therefore important to \nfind the right value for h so as to balance between the costs of \nreallocations and shortages.  \n  In this section, we show how our reallocation model can \nbe extended and applied to derive a period resupply policy. To \nachieve this purpose, we first extend our proposed model to \nmultiple (more than 2) reallocations, where the challenge is to \ndetermine the time interval between reallocations assuming \nthe intervals are the same for a given time horizon. This would \npave the way for the design of cost-optimal periodic resupply \npolicies by studying the balance between the costs of \nreallocations and shortages. We introduce more notations \nbased on those in Section 2. Instead of random reallocation \ninstants, here reallocations are assumed to take place every h \nperiod so that the total number of reallocations is m = [(H – \n1)/h] (it is unnecessary to reallocate at the end of the cycle), \nwhere [x] is the maximal integer number not greater than x. \nWe also assume h is greater than 2L, the lead time to transport \nback and forth between the depot and bases. Cr is the unit cost \nof a reallocation and Cs is the unit cost of a shortage. Thus, \nour objective is to minimize the total cost; \nmin TC = Cr × m + Cs ×  T E            ( 2 8 )  \nExtended from (27), TE here is the expected backorders over \nall bases at a series of reallocation instants and at the end of \nthe cycle, i.e. \n11 1\n1\n1\n() ( )\n()\nmn n\nii pi i\nm\np p\nTE EBO ph EBO H\nEBO t\n== =\n+\n=\n=+\n=\n∑∑ ∑\n∑\n      ( 2 9 )  \nwhere  ∑ = =\nn\ni i p ph EBO t EBO\n1 ) ( ) (  (p=1,…,m)  and \n∑ = + =\nn\ni i m H EBO t EBO\n1 1 ) ( ) (  \nAccording to (3), the expected backorders over all bases at \ntime h, just before the first reallocation is \n1\n1\n()\nn\nii\ni\ni i\nSh\nEBO t h G\nh\nλ\nλ\nλ =\n⎛⎞ −\n= ⎜⎟ ⎜⎟\n⎝⎠\n∑             ( 3 0 )  \nRespectively according to (19), the expected backorders over \nall bases at time 2h, just before the second reallocation is \n2\n2\n11\n0 11\n2\n11\n() ( )\n2\n()\nnn\nii\nii\nnn\nii ii\nnn\nii\nii\nEBO t h h\nSS h\nG\nhh\nλ λ\nλ\nλλ\n==\n==\n==\n=+\n⎛⎞\n⎜⎟ +− ⎜⎟ × ⎜⎟\n⎜⎟ + ⎜⎟\n⎝⎠\n∑ ∑\n∑∑\n∑∑\n           ( 3 1 )  \nFrom the second reallocation onward, we must consider \nthose failures that have finished repair and are sent to depot \ninventory in time to be transported to bases for the k\nth \nreallocation, Rk. We use Xt to denote the number of failures \ngenerated during period [(t  – 1)h,  th] and we know \n1 ()\nn\nti i EX h λ\n= = ∑ and \n1 ()\nn\nti i Var X h λ\n= = ∑  for all t (t=1, …, m) \nHence, for the second reallocation, we have R2 = X1[1 – e\n-(h-\n2L)/T], E(R2) = \n1\nn\ni i h λ\n= ∑ [1 – e\n-(h-2L)/T] and Var(R2) = \n1\nn\ni i h λ\n= ∑ [1 \n– e\n-(h-2L)/T]\n2.  For the third reallocation, we have R 3 = X1e\n-(h-\n2L)/T(1 – e\n-h/T)+X2[1 – e\n-(h-2L)/T], E(R3) = \n1\nn\ni i h λ\n= ∑ [1 – e\n-(2h-2L)/T] \nand  Var(R3) = \n1\nn\ni i h λ\n= ∑ {e\n-(2h-4L)/T(1 – e\n-h/T)\n2+[1 – e\n-(h-2L)/T]\n2}. \nAnd in general, for the k\nth (k = 2, …, m) reallocation, we have \n[( 1 ) 2 ]/ 2 /\n1\n(2 ) /\n1\n[( 1) 2 ]/\n1\n1\n2 2 [ (1 )2 ] / / 2 (2 ) / 2\n1\n(1 )\n[1 ]\n() [ 1 ]\n()\n{( 1 ) [ 1 ] }\nkt h L T k hT\nkt t\nhL T\nk\nn kh L T\nki i\nn\nki i\nk kt h L T h T h L T\nt\nRX e e\nXe\nER h e\nVar R h\nee e\nλ\nλ\n−− −− − −\n=\n−−\n−\n−−−\n=\n=\n− −− − − − − −\n=\n=−\n+−\n=−\n=×\n−+ −\n∑\n∑\n∑\n∑\n     ( 3 2 )  \nHence, according to (26), \n1\n2\n11 2\n1\n0 11 2\n1 2\n11 2\n() ( ) ( 1 ) ()\n()\n() ( 1 ) ( )\np nn\npi i i\nii i\nnn p\nii i ii i\nnn p\nii i ii i\nEBO t h p h Var R\nSS p h E R\nG\nhp h V a r R\nλλ\nλ\nλλ\n−\n== =\n−\n== =\n−\n== =\n=+ − +\n⎛⎞ +− + ⎜⎟ × ⎜⎟ ⎜⎟ +− + ⎝⎠\n∑∑ ∑\n∑∑ ∑\n∑∑ ∑\n   (33) \n(p = 3, …, m) and \nTABLE VII: EFFECTS OF T ON TE \nT  0 10  20  30  100 \nIdentical 3.68e-5  0.0866  2.0439  7.4840  39.1864 \nNon-identical 1.63e-3 0.0828 1.7130 6.1837 33.0417 \n \nTABLE VIII: EFFECTS OF T ON REALLOCATION INSTANTS (t1, t2) \nT  0 10  20  30  100 \nIdentical (14,19)  (14,22)  (15,24)  (14,24)  (13,24) \nNon-identical (13,19) (14,22)  (14,24)  (13,24) (14,25) \n \n TABLE IX: EFFECTS OF L ON TE \nL  0 2 5 8 \nIdentical  3.68e-5 3.68e-5  1.05e-2 4.6977 \nNon-identical  1.63e-3 1.63e-3  8.99e-3 3.8779 \n \nTABLE X: EFFECT OF L ON REALLOCATION INSTANTS (t1, t2)  \nL  0 2 5  8 \nIdentical  (14, 19)  (14, 19)  (10,21)  (8, 25) \nNon-identical  (13, 19)  (13, 19)  (10, 21)  (8, 25) T-ASE-2009-070  \n \n10\n2\n1\n11 2\n0 11 2\n2\n11 2\n() ( ) ( ) ( )\n()\n() ( ) ( )\nnn m\nmi i i\nii i\nnn m\nii i ii i\nnn m\niii ii i\nEBO t H mh mh Var R\nSS H E R\nG\nHm h m h V a r R\nλλ\nλ\nλλ\n+\n== =\n== =\n== =\n=− + +\n⎛⎞ +− + ⎜⎟ × ⎜⎟ ⎜⎟ −+ + ⎝⎠\n∑∑ ∑\n∑∑ ∑\n∑∑ ∑\n   (34) \nUsing (28)–(34), we can compute the total cost consisting \nof reallocation cost and shortage penalty cost for a given time \ninterval between periodic reallocations. Mathematically, this \ntotal cost is a function of h. Thus intuitively, we can compute \nthe first derivative and set the result to be zero (i.e. dTC/dh = \n0) to obtain the optimal periodic policy in terms of the right h \nvalue. However, due to the complexity in the equation for \nEBO as defined in (33) and (34) (especially the unit normal \nloss function G), it is computationally intensive to compute \nthe optimal h by using the first derivative.  To overcome \ncomputational inefficiency, the following heuristic approach \nmay be applied to compute EBO: \n \nfor (h = 2L+1; h < H ; h++) { \n Compute  EBO(t1),…,EBO(tm+1) by (30)–(34). \n  Compute total EBO TE by (29). \n Compute  TC(h+1) by (28). \n if  (TC(h) ≤ TC(h+1))  \n  output the optimal periodic policy h and corresponding \nstock reallocations \n} \n \nVI.  CONCLUSION AND FURTHER RESEARCH \nIn this paper, we were interested in analyzing the \nperformance of reallocation within a multi-echelon inventory \nsystem. During the replenishment cycle, we have two \nopportunities to reallocate the spares by redistributing the \ndepot stocks to the bases and by lateral transshipment. We \ndeveloped a mathematical model and use a Lagrange \nmultiplier to determine how to reallocate the spares to achieve \na minimized total expected backorders under a given \nreallocation instant pair. Then we derive a dynamic \nreallocation method to determine when to perform the first \nand second reallocation respectively. Experimental results \nshow that two-reallocation is better than single-reallocation. \nThe logic of our approach is easy to implement and efficiently \ncomputed.  \nSeveral possible avenues of extension of our work are \nworth considering: \n1)  Echelon Structure. We have considered the two-echelon \ntree structure. A natural extension is to handle more than \ntwo echelons where reallocation instants at different \nechelons can be different. Moreover, one can extend the \nsupply chain structure from a tree structure to a network \n(graph).  \n2)  Demand distribution. It is also interesting to consider \nnonstationary demand distributions, i.e. demands at each \nperiod are not identical with time-varying mean and \nstandard deviation. \n3)  Cost consideration. The objective of this paper is to \nconsider the timing of reallocation that seeks to improve \nefficiency and reduce unavailability. It is obvious that \nmore frequent reallocation yields better performance \ncompare to single reallocation if the model does not \nincorporate the cost of reallocation.  \n4)  Periodic Resupply. Finally, from the practice standpoint, \nit is interesting to experiment on the idea of computing \noptimal periodic resupply proposed in Section 5. \n \nAPPENDIX \nA.  Experiments on Approximation of Poisson distribution \nby Normal distribution \nIn the following, we investigate the effect of approximation \nof Poisson distribution by a Normal distribution when the \nmean of Poisson random variable (in our case, demand) is \nsmaller than 10. More precisely for the purpose of our paper, \nwe are concerned with the approximation error on the value of \nthe expected backorder. It is clear that the sum of Poisson \ndemands with mean λ during the interval [0, t] still follows a \nPoisson distribution with mean λ*t. Given initial inventory S, \nthe expected backorder based on Poisson demand distribution \n(denoted  EBO_Poisson) is calculated as \n()\n()\n!\nk\nt\nkS\nt\nek S\nk\nλ λ\n∞\n−\n=\n− ∑ . If we approximate the Poisson \ndistribution with a Normal distribution, then the expected \nbackorder based on Normal demand distribution (denoted \nEBO_Normal) is calculated as  ()\nSt\ntG\nt\nλ\nλ\nλ\n− . Thus, the \nrelative approximation error can be calculated as \n(EBO_Normal - EBO_Poisson)/EBO_Poisson. \nIn Table A.1, we compare the expected backorder values as \nwell as the relative approximation error computed under the \ntwo demand distributions using different values of λ*t and \nassuming  S to be equal to λ*t. We observe that the \napproximation error is very small (2% or less) when the mean \nof Poisson distribution is no smaller than 4. We conclude that \nthe Normal distribution can effectively approximate Poisson \ndistribution, for the purpose of this work. \nTABLE A1: RELATIVE APPROXIMATION ERROR ON EXPECTED BACKORDER \nλ*t  EBO_Normal EBO_Poisson  Relative approximation \nerror \n1 0.3989  0.3679  0.084 \n2 0.5642  0.5413  0.042 \n3 0.6910  0.6721  0.028 \n4 0.7979  0.7815  0.021 \n5 0.8921  0.8773  0.017 \n6 0.9772  0.9637  0.014 \n7 1.0555  1.0430  0.012 \n8 1.1284  1.1167  0.010 \n9 1.1968  1.1858  0.009 \n10 1.2616  1.2511  0.008 T-ASE-2009-070  \n \n11\nB.  Experiments on Computational performance of our two-\nreallocation algorithm \nUsing the same experimental setup as Section 4.1, we \nmeasure the computational time required to execute the two-\nreallocation algorithm for different replenishment horizons H \non a machine with CPU 1.66GHz and RAM 1GB. The result \nis shown as follows: \n \nREFERENCES \n[1]  P. Alfredsson, “Optimization of multi-echelon repairable item inventory \nsystems with simultaneous location of repair facilities”, European \nJournal of Operational Research, v99, 584-595, 1997. \n[2]  D. B. Cao and E. A. Silver, “Dynamic Allocation Heuristic for \nCentralized Safety Stock”, Naval Research Logistics, v52:6, 513-526, \n2005. \n[3]  A. Díaz and M. C. Fu, “Models for multi-echelon repairable item \ninventory systems with limited repair capacity”, European Journal of \nOperational Research, v97, 480-492, 1997. \n[4]  G. D. Eppen and L. Schrage, “Centralized ordering policies in a multi-\nwarehouse system with lead times and random demand”, In L. B. \nSchwarz (ed) TIMS Studies in the Management Sciences, v16, 51-67, \n1981. \n[5]  P. L. Jackson, “Stock allocation in a two-echelon distribution system or \nwhat to do until your ship comes in”, Management Science, v34, 880-\n895, 1988. \n[6]  P. L. Jackson and J. A. Muckstadt, “Risk pooling in a two-period, two-\nechelon inventory stocking and allocation problem”, Naval Research \nLogistics, v36, 1-26, 1989. \n[7]  H. Jönsson and E. A. Silver, “Analysis of a two-echelon inventory \ncontrol system with complete redistribution”, Management Science, v33, \n215-227, 1987. \n[8]  H. C. Lau, H. Song, C.T. See and S.Y. Cheng, “Evaluation of Time-\nVarying Availability in Multi-Echelon Spare Parts Systems with \nPassivation”, European Journal of Operational Research, v170:1, 91-\n105, 2006. \n[9]  H. C. Lau and H. Song, “Multi-Echelon Repairable Item Inventory \nSystem with Limited Repair Capacity under Nonstationary Demands”, \nInternational Journal of Inventory Research, 1(1), 67-92, 2008. \n[10]  OPUS10 User’s Reference – Logistics Support and Spares Optimization, \nSystecon AB, May 1998. \n[11]  C. C. Sherbrooke, “Optimal Inventory Modeling of System: Multi-\nEchelon Techniques”, John Wiley & Sons, New York, 1992. \n[12]  E. A. Silver, D. F. Pyke and R. Peterson, “Inventory management and \nproduction planning and scheduling”, 3\nrd edition, 1998. \n[13]  D. B. Tsao and T. Enkawa, “Optimal second replenishment policy in \ntwo-phased push control system”, Journal of Operational Research \nSociety Japan, v35, 273-289, 1992. \n[14]  V. D. R. Guide Jr. and R. Srivastava, “Repairable inventory theory: \nModels and applications”, European Journal of Operational Research, \nv102, 1-20, 1997. \n \n  \n \n \n \n \n \nHoong Chuin LAU received the B.S. and M.S. degrees from the University \nof Minnesota  in 1987 and 1988 respectively, and the D.Eng. degree from \nTokyo Institute of Technology, Japan in 1996.  He is currently Associate \nProfessor of Information Systems at the Singapore Management University, \nand holds a concurrent appointment as Director of Defense Logistics at The \nLogistics Institute Asia Pacific, National University of Singapore. His \nresearch interests are situated at the intersection of artificial intelligence and \noperations research, with application to planning and scheduling in large-scale \ntransportation, logistics and supply chain management. To date, he has \npublished more than 90 papers in journals and international conferences. Part \nof his research has resulted in innovative tools and \nsystems for industry, and one of such tools won the \nSingapore National Innovation and Quality Circles \nStar Award in 2006.  He serves actively in program \ncommittees of international conferences, including \nthe IEEE/WIC/ACM International Conference on \nIntelligent Agent Technology, IEEE Conference on \nAutomation Science and Engineering, and \nInternational Conference on Automated Planning \nand Scheduling (ICAPS). \n \n \n \nJie PAN received his B.Eng. and M.Eng. degrees from Beihang University, \nBeijing, China, in 1999 and 2002 respectively. He worked as research \nengineer in the School of Information Systems at Singapore Management \nUniversity, in 2009. He is currently Ph.D. candidate of Industrial and Systems \nEngineering at National University of Singapore. His research interests are \nsituated at operations research, computational game theory with application to \nlogistics and supply chain management. \n \n  \nHuawei Song received his B.S. from Fudan University, Shanghai, China, in \n2000, and M.S. from National University of Singapore in 2002. He worked as \na research engineer in The Logistics Institute - Asia \nPacific in 2002-2006, where he participated in the \nJAGUAR defense logistics projects. He is currently a \nPh.D. candidate of Decision Science and Engineering \nSystems at Rensselaer Polytechnic Institute. His \nresearch interests are suited at statistics, stochastic \nprocess, Monte Carlo simulation, and optimization \nwith application to system reliability, risk \nmanagement, logistics and supply chain management. \nTABLE A2 \nCOMPUTATION PERFORMANCE FOR DIFFERENT REPLENISHMENT HORIZONS \nReplenishment horizon H \n(period) \nComputation time \n(millisecond) \n30 47 \n50 94 \n80 156 \n100 219 \n200 641 ",
            "id": 5556778,
            "identifiers": [
                {
                    "identifier": "2151200858",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "oai:ink.library.smu.edu.sg:sis_research-1212",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "190688638",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1109/tase.2010.2040827",
                    "type": "DOI"
                },
                {
                    "identifier": "13242125",
                    "type": "CORE_ID"
                }
            ],
            "title": "Periodic Resource Reallocation in Two-Echelon Repairable Item Inventory Systems",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2151200858",
            "oaiIds": [
                "oai:ink.library.smu.edu.sg:sis_research-1212"
            ],
            "publishedDate": "2010-01-01T08:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 5985127,
                    "title": "[10] OPUS10 User’s Reference – Logistics Support and Spares Optimization, Systecon AB,",
                    "authors": [],
                    "date": "1998",
                    "doi": null,
                    "raw": "[10]  OPUS10 User’s Reference – Logistics Support and Spares Optimization, Systecon AB, May 1998.",
                    "cites": null
                },
                {
                    "id": 5985124,
                    "title": "Analysis of a two-echelon inventory control system with complete redistribution”,",
                    "authors": [],
                    "date": "1987",
                    "doi": null,
                    "raw": "[7]  H. Jönsson and E. A. Silver, “Analysis of a two-echelon inventory control system with complete redistribution”, Management Science, v33, 215-227, 1987.",
                    "cites": null
                },
                {
                    "id": 5985132,
                    "title": "degrees from the University of Minnesota in 1987 and",
                    "authors": [],
                    "date": "1988",
                    "doi": null,
                    "raw": "Hoong Chuin LAU received the B.S. and M.S. degrees from the University of Minnesota  in 1987 and 1988 respectively, and the D.Eng. degree from Tokyo Institute of Technology, Japan in 1996.  He is currently Associate Professor of Information Systems at the Singapore Management University, and holds a concurrent appointment as Director of Defense Logistics at The Logistics Institute Asia Pacific, National University of Singapore. His research interests are situated at the intersection of artificial intelligence and operations research, with application to planning and scheduling in large-scale transportation, logistics and supply chain management. To date, he has published more than 90 papers in journals and international conferences. Part of his research has resulted in innovative tools and systems for industry, and one of such tools won the Singapore National Innovation and Quality Circles Star Award in 2006.  He serves actively in program committees of international conferences, including the IEEE/WIC/ACM International Conference on Intelligent Agent Technology, IEEE Conference on Automation Science and Engineering, and International Conference on Automated Planning and Scheduling (ICAPS).",
                    "cites": null
                },
                {
                    "id": 5985120,
                    "title": "Dynamic Allocation Heuristic for Centralized Safety Stock”,",
                    "authors": [],
                    "date": "2005",
                    "doi": null,
                    "raw": "[2]  D. B. Cao and E. A. Silver, “Dynamic Allocation Heuristic for Centralized Safety Stock”, Naval Research Logistics, v52:6, 513-526, 2005.",
                    "cites": null
                },
                {
                    "id": 5985125,
                    "title": "Evaluation of TimeVarying Availability in Multi-Echelon Spare Parts Systems with Passivation”,",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "[8]  H. C. Lau, H. Song, C.T. See and S.Y. Cheng, “Evaluation of TimeVarying Availability in Multi-Echelon Spare Parts Systems with Passivation”, European Journal of Operational Research, v170:1, 91-105, 2006.",
                    "cites": null
                },
                {
                    "id": 5985134,
                    "title": "Huawei Song received his B.S. from Fudan University,",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "Huawei Song received his B.S. from Fudan University, Shanghai, China, in 2000, and M.S. from National University of Singapore in 2002. He worked as a research engineer in The Logistics Institute - Asia Pacific in 2002-2006, where he participated in the JAGUAR defense logistics projects. He is currently a Ph.D. candidate of Decision Science and Engineering Systems at Rensselaer Polytechnic Institute. His research interests are suited at statistics, stochastic process, Monte Carlo simulation, and optimization with application to system reliability, risk management, logistics and supply chain management. TABLE A2 COMPUTATION PERFORMANCE FOR DIFFERENT REPLENISHMENT HORIZONS Replenishment horizon H (period) Computation time (millisecond) 30 47 50 94 80 156 100 219 200 641",
                    "cites": null
                },
                {
                    "id": 5985129,
                    "title": "Inventory management and production planning and scheduling”, 3 rd edition,",
                    "authors": [],
                    "date": "1998",
                    "doi": null,
                    "raw": "[12]  E. A. Silver, D. F. Pyke and R. Peterson, “Inventory management and production planning and scheduling”, 3 rd edition, 1998.",
                    "cites": null
                },
                {
                    "id": 5985121,
                    "title": "Models for multi-echelon repairable item inventory systems with limited repair capacity”,",
                    "authors": [],
                    "date": "1997",
                    "doi": null,
                    "raw": "[3]  A. Díaz and M. C. Fu, “Models for multi-echelon repairable item inventory systems with limited repair capacity”, European Journal of Operational Research, v97, 480-492, 1997. [4]  G. D. Eppen and L. Schrage, “Centralized ordering policies in a multiwarehouse system with lead times and random demand”, In L. B.",
                    "cites": null
                },
                {
                    "id": 5985126,
                    "title": "Multi-Echelon Repairable Item Inventory System with Limited Repair Capacity under Nonstationary Demands”,",
                    "authors": [],
                    "date": "2008",
                    "doi": null,
                    "raw": "[9]  H. C. Lau and H. Song, “Multi-Echelon Repairable Item Inventory System with Limited Repair Capacity under Nonstationary Demands”, International Journal of Inventory Research, 1(1), 67-92, 2008.",
                    "cites": null
                },
                {
                    "id": 5985128,
                    "title": "Optimal Inventory Modeling of System: MultiEchelon Techniques”,",
                    "authors": [],
                    "date": "1992",
                    "doi": null,
                    "raw": "[11]  C. C. Sherbrooke, “Optimal Inventory Modeling of System: MultiEchelon Techniques”, John Wiley & Sons, New York, 1992.",
                    "cites": null
                },
                {
                    "id": 5985130,
                    "title": "Optimal second replenishment policy in two-phased push control system”,",
                    "authors": [],
                    "date": "1992",
                    "doi": null,
                    "raw": "[13]  D. B. Tsao and T. Enkawa, “Optimal second replenishment policy in two-phased push control system”, Journal of Operational Research Society Japan, v35, 273-289, 1992.",
                    "cites": null
                },
                {
                    "id": 5985119,
                    "title": "Optimization of multi-echelon repairable item inventory systems with simultaneous location of repair facilities”,",
                    "authors": [],
                    "date": "1997",
                    "doi": null,
                    "raw": "[1]  P. Alfredsson, “Optimization of multi-echelon repairable item inventory systems with simultaneous location of repair facilities”, European Journal of Operational Research, v99, 584-595, 1997.",
                    "cites": null
                },
                {
                    "id": 5985133,
                    "title": "received his B.Eng. and M.Eng. degrees from Beihang University,",
                    "authors": [],
                    "date": "1999",
                    "doi": null,
                    "raw": "Jie PAN received his B.Eng. and M.Eng. degrees from Beihang University, Beijing, China, in 1999 and 2002 respectively. He worked as research engineer in the School of Information Systems at Singapore Management University, in 2009. He is currently Ph.D. candidate of Industrial and Systems Engineering at National University of Singapore. His research interests are situated at operations research, computational game theory with application to logistics and supply chain management.",
                    "cites": null
                },
                {
                    "id": 5985131,
                    "title": "Repairable inventory theory: Models and applications”,",
                    "authors": [],
                    "date": "1997",
                    "doi": null,
                    "raw": "[14]  V. D. R. Guide Jr. and R. Srivastava, “Repairable inventory theory: Models and applications”, European Journal of Operational Research, v102, 1-20, 1997.",
                    "cites": null
                },
                {
                    "id": 5985123,
                    "title": "Risk pooling in a two-period, twoechelon inventory stocking and allocation problem”,",
                    "authors": [],
                    "date": "1989",
                    "doi": null,
                    "raw": "[6]  P. L. Jackson and J. A. Muckstadt, “Risk pooling in a two-period, twoechelon inventory stocking and allocation problem”, Naval Research Logistics, v36, 1-26, 1989.",
                    "cites": null
                },
                {
                    "id": 5985122,
                    "title": "Stock allocation in a two-echelon distribution system or what to do until your ship comes in”,",
                    "authors": [],
                    "date": "1988",
                    "doi": null,
                    "raw": "[5]  P. L. Jackson, “Stock allocation in a two-echelon distribution system or what to do until your ship comes in”, Management Science, v34, 880-895, 1988.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [],
            "updatedDate": "2022-01-12T17:27:58",
            "yearPublished": 2010,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "1545-5955"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/13242125.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/13242125"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/13242125/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/13242125/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/5556778"
                }
            ]
        },
        {
            "acceptedDate": "2010-04-20T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Feller, Frank"
                },
                {
                    "name": "Gunreben, Sebastian"
                },
                {
                    "name": "Kohn, Martin"
                },
                {
                    "name": "Mifdaoui, Ahlem"
                },
                {
                    "name": "Sab, Detlef"
                },
                {
                    "name": "Scharf, Joachim"
                },
                {
                    "name": "Sommer, Jörg"
                }
            ],
            "contributors": [
                "Universität Stuttgart (GERMANY)",
                "Institut Supérieur de l'Aéronautique et de l'Espace - ISAE-SUPAERO (FRANCE)"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/12040594"
            ],
            "createdDate": "2013-07-19T10:13:33",
            "dataProviders": [
                {
                    "id": 437,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/437",
                    "logo": "https://api.core.ac.uk/data-providers/437/logo"
                }
            ],
            "depositedDate": "2010-01-01T00:00:00",
            "abstract": "During the last decades, Ethernet progressively became the most widely used local area networking (LAN) technology. Apart from LAN installations, Ethernet became also attractive for many other fields of application, ranging from industry to avionics, telecommunication, and multimedia. The expanded application of this technology is mainly due to its significant assets like reduced cost, backward-compatibility, flexibility, and expandability. However, this new trend raises some problems concerning the services of the protocol and the requirements for each application. Therefore, specific adaptations prove essential to integrate this communication technology in each field of application. Our primary objective is to show how Ethernet has been enhanced to comply with the specific requirements of several application fields, particularly in transport, embedded and multimedia contexts. The paper first describes the common Ethernet LAN technology and highlights its main features. It reviews the most important specific Ethernet versions with respect to each application field’s requirements. Finally, we compare these different fields of application and we particularly focus on the fundamental concepts and the quality of service capabilities of each proposal",
            "documentType": "research",
            "doi": "10.1109/surv.2010.021110.00086",
            "downloadUrl": "https://core.ac.uk/download/12040594.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "  \nOpen Archive Toulouse Archive Ouverte (OATAO)  \nOATAO is an open access repository that collects the work of Toulouse researchers \nand makes it freely available over the web where possible.  \nThis is an author -deposited version published in: http://oatao.univ-toulouse.fr/  \nEprints ID: 2236 \nTo link to this article: DOI: 10.1109/SURV.2010.021110.00086 \nURL: http://dx.doi.org/10.1109/SURV.2010.021110.00086\nTo cite this version: SOMMER Jörg, GUNREBEN Sebastian, MIFDAOUI Ahlem, \nFELLER Frank,  KOHN Martin, SAB Detlef, SCHARF Joachim. Ethernet - a survey on \nits fields of application. IEEE communications surveys and tutorials, 2010, vol 12 (n° 2). \npp. 263-284. \nISSN 1553-877X.  \nAny correspondence concerning this service should be sent to the repository administrator:\nstaff-oatao@inp-toulouse.fr \n \n1Ethernet – A Survey on its Fields of Application\nJo¨rg Sommer, Sebastian Gunreben, Frank Feller, Martin Ko¨hn, Detlef Saß, Joachim Scharf\nUniversity of Stuttgart, Institute of Communication Networks and Computer Engineering\nPfaffenwaldring 47, 70569 Stuttgart, Germany\nEmail: joerg.sommer@ikr.uni-stuttgart.de\nAhlem Mifdaoui\nToulouse University, ISAE\n1 place Emile Blouin, 31056 Toulouse, France\nEmail: ahlem.mifdaoui@isae.fr\nAbstract—During the last decades, Ethernet progressively\nbecame the most widely used local area networking (LAN)\ntechnology. Apart from LAN installations, Ethernet became\nalso attractive for many other fields of application, ranging\nfrom industry to avionics, telecommunication, and multimedia.\nThe expanded application of this technology is mainly due to\nits significant assets like reduced cost, backward-compatibility,\nflexibility, and expandability. However, this new trend raises\nsome problems concerning the services of the protocol and the\nrequirements for each application. Therefore, specific adaptations\nprove essential to integrate this communication technology in\neach field of application.\nOur primary objective is to show how Ethernet has been\nenhanced to comply with the specific requirements of several\napplication fields, particularly in transport, embedded and mul-\ntimedia contexts. The paper first describes the common Ethernet\nLAN technology and highlights its main features. It reviews\nthe most important specific Ethernet versions with respect to\neach application field’s requirements. Finally, we compare these\ndifferent fields of application and we particularly focus on the\nfundamental concepts and the quality of service capabilities of\neach proposal.\nI. INTRODUCTION\nIn 1972, Robert Metcalfe and his colleagues at the Xerox\nPalo Alto Research Center developed a local area network\n(LAN) technology to interconnect stations, servers, and pe-\nripheral devices within the same building using a common\nbus system [1]. This was the birth of the first version of\nEthernet. They improved the throughput of the installed Aloha\nnetwork [2] by a collision detection algorithm [3] and filed\nthe corresponding patent in 1977 [4]. This was the creation of\nEthernet’s media access control protocol carrier sense multiple\naccess with collision detection (CSMA/CD, [5]).\nTogether with Intel and DEC, Xerox first published a mod-\nified Ethernet version in 1980, which became the well-known\nDIX Ethernet II version. The first products were available\nin the early 1980s. In 1985, the IEEE began to standardize\nthe different versions of Ethernet. They avoided the brand\nname Ethernet and named the technology 802.3 CSMA/CD [1]\ninstead.\nUntil the mid 1990-ies, several other LAN technologies\nco-existed with Ethernet, including token ring [6], fiber dis-\ntributed data interface (FDDI, ANSI X3T9.5), and attached\nresource computer network (ARCnet, ANSI/ATA 878.1-1999).\nHowever, Ethernet succeeded in becoming the dominant LAN\ntechnology for companies as it quickly adapted to upcoming\nrequirements.\nThe main requirements of a LAN technology for a com-\npany are planning reliability, future proof hardware, and easy\nmanagement (i. e. plug-and-play capabilities). This especially\nincludes scalability with respect to network size and link speed\nas well as simple migration. Consequently, the fast band-\nwidth evolution in the mid/late 1990-ies while maintaining\nbackward-compatibility laid the fundamentals of Ethernet’s\nsuccess story. Since its invention, Ethernet’s line rate has\nevolved from 2.94 Mbps to higher rates: 10 Mbps in the early\n1980s (DIX Ethernet II), 100 Mbps in 1995 (IEEE 802.3u),\n1 Gbps in 1998 (IEEE 802.3z, [5]), and 10 Gbps in 2002\n(IEEE 802.3ae). Currently, the IEEE P802.3ba task force is\nworking on 40 Gbps and 100 Gbps [7]. Figure 1 depicts this\nbandwidth growth.\nIn addition to installations limited to single departments of\nlarge companies, Ethernet became attractive for campus-wide\ndeployments as well as for small companies/offices (SOHO).\nWith an increasing penetration of permanent broadband Inter-\nnet access, the same applies to residential users. Ethernet’s\neasy adaptability enabled this success in a wide range of\nsurroundings. For residential users, Ethernet offers plug-n-\nplay operation, requiring hardly any manual configuration.\nIn corporate networks, Ethernet enables highly sophisticated\nnetworking as described in the next section. Similarly, Ethernet\ndevices range in their complexity and functionality from\nsimple, widely available mass-market products without much\nfunctionality (apart from frame forwarding) to specialized,\nhigh-performance and feature-rich devices.\nBesides wired interconnection, Ethernet also supports the\ninterconnection of wireless technologies, e. g. IEEE 802.11\ncompliant WiFi [8]. Ethernet seamlessly integrates wired and\nwireless technologies, since they show both the same logical\nlink layer interface.\nThis flexibility of Ethernet is its key to success in the\nenterprise and SOHO networks. Furthermore, thanks to its low\ncost, long lifetime, and easy integration with other network\ntechnologies, Ethernet has become an attractive option in\nother areas, too. Ethernet’s promise of being inexpensive, both\nin terms of mass-market equipment and simple operation,\nand its predominance in customer LANs make it interesting\nfor transport network operators. Therefore, Ethernet is an\n210 Gbps\n100 Gbps\n100 Mbps\n10 Mbps\n1973 1980 1990 2000 2010\nBit rate\n1 Gbps\nXerox LAN bus\n3 Mbps 10Base2\nFast Ethernet\nGigabit Ethernet\n100 GbE\n10 GbE\nCSM\nA/C\nD s\nyste\nm\nIEE\nE 80\n2.3 \nStd.\nDIX\n Eth\nerne\nt\nMA\nC B\nridg\nes\nVLA\nN &\nTra\nffic \nclas\nses\nShared Medium Point-to-Point\nFie\nld \nof\nap\npli\ncat\nion\nIndu\nstria\nl\nAVBAvi\nonic\ns\nAcc\ness\nCar\nrier\n Gra\nde\nAuto\nmot\nive\nInte\nrcon\nnec\nt \nserv\ners,\n sta\ntion\ns \nand\n per\niphe\nral\nDep\nartm\nent-\nwid\ne in\nstal\nlatio\nns\nCam\npus\n-wid\ne\ninst\nalla\ntion\ns\nTo\nday\nEn\nha\nnc\nem\nen\nt\nFig. 1. Ethernet’s story of success, its enhancements, and its fields of\napplication\nattractive replacement for metro and core network technologies\nas well as an access technology.\nWhile manufacturing companies interconnect their offices\nusing Ethernet, inter-machine communication uses fieldbuses.\nConvergence of both networks requires a common communi-\ncation infrastructure such as Ethernet in the industrial field.\nThe environments of maritime and railway applications are\nsimilar to the industrial environment. We consider them as\nspecial cases of industrial Ethernet.\nLightweight devices, mass-market products, and high band-\nwidth communication enabled the success of Ethernet in\naircraft. In addition, multimedia devices need interconnection\nin various areas of our life. The technology discussed to\ninterconnect these devices at home or inside a vehicle is\nEthernet.\nWe classify these fields of application of Ethernet according\nto their characteristics and requirements as depicted in figure 2.\nEthernet started as a local network technology in corporate\nnetworks and SOHO. From there it evolved towards public\nand private networks. The public networks include operator\nnetworks (carrier grade Ethernet) and Ethernet in the first\nmile. Private networks include embedded networks (industrial,\navionic, automotive) as well as the multimedia networks.\nSince private organizations manage these networks, they show\nsealed-off characteristics. The home multimedia networks\nactually fall in-between LANs and embedded networks. They\nshow almost all characteristics of the original Ethernet LAN,\nbut are also close to embedded networks as they are privately\nadministrated.\nIn this paper, we introduce Ethernet in different fields of\napplication and show its according evolution. We first give an\nintroduction to the common Ethernet LAN technology, used\nand formed primarily in corporate networks, and highlight the\ndrivers of this technology in section II. This is the basis for\nthe extended Ethernet versions.\nWe show the modified versions of Ethernet for transport\nHome Multimedia Networks \n▪ AVB\nLocal Area Networks\n▪ Corporate LAN\n▪ SOHO\nTransport Networks\n▪ Carrier Grade\n▪ Ethernet in the First Mile\nEmbedded Networks\n▪ Industrial\n▪ Avionics\n▪ Automotive\nFig. 2. Classification of Ethernet’s fields of application\nnetworks, namely metro and core networks, and Ethernet in\nthe first mile in sections III and IV, respectively. We address\nEthernet in embedded environments with safety and time-\ncritical applications like machine control, aircraft, and in-\nvehicle communication in sections V, VI, and VII, respec-\ntively. A further application field of Ethernet, currently in\nthe standardization process, is the audio and video domain\ndescribed in section VIII. In each of these sections, we\nintroduce the constraints and requirements of the respective\nfield of application. In section IX, we compare and contrast\nthese different versions of Ethernet with respect to common\ncharacteristics and major differences. We conclude this paper\nin section X.\nII. ETHERNET LAN TECHNOLOGY\nThis section introduces Ethernet LAN technology and its\nmain features. The main driver for Ethernet is its easy ap-\nplication in corporate and residential networks. Some of the\nfeatures of Ethernet are directly coupled with the needs in\ncorporate networks. Figure 1 shows the Ethernet features and\ntheir time of invention.\nWe first introduce the corresponding standards and the\nprotocol stack of Ethernet. In a bottom up approach, we then\nintroduce the physical layer and the MAC framing layer and\nfocus on the Ethernet features on each layer. The Ethernet\nbridging functionality and its concepts for routing and for-\nwarding of frames close this section.\nA. The IEEE 802 family\nFigure 3 classifies the Ethernet protocol stack with respect to\nthe lower layers of the ISO/OSI reference model. IEEE 802.3\nEthernet splits the functionality of the data link layer (DLL)\nin three different parts: the logical link control (LLC, IEEE\n802.2, [9]), the bridge layer (IEEE 802.1, [10]), and the media\naccess control layer (MAC, IEEE 802.3, [5]). IEEE 802.3 also\nspecifies the physical layer of the ISO/OSI reference model,\nPHY in Ethernet. Ethernet PHY offers a media independent\ninterface (xMII) to the MAC layer and a media dependent\ninterface (MDI) to the physical media. Between these layers,\nthe physical coding (PCS), the physical medium attachment\n(PMA), and physical medium dependent (PMD) layers reside.\nFor the detailed functionality of each of these layers we refer\nto IEEE 802.2 [9] and [1] as they are out of scope of this\npaper.\n3Fig. 3. Ethernet protocol stack including its functions\nB. Physical Layer\nThis section introduces the available physical media of\nEthernet, provides a short review on the evolution of the line\nrates, and auto-X features of the physical layer. IEEE 802.3 [5]\nserves as a reference for the different media.\n1) Media and Line Rates: In the early days, Ethernet\ntechnology offered a line rate of 10 Mbps using a common\nbus applying coaxial cabling. This setup was the standard for a\nlong time in the LAN. Besides coaxial cabling, IEEE 10Base-F\nalso enabled an optical transmission.\nEthernet evolved towards higher speeds of 100 Mbps (Fast\nEthernet) using twisted pair cabling or fiber. IEEE 100Base-\nTX/T4 refers to shielded and unshielded twisted pair cabling\nwith a maximum range of about 100 m using category 4 or 5\ncables. Besides copper, the IEEE also specified single-mode\nand multi-mode fiber usage. This extended the range towards\nseveral kilometers.\nThe next evolution step was 1 Gbps (Gigabit Ethernet).\nThe corresponding technologies are 1000Base-LX/SX defining\nlong-range and short-range types of single and multi-mode\nfibers each using a dedicated wavelength. Besides the optical\nmedia, Gigabit Ethernet also supports copper cables. For the\nlater 10 Gbps (10 Gigabit Ethernet) the same media types are\navailable, even on copper basis as defined by 10GBase-CX4.\nWhile Gigabit Ethernet still allows half-duplex mode and\nCSMA/CD, 10 Gigabit Ethernet operates in full duplex mode\nonly. An IEEE working group currently specifies Ethernet with\n40 Gbps and 100 Gbps line rate in IEEE 802.3ba [7].\n2) Auto-negotiation: Ethernet devices up to Gigabit\nEthernet are backward compatible, supporting down to\n10 Mbps. Two interconnected devices of different line rates\nagree on a common line rate. The auto-negotiation feature of\nthe Ethernet PHY provides this functionality. It resides in the\nlower part of the PHY if copper cabling is used (on the right in\nfigure 3). For optical attachment units, the functionality resides\nwithin the physical coding sub layer (on the left in figure 3).\nThe auto-negotiation process modulates link pulses ex-\nchanged on idle Ethernet links with line rate information.\nUpon reception of the pulses, both devices determine the\ncommon line rate and duplex mode.\n3) Auto-crossover: Connecting two Ethernet devices back-\nto-back required a special crossover cable for copper 10Base-\nT and 100Base-T Ethernet for correct pin coupling. With\n1000Base-T, the IEEE proposed the additional feature of auto-\ncrossover to eliminate the need of these special cables [5].\nA transceiver-chip internal crossover within the PHY device\nenables the auto-crossover function. The auto-configuration\nof the media dependent interface (MDI/MDI-X1) enables\nthis functionality if the device supports it. If none of the\ninterconnected devices supports auto-crossover, coupling with\ncrossover cables is necessary.\nC. MAC Framing\nThis section gives an overview on the most common\nEthernet frame formats defined in IEEE 802.3. In addition,\nwe highlight the Ethernet frame extension for virtual LANs of\nIEEE 802.1Q. Figure 4 depicts these various frame formats.\nWe start describing common fields using the Ethernet II frame\nformat.\nThe preamble (Pre) serves for synchronization between\nsender and receiver using an alternating bit sequence. After\nthe preamble, the start of frame delimiter (SFD) indicates\nthe start of the frame. The destination (DA) as well as the\nsource address (SA) are 6 byte globally unique addresses\nadministered by the IEEE or locally unique addresses assigned\nby the local network administrator. Besides unicast addresses,\nEthernet defines broadcast and multicast addresses [5]. The\ndata field contains the payload of the frame. It has a minimum\nsize of 46 byte required for collision detection by CSMA/CD.\nIn case of less payload, the MAC driver adds padding data. The\nlast field, the frame check sequence (FCS), also called cyclic\nredundancy check (CRC), contains 4 byte for error detection.\nThe 2 byte following the source address differentiate various\nEthernet versions. We discuss them in the next two sections.\n1) Frame Formats: The meaning of the 2 byte type/length\nfield (T resp. L) is context dependent. It either identifies the\ntype of the payload protocol data unit or the size of the MAC\nframe.\nIn Ethernet II, T defines the type of the protocol in the\npayload field. The Ethernet PHY determines the end of a frame\non the wire and provides this information to the MAC layer.\nAs the payload PDU indicates the data length anyhow, it is\nnot necessary to provide the length of the MAC frame in the\nMAC frame itself.\nIn contrast, the IEEE promotes the encapsulation of the\npayload PDU in a MAC-layer independent LLC frame. It\nthereby guarantees the interworking of different underlying\nLAN technologies. In this case, the MAC frame provides\ninformation on the length of the LLC frame (field L in the\n802.3 frame of figure 4). The service, i. e. the applied protocol,\nis then determined by the destination service access point\n(DSAP) and the source service access point (SSAP) in the\n1MDI-X stands for an MDI with crossover wiring\n4Fig. 4. Ethernet IEEE 802.3 frame formats\nLLC frame. The 1 byte control field (CTL) completes this\ninformation [9].\nAs there are only 256 possible SAP values available and\nthe number of protocols in use is much higher, the IEEE\ndefined special values for both DSAP and SSAP. They use\nthe value of 0xAA for DSAP/SSAP and 0x03 for the control\nfield to indicate additional layers after the LLC header. This\nsubnetwork access point (SNAP) indicates the vendor with a\n3 byte organizational unique identifier (OUI) and a 2 byte\npayload type field (TYPE). For compatibility reasons, today’s\ndevices distinguish both cases by the value of the type/length\nfield [11]:\n• a value of less than 0x0600 (153610) indicates the LLC\nframe size.\n• larger values indicate the network protocol type, e. g.\n0x0800 for the Internet protocol (IP).\nAs an example, an Ethernet frame carrying an IP packet\nlooks different depending on the Ethernet version. Ethernet II\nindicates 0x0800 in the T field, in IEEE 802.3 the SAP values\nare either 0x06 for IP or 0xAA indicating a SNAP header of\ntype 0x0800.\nFor completeness, we also mention the Novell Ethernet\nversion for the transport of Novell’s IPX [12]. It interprets the\ntype field as a length field followed by Novell’s IPX protocol\nwithout a LLC header. As the first byte of the IPX protocol\n(0xFF) is different from all possible values of the DSAP\nfield, a distinction of such Ethernet frames on the same LAN\nsegment is possible. Besides this raw encapsulation, Ethernet\ncan also transport IPX packets in an LLC frame or a special\nSNAP frame [5], [9].\nThe most dominant Ethernet version in today’s LANs is\nEthernet II, with the type/length field indicating the payload\ntype.\n2) Virtual LAN Extension: For traffic engineering purposes\nand security reasons, the IEEE extended the Ethernet frame in\nIEEE 802.1Q [13] in 1998. The extension enables a virtual\nseparation of multiple LANs on the same physical infras-\ntructure. The last row of Figure 4 shows this frame format\nextension.\nThe extension consists of an additional 4 byte field inserted\nin front of the Ethernet type field. Its first 2 byte indicate\nthe tag protocol identifier (TPID), while the second 2 byte\nrepresent the tag control information (TCI) field. The TPID\ncorresponds to the type/length field in Ethernet II and its value\nof 0x8100 identifies an IEEE 802.1Q frame. The TCI field\ncontains a VLAN identifier (12 bit), priority information (3 bit)\nand one bit canonical format indicator. The VLAN identifier\nindicates the VLAN the frame belongs to. The priority infor-\nmation indicates the user priority class. IEEE 802.1Q defines\neight different priority classes considered at the schedulers of\ninterworking units. This header extension allows an increased\noverall Ethernet frame size of 1522 byte.\nFor further reading we refer to [14] and [15] for an in-depth\ndiscussion of this concept.\nD. Optional MAC Layers\nThe IEEE standard allows optional features in the MAC\nlayer above the MAC framing functionality (cf. figure 3,\n[5]). This includes the MAC control and the link aggregation\nfunctions. One representative of the MAC control is the flow\ncontrol function. In the next subsections, we address this\nfeature and the link aggregation function.\n1) Flow Control: IEEE 802.3 defines the special value\n0x8808 for the type field (TYPE) to indicate MAC control\nframes. Within the MAC control frame an opcode distin-\nguishes different operations. One operation is the Ethernet flow\ncontrol.\nEthernet flow control is only available between directly\nconnected devices. It enables an overloaded device to ask the\nneighboring device to interrupt sending before it has to drop\nframes. The Ethernet device sends an Ethernet control frame\nto the neighbor station using a globally assigned multicast\naddress2 and the opcode for the MAC control pause operation.\n2Ethernet control frame multicast address: 01-80-c2-00-00-01\n5The frame body indicates the time to wait prior to resuming\nto send frames. This option is only applicable in full duplex\nmode and when both devices are capable of receiving these\npause frames. This is determined during the auto-negotiation\nphase.\nAs this feedback mechanism may interfere with higher layer\nprotocols’ feedback mechanisms, e. g. the congestion control\nof the transmission control protocol (TCP), Ethernet flow\ncontrol is mostly switched off in managed corporate networks.\nFeuser and Wenzel studied this topic in [16] and quantified\nthe impact on higher protocol layers’ feedback mechanisms.\n2) Link Aggregation: Ethernet link aggregation is another\noptional feature for full duplex capable Ethernet devices (IEEE\n802.3, [5], sec. 43). It introduces an additional abstraction\nlayer between the logical link control layer and the optional\nMAC control layer (cf. figure 3).\nLink aggregation provides a single logical interface to\nparallel physical links between two devices. This increases the\nlink availability and enables a linear increase in the bandwidth.\nNetwork administration may configure link aggregation man-\nually or apply the link aggregation control protocol (LACP,\nIEEE 802.3ad) for automatic discovery of parallel links.\nUsage of parallel links may cause out of sequence arrivals.\nIn general, higher layer protocols, e. g. TCP, suffer from out-\nof-sequence arrivals. Therefore, the link aggregation mecha-\nnism should avoid this, but the standard does not propose any\nmechanism. Hash functions on the source/destination pairs are\ncommon methods to ensure this.\nFor further reading, we refer to Watanabe et al. They show\nin [17] the benefits and cost savings using Ethernet link\naggregation in a computer cluster.\nE. Bridging Layer\nThis section discusses the functions of the IEEE 802.1\nEthernet bridging layer. Thereby, we focus on the inter-\nworking aspect of Ethernet. We first introduce the available\nEthernet network elements and common topologies. Frame\nforwarding and the spanning tree protocol discussed next are\nelementary principles in an Ethernet LAN. The port authenti-\ncation concept closes this section.\n1) Network Elements: Attenuation limits the spatial exten-\nsion of an Ethernet LAN segment. For each medium, the stan-\ndard defines the corresponding maximum spatial extension. In\nformer days, repeaters3 were used to interconnect segments\nup to the maximum extension of a collision domain. Today,\ntransparent bridges realize LAN segment interconnects. They\ninterwork on MAC layer and limit the collision domains to\nsingle segments. Switch4 products realize this functionality\ntoday.\nDuring the last decades, the number of stations in a LAN\nincreased. As an increasing number of stations reduces the net-\nwork throughput due to collisions, the LAN topology changed.\nToday, bridges connect stations directly using twisted pair or\nfiber cabling forming a physical star topology. They operate\nin full duplex mode avoiding collisions and do not apply\n3hubs are multi-port repeaters\n4switches are multi-port bridges\nCSMA/CD. Interconnected bridges form a mesh topology.\nHence, Ethernet evolved from a bus topology to a so-called\nmicro-segmented network with full-duplex links between sta-\ntions and bridges.\n2) Frame Forwarding: The main task of the bridges is\nframe forwarding. Thereby, the destination MAC address\ndetermines the forwarding decision, i. e. the outgoing port.\nA MAC address learning algorithm enables the mapping of\ndestination MAC address and corresponding port [10], [1].\nThis section introduces both the bridge learning algorithm and\nthe switch architectures deployed today.\nOn each received frame, bridges perform the following\nlearning algorithm. They store the source MAC address to-\ngether with the receiving port identifier in a forwarding\ndatabase (FDB). In addition, they maintain a timestamp for\neach entry to adapt to network changes. The forwarding\ndecision of a frame bases on the FDB look-up for the DA.\nIf the DA is already in the FDB, bridges forward the frame to\nthe corresponding port. If the database look-up fails, bridges\nforward the frame to all ports except the receiving one, i. e.\nthey flood it through the network.\nToday’s bridges may follow one of two widely deployed\nforwarding principles: store-and-forward or cut-through. The\nstore-and-forward technique receives the frame completely and\nforwards it after frame processing. Therefore, it is able to\ncheck frame integrity and to apply functions on the encapsu-\nlated protocols. The cut-through technique forwards the frame\nas soon as possible after the reception of parts of the Ethernet\nheader. Thus, it is in general not able to check frame integrity.\nThe cut-through technique shows a smaller forwarding\nlatency than the store-and-forward technique. As neither tech-\nnique is able to guarantee any bounded latency, only relative\ntiming guarantees are feasible using priorization. Today’s\ndevices mostly implement the store-and-forwards techniques\nas the benefit of the cut-through technique compared to its\ncomplexity is negligible [15].\n3) Spanning Tree Protocol: As today’s Ethernet networks\nhave meshed topologies, loops may occur, which may cause\ninfinite cycling of frames. As native Ethernet is not capable\nof avoiding loops, Radia Perlman developed the spanning tree\nprotocol (STP, [10]) to solve this problem. It forms a minimum\nspanning tree on a given topology by blocking selected bridge\nports. By assigning weights to links and priorities to bridges,\na network administrator is able to define an arbitrary spanning\ntree on an Ethernet mesh topology, e. g. to satisfy load-\nbalancing requirements.\nThe original spanning tree protocol shows slow convergence\nin tens of seconds. The rapid spanning tree protocol (RSTP)\nimproved the convergence time to below one second [10]. The\nmultiple spanning tree (MSTP) bases on the RSTP and pro-\nvides several instances of the RSTP within VLAN enhanced\nnetworks [13]. Today’s Ethernet networks mostly apply RSTP\nand MSTP.\nFor further reading, we refer to Olifer et al. They discuss in\n[18] the spanning tree protocol as one of the advanced features\nin LANs.\n4) Port Authentication: In native Ethernet, bridges allow\nadding of new stations to free ports. These new stations\n6immediately have access to the network resources. In some\nenvironments, especially corporate networks, adding of unau-\nthorized stations is not desired. In 802.1X [19], the IEEE\ndefines the concept of port authentication serving this purpose.\nA bridge applying port authentication only forwards frames\nof authenticated stations. First, a new station (supplicant) has\nto send an authentication message to the bridge (authenticator)\nusing the extensible authentication protocol (EAP, [20]) encap-\nsulated in Ethernet frames (EAP over LANs, EPOL). A value\nof 0x888E in the type/length field of Ethernet characterizes an\nEPOL message to the predefined EPOL group address5. The\nbridge forwards this authentication message to a centralized\nauthentication server. After successful authentication, the sta-\ntion is able to participate in the LAN, otherwise its frames are\nblocked.\nAs security issues are of major concern in today’s corporate\nnetworks, several authors address this topic and provide in-\ndepth information and tutorials on how to make LANs secure.\nWe refer to two books, [21] and [22] for further reading.\nIII. CARRIER GRADE ETHERNET\nEthernet’s promise of being inexpensive, both in terms of\nmass-market equipment and in terms of simple operation, is\nattractive to transport network operators. As the metro area is\noften considered as the initial deployment target of Ethernet in\nsuch networks, the term metro Ethernet is used synonymously\nfor carrier Ethernet or carrier grade Ethernet.\nThere are two basic approaches to introduce Ethernet into\ncarrier networks. On the one hand, Ethernet services de-\nscribe an Ethernet based interface to corporate customers who\npurchase connectivity between different sites as a transport\nservice. Since the vast majority of corporate LANs rely on\nEthernet, the Ethernet interface renders protocol conversion\nunnecessary. In addition, it enables bandwidth allocation\nin finer granularities than traditional transport services and\nchanges of this bandwidth are feasible without replacement\nof the interface hardware.\nOn the other hand, Ethernet transport stands for a class\nof new packet-oriented transport technologies intended to\nreplace traditional transport networks like synchronous digital\nhierarchy / synchronous optical networks (SDH/SONET). This\nclass of Ethernet transport technologies is manifold. Some\ntechnologies adapt Ethernet to their needs, others are not\nrelated to it at all. Nevertheless, the latter group is also\nconsidered to belong to Ethernet transport as it enables\nEthernet services. Two prominent representatives of this group\nare based on multi-protocol label switching (MPLS, [23]),\nwhich is a supplement to IP introducing connection-oriented\noperation: transport MPLS (T-MPLS, [24]), an MPLS subset\nstandardized by the ITU-T, and the upcoming MPLS transport\nprofile (MPLS-TP, [25]), currently being jointly defined by the\nIETF and the ITU-T.\nThe approaches which actually adapt the Ethernet LAN\ntechnology mainly add header fields, but may also break with\nsome of the most fundamental principles of Ethernet LANs\nlike connectionless operation. Our description of Ethernet\n5EPOL group address: 01-80-C2-00-00-03\ntransport focuses on the most prominent of the Ethernet-based\napproaches.\nA. Ethernet Services\nEthernet services describe the transparent transport of\nEthernet frames through operator networks. The Metro\nEthernet Forum (MEF), an industry alliance founded in 2001\npromoting the deployment of carrier Ethernet, proposes spec-\nifications of such services from the customer’s perspective.\nService descriptions according to MEF 6.1 [26] differentiate\nbetween port-based Ethernet (E) services and Ethernet virtual\n(EV) services. The latter support service multiplexing based\non different VLAN tags. Proposed service topologies include\npoint-to-point E-Line (resp. EV-Line) services, multipoint-to-\nmultipoint E-LAN (resp. EV-LAN) services, and E-Tree (resp.\nEV-Tree) services. The latter allow bi-directional communica-\ntion between one central instance and a number of subsidiaries.\nService attributes defined in MEF 10.1 [27] provide a more\ndetailed specification of these services. Inspired by asyn-\nchronous transfer mode (ATM) service descriptions, they base\non bandwidth profiles indicating four parameters: committed\ninformation rate (CIR), committed burst size (CBS), excess\ninformation rate (EIR) and excess burst size (EBS). Traffic\nconform to CIR and CBS receives guaranteed service; traffic\nexceeding CIR and CBS but conform to EIR and EBS is\nforwarded in a best effort fashion. Complementary to the\nMEF activities, the ITU-T study group 15 specifies Ethernet\nservices from the operator’s perspective, focusing on point-to-\npoint services in G.8011 [28], G.8011.1 [29], and G.8011.2\n[30].\nA wide range of transport technologies are able to support\nEthernet services, from traditional, circuit-switched technolo-\ngies like SDH [31]–[33] and optical transport networks (OTN)\n[34], [35] to more recent packet-switched technologies like\nMPLS and Ethernet transport.\nB. Ethernet Transport\nTransport networks face a number of requirements that\ndiffer from those of a LAN. Scalability is essential since\ntransport networks cover large spatial extensions and a huge\nnumber of attached devices, implying a large address space.\nFlooding is particularly problematic in large networks, since\nreplicated frames may travel long distances to be finally\ndiscarded, thereby wasting transmission capacity. Another\nset of requirements is due to the quality of service (QoS)\nproperties a transport network operator has to guarantee to its\ncorporate customers. Such properties, defined in a service level\nagreement (SLA), include bandwidth, availability, as well as\ndelay and jitter. Mechanisms assuring compliance with SLAs\nare essential as contract penalties may become very expensive.\nAny new transport network technology has to keep up with\nthe standards set by SDH/SONET, the traditional, widely de-\nployed technology. The benchmark for availability is 99.999%\nor above with the notorious upper bound of 50 ms for the\nrestoration time in case of a failure. This implies protection\nor fast restoration mechanisms, along with operations, ad-\nministration, and maintenance (OAM) features allowing the\n7B-DA B-SA B-VID I-SID DA SA S-VID C-VID ET Payload\nDA SA S-VID C-VID ET Payload FCS\nDA SA VID ET Payload FCS\nDA SA ET Payload FCS 802.3 Ethernet frame\n802.1Q VLAN tagging\n802.1ad Provider Bridging\n802.1ah Provider Backbone Bridging\nDA\nB-DA\nSA\nB-SA\nET\nFCS\nVID\nS-VID\nC-VID\nB-VID\nI-SID\nDestination Adress\nBackbone DA\nSource Adress\nBackbone SA\nEtherType\nFrame Check Sequence\nVirtual LAN Identifier\nService VID\nCustomer VID\nBackbonde VID\nBackbone Service Instance Identifier\n802.1Qay Provider Backbone Bridging\nwith Traffic EngineeringSwitching Label\nFCS\nFig. 5. Extensions of the Ethernet frame format\nVLAN for customer\nseparation\nProvider Bridging\n(802.1ad)\nProvider Backbone \nBridging (802.1ah)\nPBB with Traffic\nEngineering (802.1Qay)\nProvider Backbone \nBridging (802.1ah)\nProvider Bridging\n(802.1ad)\nVLAN for network\nmanagement\nPoint-to-Point\ntunnels\nVLAN for network\nmanagement\nVLAN for customer\nseparation\nAggregation Metro Core Metro Aggregation\nCorporate \nLAN 4\nCustomer B\nCorporate \nLAN 1\nCustomer A\nCorporate \nLAN 2\nCustomer B\nCorporate \nLAN 3\nCustomer A\nFig. 6. Exemplary deployment scenario of Ethernet transport technologies\ndetection and localization of failures, as well as performance\nmonitoring. A mechanism mapping frames to the respective\ncustomer, i. e. traffic separation, is required to deliver frames\nto the right interface. In addition, it facilitates SLA compliance\nby enabling the monitoring of QoS parameters on a per\ncustomer basis.\n1) Frame Format and Operation: Basic principles of\nEthernet, such as STP and address learning, enable simple set-\nup and operation of LANs, but are inappropriate to fulfill the\nrequirements stated above [36]. In order to meet those carrier\nrequirements, the IEEE defined an adaptation of Ethernet in\na series of amendments to the 802.1 standard. The extensions\ncomprise additional header fields depicted in figure 5 as well\nas changes of the operational principles.\nTraffic separation is achieved in local Ethernet networks\nby means of virtual LANs, a tagging mechanism. However,\nthe according IEEE standard 802.1Q only permits one tag\nin a frame (cf. figure 5), preventing a carrier from adding\na VLAN tag for customer separation when the customers\nalready use VLANs in their networks. Therefore, the provider\nbridging (PB) extension IEEE 802.1ad [37] allows the stacking\nof VLAN tags (VID), i. e. the introduction of a second, so-\ncalled service VLAN tag (S-VID) in addition to the customer\nVLAN tag (C-VID). Carriers can thus define VLANs within\ntheir transport networks independently of customer VLAN\nassignment. Due to the position of the service VLAN field\nin the Ethernet header, 802.1Q-compliant bridges can perform\nswitching inside a PB network.\nIn large networks, there are issues with Ethernet’s flat\naddressing scheme, which PB does not resolve. Bridges in\nthe carrier network would have to maintain a huge number\nof entries for end devices in their forwarding databases. In\npractice, this means frequent updating of table entries and sub-\nsequent flooding to newly unknown destinations. In addition,\nPB’s service VLAN tags allow for distinction of at most 4094\ncustomers, severely limiting the supportable customer base.\nThe provider backbone bridging (PBB) extension 802.1ah\n[38], approved in June 2008, addresses both issues by ex-\ntending an Ethernet frame with a so-called backbone Ethernet\nheader. Figure 5 shows such an external header, which contains\nbackbone MAC addresses assigned to edge nodes of the PBB\nnetwork, indicating the ingress point and egress point of the\nframe in the PBB network. Consequently, only the backbone\naddresses are visible inside the carrier network, resolving the\nscalability problem related to the flat addressing scheme. A\n24 bit service identifier (I-SID) introduced in addition to the\nbackbone VLAN tag allows the differentiation of a larger\nnumber of customers. As for PB, standard VLAN capable\nbridges are able to operate on PBB headers.\nThe Ethernet control plane remained unchanged from the\nLAN up to PBB. It is based on the spanning tree protocol\n(STP), flooding of frames to unknown destinations, and MAC\naddress learning. However, these mechanisms are incompatible\nwith scalability, reliability, manageability, and QoS require-\nments of transport networks. First, by deactivating physical\nlinks, the STP wastes valuable resources, prevents load balanc-\n8ing, and artificially prolongs transmission paths. In addition,\nthe only response to failures of active links or a node is the\nreconfiguration of the spanning tree. Even the improved STP\nversions, namely RSTP and MSTP, are thereby unable to meet\nthe target restoration time. Second, flooding does not scale\nwith network size. Finally, STP and MAC address learning\nalong with the connectionless forwarding principle leave little\nroom for carriers to control the operation of their networks.\nIn particular, they prevent traffic engineering desirable for\nimproved network utilization, better QoS capabilities, and fine-\ngrained protection switching mechanisms.\nSince the above-mentioned issues are intrinsically linked\nto the STP, the flooding based control plane and the connec-\ntionless operation of Ethernet, extensions similar to PB and\nPBB are insufficient to address them. Therefore, the IEEE\nworking group 802.1Qay [39] currently defines connection-\noriented operation in carrier Ethernet networks using the PBB\nframe format, dubbed provider backbone bridging – traffic\nengineering (PBB-TE). Frames are forwarded along label\nswitched paths (LSP) configured by the management plane or\na future control plane. The functionality of the Ethernet control\nplane is therefore replaced by a management system populat-\ning the forwarding databases of the bridges. The switching\nlabel consists of the backbone destination address and the\nbackbone VLAN tag, which distinguishes several paths to the\nsame destination. Since the label remains unchanged along\nthe path, the forwarding principles of the Ethernet data plane\nare retained. Therefore, 802.1Q-compliant independent VLAN\nlearning capable switches allowing the manipulation of their\nforwarding databases can operate inside a PBB-TE network.\nDue to the possibility to define multiple paths between any\ntwo network nodes, PBB-TE enables protection switching and\ntraffic engineering.\nPB, PBB and PBB-TE are not only evolutionary steps\ntowards IEEE 802.1 based Ethernet transport. All of these\nstandards have particular properties making them suitable for\ndifferent parts of a carrier Ethernet network. For instance, both\nPB and PBB natively support E-LAN/EV-LAN services by\nmeans of bridging multicast whereas PBB-TE requires the\nemulation of such behavior using several tunnels. Figure 6\ndepicts an exemplary deployment scenario where a PB net-\nwork first aggregates traffic from different customers. Several\nof such PB networks are then attached to a PBB-based metro\nnetwork. Given the high traffic volume in the core network,\nPBB-TE is a candidate technology there – among others. For\ninstance, other authors [40] combine an Ethernet MAN with\nan IP/MPLS-based core network.\nIn addition to the protocol extensions, the IEEE defined\nEthernet PHYs for carrier networks. They feature high data\nrates (work on 100 Gbps is ongoing) and extended link ranges\nof up to 40 km and beyond. Besides, there are adaptation PHYs\nfor the transport of Ethernet frames over SDH/SONET.\n2) OAM and Control: Carrier grade network management\nand restoration mechanisms require OAM features to super-\nvise the network and to quickly detect and localize failures,\nrespectively. The IEEE defines connectivity fault management\n(CFM) messages in 802.1ag [41]. They include continuity\ncheck to test an existing path, loop back check to localize\na failed link, and link trace to identify the bridges on a\ngiven path. Distinguished by a special value (0x8902) in the\ntype/length field, these messages are forwarded in the Ethernet\ndata plane and thus follow the same path as data frames.\nThis allows for their transparent transport over other tech-\nnologies (possibly having their own OAM feature set). OAM\nmessages are either directed to a particular node identified\nby its individual MAC address, or sent to a reserved OAM\nmulticast address6. Ethernet OAM can be deployed on several\nhierarchical levels, e. g. end-to-end and on individual path\nsegments. In recommendation Y.1731 [42], the ITU-T specifies\nalarm messages for the coordination of OAM reactions within\nthis hierarchy. In addition, they describe protocols for loss,\ndelay, and throughput measurement.\nA common characteristic of PBB-TE and T-MPLS is\nconnection-oriented operation by means of LSPs. These ap-\nproaches leave the control plane empty and, like SDH/SONET\nnetworks, rely on centralized management systems. However,\na future control plane based on automatically switched trans-\nport networks / generalized multi-protocol label switching\n(ASTN/GMPLS, [43], [44]) or GMPLS Ethernet label switch-\ning (GELS, [45]), respectively, is envisaged.\nThe centralized control of Ethernet transport networks in\ncombination with service contracts giving way to traffic shap-\ning at the network edge enables QoS guarantees. Assuring an\nappropriate ratio of reservations to available resources allows\nguaranteeing the bandwidth offered to a customer and an upper\nbound for the latency.\nDue to its advance in standardization, equipment vendors\nand network operators favored T-MPLS over PBB-TE. Con-\ncerns about incompatibility between T-MPLS and MPLS have\nshifted the focus to MPLS-TP. With the clear mission to\novercome this issue and pushed by the IETF as well as the\nITU-T, MPLS-TP is in a good position to become the predomi-\nnant technology for Ethernet transport. In this paper, however,\nactual Ethernet based technologies are of more interest. We\ntherefore take PBB-TE as the representative of carrier grade\nEthernet for comparison in section IX.\nIV. ETHERNET IN THE FIRST MILE\nWhile we discussed Ethernet based core and metro networks\nin the previous section, we focus in this section on the\nnetwork access and mainly on the link between a customer\npremises equipment (CPE) and a network node, i. e. the so-\ncalled local-loop. As for core networks, Ethernet promises to\nbe an inexpensive and simple to operate network technology\nfor the access part of the network. Thus it is attractive for\nnetwork operators.\nOriginating from analog and digital public switched tele-\nphone networks (PSTN), the local-loop is realized by a\ndedicated voice-grade single-pair copper cable of up to a\nfew kilometers in length. For increased data rates, Digital\nSubscriber Line (DSL) technologies according to ITU-T Rec-\nommendations G.99x are used. These imply the transport\nprotocol ATM, which is hardly used elsewhere in carrier\n601-80-c2-00-00-3y, where y varies with hierarchy level and OAM message\ntype\n9networks. In order to reduce the number of protocols and\nto simplify network management, Ethernet comes into play.\nHowever, standard Ethernet is inappropriate for the local-loop\nin two respects.\nFirst, the Ethernet PHY interfaces have been designed for\nspecific cable and fiber types as well as lengths. Also, the\ntransmission systems have been specified for in-house con-\nditions. In a first mile environment, however, the situation is\ndifferent. The cable characteristics in the field differ (frequency\nspectrum, number of wires, cable length etc.) and fibers\nare only rarely deployed in the first mile. Furthermore, the\nenvironmental conditions like temperature range and humidity\nare more challenging. Accordingly, we need either a new cable\ninfrastructure or new PHY interfaces. Hereby, two criteria\nmust be considered: the cost and the time to deploy the new\ntechnology. Analyzing the distribution of costs in the first mile,\ncabling cost is one of the major factors, and the nodes must\nbe replaced in any case. In addition, replacement of nodes\nis possible with limited time effort, whereas the roll out of\na new cabling infrastructure is time consuming. Accordingly,\nthe first mile cabling infrastructure deployed in the field has\nto be reused. Therefore, Ethernet in the first mile requires new\nPHY interfaces.\nSecond, for network operation, it is necessary to supervise\nlinks and to analyze unexpected behavior. Ethernet lacks any\nexception handling mechanism and only relies on additional\nexternal tools (e. g. for line integrity check). In LAN scenarios,\nthis is not an issue as physical access to all parts of the\nnetwork is possible and the covered area is small. However,\nin first mile scenarios, the situation is different. First, while\none of the nodes terminating the first mile link is located in\nthe operator premises, the other termination node, the cus-\ntomer premises equipment (CPE), is located in the subscribers\nproperty and is thus not accessible by the operator. Second,\ntelecom providers operate country wide networks with tens\nof thousands customers with one central management center.\nIn such an environment, provisioning and network operation\nmust be automated due to cost reasons. Therefore, Ethernet in\nthe first mile requires management functions on link level.\nIn 2001, the standardization of IEEE 802.3ah: Ethernet in\nthe First Mile (EFM) began with the approval of the project\nauthorization request. In the following years, this task force\ndeveloped a new standard for copper and fiber based PHY\ninterfaces as well as functions for management and monitoring\nof links on PHY layer. The new standard was approved in 2004\nas IEEE 802.3ah and integrated into IEEE 802.3 [5] in 2005.\nToday, products are available from several manufacturers.\nIn parallel, the Ethernet in the First Mile Alliance (EFMA)\nhas been founded in 2001 as an international non-profit\nindustry consortium. Its main objective is to position products\nand technologies relying on the new EFM standards. In 2005,\nthe EFMA became part of the Metro Ethernet Forum (MEF).\nA. PHY interfaces\nFor the PHY layer, several new interfaces have been defined\nreflecting the special requirements of outdoor cabling. EFM\nCopper (EFMC) interfaces have been standardized, which al-\nlow to reuse installed voice-grade copper cables. They employ\nSHDSL (ITU-T G.991.2, [46]) and VDSL (ITU-T G.993.1,\n[47]) transmission technology with data rates up to 10 Mbps.\nAlso designed for point-to-point links, EFM Fiber (EFMF)\ninterfaces for single-mode fibers allow an increased data rate\nof up to 1 Gbps.\nEFM Passive Optical Network (EFMP) interfaces introduce\npassive optical networks (PON), which is a completely new\nconcept in Ethernet. An Ethernet PON (EPON) is a full duplex\nsingle fiber network with point-to-multipoint topology and\ndata rates up to 1 Gbps7.\nAs shown in figure 7, it consists of a central optical line\ntermination (OLT) and several optical network units (ONU),\nwhich are connected by a passive splitter/combiner. Each\nONU connects one or several customers to the network.\nPackets from OLT to the ONUs, i. e. in downstream direction,\nare transmitted by the OLT to the splitter/combiner, which\nbroadcasts them to all ONUs irrespective of the packet’s\ndestination. The ONUs filter the packet stream and forward\nonly those for the respective receiver while all other packets\nare dropped. For this selection, the OLT assigns a logical link\nidentifier (LLID) to each ONU and marks all packets with\nthe LLID of the receiver ONU. Vice versa, i. e. in upstream\ndirection, all packets are transmitted to the splitter/combiner,\nwhich forwards the combined signal of all ONUs without any\nprocessing to the OLT. Accordingly, ONUs never receive the\nsignal transmitted by another ONU.\nB. Medium Access\nIt is obvious that in downstream direction, the medium\nis dedicated to one sender, namely the OLT, whereas in\nupstream direction it is shared. Historically, Ethernet is well\nsuited to deal with a shared medium. Nevertheless, Ethernet’s\ntypical medium access scheme CSMA/CD is not applicable as\nsensing is not possible. Thus, to control the EPON, the multi-\npoint control protocol (MPCP, [5]) has been introduced. The\nMPCP is implemented in the MAC control layer and centrally\ncoordinates the medium access of the ONUs based on the time\ndivision multiple access (TDMA) principle.\nBesides, it allows to dynamically distribute the transmission\ncapacity among the ONUs by adjusting the duration of the\ntime slots. For this, the OLT signals each ONU the start\ntime and duration of its next time slot at least every 50 ms.\nDuring this time slot the ONU may transmit user data and\nsignal the current filling level of their transmission queues to\nthe OLT. Based on this level information a scheduler in the\nOLT allocates further time slot to the ONUs. As the scheduler\nis not specified, many different scheduling strategies can be\nimplemented (cf. [48]).\nC. Link level OAM\nThe task of link level OAM is to allow a network operator to\nmanage and monitor the path between two MAC layer entities,\ne. g. between the OLT and ONU. For this, the OAM sublayer\nhas been defined as client of the MAC or MAC control layer\nwith a slow protocol, i. e. a protocol with limited data rate [49].\n7the IEEE P802.3av task force is working towards 10 Gbps\n10\nOLT\nSplitter and\nCombiner\nUpstream\nVariable length packets\nIEEE 802.3 format\nEndUser21 2 2 3 ONU\n2\n2 2\n1 2 3\n2 2\nDownstream1 2 3\nPre LLID SFD SA\n(5) (2) (1)\nDA ONU\nEndUser1\n1\n11\n1 2 3\nONU\nEndUser3\n3\n33\n1 2 3\nFig. 7. Downstream and upstream traffic flow of an EPON\nIt relies on in-band signaling and transmits dedicated OAM\nframes via the MAC and PHY that is also used for user data.\nSuch OAM messages may be sent several times in order to\nincrease the reliability in situations with high error rates. It\nmust be noted that the entire system can be used with any\nPHY and is thus neither limited to the local-loop nor to access\nnetworks.\nBesides the discovery of the OAM capabilities of the\nneighboring nodes, there are three kinds of functions provided\nby link level OAM. First, data terminals can signal critical\nand non-critical events. While non-critical events are dedicated\nto the permanent supervision of error rates in defined time\nintervals (several symbols, frames, etc.), critical events are\nsignaled if no bidirectional message exchange is possible. The\nmost prominent critical event is a link fault, which is signaled\nif the receive path is broken. Second, for fault localization the\nremote loop back mode has been introduced. In this mode,\nthe OAM sublayer reflects all received frames to the sender.\nThis allows testing the MAC-to-MAC path. Third, as the\nlower layers in Ethernet only poorly inform their neighbors\nabout errors, a variable retrieval mechanism is defined. This\nallows to read objects from the remote station’s management\ninformation base.\nV. INDUSTRIAL ETHERNET\nAfter the IEEE began to standardize Ethernet, different\nvendors (e. g. Siemens, [50]) adopted Ethernet for industrial\napplications. Today, the vendors mostly use the term indus-\ntrial Ethernet for Ethernet-based solutions. In contrast to\nthe Ethernet of corporate LANs, industrial Ethernet has to\nfulfill enhanced requirements such as time synchronization,\n(hard) real-time operation, and high availability. Furthermore,\nindustrial Ethernet connectors and hardware have to meet\nambient condition requirements of factory equipment such as\nelectrical noise, vibration, temperature, and durability [51].\nThe driver was to reduce the steadily increasing costs and\ncomplexity as well as to connect a wide range of devices in\nthe factory automation and control infrastructure. A further\nobjective was to interconnect the factory and the office net-\nwork for simple remote access to individual devices. Besides,\nEthernet bandwidth and its further development promised to\ncover future needs [52], what it does to this day.\nIn recent years, more and more devices have come with a\nbuilt-in Ethernet interface. Since Ethernet has been adapted to\nfulfill industrial requirements, it has started to replace fieldbus\ntechnologies such as Profibus [53] and Sercos [54], which\nare close to reaching their capacity limits. The trend of\nusing Ethernet in industrial environments also affects related\nfields of application with similar requirements. Nowadays,\nEthernet satisfies communication needs in many embedded\nnetworks, e. g. in marine applications and building automation.\nTraditionally, these have also been the domains of fieldbuses,\nwhich are gradually replaced by Ethernet.\nA. Networking requirements\nIndustrial networks have to transmit information (e. g.\ntemperature or position data) between sensors, actors, and\ncontrollers. In factory automation, we categorize network-\ning components and technologies according to performance\ncharacteristics, exchanged information types, and cost. The\nperformance characteristics comprise the number of attached\ndevices, data rate, physical network size, response time, pay-\nload size, and frequency of data exchange. We thereby classify\nthe networking components and technologies in hierarchical\nlevels. Typical levels are the plant level, the control level, and\nthe device level [55], [56]. The advantage of Ethernet is its\npotential to cover all levels as a single integrating networking\ntechnology.\nThe plant level comprises an entire production facility\nand connects a large number of factory cells and machine\nprocess controllers. This level tolerates reaction times up to\nseveral hundred milliseconds [57]. The control level connects\na moderate number of specialized computing devices such as\nmachine control systems over a moderately large area. In this\nlevel, the reaction times have to be smaller than 10 ms [58].\nThe device level enables the communication between sensors\nand actuators and requires normally a response time smaller\nthan 1 ms with a jitter of less than 1 µs [59] for motion\ncontrol with high frequency and small payloads [57]. Figure 8\ndepicts these differences of performance characteristics. The\nlevels also structure the information flow required for factory\nand process automation [58].\nB. Real-time Ethernet Solutions\nToday, a number of different real-time Ethernet solutions\nsupporting different classes of QoS exist. Due to the multi-\ntude of solutions and the history of their development, it is\nnot possible to give a clear-cut definition of the term real-\ntime Ethernet. One major criterion is whether a solution is\ncompatible with IEEE 802.3 compliant devices or not. De-\ncotignie [60] classifies industrial Ethernet solutions according\nto their degree of compatibility with commercial off-the-shelf\n11\nControl\nlevel\nLatency\nDevice\nlevel\nPlant\nlevel\nResponse\ntime\nMessage\nfrequency\nTransfer\nsize\nms sec MByte rare\n< 1 ms ms Bit often\nFig. 8. Networking requirements and characteristics of the levels in an\nindustrial environment [55], [56]\n(COTS) Ethernet technology: (1) interoperable homogeneous,\n(2) interoperable heterogeneous, and (3) non-interoperable. All\nthese solutions carry the generic label industrial Ethernet or\nreal-time Ethernet, even though they describe different and\nmostly incompatible technologies [60].\n1) Interoperable, homogeneous solutions: Interoperable\nand homogeneous solutions build on IEEE 802 standards, as\nshown in figure 9, and thus are compatible with IEEE 802.3\ncompliant devices, i. e. devices implementing the following\nimprovements are able to communicate with COTS Ethernet\ndevices. The use of switches and full duplex links eliminates\ncollisions. This makes QoS predictions easier and in turn\nenables the use of Ethernet as an industrial network for\ndistributed real-time applications. As defined in the IEEE\n802.1Q standard, Ethernet allows frame tagging that enables\ntraffic prioritization and a separation of real-time and best\neffort traffic (which gets the lowest priority). Bridges can\nhandle up to eight traffic classes, although in practice they\nonly use four [60].\nWith respect to higher layers, two approaches exist. On the\none hand, both real-time and best effort traffic can use the\nTCP/UDP/IP protocol suite. This solution allows transparent\ncommunication over network boundaries [59], but guarantees\nonly relative QoS. The second option implements an additional\nreal-time protocol stack, i. e. the real-time data bypasses the\nTCP/UDP/IP protocol stack and thus avoids potential delays\nintroduced by these protocols [59].\nAn example of an interoperable, homogeneous solution\nis the Ethernet industrial protocol (EtherNet/IP, [61]). This\nsolution uses tagged Ethernet frames and assigns real-time\nframes the highest priority [59].\nAlthough the use of bridges and traffic prioritization avoids\ncollisions, it does not make Ethernet deterministic. For ex-\nample, if the buffer size inside a switch is too small to store\nsimultaneously arriving frames temporarily, an overflow of the\nbuffer and frame losses occur [60]. Therefore, the industry\ndeveloped and adopted enhanced solutions. One such approach\nis the implementation of an additional traffic scheduler on\ntop of Ethernet to regulate the traffic. It can take the task\nof a traffic smoother proposed by Kweon et al. [62], which\nis located between the transport layer and the data link layer,\nas shown in Figure 9, and smoothes data streams between\nthese two layers. Its implementation requires enhancements\nof the protocol stack at the end systems. The MAC protocol,\nhowever, remains unchanged. Therefore, it runs on IEEE 802.3\ncompliant hardware.\nSuch a traffic scheduler can be a traffic shaper or a token\nbucket. This leads to smooth traffic in which messages arrive\nat a constant rate and not in a bursty manner. A traffic regulator\ndelays bursty traffic for longer periods and thus improves the\nnetwork performance [60].\nIn recent years, several authors [57], [62]–[75] have sug-\ngested and analyzed the real-time behavior of these solutions,\nsome of them making additional assumptions like limited\nnetwork load. Others analyze the delay bounds by using the\npriority field in the Ethernet frame, where the real-time traffic\nis given the highest priority, like in EtherNet/IP, and the best\neffort the lowest priority.\n2) Interoperable, heterogeneous solutions: The solutions\ndiscussed in this section are also able to communicate with\nIEEE 802.3 compliant devices. However, their key to guar-\nantee real-time capabilities is the exclusive use of modified\nswitches [60]. No other intermediate devices, hubs, switches,\nor routers are allowed. Otherwise, the communication falls\nback to the legacy best effort service of Ethernet, which is\ninsufficient for real-time applications.\nAn example of such a solution is EtheReal [76]. This so-\nlution provides bandwidth guarantees by connection-oriented\noperation. When a device wants to transmit real-time data,\nit tries to set up a connection via a reservation protocol. If\nenough resources are available, a switch forwards the connec-\ntion request to the next hop on the path. Each switch repeats\nthis process until the last switch is reached and the connection\nis established. The last switch confirms the reservation by\nsending a message back. This approach is similar to the\nresource reservation protocol (RSVP, [77]).\nAnother more prominent example of an interoperable, het-\nerogeneous solution is Profinet [50]. It differentiates several\nclasses of communication services by means of synchronous\ntime-division multiplexing: An isochronous real-time, a real-\ntime, and a non real-time service. Each time slot allows\nthe transmission of several Ethernet frames. A central entity\nmanages the assignment of isochronous transmission capacity\nin the first time slot of a cycle. In the following slots, the real-\ntime frames followed by the non real-time frames are transmit-\nted. A special Profinet switch forwards the isochronous real-\ntime frames without any interpretation [59]. In the following\ntime slots, the switch changes to normal Ethernet operation\nand interprets the destination address to forward a frame to\nthe corresponding port.\n3) Non-Interoperable solutions: Applications with high\ntiming requirements, e. g. motion control, usually base on non-\ninteroperable solutions. Such solutions use an additional, deter-\nministic MAC as depicted in figure 9 and can be implemented\non COTS IEEE 802.3 compliant hardware. Decotignie [60]\nlists different types of deterministic MAC, which have been\nused over the last years. These types include TDMA, master-\nslave, token passing, slot reservation, and time packet release.\nNetworks implementing such enhancements are isolated\nfrom regular Ethernet networks by bridges. All real-time guar-\nantees are lost in presence of traffic from regular IEEE 802.3\ndevices due to the different MAC protocols. A representative\nexample is the Ethernet for control automation technology\n12\nInteroperable solution Non-interoperablesolution\nLLC 802.2\nScheduler\napplication\nbest effort\ndata\nreal-time\ndata\nreal-time\nprotocol\nstack\nUDPTCP\nIP\nMAC 802.3 MAC 802.3\nLLC 802.2\nNon IEEE 802.3 compliant \nMAC\napplication\nbest effort\ndata\nreal-time\ndata\nreal-time\nprotocol\nstack\nUDPTCP\nIP\nFig. 9. Real-time Ethernet solutions\n(EtherCAT, [78]). It uses the Ethernet frame format in a\nring topology [58]. A master generates an Ethernet frame\nwith a slotted data field. Each device allocates a number of\nslots and removes or adds information there. This information\ncan be I/O information as well as TCP/UDP/IP data. Due\nto the on-the-fly processing, EtherCAT reaches a cycle time\nof 30 µs [58]. If required, the EtherCAT protocol may be\ntunneled through IEEE 802.3 compliant or IP-based networks\nin UDP packets, however without real-time guarantees.\nC. Topology and Availability\nUnlike in the office environment, most devices in an indus-\ntrial network include an embedded bridge, which results in\nthree possible topologies: line, ring, and tree. In the first two\ncases, each embedded bridge has three ports, the device being\nconnected to one of them. The other two bridge ports are used\nto connect to adjacent switches. Ru¨ping et al. analyze these\ntopologies in [79] with respect to the transmission delay. They\nshow that the tree topology performs best, the ring topology\nsecond, and the line topology worst. In addition, the tree\ntopology requires a smaller number of switches.\nIndustrial applications need highly reliable communication\nbetween several devices, not only between a client and a\nserver as in the corporate LAN. Therefore, redundant links\nare deployed for resilience, e. g. in a ring topology. Less time-\ncritical applications can cope with a higher recovery delay\nand the topology reconfiguration by (R)STP is sufficiently fast\nto recover from a failure. In contrast, (safety-critical) real-\ntime applications require the parallel operation of redundant\nnetworks. Today, a number of redundancy methods to ensure a\nhigh availability exist. The International Electrotechnical Com-\nmission (IEC) is working on standards that define redundancy\nschemes suitable for industrial Ethernet networks. Kirrmann\nand Dzung [80] discuss redundancy requirements and classify\nredundancy methods.\nVI. AVIONICS ETHERNET\nOver the last thirty years, the ARINC 429 [81] data bus\nhas been widely used in various civil avionics applications.\nHowever, the increasing complexity of avionics systems is\npushing this traditional data bus well beyond the limits of its\nbandwidth (100 kbps). In order to overcome these limitations,\nBoeing proposed the ARINC 629 [82] data bus. It offers a\n2 Mbps multi transmitter bus supporting up to 120 nodes.\nHowever, this protocol has not gained general acceptance,\ndue to high complexity and development cost. Hence, another\nsolution that follows the new trend of using COTS technology\nwas proposed to reduce the development costs and facilitate\nthe maintenance process. A commercial standard (ARINC\n664, [83]) and a specific aircraft implementation known as\navionics full duplex switched Ethernet (AFDX, [83]) have\nrecently been developed to define the use of switched Ethernet\nin aircraft. This latter has been successfully integrated into new\ngeneration civil aircraft like the Airbus A380.\nA. Requirements\nAvionics requirements concern both technical aspects and\ncost. The former consist of performance, safety, scalability,\nmaintainability, and environment requirements. First, perfor-\nmance requirements concern:\n1) the applicative latencies that have a magnitude order of\nmilliseconds and an allowed bounded jitter and have to\nrespect deadline constraints;\n2) the throughput that has to be at least 100 Mbps to\nguarantee the aircraft a viability of thirty years or more;\n3) deterministic behavior where the system must be able to\ndeliver correct information in a predictable manner.\nSecond, safety requirements include:\n1) reliability where a service continuity has to be guaran-\nteed and mechanisms to detect, correct, or ignore errors\nhave to be available;\n2) integrity with a very low error rate;\n3) availability that is often brought by redundant architec-\ntures;\n4) security by supporting network domains with different\nsecurity levels.\nThird, we find scalability and maintainability requirements\nwhere the system has to support easy addition, removal, and\nmaintenance of any node on the network to be viable for future\nevolutions. Finally, environment requirements impose compat-\nibility with aircraft conditions (electromagnetic interference,\nvibration, heat, etc.) on the network.\nConcerning cost requirements, a trade-off between the price\nand availability of network components and the deployment\neffort is required. Certainly, the use of a COTS technology\nhas many advantages like increasing the number of vendors\nand thus reducing the component prices. However, the impli-\ncations of a new network on the existing applications (avionics\nfunctions) have to be taken into account by adding mechanisms\nto guarantee compliance.\nWe give an overview of the AFDX in section A. Section\nB surveys other works trying to extend the Ethernet’s imple-\nmentation in military avionics context.\nB. AFDX standard\nThe AFDX standard [83] defines protocol specifications\n(ARINC 664 part 7) for the data exchange between avionics\nsubsystems. This protocol represents an analogy to the ARINC\n429 bus thanks to the virtual link (VL) concept that gives a\n13\nDA SA T IP Header\nDestination Address (DA)\nUDP Header UDP Payload (min. 17, max. 1471 B)Avionics Subsystem Message\nSeq.\nNo. FCS\nEthernet Header\nAFDX Frame\nEthernet Frame\n6 6 2 20 8 variable 1 4\nConstant Field Virtual Link ID\n24\nSource Address (SA)\nConstant Field\n13\nNetwork\nID\nEquipment\nID\nInterface\nID\n11\nInterf.\nID 00000\nLocation\nID\nSide\nID\nDomain\nID0000\nFig. 10. AFDX frame format\nway to reserve a guaranteed bandwidth and provides a simple\ntransition for existing avionics functions. The VL defines a\nlogical unidirectional connection between one source and one\nor more destinations. Thus, it shows multicast characteristics\nwith one end system representing the source of a given VL.\n1) Functional description: The AFDX topology is a closed\nnetwork topology and consists of interconnected switches\nand end systems. An end system is used to connect each\navionics subsystem to the network, but it can also support\nmultiple avionics subsystems where partitions are used to\nprovide subsystem isolation within the same end system. Each\nend system has two direct bi-directional connections to two\nredundant switches to guarantee the system’s availability. A\nMAC address, as depicted in figure 10, identifies the end\nsystems. The source address is a unicast address and represents\na unique end system, whereas the destination address is a\nmulticast address where the VL identifier is embedded.\nEach end system can send data to multiple VLs. Thus, like\nthe partition mechanism used to isolate subsystems within an\nend system, a similar mechanism is needed to isolate VLs to\nprevent interference between them. Hence, for each VL, the\ntransmission rate and the maximal frame size are limited and\ndefined a priori. Each VL has two main parameters:\n• The bandwidth allocation gap (BAG) that presents the\nminimal inter-arrival time between two frames sent on\nthe VL. It ranges in powers of 2 from 1 ms to 128 ms;\n• Lmax that is the largest Ethernet frame length transmitted\non the virtual link.\nAll VLs generated by the same end system are controlled and\nscheduled by a VL scheduler inside each end system. This\nscheduler ensures that each VL respects its associated rate\nand length limitations and is responsible of multiplexing the\ndifferent VLs. The management of the VLs by the end system\nis called traffic shaping. Each end system also integrates\nredundancy management to deal with the data received and\ntransmitted on the redundant connections. The transmitting end\nsystem sends the same frame using two redundant paths to the\nsame receiving end system. The receiving end systems choose\nthe first valid frame.\nThe switches used in AFDX implement static configuration\ntables to define the associated physical ports for each VL.\nWhen a message is received, it is routed to its destination\nport(s) based on the VL. Hence, the STP and the address\nresolution protocol (ARP, RFC 826 [84]) mechanisms are not\nnecessary and disabled. These switches work in store-and-\nforward mode to check frame integrity and discard invalid\nframes. Furthermore, they ensure that each port sends correct\nVLs by means of policers. Frames that do not respect the\nassociated VLs characteristics are discarded.\n2) Extensions to IEEE 802.3 and 802.1D: The AFDX\nspecification [83] defines compliance degrees with regard to\nEthernet and IP standards and distinguishes two types of\nnetworks: compliant network and profiled network. Compliant\nnetworks apply standard specifications. They are used for non-\ncritical applications. Profiled networks require some exten-\nsions to the Ethernet standard to support avionics require-\nments. They include two major extensions.\n• Extensions to 802.3 standard: The traffic is controlled\ninside each source by using the VL concept and traffic\nshaping technique to guarantee the characteristics of each\ngenerated flow. For each VL, a sequence number is\nadded to the Ethernet frame (1 byte field that occurs just\nbefore the FCS field). This is mainly used within each\nend system for redundant networks and for the integrity\nchecking of received frames on the same VL to control\nthe arrival order.\n• Extensions to 802.1D standard: AFDX switches are\naware of VL characteristics and include policers that\nenforce the VL traffic contracts. Violating frames are\nrejected.\nExtensions to standards and conception choices, like a\nstatically defined topology and VLs, enable the deterministic\nbehavior of the network [85], [86].\nC. Extension to Military Avionics\nThe integration of the AFDX standard in the Airbus A380\nwas a great success in civil avionics. Therefore, full duplex\nswitched Ethernet became attractive for next generation mil-\nitary aircraft. It is worth to note that AFDX could not be\nused for military applications. In fact, the requirements of\ncommunication protocols are distinct in civil and military ap-\nplications. AFDX has been defined to provide traffic isolation\nand bandwidth guarantee along with a guaranteed latency to all\ntraffic flows, whereas military applications need several service\nclasses in severe military environments where bandwidth may\nnot be the primary design concern.\n14\nLimitations of current military avionics networks motivate\nthe integration of switched Ethernet in military applications.\nWith the increasing complexity of interconnected subsystems\nand the expansion of exchanged data quantities, the current\nnetworks no longer meet the emerging requirements of new\nmilitary applications in terms of bandwidth and latency. There-\nfore, in order to fulfill these increasing needs, two communi-\ncation networks based upon micro-segmented Ethernet were\nproposed to replace the current military network [87].\nThe first proposal uses a distributed communication scheme\nwhere devices can emit data simultaneously [88]. This solu-\ntion improves global throughput and the system’s flexibility.\nHowever, the existing subsystems are implemented to work in\na centralized communication scheme, due to the widely used\ncommand/response data bus MIL STD 1553B [89]. Therefore,\nadapting all existing applications for distributed communi-\ncation would be expensive. In order to avoid this process,\nthe second proposal keeps the current centralized communi-\ncation scheme and deploys an optimized master/multi-slave\ntransmission control. However, this profiled network seems\nmore constrained than the one with distributed communication\nscheme since a higher data rate is necessary to satisfy all real-\ntime constraints [87]. The selection of the appropriate network\nrequires a trade-off between real-time guarantees and cost.\nThe standardization of these proposals for military avionics\napplications is an open issue.\nVII. AUTOMOTIVE ETHERNET\nIn recent years, the importance of automotive electronic sys-\ntems has grown rapidly as original equipment manufacturers\n(OEMs) keep adding new functions and applications. Today,\nelectronic systems and software directly or indirectly enable\nmost innovations. For example, the electronic stability control\n(ESC) improves safety by controlling individual brakes auto-\nmatically. At the beginning of automotive electronic systems,\nthe OEMs implemented each application on a stand-alone\nelectronic control unit (ECU), which is a subsystem composed\nof a micro-controller and a set of sensors and actuators [90].\nLater, the ECUs exchanged data through point-to-point\nlinks. With an increasing number of ECUs and distributed\napplications, this approach became insufficient and the OEMs\nintroduced networks with multiplexed communication. Later\non, the OEMs grouped ECUs with similar characteristics\nand requirements together in one domain. For example, the\ninfotainment domain groups ECUs which offer information\nand entertainment services and applications such as DVD\nplayer, navigation system, and remote diagnostic. Further\ntypical in-vehicle domains are chassis, powertrain, body, and\ncomfort. The different domains have separate communication\nsystems, which are connected via gateways. Today, an upper-\nclass passenger car contains up to 70 ECUs with several\nhundred software-enabled functions distributed over different\nnetwork technologies.\nThe different in-vehicle network technologies came from\nthe diversity of requirements. These requirements are unique\nin their combination of innovation and cost-driven mass-\nmarket characteristics with high demands on data rate, safety,\nreliability, usability, and a wide spectrum of other quality\nproperties [91]. The installation during manufacturing has to\nbe as simple as possible, too, and the components have to\nbe automotive qualified concerning temperature compatibility,\nelectromagnetic compatibility, etc.\nCurrently, the most commonly deployed in-vehicle network\ntechnologies are LIN [92], CAN [93], and MOST [94]. These\nnetwork technologies are automotive-specific. Consequently,\nfurther developments, enhancements, and standardization are\nOEM- or supplier-driven. These network technologies have\nbecome inflexible, complex, and costly, and some of them are\nreaching their capacity limits [95].\nA. Ethernet’s drivers\nIncreasing the offered bandwidth of the applied technologies\nsuch as CAN and MOST would lead to extra costs. Due to\nthe cost sensitivity of the automotive industry, the OEMs and\nsuppliers keep an eye on other low-cost network technologies\navailable, which can also fulfill requirements of an in-vehicle\nnetwork technology. Ethernet is an eligible candidate and\nbecomes more and more attractive [95]–[98].\nThe automotive industry aims at reducing the development\ntimes. Using standardized technologies and mass-market com-\nponents instead of developing automotive-specific technolo-\ngies achieves this objective. Ethernet is a flexible technology\nthat is suitable for both quickly changing platforms and\ndifferent car series. Ethernet enables a simpler development\nprocess thanks to the reuse of components. Many solutions that\nintegrate Ethernet and higher-layer protocols such as TCP/IP\non a single chip are already available. In addition, Ethernet\ncould be a technology that fits for nearly all above-mentioned\ndomains. Thus, this would avoid the parallel and independent\ndevelopment of different technologies. Therefore, the exclusive\nuse of Ethernet reduces cost and complexity. With low-cost\nbridges, the interconnection of Ethernet subnetworks is also\nsimple. Another solution is to separate different domains by\nVLANs. Consequently, there is no need for complex gateways.\nReaching more than 30 years (including development),\nthe life cycle of a car series is much longer compared to\nconsumer electronic. In case consumers want to integrate their\nelectronic devices, the in-vehicle communication systems have\nto be scalable over this time. There is a relation between\ninnovation cycles of customer electronic devices such as\npersonal digital assistant (PDA), semiconductor technology,\nand cars’ product lifespan. Processors and chips, or backward\ncompatible components, have to be available during cars’\nlifetime. Next generation in-vehicle communication systems\nhave to be robust with respect to possible future evolutions.\nThe history of Ethernet has proven that it is a sustainable\ntechnology, which has not yet reached its limits and can fulfill\nupcoming requirements.\nIn a car, electromagnetic interference (EMI) plays an im-\nportant role. The installation of polymer optical fiber (POF) is\nsimple and avoids EMI. In addition, POFs are a cost effective\nsolution and will support up to 1 Gbps in the future. Some\nsuppliers already work on Ethernet over POF solutions. For\nexample, Infineon offers full duplex POF transceivers and POF\nmedia converters [99].\n15\nControl\ndomains\n100 ms MByte\n10 ms\nInteractive\nInfotainment\ndomain\nStreaming\nSimilar network \ncharacteristics\nand requirements \nAudio/Video\nBridging\nEthernet\nIndustrial Ethernet\n(Control and \nDevice level)\nHard \nreal-time\n(safety) Bit\nIn-vehicle\ndomains Application Delay\nPayload\nsize\nFig. 11. Networking requirements and characteristics of the in-vehicle\ndomains\nB. Networking requirements\nSo far, we discussed non-functional requirements. How-\never, Ethernet has to fulfill networking QoS requirements,\ntoo. In [95], Rahmani et al. categorize in-vehicle traffic into\nfour classes based on their QoS requirements: Hard real-time\ncontrol data, real-time audio and video data, multimedia data,\nand best effort data. The hard real-time control data contains\nsensor information from the control domains. Typically, this\ninformation is only a few byte long and the data rate is\nlow. The maximum end-to-end delay of packet transmission,\nderived from CAN cycle time, amounts to 10 ms.\nSafety applications like ESC have such hard real-time re-\nquirements. In an isolated, closed domain with time-triggered,\ndeterministic applications, the worst-case end-to-end delay and\njitter is calculable. Typically, control domains with safety\napplications are isolated and an extra scheduler controls the\ntraffic coming from other devices outside of the domain. As\nshown in figure 11, the control domain has similar require-\nments and communication characteristics as the control and\ndevice level in industrial communication systems discussed in\nsection V. Therefore, it offers opportunity to adopt existing\nindustrial Ethernet solutions.\nIn the infotainment domain, we have to distinguish between\ninteractive and streaming applications as depicted in figure 11.\nStreaming applications basically have strong requirements\nconcerning the delay and jitter. However, the data streams\nallow buffering on the receiver side and thus end-to-end delays\nof up to 100 ms are tolerable. In contrast, it is not possible\nto buffer the data stream of an interactive application, e. g.\nof a rear view camera. Thus, the maximum end-to-end delay\nshould be less than 45 ms. Both categories are time-sensitive,\nhave a variable data rate, and require a packet loss of less\nthan 0.1 %. The multimedia domain has similar requirements\nand communication characteristics as in-house multimedia\nnetworks. Thus, solutions discussed in section VIII provide a\ngood basis for developing an Ethernet in-vehicle infotainment\ncommunication system.\nEthernet’s flexible frame length supports both the transmis-\nsion of control data of only a few bytes at a high frequency,\nand multimedia streams with large frames sizes. Therefore,\nit fits control and streaming applications. The dimensioning\nof switches’ and NICs’ buffers guarantee a low packet loss\nrate. Thanks to its high data rates, Ethernet satisfies the high\nbandwidth demands of upcoming applications, especially in\nthe infotainment domain where HD-Video and Dolby-surround\nwill become essential.\nIn the future, customers may connect their personal elec-\ntronic devices, which leads to an additional amount of traffic.\nIn this case, the safety critical domains have to be protected\nagainst unknown traffic, which would cause unknown de-\nlay and jitter. One approach to prevent this uses physically\nseparated domains connected by gateways. A quite different\napproach is to configure VLANs. For this purpose, VLANs\npartition a single physical communication system and the\nEthernet frame tags are used for traffic prioritization. In one\nsuch VLAN, which presents a separated domain, customers\ncan integrate their devices without affecting the existing ap-\nplications. This approach is similar to the VL concept in the\navionics field discussed in section VI.\nC. Topology\nIn a car, weight and packaging, and therewith the topology,\nplay an important role. The topology minimize the total\ncable length and fulfill installation demands. There are several\napproaches to design an Ethernet in-vehicle communication\nsystem without redundant paths. Typical Ethernet networks use\ntree topologies. Theoretically, this solution is applicable, too.\nHowever, deploying for example one central switch located\nanywhere inside the car would lead to cable bundles.\nThe use of more than one switch avoids such cable bundles.\nWe thereby have two possibilities: (1) the switches are located\nanywhere inside the car or (2) the switches are embedded\nwithin the devices. In the former case, additional cabling for\nthe power supply of the switches is needed. The latter solution\nleads to a line or ring topology. In section V-C, we discuss the\nperformance of different solutions. Rahmani et al. compare\nseveral in-vehicle network topologies, e. g. a unidirectional\nring, by their QoS performance and production cost in [95].\nD. Outlook\nAutomotive Ethernet is not a standardized term or tech-\nnology. It is rather used for an ongoing discussion whether\nEthernet has the potential for an in-vehicle communication\nsystem. A first step could be the deployment of Ethernet in\nthe multimedia domain instead of MOST.\nDue to the large production volume, cost per piece plays\nan important role for the automotive industry. If the cost\nbetween hubs and switches differs significantly, hubs might be\ndeployed. This would mean the rebirth of collision domains\nand the CSMA/CD MAC protocol. In such a solution, the\nunfairness between nodes to access the shared medium and\nunpredictable collisions will lead to an unpredictable timing\nbehavior. However, approaches which fix this problem and\nenforce real-time behavior over a shared Ethernet medium\nalready exist [100]–[102].\nIn summary, the decision to bring Ethernet into cars is moti-\nvated by possible cost reductions with respect to development,\nmanufacturing, and maintenance as well as the satisfaction of\nincreased application demands. In the automotive industry, we\nfind similar, challenging networking problems as in other fields\nof application. Thus, we can adopt solutions from these fields.\nWe can bring ideas from industrial Ethernet (section V) and\n16\navionics Ethernet (section VI) into the car and improve them.\nHome entertainment (AVB) networks (section VIII) also have\na lot in common with the infotainment domain in a car.\nVIII. AUDIO AND VIDEO BRIDGING ETHERNET\nThis section presents the intentions of the IEEE audio/video\nbridging task group (AVB) [103] and the enhancements of the\nIEEE 802 standards they are developing. For a few years, the\nmarket of home multimedia networks has been growing. Con-\nsumers want to access multimedia content and resources stored\nanywhere in the house using their computers and entertainment\ndevices. Though not widely adopted for streaming multimedia\napplications with real-time requirements yet, Ethernet has the\npotential of serving both data and multimedia networking\nneeds and thus is suited for the backbone of an integrated\ndigital home network. Hence, there is a growing need for an\nEthernet network able to distribute high quality digital audio\nand video reliably [104].\nA. Enhancements\nThe current IEEE 802 standards do not provide robust\nQoS guarantees concerning latency and bandwidth. Therefore,\nthe IEEE founded the 802.1 AVB task group to specify\nprotocols and mechanism supporting services for streaming\nmultimedia applications, which have time-synchronized, real-\ntime requirements, and need high data rates at low latencies.\nAVB Ethernet will guarantee an isochronous service with\nless than 2ms end-to-end latency and only 250µs through\none bridge. This maximum end-to-end latency is based on a\ntotal trip budget of 10–15ms for lip to ear synchronization.\nThe maximum number of bridges is limited to 7 [105]. The\nisochronous service is only supported over 100Mbps or faster\nfull duplex links.\nToday, Ethernet does not feature a mechanism to guaran-\ntee a certain bandwidth to an application. However, this is\nessential for multimedia streaming applications, which need\ndeterministically low latency and low jitter. Therefore, the\nupcoming IEEE amendment 802.1Qat [106] specifies a stream\nreservation protocol (SRP). Besides the protocol specification,\nthe amendment will comprise procedures and managed objects\nto be used by existing higher-layer mechanisms that allow\nresource reservation for specific traffic streams. The SRP\nreserves network resources, i. e. bandwidth shares, if they are\navailable along the entire path from the talker (source) to one\nor more listeners (destinations). The procedure is similar to the\none described in section V-B.2. A listener sends a join request\ncontrol frame to the talker. This frame contains information\nincluding the amount of required bandwidth and the channel\nnumber. If the resources are available, the talker receives the\nrequest and returns a join response message. Otherwise, the\nintermediate switches mark the message as resource not avail-\nable, but still forward it to the talker. This admission control\nscheme enables connection-oriented bandwidth reservation by\nend-to-end management.\nIn addition, a bridge has to obey frame-forwarding rules to\nprovide guarantees for time-sensitive multimedia streams, i. e.\nbounded latency and delivery variation. The IEEE amendment\n802.1Qav [107] specifies forwarding and queuing enhance-\nments for bridges, including per priority ingress metering,\npriority regeneration, and timing-aware queue draining algo-\nrithms. Besides a strict priority transmission selection scheme,\nthe bridges have to support a credit-shaper algorithm. Again,\nwe see similarities to the industrial solutions deploying addi-\ntional schedulers.\nA uniform time basis is essential for meeting the jit-\nter, wander, and time synchronization requirements of time-\nsensitive streaming applications, especially for applications in-\nvolving multiple streams delivered to multiple endpoints. The\nupcoming IEEE standard 802.1AS [108] therefore specifies\ntiming and synchronizing services on the data link layer. This\nstandard includes both time-stamping and media coordination\nservices [105] with a timing difference of less than 1µs\nbetween devices. For this purpose, the root of the clock\nspanning tree distributes the accurate time throughout the AVB\ndomain by means of time measurements between adjacent\ntime-aware devices. However, Ethernet components require\nmodifications in order to provide these features. The MAC\nlayer needs an accurate frame timer and separated queues\nto provide an additional traffic class. The driver firmware\nand bridges need an update to support admission control\nand bandwidth allocation mechanisms. Furthermore, a real-\ntime clock module and a time synchronization method are\nnecessary.\nIn summary, the new IEEE standard [108] and amendments\n[106], [107] enable the legacy Ethernet technology to provide\nan isochronous and deterministic low-latency service for multi-\nmedia streaming applications. The proposed standards require\nsmall changes to Ethernet’s MAC and some more significant\nchanges to the 802.1D bridges. These enhancements are sim-\nilar to some solutions in the industrial field as discussed in\nsection V.\nB. Interworking\nSince multimedia data is transported in normal, tagged\nEthernet frames, AVB enabled devices can interoperate with\nIEEE 802.3 compliant Ethernet hardware. However, all QoS\nguarantees are limited to the AVB domain as we illustrate\nin figure 12. Just like for the interoperable, heterogeneous\nindustrial solutions, the key to guarantee QoS is the exclusive\nuse of AVB compliant bridges and devices – along with\nfull duplex links. Outside of the AVB domain (grayed out\nin figure 12), we have no QoS guarantees. If there are any\nhalf duplex links, hubs, legacy switches, or legacy devices,\nwe leave the AVB domain and fall back to the legacy best\neffort service of Ethernet, which is insufficient for streaming\napplications. An AVB domain can only contain a limited\nnumber of bridges, but may have any physical structure. The\nSTP assures a loop-free logical topology.\nIX. COMPARISON\nThe deployment of some Ethernet technology is common\nto all fields of application. Since these fields show huge\ndifferences in their requirements and constraints, the according\nEthernet variants also do. This section compares the different\n17\nTransport Networks Embedded Networks\nCorporate\nLAN\nCarrier Grade\n(PBB-TE)\nFirst Mile Industrial1 Avionics Automotive2 AVB\nConsortia IEEE IEEE, ITU-T,\nMEF\nIEEE, MEF\n(previous:\nEFMA)\nIEEE, IEC,\nISO\nARINC 664 consortium of\nOEMs and\nsuppliers\nIEEE\nData rate up to 10Gbps,\n100Gbps1\nup to 10Gbps,\n100Gbps1\n2Mbps–\n1Gbps,\n10Gbps1\n100Mbps\n(predominant),\n1Gbps\n100Mbps 100Mbps ≥ 100Mbps\nPhysical topology mesh mesh point-to-point,\ntree\nline, ring, tree mesh line, ring, tree mesh\nLogical topology tree mesh point-to-point tree or mesh mesh tree or mesh tree\nMAC protocol dedicated\nchannel\ndedicated\nchannel\ndedicated\nchannel, PON:\nTDMA\ndedicated\nchannel and\nadditional\nMAC\nprotocols2\ndedicated\nchannel\ndedicated\nchannel\ndedicated\nchannel\nSwitching identifier MAC\naddresses\npath identifier\n(internal MAC\n+ B-VID)\nMAC\naddresses,\nPON:\nadditional\nlogical link ID\nMAC\naddresses\nVL identifier MAC\naddresses\nMAC\naddresses\nControl philosophy plug-n-play or\nfully managed\ncentral\nmanagement\nand resource\nprovisioning\ncentral\nmanagement\nand resource\nprovisioning\nstatically\nconfigured,\n(predomi-\nnantly)\nisolated, and\nautonomous\nnetwork\nstatically\nconfigured and\nautonomous\nnetwork\n(FDBs and\nVLs\npredefined)\nstatically\nconfigured,\n(predomi-\nnantly)\nisolated, and\nautonomous\nnetwork\nplug-n-play\nand\nautomatically\nconfigured\nFault management STP or manual\ninteraction\nOAM and hard\nguarantee by\nprotection\nswitching\n(50ms\nrestoration\ntime)\nlink level\nOAM, outage\ndetection and\nrecovery, but\nno physical\ndisjoint paths\nmanual\ninteraction and\nproprietary\nsolutions;\npartly\nredundant\nnetworks\ntwo redundant\nnetworks\nrestoration /\nconfiguration\nonly in\nworkshops\nSTP\n1 IEEE 802.3 compliant solutions\n2 Work in progress; it depends on the final solution (see different solutions in section V-B)\nTABLE I\nFUNDAMENTALS\nAVB\ndev\nAVB AVB\nLegacy\nswitch\nAVB compatible\ndevice\nAVB\ndev\nAVB\nAP\nAVB\ndev\nHub\nAVB domain\nDoes not work\non half duplex links\nAVB link\nLegacy Link\nStreaming QoS only\nwithin the AVB domain\nAVB compatible\nswitch\ndev\ndev\nAVB\ndev\nAVB\ndev\nAVB compatible\naccess point\nFig. 12. Topology and interworking of an AVB domain [105]\nEthernet technologies and highlights their differences and\nsimilarities.\nTelecom operators selling transport services to their cus-\ntomers run transport networks like carrier grade Ethernet and\nEFM. They have to comply with stringent service guarantees,\nparticularly with respect to availability and bandwidth. OAM\nmechanisms are therefore essential. Deployed network tech-\nnologies do not only have to scale to huge network sizes, but\nalso have to provide means to cope with an ever-increasing\nbandwidth demand. Traffic engineering allows for an efficient\nutilization of installed resources in order to assure the return\non investment.\nThe applications and the number of nodes dictate the\nrequirements for embedded networks. They need to support\nreal-time communication for time-critical applications like\nfactories, aircraft, and cars. Such networks are mostly closed,\ni. e. all occurring traffic is predictable (or shaped). They can\ntherefore be dimensioned to fulfill safety requirements like\ntimeliness.\nWith respect to requirements and characteristics, a home\nmultimedia network is a mixture of a SOHO and an embedded\nnetwork. Like the latter, it faces QoS requirements such as\ndeterministic service and low, bounded delays. However, it\nsupports self-configuration and plug-and-play style attachment\nand removal of devices, as an Ethernet LAN does. However,\nthe number of bridges in an AVB domain is limited.\nFollowing this classification, we assess and compare the\ncharacteristics and provided services of the Ethernet variants.\nWe thereby focus on fundamentals and network QoS capabil-\nities depicted in table I and table II, respectively.\n18\nA. Fundamentals\nThe fundamentals describe specific characteristics of the\ndifferent Ethernet variants. Following the rows of table I,\nwe will outline the most important ones including data rate,\ntopology, and control mechanisms.\nThe IEEE plays a major role in the standardization of\nalmost all extensions to Ethernet. In addition, further consortia,\nnamely the ITU-T, MEF, ISO, IEC, and ARINC 664, published\nspecific enhancements to the IEEE 802 standards. In the au-\ntomotive sector, the ongoing discussion about the deployment\nof Ethernet is mostly driven by OEMs and suppliers.\nThe data rate defines the transmission capacity of one link.\nTransport network applications are driving the development\ntowards ever-higher data rates, currently 100 Gbps, since new\napplications and a growing number of users increase the\nbandwidth demand. The extension of the transmission capacity\nper physical link, i. e. fiber, is preferred to the installation of\nmore fibers, since such works in the field are very expensive.\nFor embedded networks, the situation is different. Data rates\nof 100 Mbps are mostly sufficient for their real-time control\napplications. Major design criteria include the robustness to\nenvironmental conditions like EMI, vibration, and heat. AVB\nnetworks may support various data rates, though the bandwidth\ndemand of multimedia streaming applications imposes a lower\nbound of 100 Mbps.\nConcerning network topology, we have to distinguish be-\ntween physical set-up and logical structure. In the majority\nof application fields, the physical topology is some kind of\nmesh, potentially ranging from tree to fully meshed structures.\nSome embedded networks, however, restrict permitted meshes\nto simple ring structures, and EFM is limited to tree topologies\ndue to its deployment at the network’s edge. The logical\ntopology often differs from the physical one, most prominently\nwhen the STP creates a loop-free structure by deactivating\nlinks, e. g. in LANs and AVB networks. In PBB-TE carrier net-\nworks, in contrast, both topologies are generally meshes, since\ntraffic-engineered paths are insensitive to potential forwarding\nloops. While EFM only supports logical point-to-point links\nto connect individual subscribers, a potentially meshed logical\ntopology is statically configured in embedded networks. If\nloops exist, the forwarding behavior of bridges, i. e. their FDB\nentries, needs to be defined statically as well.\nThe Ethernet variants also differ in the network operation\nwith respect to medium access control, switching identifiers,\nand network control. Dedicated channels, i. e. full duplex links\nand switches, are common to almost all application fields.\nThus, Ethernet’s initial CSMA/CD mechanism is no longer in\nuse. Even EFM, the only variant still using a shared medium,\nrelies on a slotted TDMA scheme instead of CSMA/CD.\nThe frame forwarding decision bases in general on the MAC\naddress. When tagged frames are used, this address may show\nper-VLAN significance if the VLAN tag is also taken into\naccount. This particularly applies to PBB-TE, where the B-\nVID serves to distinguish several paths to the same destination.\nIn avionic networks, the destination address field also serves\nas a switching identifier, but does not contain a normal MAC\naddress. Instead, it features a VL identifier, which is basically\na multicast address.\nThe classical control mechanisms of Ethernet like STP and\nMAC address learning, which are the basis of its plug-n-\nplay style self-configuration, are only retained in LAN and\nAVB networks. In corporate LANs, they can provide interfaces\nfor network management, e. g. to influence the spanning tree.\nIn transport networks, operators do not rely on distributed\ncontrol protocols. They centralize network control and explic-\nitly assign resources to possibly dynamic traffic flows. OAM\nfeatures thereby provide feedback on the state of the network.\nConversely, embedded networks are isolated, thus need to\nfunction autonomously, without external control. However, the\ntraffic circulating in such closed networks is generally known\nin advance, giving way to static configuration. If an embedded\nnetwork is interconnected with, e. g. an office LAN, traffic\nshapers and policers are applied to restrict the unforeseen\ntraffic.\nFault management describes mechanisms and protocols to\ndetect and recover from failures. Ethernet’s classical mecha-\nnism, the reconfiguration of the spanning tree, is still active\nin LAN and AVB networks. Carrier networks facing more\nstringent availability requirements deploy OAM mechanisms\nto detect and localize failures. Affected connections are then\nswitched to a pre-established backup path. EFM networks\nallow for failure detection by link-level OAM, but do not\nsupport automated recovery. Most embedded networks in the\nindustrial and automotive fields do not support fault manage-\nment features, i. e. failures need to be resolved by manual\nintervention. In contrast, a redundant structure increases the\nresilience of avionic networks as well as of some highly safety-\ncritical industrial applications.\nB. Network QoS Capabilities\nIn this section, we discuss the network QoS capabilities\nof the Ethernet versions for different fields of application\nas presented in table II. We thereby focus on bandwidth as\nwell as real-time metrics like latency and jitter, and evoke the\nmechanisms allowing for these guarantees.\nThe table row labeled bandwidth indicates the network’s\nability to provide guaranteed transmission capacities for indi-\nvidual data flows or traffic classes. Corporate networks only\nallow the assignment of bandwidth shares to different traffic\nclasses, i. e. a relative guarantee. In embedded and AVB net-\nworks, in contrast, deterministic guarantees are enabled. The\nsame applies to centrally managed transport networks, where\ntransmission resources are reserved for traffic flows. However,\nif operators exploit traffic fluctuations for multiplexing gains,\nonly statistical guarantees are feasible.\nThe latency is the time needed to transmit an Ethernet\nframe from the source to the destination node. It comprises\nthe processing delays in the source and destination nodes,\nthe processing and queuing delays in all intermediate nodes,\nand the propagation delay between all these nodes. Networks\nexposed to uncoordinated sources of unknown traffic like LAN\nand EFM networks are unable to provide timing guarantees.\nDue to service contracts and traffic shaping at their edge, car-\nrier networks allow for statistical guarantees. Embedded and\n19\nTransport Networks Embedded Networks\nCorporate\nLAN\nCarrier Grade\n(PBB-TE)\nFirst Mile Industrial1 Avionics Automotive2 AVB\nBandwidth relative hard or\nstatistical\nguarantee\nhard guarantee hard or\nstatistical\nguarantee\nhard guarantee similar to\nindustrial\nand/or AVB\nEthernet\nhard guarantee\nLatency no guarantee statistical\nguarantee\nno guarantee hard or\nstatistical\nguarantee\nguaranteed similar to\nindustrial\nand/or AVB\nEthernet\nhard guarantee\n(end-to-end\n2ms)\nJitter no guarantee statistical\nguarantee\nno guarantee hard or\nstatistical\nguarantee\nguaranteed\nbounded jitter\nsimilar to\nindustrial\nand/or AVB\nEthernet\nlow jitter\nMechanisms traffic classes\n(802.1D) and\nVLANs\nresource\nreservation and\ntraffic\nengineering\nresource\nreservation\ntraffic\nschedulers,\nadditional\nMAC\nprotocols, and\nresource\nreservation\nVL concept,\npolicers\nsimilar to\nindustrial\nand/or AVB\nEthernet\ntraffic classes\n(802.1D) and\nresource\nreservation\n(SRP)\n1 IEEE 802.3 compliant solutions\n2 Work in progress; it depends on the final solution (see different solutions in section V-B)\nTABLE II\nNETWORK QOS CAPABILITIES\nAVB networks designed for real-time applications implement\nmechanisms to deterministically bound the latency.\nThe jitter is the variation of the end-to-end latency, i. e.\nthe deviation of the inter-arrival times from the respective\ninter-departure times. It is therefore closely related to the\nlatency, and guaranteed jitter bounds coincide with guaranteed\nlatencies.\nThe mechanisms providing these guarantees vary between\napplication fields. Resource reservation and some form of\nlimitation of the incoming traffic, however, are fundamental\nfor any hard guarantee. Lacking such mechanisms, corporate\nLANs only provide relative guarantees based on traffic classes.\nAlso based on VLANs and traffic classes, AVB Ethernet\nintroduces an additional stream reservation protocol in order\nto give deterministic guarantees. In carrier networks, resource\nreservation is supported by traffic engineering and shapers en-\nforcing service contracts limit the traffic. Embedded networks\nsupporting safety-critical real-time applications use schedulers,\nshapers, and policers along with resource reservation pro-\ntocols. Resource reservation in avionic networks is finally\nachieved by the VL concept.\nX. SUMMARY\nStarting as a local network technology, Ethernet diffused in\na large number of fields of application, including transport,\naccess, industrial, automotive, avionics and home entertain-\nment networks. All these networks claim to rely on Ethernet\ntechnology. We surveyed the Ethernet technology of each of\nthese application fields and compared the resulting Ethernet\nvariants.\nWe classified the fields of application of Ethernet in three\nmajor categories: (1) the operated and managed networks of\ncarriers in the core and access part of a public or private\nnetwork; (2) the embedded networks in the manufacturing en-\nvironment, in aircraft, and in cars; (3) the home entertainment\n(AVB) networks residing between LAN and category two.\nIn each of these fields of application, Ethernet has adapted\n(or is on the way to adapt) to specific requirements. This\nadaptation process included changes in most ISO/OSI layers\nand additional extensions to the original IEEE 802 standards\nfamily. We identified the main changes to Ethernet in the\nfollowing fields: OAM support and QoS capabilities, switching\nprinciple, and frame format. Besides, vendor specific solutions\nexist. Consequently, a unique definition of the term Ethernet\nis not possible anymore. The use of some frame structure\nbeing the only similarity in all fields of application, each\nimplementation represents some kind of Ethernet technology.\nMore than 30 years after the birth of Ethernet, only two\nattributes remain common to all technologies: one is framing\nand the other is the name – Ethernet.\nACKNOWLEDGMENT\nThe authors would like to thank Christian Mu¨ller, Rainer\nBlind, and Martin Neubauer for fruitful discussions. Special\nthanks also go to the reviewers who helped to improve the\npaper.\nThe work presented in this paper was partly funded within\nthe 100GET project 100G ARP by the German Bundesmin-\nisterium fu¨r Bildung und Forschung under contract No.\n01BP0768.\nREFERENCES\n[1] C. E. Spurgeon, Ethernet: The Definitive Guide. O’Reilly Media,\nFebruary 2000, vol. 1.\n[2] F. Gebali, Analysis of Computer and Communication Networks.\nSpringer Publishing Company, Incorporated, 2008.\n[3] R. M. Metcalfe and D. R. Boggs, “Ethernet: Distributed packet switch-\ning for local computer networks,” XEROX Palo Alto Research Center,\n3333 Coyote Hill Road, Palo Alto, California, USA, Tech. Rep., 1975.\n20\n[4] R. M. Metcalfe, D. R. Boggs, C. P. Thacker, and B. W. Lampson,\n“Multipoint data communication system with collision detection,”\nUnited States Patent 4063220, December 1977.\n[5] IEEE Computer Society, “802.3: IEEE Standard for Local and\nMetropolitan Area Networks–Carrier sense multiple access with colli-\nsion detection (CSMA/CD) access method and physical layer specifi-\ncations,” 2005.\n[6] ——, “802.5: IEEE Standard for Local and Metropolitan Area\nNetworks–Specific requirements, Part 5: Token ring access method and\nPhysical Layer specifications,” May 1998.\n[7] ——, “P802.3ba Draft Standard for Local and Metropolitan Area\nNetworks–Carrier Sense Multiple Access with Collision Detection\n(CSMA/CD) Access Method and Physical Layer Specifications -\nAmendment: Media Access Control Parameters, Physical Layers and\nManagement Parameters for 40 Gb/s and 100 Gb/s Operation,” 2008.\n[8] ——, “802.11: IEEE Standards for Local and Metropolitan Area\nNetworks–Wireless LAN Medium Access Control (MAC) and Physical\nLayer (PHY) Specifications,” 2003.\n[9] ——, “802.2: IEEE Standard for Local and Metropolitan Area\nNetworks–Logical Link Control,” 1998.\n[10] ——, “802.1D: IEEE Standard for Local and Metropolitan Area\nNetworks–Media Access Control (MAC) Bridges,” 2004.\n[11] Internet Assigned Numbers Authority (IANA), “IANA Ethernet\nnumbers assignment,” August 2008. [Online]. Available:\nhttp://www.iana.org/assignments/ethernet-numbers\n[12] L. A. Chappell, Novell’s Guide to LAN/WAN Analysis. Foster City,\nCA, USA: IDG Books Worldwide, Inc., 1998.\n[13] IEEE Computer Society, “802.1Q: IEEE Standard for Local and\nMetropolitan Area Networks–Virtual Bridged Local Area Networks,”\n2005.\n[14] G. Donahue, Network Warrior. O’Reilly Media, Inc., 2007.\n[15] R. Seifert and J. Edwards, The All-New Switch Book: The Complete\nGuide to LAN Switching Technology. Wiley Publishing, 2008.\n[16] O. Feuser and A. Wenzel, “On the effects of the IEEE 802.3x flow\ncontrol in full-duplex Ethernet LANs,” Local Computer Networks,\n1999. LCN ’99. Conference on, pp. 160–161, October 1999.\n[17] W. Takafumi, N. Masahiro, T. Hiroyasu, T. Otsuka, and M. Koibuchi,\n“Impact of topology and link aggregation on a PC cluster with\nEthernet,” Cluster Computing, 2008 IEEE International Conference on,\npp. 280–285, October 2008.\n[18] N. Olifer and V. Olifer, Computer Networks: Principles, Technologies\nand Protocols for Network Design. Wiley, 2006.\n[19] IEEE Computer Society, “802.1X: IEEE Standard for Local and\nMetropolitan Area Networks–Port-Based Network Access Control ,”\nOct. 2001.\n[20] B. Aboba, L. Blunk, J. Vollbrecht, J. Carlson, and H. Levkowetz,\n“Extensible Authentication Protocol (EAP),” IETF, RFC 3748, June\n2004.\n[21] J. Geier, Implementing 802.1X Security Solutions for Wired and Wire-\nless Networks. Wiley, 2008.\n[22] E. L. Brown, 802.1X Port-Based Authentication. Boston, MA, USA:\nAuerbach Publications, 2006.\n[23] E. Rosen, A. Viswanathan, and R. Callon, “Multiprotocol Label\nSwitching Architecture,” IETF, RFC 3031, Jan. 2001.\n[24] ITU, “Rec. G.8110.1/Y.1370.1 : Architecture of Transport MPLS (T-\nMPLS) layer network,” International Telecommunication Union, ITU-\nT, Nov. 2006.\n[25] B. Niven-Jenkins, D. Brungard, M. Betts, N. Sprecher, and S. Ueno,\n“MPLS-TP Requirements,” IETF, Internet Draft, Feb. 2009. [Online].\nAvailable: http://tools.ietf.org/id/draft-ietf-mpls-tp-requirements-04.txt\n[26] MEF, “MEF Technical Specification 6.1: Ethernet Services Definition\n– Phase 2,” The Metro Ethernet Forum, April 2008.\n[27] ——, “MEF Technical Specification 10.1: Ethernet Services Attributes\nPhase 2,” The Metro Ethernet Forum, Nov. 2006.\n[28] ITU, “Rec. G.8011/Y.1307: Ethernet over Transport - Ethernet services\nframework,” International Telecommunication Union, ITU-T, Aug.\n2004.\n[29] ——, “Rec. G.8011.1/Y.1307.1: Ethernet private line service,” Interna-\ntional Telecommunication Union, ITU-T, Aug. 2004.\n[30] ——, “Rec. G.8011.2/Y.1307.2: Ethernet virtual private line service,”\nInternational Telecommunication Union, ITU-T, Sept. 2005.\n[31] ——, “Rec. G.707/Y.1322: Network node interface for the synchronous\ndigital hierarchy (SDH),” International Telecommunication Union,\nITU-T, Dec. 2003.\n[32] ——, “Rec. G.783: Characteristics of synchronous digital hierarchy\n(SDH) equipment functional blocks,” International Telecommunication\nUnion, ITU-T, Mar. 2006.\n[33] ——, “Rec. G.803 : Architecture of transport networks based on the\nsynchronous digital hierarchy (SDH),” International Telecommunica-\ntion Union, ITU-T, Mar. 2000.\n[34] ——, “Rec. G.709/Y.1331: Interfaces for the Optical Transport Net-\nwork (OTN),” International Telecommunication Union, ITU-T, Mar.\n2003.\n[35] ——, “Rec. G.872: Architecture of optical transport networks,” Inter-\nnational Telecommunication Union, ITU-T, 2001.\n[36] TPACK, “PBB-TE, PBT - Carrier Grade Ethernet Transport,” TPACK,\nTech. Rep., 2007. [Online]. Available: www.tpack.com\n[37] IEEE Computer Society, “802.1ad: IEEE Standard for Local and\nMetropolitan Area Networks–Virtual Bridged Local Area Networks,\nAmendment 4: Provider Bridges,” May 2006.\n[38] ——, “802.1ah: Draft Standard for Local and Metropolitan Area\nNetworks–Virtual Bridged Local Area Networks, Amendment 6:\nProvider Backbone Bridges,” Nov. 2007.\n[39] ——, “802.1Qay: Draft Standard for Local and Metropolitan\nArea Networks–Virtual Bridged Local Area Networks, Amendment:\nProvider Backbone Bridge–Traffic Engineering,” Dec. 2007.\n[40] L. Fang, R. Zhang, and M. Taylor, “The evolution of carrier ethernet\nservices-requirements and deployment case studies [next-generation\ncarrier ethernet],” IEEE Communications Magazine, vol. 46, no. 3, pp.\n69–76, 2008.\n[41] IEEE Computer Society, “802.1ag: IEEE Standard for Local and\nMetropolitan Area Networks–Virtual Bridged Local Area Networks,\nAmendment 5: Connectivity Fault Management,” Dec. 2007.\n[42] ITU, “Rec. Y.1731: OAM functions and mechanisms for Ethernet based\nnetworks,” International Telecommunication Union, ITU-T, May 2006.\n[43] ——, “Rec. G.807/Y.1302: Requirements for automatic switched trans-\nport networks (ASTN),” International Telecommunication Union, ITU-\nT, 2001, wITHDRAWN.\n[44] E. Mannie and Ed., “Generalized Multi-Protocol Label Switching\n(GMPLS) Architecture,” IETF, RFC 3945, Oct. 2004.\n[45] D. Fedyk, L. Berger, and L. Andersson, “GMPLS Ethernet Label\nSwitching Architecture and Framework,” IETF, Internet Draft,\nOct. 2008. [Online]. Available: http://tools.ietf.org/id/draft-ietf-ccamp-\ngmpls-ethernet-arch-03.txt\n[46] ITU, “Rec. G.991.2: Single-pair high-speed digital subscriber line\n(SHDSL) transceivers,” International Telecommunication Union, ITU-\nT, Dec. 2003.\n[47] ——, “Rec. G.993.1: Very high speed digital subscriber line\ntransceivers,” International Telecommunication Union, ITU-T, June\n2004.\n[48] M. P. McGarry, M. Reisslein, and M. Maier, “Ethernet passive optical\nnetwork architectures and dynamic bandwidth allocation algorithms,”\nIEEE Communications Surveys & Tutorials, vol. 10, no. 3, pp. 46–60,\n2008.\n[49] M. Beck, Ethernet in the First Mile – The IEEE 802.3ah EFM\nStandard. McGraw-Hill, 2005.\n[50] R. Pigan and M. Metter, Automating with PROFINET: Industrial\ncommunication based on Industrial Ethernet. Wiley, June 2006.\n[51] P. S. Marshall and J. S. Rinaldi, Industrial Ethernet. ISA, September\n2004.\n[52] J.-D. Decotignie, “A perspective on Ethernet-TCP/IP as a fieldbus,” in\nProceedings volume from the 4th IFAC Conference, 2001.\n[53] International Electrotechnical Commission, “IEC/TR 61158 Industrial\ncommunication networks – Fieldbus specifications,” IEC, Tech. Rep.,\nNovember 2007.\n[54] ——, “IEC 61491 Electrical equipment of industrial machines – Serial\ndata link for real-time communication between controls and drives,”\nIEC, Tech. Rep., October 2007.\n[55] J. Jasperneite and P. Neumann, “Switched ethernet for factory commu-\nnication,” in 8th IEEE International Conference on Emerging Technolo-\ngies and Factory Automation (ETFA’01), October 2001, pp. 205–212.\n[56] J. Donald J. Sterling and S. P. Wissler, The Industrial Ethernet\nNetworking Guide. Thomson Delmar Learning, October 2002.\n[57] J. Jasperneite and P. Neumann, “How to gurantee realtime behavior\nusing ethernet,” in 11th IFAC Symposium on Information Control\nProblems in Manufacturing (INCOM’2004), April 2004.\n[58] R. Zurawski, The Industrial Communication Technology Handbook.\nCRC Press, February 2005.\n[59] M. Felser, “Real-time ethernet – industry prospective,” Proceedings of\nthe IEEE, vol. 93, no. 6, pp. 1118–1129, June 2005.\n[60] J.-D. Decotignie, “Ethernet-based real-time and industrial communica-\ntions,” Proceedings of the IEEE, vol. 93, no. 6, pp. 1102–1117, June\n2005.\n21\n[61] Open DeviceNet Vendors Association (ODVA), “EtherNet/IP Library,”\n2009. [Online]. Available: http://www.odva.org\n[62] S.-K. Kweon, K. G. Shin, and Q. Zheng, “Statistical Real-time Com-\nmunication over Ethernet for Manufacturing Automation Systems,”\nin Proceedings 5th IEEE Real-Time Technology and Applications\nSymposium, 1999, pp. 192–202.\n[63] X. Fan, Z. Wang, and Y. Sun, “How to guarantee factory communica-\ntion with switched ethernet: survey of its emerging technology,” vol. 3,\nNovember 2002, pp. 2525–2530.\n[64] Q. Zhang and W. Zhang, “Priority scheduling in switched industrial\nethernet,” Proceedings of the 2005 American Control Conference, pp.\n3366–3370, June 2005.\n[65] T. Skeie, S. Johannessen, and O. Holmeide, “The road to an end-to-end\ndeterministic ethernet,” in 4th IEEE International Workshop on Factory\nCommunication Systems, 2002, pp. 3–9.\n[66] Z. Wang, Y.-Q. Song, J.-M. Chen, and Y.-X. Sun, “Real time charac-\nteristics of ethernet and its improvement,” in Proceedings of the 4th\nWorld Congress on Intelligent Control and Automation, vol. 2, June\n2002, pp. 1311–1318.\n[67] J. Chen, Z. Wang, and Y. Sun, “Real-time capability analysis for switch\nindustrial ethernet traffic priority-based,” in Proceedings of the 2002\nInternational Conference on Control Applications, vol. 1, 2002, pp.\n525–529.\n[68] J. Jasperneite, P. Neumann, M. Theis, and K. Watson, “Deterministic\nreal-time communication with switched Ethernet,” in Proceedings of\n4th IEEE International Workshop on Factory Communication Systems\n(WFCS’02), 2002, pp. 28–30.\n[69] K. C. Lee and S. Lee, “Performance evaluation of switched ethernet\nfor real-time industrial communications,” Comput. Stand. Interfaces,\nvol. 24, no. 5, pp. 411–423, 2002.\n[70] M. Alves, E. Tovar, and F. Vasques, “Ethernet goes real-time: a survey\non research and technological developments,” Polytechnic Institute of\nPorto, School of Engineering, Tech. Rep., January 2000.\n[71] M. Azarov, “Approach to a latency-bound ethernet,” IEEE 802.1 AVB\ngroup, Tech. Rep., April 2006.\n[72] J. Lo¨ser and H. Ha¨rtig, “Low-latency hard real-time communication\nover switched ethernet,” in Proceedings of th 6th Euromicro Conference\non Real-Time Systems (ECRTS 2004), June 2004, pp. 13–22.\n[73] Y. Song, A. Koubaa, and L. I. Lorraine, “Switched ethernet for real-\ntime industrial communication: Modelling and message buffering delay\nevaluation,” in 4th IEEE WFCS 2002, Vasteras (Sweden). Springer-\nVerlag, 2002, pp. 27–30.\n[74] Y. Song, “Time constrained communication over switched ethernet,” in\nIFAC international conference on fieldbus systems and their applica-\ntions, November 2001, pp. 152–159.\n[75] E. Vonnahme, S. Ruping, and U. Ruckert, “Measurements in switched\nethernet networks used for automation systems,” Proceedings of the\nIEEE International Workshop on Factory Communication Systems, pp.\n231–238, 2000.\n[76] S. Varadarajan and T. Chiueh, “Ethereal: a host-transparent real-\ntime fast ethernet switch,” in Proceedings of the sixth International\nConference on Network Protocols, 1998, October 1998, pp. 12–21.\n[77] R. Braden, Ed., L. Zhang, S. Berson, S. Herzog, and S. Jamin,\n“Resource ReSerVation Protocol (RSVP) – Version 1 Functional Spec-\nification,” IETF, RFC 2205, Sept. 1997.\n[78] E. T. Group, “Website of the ethercat technology group,” 2008.\n[Online]. Available: http://www.ethercat.org\n[79] S. Ru¨ping, E. Vonnahme, and J. Jasperneite, “Analysis of Switched\nEthernet Networks with different Topologies used in Automation\nSystems,” in Field Bus Technology. Springer-Verlag, 1999, pp. 351–\n358.\n[80] H. Kirrmann and D. Dzung, “Selecting a Standard Redundancy Method\nfor Highly Available Industrial Networks,” 2006 IEEE International\nWorkshop on Factory Communication Systems, pp. 386–390, June\n2006.\n[81] Condor Engineering Incorporated, ARINC 429 protocol Tutorial, 2004.\n[Online]. Available: http://www.429-arinc.com/arinc-429-tutorial.html\n[82] N. C. Audsley and A. Grigg, “Timing analysis of the arinc 629 data bus\nfor real-time applications,” Microprocessors and Microsystems, vol. 21,\nno. 1, pp. 55–61, July 1997.\n[83] ARINC 664, Aircraft Data Network, Part 7: Deterministic Networks,\n2003.\n[84] D. Plummer, “Ethernet Address Resolution Protocol: Or Converting\nNetwork Protocol Addresses to 48.bit Ethernet Address for Transmis-\nsion on Ethernet Hardware,” IETF, RFC 0826, Nov. 1982.\n[85] F. Frances, C. Fraboul, and J. Grieu, “Using network calculus to\noptimize the afdx network,” in Proceedings of the 3rd European\nCongress Embedded Real Time Software, Toulouse, 2006.\n[86] H. Charara, J. Scharbarg, J. Ermont, and C. Fraboul, “Methods for\nbounding end-to end delays on an afdx network,” in Proceedings of\nthe 18th Euromicro Conference on Real-Time Systems (ECRTS06),\nDresden, 2006.\n[87] A. Mifdaoui, “Specification and validation of a communication network\nbased on switched ethernet for next generation military avionics\nsystems,” Ph.D. dissertation, INP, Toulouse, 2007.\n[88] A. Mifdaoui, F. Frances, and C. Fraboul, “Full-duplex switched\nethernet for next generation ”1553b”-based applications,” in 13th IEEE\nReal-Time and Embedded Technology and Applications Symposium\n(RTAS07), Bellevue, WA, United States, 2007.\n[89] Avionic Systems Standardisation Committee, Guide to low and medium\nspeed digital interface standards for avionic applications, June 2000.\n[90] N. Navet, Y. Song, F. Simonot-Lion, and C. Wilwert, “Trends in\nautomotive communication systems,” Proceedings of the IEEE, vol. 93,\nno. 6, pp. 1204–1223, June 2005.\n[91] M. Broy, I. H. Kru¨ger, A. Pretschner, and C. Salzmann, “Engineering\nautomotive software,” Proceedings of the IEEE, vol. 95, no. 2, pp.\n356–373, February 2007.\n[92] L. Consortium, LIN Specification Package Rev. 2.1, November 2006.\n[93] Robert Bosch GmbH, CAN Specification Version 2.0, September 1991.\n[94] MOST Cooperation, MOST Specification – Version 2.5-00, October\n2006.\n[95] M. Rahmani, R. Steffen, K. Tappayuthpijarn, E. Steinbach, and\nG. Giordano, “Performance analysis of different network topologies\nfor in-vehicle audio and video communication,” in 4th. International\nTelecommunication Networking Workshop on QoS in Multiservice IP\nNetworks, February 2008, pp. 179–184.\n[96] M. Rahmani, J. Hillebrand, W. Hintermaier, R. Bogenberger, and\nE. Steinbach, “A novel network architecture for in-vehicle audio and\nvideo communication,” in 2nd IEEE/IFIP International Workshop on\nBroadband Convergence Networks, May 2007, pp. 1–12.\n[97] J. Hillebrand, M. Rahmani, R. Bogenberger, and E. Steinbach, “Co-\nexistence of time-triggered and event-triggered traffic in switched full-\nduplex ethernet networks,” in Proceedings of the IEEE Second Interna-\ntional Symposium on Industrial Embedded Systems – SIES’2007, 2007,\npp. 217–224.\n[98] R. Steffen, R. Bogenberger, J. Hillebrand, W. Hintermaier, A. Winckler,\nand M. Rahmani, “Design and realization of an ip-based in-car net-\nwork architecture,” in Proceedings of the First Annual International\nSymposium on Vehicular Computing Systems (ISVCS 2008), Dublin,\nJuly 2008.\n[99] Infineon, “Fast ethernet over plastic optical fiber,” October 2006.\n[100] R. Yavatkar, P. Pai, and R. Finkel, “A reservation-based csma protocol\nfor integrated manufacturing networks,” IEEE Transactions on Systems,\nMan and Cybernetics, vol. 24, pp. 1247–1258, 1994.\n[101] D. Pritty, J. Malone, D. Smeed, S. Banerjee, and N. Lawrie, “A real-\ntime upgrade for ethernet based factory networking,” Proceedings of\nthe 1995 IEEE IECON 21st International Conference on Industrial\nElectronics, Control, and Instrumentation, vol. 2, pp. 1631–1637,\nNovember 1995.\n[102] C. Venkatramani and T. Chiueh, “Supporting real-time traffic on\nethernet,” Proceedings of the Real-Time Systems Symposium, pp. 282–\n286, December 1994.\n[103] IEEE Computer Society, “IEEE 802.1 AV Bridging Task Group,”\n2008. [Online]. Available: http://ieee802.org/1/pages/avbridges.html\n[104] M. J. Teener, J. Battaglia, A. Beliaev, E. H. Ryu, and Y. Kim,\n“Residential ethernet tutorial,” March 2005. [Online]. Available:\nhttp://ieee802.org/3/tutorial/mar05/tutorial 1 0305.pdf\n[105] M. J. Teener, “AV bridging and Ethernet AV,” 2007.\n[Online]. Available: www.ieee802.org/1/files/public/docs2007/as-mjt-\nAVB-Combo-For-ITU-0307.pdf\n[106] IEEE Computer Society, “P802.1Qat/D1.3 Draft Standard for Local and\nMetropolitan Area Networks–Virtual Bridged Local Area Networks–\nAmendment: Stream Reservation Protocol (SRP),” IEEE, Tech. Rep.,\nMay 2008, this is an unapproved IEEE Standards Draft.\n[107] ——, “P802.1Qav/D3.0 Draft Standard for Local and Metropolitan\nArea Networks–Virtual Bridged Local Area Networks–Amendment:\nForwarding and Queuing Enhancements for Time-Sensitive Streams,”\nIEEE, Tech. Rep., July 2008, this is an unapproved IEEE Standards\nDraft.\n[108] ——, “P802.1as/D4.0 Draft Standard for Local and Metropolitan Area\nNetworks–Timing and Synchronization for Time–Sensitive Applica-\n22\ntions in Bridged Local Area Network,” IEEE, Tech. Rep., August 2008,\nthis is an unapproved IEEE Standards Draft.\nJo¨rg Sommer received his Diploma degree (Dipl.-\nIng. (BA)) in information technology from the Uni-\nversity of Cooperative Education (Berufsakademie)\nHeidenheim, Germany, and his Master degree in\ncomputer science (M. Comp. Sc.) from the Univer-\nsity of Ulm, Germany, in 2002 and 2004 respec-\ntively. Since 2005, he is a member of the research\nstaff of the Institute of Communication Networks\nand Computer Engineering (IKR), University of\nStuttgart. His research interests include performance\nevaluation of communication networks with a focus\non embedded networks, and topology design and optimization problems. He\nis a member of the German Gesellschaft fu¨r Informatik (Computer Science\nSociety).\nSebastian Gunreben received his Dipl.-Ing. degree\nin Mechatronics in 2004 from the University of\nStuttgart, Germany. Since then he has been with the\nInstitute of Communication Networks and Computer\nEngineering (IKR) at the University of Stuttgart\nwhere he works on traffic engineering for IP-over-\nWDM networks in several projects. He focuses on\ncontrol plane aspects of multi-layer networks as\nwell as on the formal description of out-of-sequence\npacket arrivals.\nFrank Feller received the Dipl.-Ing degree in\nelectrical engineering and information technology\nfrom the University of Stuttgart, Germany, and the\nDiploˆme d’inge´nieur degree from the Ecole Na-\ntionale Supe´rieure des Te´le´communications (ENST)\nin Paris, France, in 2007. Since 2008 he has been\nwith the Institute of Communication Networks and\nComputer Engineering (IKR) at the University of\nStuttgart. His research focus is on packet-switched\ntransport networks.\nMartin Ko¨hn received his Diploma degree (Dipl.-\nIng.) in Electrical Engineering and Information\nTechnology from the University of Stuttgart in 2002.\nSince then he is a member of the research staff of\nthe Institute of Communication Networks and Com-\nputer Engineering (IKR), University of Stuttgart. His\nmajor research interests include traffic engineering\nand network dimensioning of dynamic multi-layer\ntransport networks.\nAhlem Mifdaoui received the M.S. degree in com-\nputer science and air traffic management engineering\nin 2004 from the Ecole Nationale de l’Aviation civile\n(ENAC) in Toulouse, and the Ph.D. in computer\nand telecommunications engineering from the In-\nstitut National Polytechnique of Toulouse (INPT)\nin 2007. She is an associate professor of computer\nengineering at Institut Superieur de l’Aeronautique\net de l’Espace (ISAE) in Toulouse, France. Her\nmain research interests lie in the fields of real-time\nnetworks and embedded systems especially targeted\nto avionics and satellites applications.\nDetlef Saß received his Diploma degree (dipl.math.)\nin mathematics from the University of Stuttgart in\n2000. During 2001 - 2002, he was with DeTeLine\n(a subsidiary of Deutsche Telekom), as a member\nof the technical and consulting staff focused on the\narea of IP telephony systems. In 2003, he joined\nthe Institute of Communication Networks and Com-\nputer Engineering (IKR), University of Stuttgart, as\nmember of the research staff. His major research\ninterests include the modeling, characterization and\nmeasurement of network traffic in transport networks\nand in the Internet.\nJoachim Scharf received the Dipl.-Ing. degree\nin electrical engineering from the University of\nStuttgart in 2005. Since 2006 he has been working\nas a scientific staff member at the Institute of Com-\nmunication Networks and Computer Engineering\n(IKR), University of Stuttgart. His main research\nareas include network design and survivability.\n",
            "id": 5185196,
            "identifiers": [
                {
                    "identifier": "oai:oatao.univ-toulouse.fr:2236",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "12040594",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1109/surv.2010.021110.00086",
                    "type": "DOI"
                }
            ],
            "title": "Ethernet - a survey on its fields of application",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:oatao.univ-toulouse.fr:2236"
            ],
            "publishedDate": "2010-04-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 10427514,
                    "title": "61491 Electrical equipment of industrial machines – Serial data link for real-time communication between controls and drives,” IEC,",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.3403/30176598",
                    "raw": "——, “IEC 61491 Electrical equipment of industrial machines – Serial data link for real-time communication between controls and drives,” IEC, Tech. Rep., October 2007.",
                    "cites": null
                },
                {
                    "id": 10427450,
                    "title": "802.1X Port-Based Authentication.",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1201/9781420044652",
                    "raw": "E. L. Brown, 802.1X Port-Based Authentication. Boston, MA, USA: Auerbach Publications, 2006.",
                    "cites": null
                },
                {
                    "id": 10427583,
                    "title": "A novel network architecture for in-vehicle audio and video communication,”",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1109/bcn.2007.372741",
                    "raw": "M. Rahmani, J. Hillebrand, W. Hintermaier, R. Bogenberger, and E. Steinbach, “A novel network architecture for in-vehicle audio and video communication,” in 2nd IEEE/IFIP International Workshop on Broadband Convergence Networks, May 2007, pp. 1–12.",
                    "cites": null
                },
                {
                    "id": 10427513,
                    "title": "A perspective on Ethernet-TCP/IP as a ﬁeldbus,”",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1007/978-3-7091-6421-1_37",
                    "raw": "J.-D. Decotignie, “A perspective on Ethernet-TCP/IP as a ﬁeldbus,” in Proceedings volume from the 4th IFAC Conference, 2001.",
                    "cites": null
                },
                {
                    "id": 10427590,
                    "title": "A realtime upgrade for ethernet based factory networking,”",
                    "authors": [],
                    "date": "1995",
                    "doi": "10.1109/iecon.1995.484195",
                    "raw": "D. Pritty, J. Malone, D. Smeed, S. Banerjee, and N. Lawrie, “A realtime upgrade for ethernet based factory networking,” Proceedings of the 1995 IEEE IECON 21st International Conference on Industrial Electronics, Control, and Instrumentation, vol. 2, pp. 1631–1637, November 1995.",
                    "cites": null
                },
                {
                    "id": 10427589,
                    "title": "A reservation-based csma protocol for integrated manufacturing networks,”",
                    "authors": [],
                    "date": "1994",
                    "doi": "10.1109/21.299705",
                    "raw": "R. Yavatkar, P. Pai, and R. Finkel, “A reservation-based csma protocol for integrated manufacturing networks,” IEEE Transactions on Systems, Man and Cybernetics, vol. 24, pp. 1247–1258, 1994.",
                    "cites": null
                },
                {
                    "id": 10427573,
                    "title": "Aircraft Data Network, Part 7: Deterministic Networks,",
                    "authors": [],
                    "date": "2003",
                    "doi": null,
                    "raw": "ARINC 664, Aircraft Data Network, Part 7: Deterministic Networks, 2003.",
                    "cites": null
                },
                {
                    "id": 10427428,
                    "title": "Analysis of Computer and Communication Networks.",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1007/978-0-387-74437-7",
                    "raw": "F. Gebali, Analysis of Computer and Communication Networks. Springer Publishing Company, Incorporated, 2008.",
                    "cites": null
                },
                {
                    "id": 10427557,
                    "title": "Analysis of Switched Ethernet Networks with different Topologies used in Automation Systems,”",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1007/978-3-7091-6421-1_46",
                    "raw": "S. R¨ uping, E. Vonnahme, and J. Jasperneite, “Analysis of Switched Ethernet Networks with different Topologies used in Automation Systems,” in Field Bus Technology. Springer-Verlag, 1999, pp. 351– 358.",
                    "cites": null
                },
                {
                    "id": 10427543,
                    "title": "Approach to a latency-bound ethernet,”",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "M. Azarov, “Approach to a latency-bound ethernet,” IEEE 802.1 AVB group, Tech. Rep., April 2006.",
                    "cites": null
                },
                {
                    "id": 10427511,
                    "title": "Automating with PROFINET: Industrial communication based on Industrial Ethernet.",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "R. Pigan and M. Metter, Automating with PROFINET: Industrial communication based on Industrial Ethernet. Wiley, June 2006.",
                    "cites": null
                },
                {
                    "id": 10427601,
                    "title": "AV bridging and Ethernet AV,”",
                    "authors": [],
                    "date": "2007",
                    "doi": null,
                    "raw": "M. J. Teener, “AV bridging and Ethernet AV,” 2007.",
                    "cites": null
                },
                {
                    "id": 10427579,
                    "title": "Avionic Systems Standardisation Committee, Guide to low and medium speed digital interface standards for avionic applications,",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "Avionic Systems Standardisation Committee, Guide to low and medium speed digital interface standards for avionic applications, June 2000.",
                    "cites": null
                },
                {
                    "id": 10427584,
                    "title": "Coexistence of time-triggered and event-triggered trafﬁc in switched fullduplex ethernet networks,”",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1109/sies.2007.4297338",
                    "raw": "J. Hillebrand, M. Rahmani, R. Bogenberger, and E. Steinbach, “Coexistence of time-triggered and event-triggered trafﬁc in switched fullduplex ethernet networks,” in Proceedings of the IEEE Second International Symposium on Industrial Embedded Systems – SIES’2007, 2007, pp. 217–224.",
                    "cites": null
                },
                {
                    "id": 10427571,
                    "title": "Condor Engineering Incorporated, ARINC 429 protocol Tutorial,",
                    "authors": [],
                    "date": "2004",
                    "doi": null,
                    "raw": "Condor Engineering Incorporated, ARINC 429 protocol Tutorial, 2004.",
                    "cites": null
                },
                {
                    "id": 10427586,
                    "title": "Design and realization of an ip-based in-car network architecture,”",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.4108/icst.isvcs2008.3543",
                    "raw": "R. Steffen, R. Bogenberger, J. Hillebrand, W. Hintermaier, A. Winckler, and M. Rahmani, “Design and realization of an ip-based in-car network architecture,” in Proceedings of the First Annual International Symposium on Vehicular Computing Systems (ISVCS 2008), Dublin, July 2008.",
                    "cites": null
                },
                {
                    "id": 10427537,
                    "title": "Deterministic real-time communication with switched Ethernet,”",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1109/wfcs.2002.1159695",
                    "raw": "J. Jasperneite, P. Neumann, M. Theis, and K. Watson, “Deterministic real-time communication with switched Ethernet,” in Proceedings of 4th IEEE International Workshop on Factory Communication Systems (WFCS’02), 2002, pp. 28–30.",
                    "cites": null
                },
                {
                    "id": 10427499,
                    "title": "Draft Standard for Local and Metropolitan Area Networks–Virtual Bridged Local Area Networks, Amendment 6: Provider Backbone Bridges,”",
                    "authors": [],
                    "date": "2007",
                    "doi": null,
                    "raw": "——, “802.1ah: Draft Standard for Local and Metropolitan Area Networks–Virtual Bridged Local Area Networks, Amendment 6: Provider Backbone Bridges,” Nov. 2007.",
                    "cites": null
                },
                {
                    "id": 10427500,
                    "title": "Draft Standard for Local and Metropolitan Area Networks–Virtual Bridged Local Area Networks, Amendment: Provider Backbone Bridge–Trafﬁc Engineering,”",
                    "authors": [],
                    "date": "2007",
                    "doi": null,
                    "raw": "——, “802.1Qay: Draft Standard for Local and Metropolitan Area Networks–Virtual Bridged Local Area Networks, Amendment: Provider Backbone Bridge–Trafﬁc Engineering,” Dec. 2007.",
                    "cites": null
                },
                {
                    "id": 10427581,
                    "title": "Engineering automotive software,”",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1109/jproc.2006.888386",
                    "raw": "M. Broy, I. H. Kr¨ uger, A. Pretschner, and C. Salzmann, “Engineering automotive software,” Proceedings of the IEEE, vol. 95, no. 2, pp. 356–373, February 2007.",
                    "cites": null
                },
                {
                    "id": 10427553,
                    "title": "Ethereal: a host-transparent realtime fast ethernet switch,”",
                    "authors": [],
                    "date": "1998",
                    "doi": "10.1109/icnp.1998.723721",
                    "raw": "S. Varadarajan and T. Chiueh, “Ethereal: a host-transparent realtime fast ethernet switch,” in Proceedings of the sixth International Conference on Network Protocols, 1998, October 1998, pp. 12–21.",
                    "cites": null
                },
                {
                    "id": 10427574,
                    "title": "Ethernet Address Resolution Protocol: Or Converting Network Protocol Addresses to 48.bit Ethernet Address for Transmission on Ethernet Hardware,”",
                    "authors": [],
                    "date": "1982",
                    "doi": null,
                    "raw": "D. Plummer, “Ethernet Address Resolution Protocol: Or Converting Network Protocol Addresses to 48.bit Ethernet Address for Transmission on Ethernet Hardware,” IETF, RFC 0826, Nov. 1982.",
                    "cites": null
                },
                {
                    "id": 10427541,
                    "title": "Ethernet goes real-time: a survey on research and technological developments,”",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "M. Alves, E. Tovar, and F. Vasques, “Ethernet goes real-time: a survey on research and technological developments,” Polytechnic Institute of Porto, School of Engineering, Tech. Rep., January 2000.",
                    "cites": null
                },
                {
                    "id": 10427510,
                    "title": "Ethernet in the First Mile",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1002/0470022515.ch12",
                    "raw": "M. Beck, Ethernet in the First Mile – The IEEE 802.3ah EFM Standard. McGraw-Hill, 2005.",
                    "cites": null
                },
                {
                    "id": 10427509,
                    "title": "Ethernet passive optical network architectures and dynamic bandwidth allocation algorithms,”",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1109/comst.2008.4625804",
                    "raw": "M. P. McGarry, M. Reisslein, and M. Maier, “Ethernet passive optical network architectures and dynamic bandwidth allocation algorithms,” IEEE Communications Surveys & Tutorials, vol. 10, no. 3, pp. 46–60, 2008.",
                    "cites": null
                },
                {
                    "id": 10427524,
                    "title": "Ethernet-based real-time and industrial communications,”",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1109/jproc.2005.849721",
                    "raw": "J.-D. Decotignie, “Ethernet-based real-time and industrial communications,” Proceedings of the IEEE, vol. 93, no. 6, pp. 1102–1117, June 2005.21",
                    "cites": null
                },
                {
                    "id": 10427430,
                    "title": "Ethernet: Distributed packet switching for local computer networks,”",
                    "authors": [],
                    "date": "1975",
                    "doi": "10.1145/357980.358015",
                    "raw": "R. M. Metcalfe and D. R. Boggs, “Ethernet: Distributed packet switching for local computer networks,” XEROX Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, California, USA, Tech. Rep., 1975.20",
                    "cites": null
                },
                {
                    "id": 10427426,
                    "title": "Ethernet: The Deﬁnitive Guide. O’Reilly",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "C. E. Spurgeon, Ethernet: The Deﬁnitive Guide. O’Reilly Media, February 2000, vol. 1.",
                    "cites": null
                },
                {
                    "id": 10427588,
                    "title": "Fast ethernet over plastic optical ﬁber,”",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "Inﬁneon, “Fast ethernet over plastic optical ﬁber,” October 2006.",
                    "cites": null
                },
                {
                    "id": 10427578,
                    "title": "Full-duplex switched ethernet for next generation ”1553b”-based applications,”",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1109/rtas.2007.13",
                    "raw": "A. Mifdaoui, F. Frances, and C. Fraboul, “Full-duplex switched ethernet for next generation ”1553b”-based applications,” in 13th IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS07), Bellevue, WA, United States, 2007.",
                    "cites": null
                },
                {
                    "id": 10427488,
                    "title": "G.707/Y.1322: Network node interface for the synchronous digital hierarchy",
                    "authors": [],
                    "date": "2003",
                    "doi": null,
                    "raw": "——, “Rec. G.707/Y.1322: Network node interface for the synchronous digital hierarchy (SDH),” International Telecommunication Union, ITU-T, Dec. 2003.",
                    "cites": null
                },
                {
                    "id": 10427494,
                    "title": "G.709/Y.1331: Interfaces for the Optical Transport Network",
                    "authors": [],
                    "date": "2003",
                    "doi": null,
                    "raw": "——, “Rec. G.709/Y.1331: Interfaces for the Optical Transport Network (OTN),” International Telecommunication Union, ITU-T, Mar. 2003.",
                    "cites": null
                },
                {
                    "id": 10427490,
                    "title": "G.783: Characteristics of synchronous digital hierarchy (SDH) equipment functional blocks,” International Telecommunication",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "——, “Rec. G.783: Characteristics of synchronous digital hierarchy (SDH) equipment functional blocks,” International Telecommunication Union, ITU-T, Mar. 2006.",
                    "cites": null
                },
                {
                    "id": 10427479,
                    "title": "G.8011.1/Y.1307.1: Ethernet private line service,” International Telecommunication",
                    "authors": [],
                    "date": "2004",
                    "doi": null,
                    "raw": "——, “Rec. G.8011.1/Y.1307.1: Ethernet private line service,” International Telecommunication Union, ITU-T, Aug. 2004.",
                    "cites": null
                },
                {
                    "id": 10427481,
                    "title": "G.8011.2/Y.1307.2: Ethernet virtual private line service,” International Telecommunication",
                    "authors": [],
                    "date": "2005",
                    "doi": null,
                    "raw": "——, “Rec. G.8011.2/Y.1307.2: Ethernet virtual private line service,” International Telecommunication Union, ITU-T, Sept. 2005.",
                    "cites": null
                },
                {
                    "id": 10427477,
                    "title": "G.8011/Y.1307: Ethernet over Transport - Ethernet services framework,” International Telecommunication",
                    "authors": [],
                    "date": "2004",
                    "doi": null,
                    "raw": "ITU, “Rec. G.8011/Y.1307: Ethernet over Transport - Ethernet services framework,” International Telecommunication Union, ITU-T, Aug. 2004.",
                    "cites": null
                },
                {
                    "id": 10427492,
                    "title": "G.803 : Architecture of transport networks based on the synchronous digital hierarchy",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "——, “Rec. G.803 : Architecture of transport networks based on the synchronous digital hierarchy (SDH),” International Telecommunication Union, ITU-T, Mar. 2000.",
                    "cites": null
                },
                {
                    "id": 10427504,
                    "title": "G.807/Y.1302: Requirements for automatic switched transport networks",
                    "authors": [],
                    "date": "2001",
                    "doi": null,
                    "raw": "——, “Rec. G.807/Y.1302: Requirements for automatic switched transport networks (ASTN),” International Telecommunication Union, ITUT, 2001, wITHDRAWN.",
                    "cites": null
                },
                {
                    "id": 10427507,
                    "title": "G.991.2: Single-pair high-speed digital subscriber line (SHDSL) transceivers,” International Telecommunication",
                    "authors": [],
                    "date": "2003",
                    "doi": null,
                    "raw": "ITU, “Rec. G.991.2: Single-pair high-speed digital subscriber line (SHDSL) transceivers,” International Telecommunication Union, ITUT, Dec. 2003.",
                    "cites": null
                },
                {
                    "id": 10427508,
                    "title": "G.993.1: Very high speed digital subscriber line transceivers,” International Telecommunication",
                    "authors": [],
                    "date": "2004",
                    "doi": null,
                    "raw": "——, “Rec. G.993.1: Very high speed digital subscriber line transceivers,” International Telecommunication Union, ITU-T, June 2004.",
                    "cites": null
                },
                {
                    "id": 10427505,
                    "title": "Generalized Multi-Protocol Label Switching (GMPLS)",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1109/glocomw.2004.1417605",
                    "raw": "E. Mannie and Ed., “Generalized Multi-Protocol Label Switching (GMPLS) Architecture,” IETF, RFC 3945, Oct. 2004.",
                    "cites": null
                },
                {
                    "id": 10427528,
                    "title": "How to guarantee factory communication with switched ethernet: survey of its emerging technology,”",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1109/iecon.2002.1185371",
                    "raw": "X. Fan, Z. Wang, and Y. Sun, “How to guarantee factory communication with switched ethernet: survey of its emerging technology,” vol. 3, November 2002, pp. 2525–2530.",
                    "cites": null
                },
                {
                    "id": 10427518,
                    "title": "How to gurantee realtime behavior using ethernet,”",
                    "authors": [],
                    "date": "2004",
                    "doi": null,
                    "raw": "J. Jasperneite and P. Neumann, “How to gurantee realtime behavior using ethernet,” in 11th IFAC Symposium on Information Control Problems in Manufacturing (INCOM’2004), April 2004.",
                    "cites": null
                },
                {
                    "id": 10427439,
                    "title": "IEEE Standard for Local and Metropolitan Area Networks–Logical Link Control,”",
                    "authors": [],
                    "date": "1998",
                    "doi": "10.3403/00411203",
                    "raw": "——, “802.2: IEEE Standard for Local and Metropolitan Area Networks–Logical Link Control,” 1998.",
                    "cites": null
                },
                {
                    "id": 10427440,
                    "title": "IEEE Standard for Local and Metropolitan Area Networks–Media Access Control (MAC)",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.3403/30293683",
                    "raw": "——, “802.1D: IEEE Standard for Local and Metropolitan Area Networks–Media Access Control (MAC) Bridges,” 2004.",
                    "cites": null
                },
                {
                    "id": 10427436,
                    "title": "IEEE Standard for Local and Metropolitan Area Networks–Speciﬁc requirements, Part 5: Token ring access method and Physical Layer speciﬁcations,”",
                    "authors": [],
                    "date": "1998",
                    "doi": "10.3403/00327026",
                    "raw": "——, “802.5: IEEE Standard for Local and Metropolitan Area Networks–Speciﬁc requirements, Part 5: Token ring access method and Physical Layer speciﬁcations,” May 1998.",
                    "cites": null
                },
                {
                    "id": 10427438,
                    "title": "IEEE Standards for Local and Metropolitan Area Networks–Wireless LAN Medium Access Control (MAC) and Physical Layer (PHY) Speciﬁcations,”",
                    "authors": [],
                    "date": "2003",
                    "doi": null,
                    "raw": "——, “802.11: IEEE Standards for Local and Metropolitan Area Networks–Wireless LAN Medium Access Control (MAC) and Physical Layer (PHY) Speciﬁcations,” 2003.",
                    "cites": null
                },
                {
                    "id": 10427447,
                    "title": "Impact of topology and link aggregation on a PC cluster with Ethernet,” Cluster Computing,",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1109/clustr.2008.4663782",
                    "raw": "W. Takafumi, N. Masahiro, T. Hiroyasu, T. Otsuka, and M. Koibuchi, “Impact of topology and link aggregation on a PC cluster with Ethernet,” Cluster Computing, 2008 IEEE International Conference on, pp. 280–285, October 2008.",
                    "cites": null
                },
                {
                    "id": 10427449,
                    "title": "Implementing 802.1X Security Solutions for Wired and Wireless Networks.",
                    "authors": [],
                    "date": "2008",
                    "doi": null,
                    "raw": "J. Geier, Implementing 802.1X Security Solutions for Wired and Wireless Networks. Wiley, 2008.",
                    "cites": null
                },
                {
                    "id": 10427512,
                    "title": "Industrial Ethernet.",
                    "authors": [],
                    "date": "2004",
                    "doi": null,
                    "raw": "P. S. Marshall and J. S. Rinaldi, Industrial Ethernet. ISA, September 2004.",
                    "cites": null
                },
                {
                    "id": 10427441,
                    "title": "Internet Assigned Numbers Authority (IANA), “IANA Ethernet numbers assignment,”",
                    "authors": [],
                    "date": "2008",
                    "doi": null,
                    "raw": "Internet Assigned Numbers Authority (IANA), “IANA Ethernet numbers assignment,” August 2008. [Online]. Available: http://www.iana.org/assignments/ethernet-numbers",
                    "cites": null
                },
                {
                    "id": 10427545,
                    "title": "Low-latency hard real-time communication over switched ethernet,” in",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1109/emrts.2004.1310992",
                    "raw": "J. L¨ oser and H. H¨ artig, “Low-latency hard real-time communication over switched ethernet,” in Proceedings of th 6th Euromicro Conference on Real-Time Systems (ECRTS 2004), June 2004, pp. 13–22.",
                    "cites": null
                },
                {
                    "id": 10427551,
                    "title": "Measurements in switched ethernet networks used for automation systems,”",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1109/wfcs.2000.882554",
                    "raw": "E. Vonnahme, S. Ruping, and U. Ruckert, “Measurements in switched ethernet networks used for automation systems,” Proceedings of the IEEE International Workshop on Factory Communication Systems, pp. 231–238, 2000.",
                    "cites": null
                },
                {
                    "id": 10427576,
                    "title": "Methods for bounding end-to end delays on an afdx network,”",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/ecrts.2006.15",
                    "raw": "H. Charara, J. Scharbarg, J. Ermont, and C. Fraboul, “Methods for bounding end-to end delays on an afdx network,” in Proceedings of the 18th Euromicro Conference on Real-Time Systems (ECRTS06), Dresden, 2006.",
                    "cites": null
                },
                {
                    "id": 10427432,
                    "title": "Multipoint data communication system with collision detection,” United States Patent 4063220,",
                    "authors": [],
                    "date": "1977",
                    "doi": null,
                    "raw": "R. M. Metcalfe, D. R. Boggs, C. P. Thacker, and B. W. Lampson, “Multipoint data communication system with collision detection,” United States Patent 4063220, December 1977.",
                    "cites": null
                },
                {
                    "id": 10427444,
                    "title": "Network Warrior.",
                    "authors": [],
                    "date": "2007",
                    "doi": null,
                    "raw": "G. Donahue, Network Warrior. O’Reilly Media, Inc., 2007.",
                    "cites": null
                },
                {
                    "id": 10427442,
                    "title": "Novell’s Guide to LAN/WAN Analysis.",
                    "authors": [],
                    "date": "1998",
                    "doi": null,
                    "raw": "L. A. Chappell, Novell’s Guide to LAN/WAN Analysis. Foster City, CA, USA: IDG Books Worldwide, Inc., 1998.",
                    "cites": null
                },
                {
                    "id": 10427446,
                    "title": "On the effects of the IEEE 802.3x ﬂow control in full-duplex Ethernet LANs,” Local Computer Networks,",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1109/lcn.1999.802012",
                    "raw": "O. Feuser and A. Wenzel, “On the effects of the IEEE 802.3x ﬂow control in full-duplex Ethernet LANs,” Local Computer Networks, 1999. LCN ’99. Conference on, pp. 160–161, October 1999.",
                    "cites": null
                },
                {
                    "id": 10427496,
                    "title": "PBT - Carrier Grade Ethernet Transport,”",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1002/9781118435960.ch11",
                    "raw": "TPACK, “PBB-TE, PBT - Carrier Grade Ethernet Transport,” TPACK, Tech. Rep., 2007. [Online]. Available: www.tpack.com",
                    "cites": null
                },
                {
                    "id": 10427582,
                    "title": "Performance analysis of different network topologies for in-vehicle audio and video communication,”",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1109/itnews.2008.4488150",
                    "raw": "M. Rahmani, R. Steffen, K. Tappayuthpijarn, E. Steinbach, and G. Giordano, “Performance analysis of different network topologies for in-vehicle audio and video communication,” in 4th. International Telecommunication Networking Workshop on QoS in Multiservice IP Networks, February 2008, pp. 179–184.",
                    "cites": null
                },
                {
                    "id": 10427539,
                    "title": "Performance evaluation of switched ethernet for real-time industrial communications,”",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1016/s0920-5489(02)00070-3",
                    "raw": "K. C. Lee and S. Lee, “Performance evaluation of switched ethernet for real-time industrial communications,” Comput. Stand. Interfaces, vol. 24, no. 5, pp. 411–423, 2002.",
                    "cites": null
                },
                {
                    "id": 10427530,
                    "title": "Priority scheduling in switched industrial ethernet,”",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1109/acc.2005.1470490",
                    "raw": "Q. Zhang and W. Zhang, “Priority scheduling in switched industrial ethernet,” Proceedings of the 2005 American Control Conference, pp. 3366–3370, June 2005.",
                    "cites": null
                },
                {
                    "id": 10427533,
                    "title": "Real time characteristics of ethernet and its improvement,”",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1109/wcica.2002.1020794",
                    "raw": "Z. Wang, Y.-Q. Song, J.-M. Chen, and Y.-X. Sun, “Real time characteristics of ethernet and its improvement,” in Proceedings of the 4th World Congress on Intelligent Control and Automation, vol. 2, June 2002, pp. 1311–1318.",
                    "cites": null
                },
                {
                    "id": 10427535,
                    "title": "Real-time capability analysis for switch industrial ethernet trafﬁc priority-based,”",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1109/cca.2002.1040240",
                    "raw": "J. Chen, Z. Wang, and Y. Sun, “Real-time capability analysis for switch industrial ethernet trafﬁc priority-based,” in Proceedings of the 2002 International Conference on Control Applications, vol. 1, 2002, pp. 525–529.",
                    "cites": null
                },
                {
                    "id": 10427522,
                    "title": "Real-time ethernet – industry prospective,”",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1109/jproc.2005.849720",
                    "raw": "M. Felser, “Real-time ethernet – industry prospective,” Proceedings of the IEEE, vol. 93, no. 6, pp. 1118–1129, June 2005.",
                    "cites": null
                },
                {
                    "id": 10427592,
                    "title": "Residential ethernet tutorial,”",
                    "authors": [],
                    "date": "2005",
                    "doi": null,
                    "raw": "M. J. Teener, J. Battaglia, A. Beliaev, E. H. Ryu, and Y. Kim, “Residential ethernet tutorial,” March 2005. [Online]. Available: http://ieee802.org/3/tutorial/mar05/tutorial 1 0305.pdf",
                    "cites": null
                },
                {
                    "id": 10427570,
                    "title": "Selecting a Standard Redundancy Method for Highly Available Industrial Networks,”",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/wfcs.2006.1704184",
                    "raw": "H. Kirrmann and D. Dzung, “Selecting a Standard Redundancy Method for Highly Available Industrial Networks,” 2006 IEEE International Workshop on Factory Communication Systems, pp. 386–390, June 2006.",
                    "cites": null
                },
                {
                    "id": 10427498,
                    "title": "Society, “802.1ad: IEEE Standard for Local and Metropolitan Area Networks–Virtual Bridged Local Area Networks, Amendment 4: Provider Bridges,”",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "IEEE Computer Society, “802.1ad: IEEE Standard for Local and Metropolitan Area Networks–Virtual Bridged Local Area Networks, Amendment 4: Provider Bridges,” May 2006.",
                    "cites": null
                },
                {
                    "id": 10427502,
                    "title": "Society, “802.1ag: IEEE Standard for Local and Metropolitan Area Networks–Virtual Bridged Local Area Networks, Amendment 5: Connectivity Fault Management,”",
                    "authors": [],
                    "date": "2007",
                    "doi": null,
                    "raw": "IEEE Computer Society, “802.1ag: IEEE Standard for Local and Metropolitan Area Networks–Virtual Bridged Local Area Networks, Amendment 5: Connectivity Fault Management,” Dec. 2007.",
                    "cites": null
                },
                {
                    "id": 10427443,
                    "title": "Society, “802.1Q: IEEE Standard for Local and Metropolitan Area Networks–Virtual Bridged Local Area Networks,”",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1109/ieeestd.2002.93395",
                    "raw": "IEEE Computer Society, “802.1Q: IEEE Standard for Local and Metropolitan Area Networks–Virtual Bridged Local Area Networks,” 2005.",
                    "cites": null
                },
                {
                    "id": 10427448,
                    "title": "Society, “802.1X: IEEE Standard for Local and Metropolitan Area Networks–Port-Based Network Access Control ,”",
                    "authors": [],
                    "date": "2001",
                    "doi": null,
                    "raw": "IEEE Computer Society, “802.1X: IEEE Standard for Local and Metropolitan Area Networks–Port-Based Network Access Control ,” Oct. 2001.",
                    "cites": null
                },
                {
                    "id": 10427434,
                    "title": "Society, “802.3: IEEE Standard for Local and Metropolitan Area Networks–Carrier sense multiple access with collision detection (CSMA/CD) access method and physical layer speciﬁ-cations,”",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.3403/00280958",
                    "raw": "IEEE Computer Society, “802.3: IEEE Standard for Local and Metropolitan Area Networks–Carrier sense multiple access with collision detection (CSMA/CD) access method and physical layer speciﬁ-cations,” 2005.",
                    "cites": null
                },
                {
                    "id": 10427602,
                    "title": "Society, “P802.1Qat/D1.3 Draft Standard for Local and Metropolitan Area Networks–Virtual Bridged Local Area Networks– Amendment:",
                    "authors": [],
                    "date": "2008",
                    "doi": null,
                    "raw": "IEEE Computer Society, “P802.1Qat/D1.3 Draft Standard for Local and Metropolitan Area Networks–Virtual Bridged Local Area Networks– Amendment: Stream Reservation Protocol (SRP),” IEEE, Tech. Rep., May 2008, this is an unapproved IEEE Standards Draft.",
                    "cites": null
                },
                {
                    "id": 10427577,
                    "title": "Speciﬁcation and validation of a communication network based on switched ethernet for next generation military avionics systems,”",
                    "authors": [],
                    "date": "2007",
                    "doi": null,
                    "raw": "A. Mifdaoui, “Speciﬁcation and validation of a communication network based on switched ethernet for next generation military avionics systems,” Ph.D. dissertation, INP, Toulouse, 2007.",
                    "cites": null
                },
                {
                    "id": 10427605,
                    "title": "Standard for Local and Metropolitan Area Networks–Timing and Synchronization for Time–Sensitive Applica-22 tions in Bridged Local Area Network,”",
                    "authors": [],
                    "date": "2008",
                    "doi": null,
                    "raw": "——, “P802.1as/D4.0 Draft Standard for Local and Metropolitan Area Networks–Timing and Synchronization for Time–Sensitive Applica-22 tions in Bridged Local Area Network,” IEEE, Tech. Rep., August 2008, this is an unapproved IEEE Standards Draft. J¨ org Sommer received his Diploma degree (Dipl.-Ing. (BA)) in information technology from the University of Cooperative Education (Berufsakademie) Heidenheim, Germany, and his Master degree in computer science (M. Comp. Sc.) from the University of Ulm, Germany, in 2002 and 2004 respectively. Since 2005, he is a member of the research staff of the Institute of Communication Networks and Computer Engineering (IKR), University of Stuttgart. His research interests include performance evaluation of communication networks with a focus on embedded networks, and topology design and optimization problems. He is a member of the German Gesellschaft f¨ ur Informatik (Computer Science Society). Sebastian Gunreben received his Dipl.-Ing. degree in Mechatronics in 2004 from the University of Stuttgart, Germany. Since then he has been with the Institute of Communication Networks and Computer Engineering (IKR) at the University of Stuttgart where he works on trafﬁc engineering for IP-overWDM networks in several projects. He focuses on control plane aspects of multi-layer networks as well as on the formal description of out-of-sequence packet arrivals. Frank Feller received the Dipl.-Ing degree in electrical engineering and information technology from the University of Stuttgart, Germany, and the Diplˆ ome d’ing´ enieur degree from the Ecole Nationale Sup´ erieure des T´ el´ ecommunications (ENST) in Paris, France, in 2007. Since 2008 he has been with the Institute of Communication Networks and Computer Engineering (IKR) at the University of Stuttgart. His research focus is on packet-switched transport networks. Martin K¨ ohn received his Diploma degree (Dipl.-Ing.) in Electrical Engineering and Information Technology from the University of Stuttgart in 2002. Since then he is a member of the research staff of the Institute of Communication Networks and Computer Engineering (IKR), University of Stuttgart. His major research interests include trafﬁc engineering and network dimensioning of dynamic multi-layer transport networks. Ahlem Mifdaoui received the M.S. degree in computer science and air trafﬁc management engineering in 2004 from the Ecole Nationale de l’Aviation civile (ENAC) in Toulouse, and the Ph.D. in computer and telecommunications engineering from the Institut National Polytechnique of Toulouse (INPT) in 2007. She is an associate professor of computer engineering at Institut Superieur de l’Aeronautique et de l’Espace (ISAE) in Toulouse, France. Her main research interests lie in the ﬁelds of real-time networks and embedded systems especially targeted to avionics and satellites applications. Detlef Saß received his Diploma degree (dipl.math.) in mathematics from the University of Stuttgart in 2000. During 2001 - 2002, he was with DeTeLine (a subsidiary of Deutsche Telekom), as a member of the technical and consulting staff focused on the area of IP telephony systems. In 2003, he joined the Institute of Communication Networks and Computer Engineering (IKR), University of Stuttgart, as member of the research staff. His major research interests include the modeling, characterization and measurement of network trafﬁc in transport networks and in the Internet. Joachim Scharf received the Dipl.-Ing. degree in electrical engineering from the University of Stuttgart in 2005. Since 2006 he has been working as a scientiﬁc staff member at the Institute of Communication Networks and Computer Engineering (IKR), University of Stuttgart. His main research areas include network design and survivability.",
                    "cites": null
                },
                {
                    "id": 10427604,
                    "title": "Standard for Local and Metropolitan Area Networks–Virtual Bridged Local Area Networks–Amendment: Forwarding and Queuing Enhancements for Time-Sensitive Streams,”",
                    "authors": [],
                    "date": "2008",
                    "doi": null,
                    "raw": "——, “P802.1Qav/D3.0 Draft Standard for Local and Metropolitan Area Networks–Virtual Bridged Local Area Networks–Amendment: Forwarding and Queuing Enhancements for Time-Sensitive Streams,” IEEE, Tech. Rep., July 2008, this is an unapproved IEEE Standards Draft.",
                    "cites": null
                },
                {
                    "id": 10427526,
                    "title": "Statistical Real-time Communication over Ethernet for Manufacturing Automation Systems,”",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1109/rttas.1999.777672",
                    "raw": "S.-K. Kweon, K. G. Shin, and Q. Zheng, “Statistical Real-time Communication over Ethernet for Manufacturing Automation Systems,” in Proceedings 5th IEEE Real-Time Technology and Applications Symposium, 1999, pp. 192–202.",
                    "cites": null
                },
                {
                    "id": 10427591,
                    "title": "Supporting real-time trafﬁc on ethernet,”",
                    "authors": [],
                    "date": "1994",
                    "doi": "10.1109/real.1994.342706",
                    "raw": "C. Venkatramani and T. Chiueh, “Supporting real-time trafﬁc on ethernet,” Proceedings of the Real-Time Systems Symposium, pp. 282– 286, December 1994.",
                    "cites": null
                },
                {
                    "id": 10427515,
                    "title": "Switched ethernet for factory communication,”",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1109/etfa.2001.996370",
                    "raw": "J. Jasperneite and P. Neumann, “Switched ethernet for factory communication,” in 8th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA’01), October 2001, pp. 205–212.",
                    "cites": null
                },
                {
                    "id": 10427547,
                    "title": "Switched ethernet for realtime industrial communication: Modelling and message buffering delay evaluation,”",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1109/wfcs.2002.1159697",
                    "raw": "Y. Song, A. Koubaa, and L. I. Lorraine, “Switched ethernet for realtime industrial communication: Modelling and message buffering delay evaluation,” in 4th IEEE WFCS 2002, Vasteras (Sweden). SpringerVerlag, 2002, pp. 27–30.",
                    "cites": null
                },
                {
                    "id": 10427475,
                    "title": "Technical Speciﬁcation 10.1: Ethernet Services Attributes Phase 2,” The Metro Ethernet Forum,",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "——, “MEF Technical Speciﬁcation 10.1: Ethernet Services Attributes Phase 2,” The Metro Ethernet Forum, Nov. 2006.",
                    "cites": null
                },
                {
                    "id": 10427473,
                    "title": "Technical Speciﬁcation 6.1: Ethernet Services Deﬁnition – Phase 2,” The Metro Ethernet Forum,",
                    "authors": [],
                    "date": "2008",
                    "doi": null,
                    "raw": "MEF, “MEF Technical Speciﬁcation 6.1: Ethernet Services Deﬁnition – Phase 2,” The Metro Ethernet Forum, April 2008.",
                    "cites": null
                },
                {
                    "id": 10427445,
                    "title": "The All-New Switch Book: The Complete Guide to LAN Switching Technology.",
                    "authors": [],
                    "date": "2008",
                    "doi": null,
                    "raw": "R. Seifert and J. Edwards, The All-New Switch Book: The Complete Guide to LAN Switching Technology. Wiley Publishing, 2008.",
                    "cites": null
                },
                {
                    "id": 10427501,
                    "title": "The evolution of carrier ethernet services-requirements and deployment case studies [next-generation carrier ethernet],”",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1109/mcom.2008.4463774",
                    "raw": "L. Fang, R. Zhang, and M. Taylor, “The evolution of carrier ethernet services-requirements and deployment case studies [next-generation carrier ethernet],” IEEE Communications Magazine, vol. 46, no. 3, pp. 69–76, 2008.",
                    "cites": null
                },
                {
                    "id": 10427520,
                    "title": "The Industrial Communication Technology Handbook.",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1201/9781420037821",
                    "raw": "R. Zurawski, The Industrial Communication Technology Handbook. CRC Press, February 2005.",
                    "cites": null
                },
                {
                    "id": 10427516,
                    "title": "The Industrial Ethernet Networking Guide. Thomson Delmar Learning,",
                    "authors": [],
                    "date": "2002",
                    "doi": null,
                    "raw": "J. Donald J. Sterling and S. P. Wissler, The Industrial Ethernet Networking Guide. Thomson Delmar Learning, October 2002.",
                    "cites": null
                },
                {
                    "id": 10427531,
                    "title": "The road to an end-to-end deterministic ethernet,”",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1109/wfcs.2002.1159694",
                    "raw": "T. Skeie, S. Johannessen, and O. Holmeide, “The road to an end-to-end deterministic ethernet,” in 4th IEEE International Workshop on Factory Communication Systems, 2002, pp. 3–9.",
                    "cites": null
                },
                {
                    "id": 10427549,
                    "title": "Time constrained communication over switched ethernet,”",
                    "authors": [],
                    "date": "2001",
                    "doi": null,
                    "raw": "Y. Song, “Time constrained communication over switched ethernet,” in IFAC international conference on ﬁeldbus systems and their applications, November 2001, pp. 152–159.",
                    "cites": null
                },
                {
                    "id": 10427572,
                    "title": "Timing analysis of the arinc 629 data bus for real-time applications,”",
                    "authors": [],
                    "date": "1997",
                    "doi": "10.1016/s0141-9331(97)00020-3",
                    "raw": "N. C. Audsley and A. Grigg, “Timing analysis of the arinc 629 data bus for real-time applications,” Microprocessors and Microsystems, vol. 21, no. 1, pp. 55–61, July 1997.",
                    "cites": null
                },
                {
                    "id": 10427580,
                    "title": "Trends in automotive communication systems,”",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1109/jproc.2005.849725",
                    "raw": "N. Navet, Y. Song, F. Simonot-Lion, and C. Wilwert, “Trends in automotive communication systems,” Proceedings of the IEEE, vol. 93, no. 6, pp. 1204–1223, June 2005.",
                    "cites": null
                },
                {
                    "id": 10427575,
                    "title": "Using network calculus to optimize the afdx network,”",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/wfcs.2008.4638728",
                    "raw": "F. Frances, C. Fraboul, and J. Grieu, “Using network calculus to optimize the afdx network,” in Proceedings of the 3rd European Congress Embedded Real Time Software, Toulouse, 2006.",
                    "cites": null
                },
                {
                    "id": 10427555,
                    "title": "Website of the ethercat technology group,”",
                    "authors": [],
                    "date": "2008",
                    "doi": null,
                    "raw": "E. T. Group, “Website of the ethercat technology group,” 2008.",
                    "cites": null
                },
                {
                    "id": 10427503,
                    "title": "Y.1731: OAM functions and mechanisms for Ethernet based networks,” International Telecommunication",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "ITU, “Rec. Y.1731: OAM functions and mechanisms for Ethernet based networks,” International Telecommunication Union, ITU-T, May 2006.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "http://oatao.univ-toulouse.fr/2236/1/Mifdaoui_2236.pdf"
            ],
            "updatedDate": "2022-02-28T02:21:22",
            "yearPublished": 2010,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "1553-877X"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/12040594.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/12040594"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/12040594/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/12040594/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/5185196"
                }
            ]
        },
        {
            "acceptedDate": "2015-09-11T00:00:00",
            "arxivId": "1507.04761",
            "authors": [
                {
                    "name": "Kereliuk, C"
                },
                {
                    "name": "Larsen, J"
                },
                {
                    "name": "STURM, BLT"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/43254355",
                "https://api.core.ac.uk/v3/outputs/285366754",
                "https://api.core.ac.uk/v3/outputs/30698000"
            ],
            "createdDate": "2015-09-24T01:08:33",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 619,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/619",
                    "logo": "https://api.core.ac.uk/data-providers/619/logo"
                },
                {
                    "id": 525,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/525",
                    "logo": "https://api.core.ac.uk/data-providers/525/logo"
                }
            ],
            "depositedDate": "2015-11-01T00:00:00",
            "abstract": "OA Monitor ExerciseOA Monitor ExerciseAn {\\em adversary} is essentially an algorithm intent on making a classification system perform in some particular way given an input, e.g., increase the probability of a false negative. Recent work builds adversaries for deep learning systems applied to image object recognition, which exploits the parameters of the system to find the minimal perturbation of the input image such that the network misclassifies it with high confidence. We adapt this approach to construct and deploy an adversary of deep learning systems applied to music content analysis. In our case, however, the input to the systems is magnitude spectral frames, which requires special care in order to produce valid input audio signals from network-derived perturbations. For two different train-test partitionings of two benchmark datasets, and two different deep architectures, we find that this adversary is very effective in defeating the resulting systems. We find the convolutional networks are more robust, however, compared with systems based on a majority vote over individually classified audio frames. Furthermore, we integrate the adversary into the training of new deep systems, but do not find that this improves their resilience against the same adversary",
            "documentType": "research",
            "doi": "10.1109/tmm.2015.2478068",
            "downloadUrl": "https://core.ac.uk/download/30698000.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "Deep Learning and Music AdversariesSTURM, BLT; Kereliuk, C; Larsen, J     © 2015, IEEE  For additional information about this publication click this link.http://qmro.qmul.ac.uk/xmlui/handle/123456789/11150   Information about this research object was correct at the time of download; we occasionallymake corrections to records, please therefore check the published record when citing. Formore information contact scholarlycommunications@qmul.ac.uk1Deep Learning and Music AdversariesCorey Kereliuk, Member, IEEE, Bob L. Sturm, Member, IEEE, Jan Larsen Member, IEEEAbstract—An adversary is essentially an algorithm intent onmaking a classification system perform in some particular waygiven an input, e.g., increase the probability of a false negative.Recent work builds adversaries for deep learning systems appliedto image object recognition, which exploits the parameters ofthe system to find the minimal perturbation of the input imagesuch that the network misclassifies it with high confidence. Weadapt this approach to construct and deploy an adversary ofdeep learning systems applied to music content analysis. In ourcase, however, the input to the systems is magnitude spectralframes, which requires special care in order to produce validinput audio signals from network-derived perturbations. For twodifferent train-test partitionings of two benchmark datasets, andtwo different deep architectures, we find that this adversary isvery effective in defeating the resulting systems. We find theconvolutional networks are more robust, however, compared withsystems based on a majority vote over individually classifiedaudio frames. Furthermore, we integrate the adversary into thetraining of new deep systems, but do not find that this improvestheir resilience against the same adversary.Index Terms—EDICS: MLSAS-PATT Pattern recognition andclassification; AEA-MIR Content-based Processing and MusicInformation RetrievalI. INTRODUCTIONDeep learning is impacting the research domain of mu-sic content analysis and music information retrieval (MIR)[19], [28], [31], [34], [41], [44], [57], [63], [65], but recentdevelopments raise the spectre that the high performance ofthese systems does not reflect how well they have learned tosolve high-level problems of music listening. MIR aims toproduce systems that help make “music, or information aboutmusic, easier to find” [14]. This is of principal importancefor confronting the vast amount of music data that exists andcontinues to be created. Listening machines that can flexiblyproduce accurate, meaningful and searchable descriptions ofmusic can greatly reduce the cost of processing music data,and can facilitate a diversity of applications. These extendfrom music identification [59], author attribution [13], recom-mendation [57], transcription [21], and playlist generation [2],to extracting semantic descriptors such as genre and mood[9], [49], [64], to computational musicology [15], and evensynthesis and music composition [43].Recent surveys of the domain of deep learning recordimpressive results for several benchmark problems [6], [17]. Inaddition to these major successes, deep learning methods arevery attractive for three other reasons: there now exist efficientand effective training algorithms for deep learning, not to men-tion completely free and open cross-platform implementations,e.g., Theano [4], [8]; they entail jointly optimising featureC. Kereliuk and J. Larsen are with DTU.B. L. Sturm is with the School of Electronic Engineering and ComputerScience, Queen Mary University of London.learning and classification, thus allowing one to forgo manydifficulties inherent to formally encoding expert knowledgeinto a machine; and their layered structures seems to favourhierarchical representations of structures in data. One caveat,however, is that these methods require a lot of data in orderto estimate parameters and generalise well [37].In MIR, the works in [28], [34], [35] are among the firstto apply deep learning to music content analysis, and eachdescribes results pointing to the conclusion that these systemscan automatically learn features relevant for complex musiclistening tasks, e.g., recognition of genre or style. Resultssince then point to the same conclusion [19], [41], [44], [63],[65]. Humphrey et al. [31] highlight this fact to argue deeplearning is naturally suited to learn relevant abstractions formusic content analysis, provided enough data is available.Since music can be seen as a “whole greater than the sumof its parts” [31], deep learning can help MIR narrow the“semantic gap” [62], and move beyond what has been calleda “glass ceiling” in performance [3].However, it is now known how deceiving the appearanceof high performance can be: an MIR system can appear tobe very successful in solving a high-level music listeningproblem when in fact it is just exploiting some independentvariables of questionable relevance unknowingly confoundedwith the ground truth of a music dataset by a poor experimentaldesign [22], [23], [39], [47]–[49], [49], [51]–[53], [56]. Inaddition, recent work in machine learning has demonstrateddeep learning systems behaving in ways that contradict theirappearance of solving content-recognition problems. Nguyenet al. [38] show how a high-performing image object recog-nition system can label with high confidence non-sensicalsynthetic images. In a similar direction, we have shown [51]how a deep system that appears highly capable of recognisingdifferent musical rhythms confidently classifies synthesisedrhythms, though they bear little similarity to the rhythmsthey supposedly represent. Szegedy et al. [54] show howdeep high-performing image object recognition systems arehighly sensitive to imperceptible perturbations created by anadversary: an agent that actively seeks to fool a classifier byperturbing the input such that it results in an incorrect outputbut with high confidence [16].All of these results motivate several timely questions ofdeep learning systems for music content analysis specifically,and multimedia in general. First, how do the adversaries ofSzegedy et al. [54] translate to the context of deep learningapplied to music content analysis? The input of the systemsstudied by Szegedy et al. [54] is raw pixel data; however, inmusic content analysis only the system studied in [19] takesas input raw audio samples. The inputs to other deep learningsystems have been features: windowed magnitude spectra[28], [44], sonograms [34], [57], autocorrelations of spectral2energies [41], [51], or statistics of features [63], [65]. Second,can we generate an adversary for such deep learning musiccontent analysis systems that produce adversarial examplesthat are perceptually identical to the originals? Third, can we“harness” an adversary to train deep learning systems that arerobust to its “malfeasance”? Finally, and more broadly, whatis deep learning contributing to music content analysis? Canwe use adversaries to reveal whether these deep systems areusing better models of the content than other state of the artsystems using hand-crafted features?Our preliminary work [33] shows that it is possible to createhighly effective adversaries of the music content analysisdeep neural networks (DNN) studied in [28], [44]. Theseadversaries can make the systems always wrong, always right,and anywhere in-between, with high confidence by applyingonly minor perturbations of the input magnitude spectra.Furthermore, we created an ensemble of adversaries that cancoax the DNN into assigning with high confidence any label tothe same music by perturbing the input by very small amounts(e.g., 26.8 dB SNR). In this article, we greatly expand uponour prior work [33] to include convolutional deep learningsystems, more extensive testing in a larger benchmark MIRdataset, and the results of incorporating an adversary into thetraining of these different deep learning systems.In the next section, we provide an overview of workapplying deep learning to music content analysis and MIR. Wethen review two different deep learning architectures, and ourconstruction of several music content analysis systems usingtwo partitions of two MIR benchmark datasets. In Sec. IIIwe review adversaries, and design an adversary for our deepsystems. We then present in Sec. IV a series of experimentsusing our adversary. In Section V we provide a discussionof our work in wider contexts. We conclude in section VI.Some of our results can be produced with the software here:https://github.com/coreyker/dnn-mgr.II. DEEP LEARNING FOR MUSIC CONTENT ANALYSISWe first provide an overview of research in applying deeplearning approaches to music content analysis. We then discusstwo different architectures, train two music content analysissystems, and test them in two benchmark MIR datasets. Thesesystems are the subjects of our experiments in Section IV.A. OverviewArtificial neural networks have been applied to many musiccontent analysis problems, [26], for instance, fingerprinting[12], genre recognition [36], emotion recognition [58], artistrecognition [61], and even composition [40]. Advances intraining have enabled the creation of more advanced anddeeper architectures. Deng and Yu [17] (Chapter 7) providea review of successful applications of deep learning to theanalysis of audio, highlighting in particular its significantcontributions to speech recognition in conversational settings.Humphrey et al. [31] provide a review for applications tomusic in particular, and motivate the capacity of deep ar-chitectures to automatically learn hierarchical relationships inaccordance with the hierarchical nature of music: “pitch andloudness combine over time to form chords, melodies andrhythms.” They argue that this is key for moving beyond thereliance on “shallow” and hand-designed features that weredesigned for different tasks.Lee et al. [34] are perhaps the first to apply deep learn-ing to music content analysis, specifically genre and artistrecognition. They train a convolutional deep belief network(CDBN) with two hidden layers in an unsupervised mannerin an attempt to make the hidden layer activations producemeaningful features from a pre-processed spectrogram in-put computed using 20 ms 50%-overlapped windows. Thespectrogram is “PCA-whitened”, which involves projectingit onto a lower-dimensional space using scaled eigenvectors.Important details are missing in the description of the work,but it appears they use the activations as features in sometrain/test task using a standard machine learning approach.A table of their experimental results, using some portionof the dataset ISMIR2004, shows higher accuracies for theirdeep learned features compared to those for standard MFCCs.For genre recognition, Li et al. [35] use convolutional deepneural networks (CDNN) with three hidden layers, into whichthey input a sequence of 190 13-dimensional MFCC featurevectors. The architecture of their CDNN is such that the firsthidden layer considers data from 127 ms duration, and thelast hidden layer is capable of summarising events over a 2.2s duration. van den Oord et al. [57] apply CDNN to mel-frequency spectrograms for automatic music content analysis.For genre recognition and more general descriptors, Hameland Eck [28] train a DNN with three hidden layers of 50units each, taking as input 513 discrete Fourier transform(DFT) magnitudes computed from a single 46 ms audioframe. They use a train/valid/test partition of the benchmarkmusic genre dataset GTZAN [49], [55]. They also explore“aggregated” features, which are the mean and variance ineach dimension of activations over 5 second durations. Theyfind in the test set, and for both short-term and aggregatedfeatures, that SVM classifiers trained with features built fromhidden layer activations reproduce more ground truth than anSVM classifier trained with features built from MFCCs. Theyreport an accuracy of over 0.84 for features that aggregateactivations of all three hidden layers. Sigtia and Dixon [44]explore modifications to the system in [28], in particular usingdifferent combinations of architectures, training procedures,and regularisation. They use the activations of their trainedDNN as features for a train/test task using a random for-est classifier. They report an accuracy of about 0.83 usingfeatures aggregating activations of all hidden layers of 500units each. For genre recognition, Yang et al. [63] combine263-dimensional modulation features with a DBN. For musicrhythm classification, Pikrakis [41] employs a DBN, which westudied further in [51]–[53].Dieleman et al. [18] build and apply CDBN to musickey detection, artist recognition, and genre recognition. Thereare three major differences with respect to the work above[28], [34], [35], [44], [63]. First, Dieleman et al. employ 24-dimensional input features computed by averaging short-timechroma and timbre features over the time scales of singlemusical beats. Second, they employ expert musical knowl-3Input     1st conv. layer(4/32 filters shown)   1st pooling layer(4/32 filters shown)      2nd pooling layer(4/32 filters shown)50 units     2nd conv. layer(4/32 filters shown)SoftmaxFig. 1. Illustration of the CDNN architecture we use for our experiments. The CDNN first applies narrow vertical filters to the input sonogram (left) tocapture harmonic structure. Then, it applies 32 different filters in the first convolutional layer (we show only 4). This is followed by the first max-poolinglayer, and then a 2nd pair of convolutional and max-pooling layers. Finally, the output of the final max-pooling layer is fully connected to a final hidden layerof 50 units, followed by a softmax output unit. The input spectrogram contains 100 time slices, which means that the final layer of the CDNN summarisesinformation over a total duration of 2.35 seconds.edge to guide decisions about the architecture of the system.Finally, they use the output posteriors of their system forclassification, instead of using the hidden layer activations asfeatures for a separate classifier. Their experiments in a portionof the “million song dataset” [10] show large differences inclassification accuracies between their systems and a naiveBayesian classifier using the same input features. In a uniquedirection for audio, Dieleman and Schrauwen [19] explore“end-to-end” learning, where a CDNN is trained with input ofabout 3 s of raw audio samples for a music content analysistask (autotagging). They find that the lowest layer of thetrained CDNN appears to learn some filters that are frequencyselective. They evaluate this system for a multilabel problem.To recognise music mood, Weninger et al. [60] use recurrentDNN with input constructed of several statistics of low-level features computed over second-long excerpts of musicrecordings. Battenberg and Wessel [5] apply DBN for iden-tifying the beat numbers over several measures of percussivemusic, with input features consisting of quantised onset timesand magnitudes. Boulanger-Lewandowski et al. [11] train arecurrent neural network to produce chord classifications usinginput of PCA-whitened magnitude DFT. In a similar direction,Humphrey and Bello [32] build a DNN that maps inputspectrogram features to guitar-specific fingerings of chords.B. Two types of deep architecturesWe now review two different architectures of deep learningsystems, and the way they are trained. A DNN is an artificialneural network with several hidden layers [17]. The outputof each layer is a non-linear function of its inputs, obtainedby a matrix multiplication cascaded with a non-linearity,e.g., tanh, sigmoid and rectifier. By chaining together severalhidden layers, composite representations of the input emergein deeper layers. This fact can give deep networks greaterrepresentational power than shallower networks containing anequivalent number of parameters [7].A CDNN is a special type of DNN with weights that areshared between multiple points between adjacent layers. Theweight sharing in CDNNs not only reduces the number oftrainable parameters, but also causes matrix multiplications toreduce to convolutions, which can be implemented efficiently.Furthermore, many natural signals have local spatial or tempo-ral structures that are repeated globally. For example, naturalimages often consist of oriented edges; and audio signals oftenconsist of harmonic and repetitive structures. CDNNs can learnthese types of structures very well. Figure 1 illustrates ourCDNN, which we discuss in the following subsection.The contemporary success of deep learning comes withcomputationally efficient training methods. Systems that havesuch deep architectures are usually trained using gradientdescent, which consists of backpropagating error derivativesfrom the cost function through the network. There are aplethora of useful tips and tricks to augment training, includingstochastic gradient descent, dropout regularisation, weightdecay, momentum, learning rate decay, and so on [37].C. Deep learning with two music genre benchmarksWe now build DNNs and CDNNs using two music genrebenchmarks: GTZAN [49], [55] and the Latin Music Database(LMD) [45]. GTZAN consists of 100 30-second music record-ing excerpts in each of ten categories, and is the most-usedpublic dataset in MIR research [50]. LMD is a private dataset,consisting of 3,229 full-length music track recordings non-uniformly distributed among ten categories, and has been usedin the annual MIREX audio latin music genre classificationevaluation campaign since 2008.1 We use the first 30 secondsof each track in LMD.We build several DNNs and CDNNs using different par-titionings of these datasets. One partitioning of GTZANwe create by randomly selecting 500/250/250 excerpts fortraining/validation/testing. The other partitioning of GTZANis “fault-filtered,” which we construct by hand to include443/197/290 excerpts. This involves removing 70 files includ-ing exact replicas, recording replicas, and distorted files [49],and then dividing the excerpts such that no artist is repeatedacross the training, validation, and test partitions. We partitionLMD in two ways: 1) partitioning by 60/20/20% sampling1http://www.music-ir.org/mirex/wiki/MIREX HOME4in each class; 2) a hand-constructed artist-filtered partitioningcontaining approximately the same division of excerpts in eachclass. We retain all 213 replicas in LMD.2The input to our systems is derived from the short-timeFourier transform (STFT) of a sampled audio signal x [1]:F(x)[m,u] =L−1∑l=0w[l]x[l − uH]e−j2piml/L (1)where the parameter L defines both the window length and thenumber of frequency bins. We define w as a Hann window oflength L = 1024, which corresponds to a duration of 46msfor recordings sampled at 22050 Hz. The window is hoppedalong x with a stride of H = 512 samples (adjacent windowsoverlap by 50%).Since audio signals can be of any duration, we define theinput to our systems as a sequence X = (Xn)N−1n=0 , where thesequence length depends on the input audio’s duration. Wedefine the nth element of the input sequence X to beXn ,(∣∣∣F(x)[m,u]∣∣∣ : m ∈ [0, 512], u ∈ [nT, (n+1)T [) (2)where T = 1 for each DNN and T = 100 for each CDNN.Thus, when T = 1, X is a sequence of 513×1 vectors; whenT = 100, X is a sequence of 513× 100 matrices.A (C)DNN processes each element in this sequence inde-pendently, outputting a sequence P = (Pn)N−1n=0 from the final(softmax) layer. The output vector Pn ∈ [0, 1]K , ‖Pn‖1 = 1,is the posterior distribution of labels assigned to the nthelement in the input sequence by the network. Therefore, wemay write Pn(i|Xn,Θ) ≡ Pn(i) ∈ [0, 1] where Θ representsthe trainable network parameters, i.e., the set of weights andbiases. We define the confidence of a (C)DNN in a particularlabel k ∈ {1, . . . ,K} for an input sequence X as the sum ofall posteriors, i.e.,R(k|X,Θ) = 1NN−1∑n=0Pn(k|Xn,Θ). (3)We apply a label to an input sequence X as the one maximis-ing the confidencey(X,Θ) = arg maxk∈{1,...,K}R(k|X,Θ). (4)Paralleling the work in [44], we build DNNs with 3 fullyconnected hidden layers, and either 50 or 500 units per layer.Our CDNN has two convolutional layers (accompanied bymax pooling layers) followed by a fully connected hiddenlayer with 50 units. Figure 1 illustrates the architecture of ourCDNN. Its first convolutional layer contains 32 filters, eacharranged in a rectangular 400 × 4 grid. We choose this longrectangular shape instead of the small square patches typicallyused when training on images based on our knowledge thatmany sounds exhibit strong harmonic structures that span alarge portion of the audible spectrum. The second convolu-tional layer contains 32 filters, each connected in an 8 × 8pattern. Our two pooling neighborhoods are 4 × 4 and havestrides of 2×2. All of our deep learning systems use rectified2https://highnoongmt.wordpress.com/2014/02/08/faults in the latin music databaselinear units (ReLUs), and have a softmax unit in the final layer.As is typical, we standardise the (C)DNN inputs by subtractingthe training set mean and dividing by the standard deviationin each of the input dimensions. We perform this with a linearlayer above the input layer of each network. The raw inputsto the network are still Xn.Also paralleling [44], we build several music classificationsystems treating our DNN as a feature extractor. In this case,we construct a set of features by concatenating the activationsfrom the DNN’s three hidden layers, and aggregating themover 5-second texture windows (hopped by 50%). The ag-gregation summarises the mean and standard deviation of thefeature dimensions over the texture window and may be seenas a form of late-integration of temporal information. We usethis new set of features to train a random forest (RF) classifier[29] with 500 trees. Thus, to classify a music audio recordingx from its set of aggregated features, we use majority votingover all classifications, which is also used in [44].D. Preliminary evaluationFigure 2 and Table I show the results of RF classificationusing the features produced by the DNN when trained onGTZAN with the two different partitioning strategies; and Fig.3 shows those for the (C)DNNs we train and test in LMD.Across each partition strategy we see significant differencesin performance. The mean recall in each class in Figure 2on the fault-filtered partition is much lower than that onthe random test partition — involving drops higher than 30percentage points in most cases. Table I shows similar dropsin performance that persist over the inclusion of drop-outregularisation. Such significant drops in performance frompartitioning based on artists is not unusual, and has beenstudied before as a bias coming from the experimental design[23], [39], [49]. Partitioning a music genre recognition datasetalong artist lines has been recommended to avoid this bias[23], [39], and is in fact used in several MIREX audioclassification tasks.3 Experiments using GTZAN with fault-filtering partitioning has not been used in many benchmarkexperiments with GTZAN because its artist information hasonly recently been made available [49].III. ADVERSARIES IN MUSIC CONTENT ANALYSISAn adversary is an agent that tries to defeat a classificationsystem in order to maximise its gain, e.g., SPAM detection.Dalvi et al. [16] pose this problem as a game between aclassifier and adversary, and analyse the strategies involvedfor an adversary with complete knowledge of the classificationsystem, and for a classifier to adapt to such an adversary.Szegedy et al. [54] propose using adversaries for testing theassumption that deep learning systems are “smooth classifiers,”i.e., stable in their classification to small perturbations aroundexamples in the training data. They define an adversary ofa classifier f : Rm → {1, . . . ,K} as an algorithm usingcomplete knowledge of the classifier to perturb an observationx ∈ Rm such that f(x + r) 6= f(x), where r ∈ Rm is some3http://www.music-ir.org/mirex/wiki/MIREX HOME5bluesclassicalcountrydiscohiphopjazzmetalpopreggaerockPrbluesclassicalcountrydiscohiphopjazzmetalpopreggaerockF92.0 0.0 0.0 0.0 0.0 4.0 0.0 0.0 4.0 8.0 85.20.0 84.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 100.00.0 4.0 92.0 0.0 0.0 0.0 0.0 0.0 0.0 4.0 92.08.0 4.0 4.0 80.0 0.0 0.0 0.0 4.0 12.0 16.0 62.50.0 0.0 0.0 0.0 76.0 0.0 0.0 0.0 8.0 0.0 90.50.0 8.0 0.0 0.0 0.0 92.0 0.0 0.0 4.0 0.0 88.50.0 0.0 0.0 0.0 20.0 0.0 92.0 0.0 4.0 0.0 79.30.0 0.0 0.0 12.0 0.0 4.0 0.0 92.0 0.0 16.0 74.20.0 0.0 0.0 0.0 0.0 0.0 0.0 4.0 64.0 8.0 84.20.0 0.0 4.0 8.0 4.0 0.0 8.0 0.0 4.0 48.0 63.288.5 91.3 92.0 70.2 82.6 90.2 85.2 82.1 72.7 54.5 81.2(a) Random partitioningbluesclassicalcountrydiscohiphopjazz metalpop reggaerock PrbluesclassicalcountrydiscohiphopjazzmetalpopreggaerockF0.0 0.0 6.7 10.3 0.0 0.0 0.0 0.0 0.0 3.1 0.00.0 100.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 100.012.9 0.0 56.7 3.4 0.0 0.0 3.7 0.0 3.8 12.5 60.79.7 0.0 23.3 24.1 3.7 37.0 3.7 3.3 7.7 40.6 15.619.4 0.0 0.0 17.2 70.4 0.0 3.7 6.7 30.8 12.5 42.216.1 0.0 0.0 3.4 0.0 29.6 0.0 3.3 7.7 0.0 47.10.0 0.0 0.0 3.4 0.0 0.0 85.2 0.0 0.0 6.2 88.50.0 0.0 6.7 3.4 22.2 25.9 0.0 83.3 7.7 6.2 55.638.7 0.0 6.7 20.7 3.7 7.4 3.7 3.3 42.3 15.6 26.83.2 0.0 0.0 13.8 0.0 0.0 0.0 0.0 0.0 3.1 16.70.0 100.0 58.6 18.9 52.8 36.4 86.8 66.7 32.8 5.3 49.0(b) Artist-filtered partitioningFig. 2. Figure of merit (FoM, ×100) in GTZAN with two different partitionings for random forest classification (majority vote) of DNN-based features (alllayers) aggregated over 5 second windows (mean and standard deviations). Each DNN has 500 rectified linear units in each hidden layer. Columns representthe true class; rows denote labels chosen by system; the diagonal contains the per-class recall; the off-diagonal entries are confusions; the rightmost columnis the precision; the bottom row is the F-score; and the last element along the diagonal is the mean recall (normalised classification accuracy).Hidden Units Layer ReLU ReLU+Dropout501 76.00 (40.69) 80.40 (45.17)2 78.80 (45.17) 80.40 (43.10)3 79.60 (43.79) 78.80 (44.48)All 80.40 (43.79) 80.00 (43.79)5001 68.40 (40.34) 75.60 (40.69)2 74.40 (40.69) 80.00 (50.34)3 77.60 (43.79) 79.20 (48.62)All 76.00 (42.41) 81.20 (48.97)TABLE IMEAN NORMALISED CLASSIFICATION ACCURACY (×100) IN GTZAN FORRANDOM FOREST CLASSIFICATION OF DNN-BASED FEATURES FROMLAYER SHOWN AGGREGATED OVER 5-SECOND WINDOWS. NUMBEROUTSIDE BRACKETS IS FROM RANDOM PARTITION IN FIG. 2(A); AND THATINSIDE BRACKETS IS FROM FAULT-FILTERED PARTITION IN FIG. 2(B).small perturbation. Specifically, their adversary solves theconstrained optimisation problem for a given k ∈ {1, . . . ,K}:min ‖r‖2 subject to f(x+ r) = k. (5)For k 6= f(x), Szegedy et al. [54] employ a line searchalong the direction of the loss function of the network startingfrom x until the classifier produces the requested class. Theyfind that adversarial examples of one classifier can fool otherclassifiers trained on independent data; hence, one need nothave complete knowledge of a classifier in order to fool it.Goodfellow et al. [24] provide an intuitive explanationof these adversaries: even though the perturbations in eachdimension might be small, their contribution to the magnitudeof a projection grows linearly with input dimensionality. Witha deep neural network involving many such projections in eachlayer, a small perturbation at its high-dimensional input layercan create major consequences at the output layer. Goodfellowet al. [24] show that adversarial examples can be easilygenerated by making the perturbation proportional to the signof the partial derivative of the loss function used to train aparticular network, evaluated with the requested class. Theyalso find that the direction of perturbation is important, notnecessarily its size. Hence, it seems adversarial examples ofone model will likely fool other models because they occur inlarge volumes in high-dimensional spaces. This is also foundby Gu and Rigazio [27].As for Szegedy et al. [54], we are interested the robustnessof our deep learning music content analysis systems to anadversary. Do these systems suffer just as dramatically asthe image content recognition systems in [24], [27], [54]? Inother words, can we find imperceptible perturbations of audiorecordings, yet make the systems produce any label with highconfidence? If so, can we adapt the training of the systemssuch that they become more robust? In the next subsections,we define an adversary as an optimisation problem, but withcare of the fact that the input to our deep learning systems aremagnitude STFT (2). We then present an approach to integrateadversaries into the training of our systems. We present ourexperimental results in Section IV.A. Adversaries for music audioThe explicit goal of our adversary is to perturb a musicrecording x such that a system will confidently classify itwith some class y ∈ {1, . . . ,K}. Specifically, we define theadversary as the constrained optimisation problem:Xˆ(y) = arg minZ∈C(X)N−1∑n=0L(Zn, y|Θ) (6)6AxeBachataBoleroForroGauchaMerenguePagodeSalsaSertanejaTangoPrAxeBachataBoleroForroGauchaMerenguePagodeSalsaSertanejaTangoF98.0 2.0 10.0 8.0 12.0 0.0 6.0 2.0 20.0 0.0 62.00.0 94.0 0.0 0.0 2.0 0.0 0.0 0.0 0.0 0.0 97.90.0 4.0 78.0 2.0 4.0 2.0 4.0 12.0 22.0 10.0 56.50.0 0.0 0.0 68.0 0.0 0.0 0.0 0.0 0.0 0.0 100.00.0 0.0 0.0 6.0 74.0 0.0 0.0 0.0 4.0 2.0 86.00.0 0.0 0.0 0.0 0.0 90.0 0.0 0.0 0.0 0.0 100.02.0 0.0 10.0 16.0 6.0 6.0 90.0 6.0 0.0 2.0 65.20.0 0.0 2.0 0.0 0.0 2.0 0.0 80.0 0.0 0.0 95.20.0 0.0 0.0 0.0 2.0 0.0 0.0 0.0 54.0 0.0 96.40.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 86.0 100.076.0 95.9 65.5 81.0 79.6 94.7 75.6 87.0 69.2 92.5 81.2(a) DNN Random partitioningAxeBachataBoleroForroGauchaMerenguePagodeSalsaSertanejaTangoPrAxeBachataBoleroForroGauchaMerenguePagodeSalsaSertanejaTangoF89.4 9.7 7.9 28.1 3.2 12.9 27.9 6.5 29.0 1.2 42.80.0 64.5 1.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 97.61.5 9.7 71.4 8.8 0.0 1.6 1.6 9.7 13.0 19.8 50.00.0 0.0 0.0 8.8 0.0 0.0 0.0 0.0 0.0 0.0 100.00.0 1.6 0.0 14.0 85.7 1.6 0.0 1.6 15.9 2.5 69.20.0 0.0 0.0 0.0 0.0 80.6 0.0 4.8 0.0 0.0 94.37.6 9.7 12.7 19.3 11.1 3.2 67.2 16.1 15.9 0.0 40.60.0 1.6 1.6 0.0 0.0 0.0 0.0 61.3 0.0 2.5 90.51.5 0.0 0.0 19.3 0.0 0.0 1.6 0.0 23.2 2.5 51.60.0 3.2 4.8 1.8 0.0 0.0 1.6 0.0 2.9 71.6 86.657.8 77.7 58.8 16.1 76.6 87.0 50.6 73.1 32.0 78.4 62.8(b) DNN Artist-filtered partitioningAxeBachataBoleroForroGauchaMerenguePagodeSalsaSertanejaTangoPrAxeBachataBoleroForroGauchaMerenguePagodeSalsaSertanejaTangoF92.0 0.0 2.0 2.0 4.0 2.0 12.0 2.0 8.0 0.0 74.20.0 100.0 2.0 0.0 2.0 0.0 0.0 6.0 0.0 0.0 90.92.0 0.0 76.0 2.0 4.0 2.0 0.0 8.0 20.0 8.0 62.30.0 0.0 0.0 86.0 6.0 0.0 2.0 0.0 4.0 0.0 87.80.0 0.0 0.0 0.0 68.0 4.0 0.0 0.0 0.0 0.0 94.40.0 0.0 0.0 0.0 2.0 88.0 0.0 4.0 0.0 0.0 93.60.0 0.0 4.0 2.0 0.0 0.0 78.0 0.0 2.0 0.0 90.72.0 0.0 10.0 2.0 2.0 4.0 2.0 80.0 0.0 0.0 78.44.0 0.0 6.0 6.0 12.0 0.0 4.0 0.0 64.0 0.0 66.70.0 0.0 0.0 0.0 0.0 0.0 2.0 0.0 2.0 92.0 95.882.1 95.2 68.5 86.9 79.1 90.7 83.9 79.2 65.3 93.9 82.4(c) CDNN Random partitioningAxeBachataBoleroForroGauchaMerenguePagodeSalsaSertanejaTangoPrAxeBachataBoleroForroGauchaMerenguePagodeSalsaSertanejaTangoF36.4 0.0 0.0 3.5 3.2 1.6 0.0 1.6 1.4 0.0 77.41.5 85.5 1.6 0.0 1.6 1.6 0.0 0.0 5.8 0.0 86.916.7 6.5 77.8 10.5 4.8 1.6 13.1 0.0 27.5 45.7 35.50.0 1.6 4.8 52.6 6.3 0.0 6.6 0.0 7.2 0.0 63.813.6 1.6 1.6 1.8 69.8 1.6 4.9 0.0 15.9 0.0 62.00.0 1.6 0.0 1.8 0.0 88.7 4.9 1.6 0.0 0.0 90.21.5 1.6 1.6 8.8 6.3 0.0 52.5 3.2 5.8 0.0 64.021.2 1.6 3.2 1.8 4.8 4.8 13.1 93.5 4.3 3.7 60.49.1 0.0 9.5 19.3 3.2 0.0 4.9 0.0 31.9 0.0 44.00.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 50.6 100.049.5 86.2 48.8 57.7 65.7 89.4 57.7 73.4 37.0 67.2 63.2(d) CDNN Artist-filtered partitioningFig. 3. FoM for deep learning systems with two different partitioning strategies of LMD. Interpretation as in Fig. 2; but note that in this case we are usingthe deep learning systems as the classifiers, instead of performing classification using a random forest with features derived from hidden layer activations.where we define the feasible set of adversarial examples toinput sequence X as:C(X) ={Z = (Zn)N−1n=0 :√∑N−1n=0 ‖Zn −Xn‖22 ≤ N\u000f(SNR)}(7)with the parameter\u000f(SNR) =1N√∑N−1n=0 ‖Xn‖2210SNR/20(8)limiting the maximum acceptable perturbation caused by theadversary. The loss function in (6) is the cross-entropy lossfunction, L(Xn, y|Θ) := − logPn(y|Xn,Θ), which we usein training our (C)DNNs. Given the network parameters Θ,this adversary can compute the derivative of this loss functionby backpropagating derivatives through the network. This sug-gests that our adversary can accomplish its goal by searchingfor a new input sequence Xˆ via gradient descent on the lossfunction with any label y that differs from the ground truth.This is the approach used by Szegedy et al. [54] in the contextof image object recognition.A local minimum of (6) can be found using projectedgradient descent, initialised with the exemplar Xˆ(0) ← X ,and iteratingXˆ(k+1) ← PC(Xˆ(k) + µ∇L(Xˆ(k), y|Θ)) (9)where the scalar µ is the gradient descent step size, and7Algorithm 1 From exemplar sequence X search for adver-sarial sequence Xˆ with maximal perturbation SNR in at mostkmax steps that makes a (C)DNN with parameters Θ applylabel y with confidence Rmin.1: parameters: y, SNR, µ,Rmin,Θ, kmax2: init: X(0) = X, k = 03: repeat4: V ← X(k) + µ∇L(X(k), y|Θ) {Gradient step}5: W ← PGL(max(0, V )) {Find valid sequence}6: ν ← max(0, 1N√∑N−1n=0 ||Wn −Xn||22/\u000f(SNR) − 1){Lagrange mult.}7: X(k+1) ← (1 + ν)−1(W + νX) {SNR constraint}8: k ← k + 19: until 1N∑N−1n=0 Pn(y|X(k+1),Θn ) ≥ Rmin or k = kmax10: return: Xˆ = X(k)PC(·) computes the least squares projection of its argumentonto the set C(X) defined in (7). Note that we define op-erations on sequences element-wise, e.g., ∇L(Xˆ(k), y|Θ) =(∇L(Xˆ(k)n , y|Θ))N−1n=0 .The main difficulty with this approach is that not allsequences Xˆ can be mapped back to valid time-domainsignals xˆ. This is because the analysis in (1) uses overlappingwindows, which causes adjacent elements in the sequence X tobecome dependent. This means that individual elements fromthe sequence X cannot be adjusted arbitrarily if we want Xto have an analog in the time-domain. Therefore, in order togenerate valid adversarial examples, we include an additionalprocessing step that projects the sequence Xˆ onto the spaceof time-frequency coefficients arising from valid time-domainsequences. This is done using the Griffin and Lim algorithm[25], which seeks to minimisePGL(Xˆ) = minZ∈XN−1∑n=0||Zn − Xˆn||22 (10)where X = {X = (Xn)N−1n=0 : PGL(X) = X} denotes the setof all valid sequences. This minimization can be performedusing alternating projections, and we have found that inpractice it is sufficient to apply a single set of projections. Wedo this by first rebuilding a complex valued time-frequencyrepresentation from the sequence XˆU [m,u] ={Xˆbu/Tc[m,u mod T ]ejΦ[m,u] 0 ≤ m < DXˆbu/Tc[D −m,u mod T ]ejΦ[m,u] D ≤ m < L.(11)where D = L/2 + 1 and Φ[m,u] , ∠F(x) is the phasefrom the exemplar’s Fourier transform. The inverse Fouriertransform F−1(U) is a time-domain signal, and so the Fouriertransform of this signal, F ◦ F−1(U), will yield a valid DFTspectrum that can be used to build a valid input sequence forour (C)DNN, i.e., by replacing F(x) by F ◦ F−1(U) in (2).The pseudo-code in Alg. 1 summarises this approach. Thealgorithm may be terminated when the mean posterior of thetarget adversarial label exceeds the threshold Rmin, or after aAlgorithm 2 Train (C)DNN using database of labeled se-quences (X,Y) and fast adversarial generation [24], withε and µ the gradient descent step sizes for adjusting theadversarial inputs and network weights, respectively.1: parameters: ε, µ2: init: (C)DNN parameters Θ to small random weights3: repeat4: select Yˆ uniformly {1, . . . ,K}N5: Xˆ← X+ ε∇L(X, Yˆ|Θ) {Generate adversarial ex.}6: Θ← Θ + µ∇L(Xˆ,Y|Θ) {Model update}7: until Stopping conditionmaximum number of epochs kmax (in which case an adversarycannot be found above the minimum SNR).B. Training with adversaries for music audioAs per [54] and [24], we can attempt to use our adversary asa regulariser, and to create systems robust against adversarialinputs. In particular, we create adversaries for the (C)DNNdiscussed above, and use them to generate a (possibly) infinitesupply of new samples during training. The iterative procedurefor generating adversaries in Alg 1 is too slow to be practicalfor training, which requires on the order of 50 to 200 trainingepochs. Therefore, we apply the single gradient step proceduresuggested in [24]. In our experience, this procedure often gen-erates inputs that confuse the network, although not typicallywith a high confidence. The pseudo-code in Alg. 2 illustratesour training algorithm, where (X,Y) represent the trainingdata, i.e., the set of input audio sequences and their labels,and Yˆ is a set of adversarial labels.IV. EXPERIMENTAL RESULTSWe can design an adversary (Alg. 1) such that it will attemptto make a system behave in different ways. For instance,an adversary could attempt to perturb an input within somelimit (SNR) such that the (C)DNN makes a high-confidenceclassification (Rmin ≈ 1) that is correct with probability p.Another adversary could attempt to make the system label anyinput using the same label. We can also make an ensemble ofadversaries such that they produce adversarial examples thata (C)DNN classifies in every possible way.We define our adversaries (Alg. 1) using: Rmin = 0.9, SNR= 15dB, µ = 0.1, and kmax = 100, and with the directive tomake the (C)DNN correct with probability p = 0.1. Moreconcretely, for each test observations, the adversary drawsuniformly one of the dataset labels y, then seeks to find inno more than kmax = 100 iterations using step size µ = 0.1a valid perturbation no larger than 15dB SNR, and which the(C)DNN labels as y with confidence Rmin = 0.9. Figure 4(a)shows the FoM of the DNN-based classification system in Fig.2(b), but with input intercepted by this adversary. Note that inthis case the classification is performed by the same randomforest classifier using the aggregated hidden layer activations,but the adversary is unaware of this. In other words, it is onlytrying to force the DNN to misclassify inputs that have beensubject to minor perturbations. Compared with a normalised8bluesclassicalcountrydiscohiphopjazz metalpop reggaerock PrbluesclassicalcountrydiscohiphopjazzmetalpopreggaerockF12.9 3.2 0.0 10.3 7.4 11.1 7.4 6.7 7.7 9.4 18.23.2 16.1 13.3 10.3 22.2 0.0 18.5 6.7 11.5 12.5 15.20.0 12.9 10.0 3.4 0.0 22.2 11.1 3.3 11.5 18.8 11.16.5 16.1 6.7 10.3 3.7 7.4 18.5 13.3 11.5 12.5 9.712.9 16.1 0.0 13.8 7.4 18.5 11.1 23.3 11.5 9.4 5.616.1 9.7 23.3 10.3 18.5 11.1 11.1 16.7 7.7 6.2 7.916.1 0.0 16.7 13.8 3.7 3.7 14.8 3.3 0.0 9.4 16.722.6 16.1 6.7 10.3 11.1 3.7 0.0 10.0 15.4 6.2 10.00.0 6.5 6.7 3.4 11.1 14.8 7.4 13.3 19.2 9.4 19.29.7 3.2 16.7 13.8 14.8 7.4 0.0 3.3 3.8 6.2 8.715.1 15.6 10.5 10.0 6.3 9.2 15.7 10.0 19.2 7.3 11.7(a) GTZAN fault-filtered: 23.0± 4.5dBAxeBachataBoleroForroGauchaMerenguePagodeSalsaSertanejaTangoPrAxeBachataBoleroForroGauchaMerenguePagodeSalsaSertanejaTangoF12.1 0.0 0.0 0.0 1.6 3.2 1.6 1.6 2.9 0.0 53.30.0 69.4 0.0 0.0 4.8 3.2 3.3 0.0 4.3 0.0 81.133.3 12.9 55.6 12.3 17.5 1.6 18.0 6.5 29.0 76.5 19.36.1 0.0 1.6 38.6 1.6 0.0 4.9 4.8 5.8 0.0 57.915.2 3.2 1.6 7.0 28.6 0.0 4.9 3.2 24.6 1.2 31.01.5 1.6 0.0 3.5 0.0 79.0 1.6 1.6 1.4 0.0 87.515.2 3.2 14.3 14.0 15.9 1.6 31.1 1.6 4.3 0.0 30.29.1 6.5 17.5 8.8 17.5 6.5 27.9 80.6 8.7 7.4 41.77.6 0.0 6.3 15.8 12.7 1.6 6.6 0.0 15.9 0.0 26.20.0 3.2 3.2 0.0 0.0 3.2 0.0 0.0 2.9 14.8 60.019.8 74.8 28.7 46.3 29.8 83.1 30.6 54.9 19.8 23.8 41.3(b) LMD artist-filtered: 15.78± 4.65dBFig. 4. For the DNN-based classifier in Fig. 2(b) and the CDNN in Fig. 3(d), but with all input intercepted by an adversary intent on making the maximumposterior correct with probability p = 0.1. For this adversary, Rmin = 0.9, SNR = 15dB, µ = 0.1, and kmax = 100. Sub-captions show the resulting SNR(mean ± standard deviation) for the adversarial test sets of GTZAN (N = 290) and LMD (N = 646).Classification in GTZANMusic excerpt Blues Classical Country Disco Hiphop Jazz Metal Pop Reggae RockLittle Richard, “Last Year’s Race Horse” 32 (23) 29 (23) 36 (25) 36 (26) 36 (25) 33 (24) 32 (24) 31 (25) 42 (26) 36 (25)Rossini, “William Tell Overture” 32 (25) 37 (30) 40 (29) 43 (28) 34 (24) 36 (29) 33 (25) 34 (26) 37 (26) 37 (28)Willie Nelson, “A Horse Called Music” 25 ( ) 25 (20) 30 (27) 30 (20) 26 (19) 30 (25) 27 (23) 21 (20) 30 (23) 29 (23)Simian Mobile Disco, “10000 Horses Can’t Be Wrong” 31 (30) 36 (31) 38 (32) 45 (34) 41 (33) 40 (32) 33 (31) 47 (34) 42 (33) 38 (33)Rubber Bandits, “Horse Outside” 27 (27) 27 (27) 36 (29) 42 (31) 38 (29) 34 (28) 32 (28) 37 (29) 36 (29) 35 (29)Leonard Gaskin, “Riders in the Sky” 32 (23) 30 (25) 32 (23) 35 (25) 31 (22) 35 (29) 34 (23) 26 (23) 35 (25) 35 (24)Jethro Tull, “Heavy Horses” 29 (26) 28 (26) 40 (29) 42 (29) 38 (28) 36 (28) 34 (28) 34 (28) 37 (28) 36 (29)Echo and The Bunnymen, “Bring on the Dancing Horses” 29 (25) 28 (26) 38 (28) 43 (28) 35 (26) 34 (26) 33 (26) 33 (26) 36 (27) 38 (28)Count Prince Miller, “Mule Train” 32 (30) 29 (30) 41 (33) 37 (34) 43 (33) 36 (31) 33 (31) 42 (34) 40 (33) 33 (33)Rolling Stones, “Wild Horses” 30 (22) 32 (24) 37 (25) 40 (25) 31 (22) 34 (25) 31 (26) 32 (23) 37 (25) 37 (26)TABLE IISNR OF PERTURBATIONS PRODUCED BY TWO ENSEMBLES OF ADVERSARIES THAT INTERCEPT THE INPUT TO THE SYSTEM IN FIG. 2(B) AND HAVE ITPRODUCE ALL CLASSIFICATIONS POSSIBLE WITH CONFIDENCE THRESHOLDS Rmin = 0.5 (Rmin = 0.9 IN BRACKETS). THE AVERAGE SNR IS 34.5(26.8) DB. THIS TABLE CAN BE HEARD AT HTTP://WWW.EECS.QMUL.AC.UK/∼STURM/RESEARCH/DNN ADVERSARIES.accuracy of 0.49 in Fig. 2(b), we see our adversary hassuccessfully confused the random forest classifier to be nobetter than random. Figure 5 shows one of the adversarialexamples from this experiment. Apart from some significanthigh-frequency deviations, the spectrum of the adversary isvery similar to that of the original. The SNR in this exampleis 21.1dB.Figure 4(b) shows the FoM of the CDNN classificationsystem in Fig. 3(d) attacked by the same adversary. In thiscase, the CDNN proved more difficult to fool, but still theadversary is able to significantly reduce the normalised clas-sification accuracy from 0.63 to 0.41 with high confidenceclassifications at rather high SNR. If we reduce the minimumconfidence Rmin = 0.5 and lessen the SNR constraint to−300 dB, then the adversary makes the CDNN perform evenworse: a normalised accuracy of 0.28 with a mean SNR of11.15± 8.32 dB.For the same system in Fig. 2(b), and using Rmin = 0.9,SNR = 15dB, µ = 0.1 and kmax = 100, we show in [33] thatwe able to create adversaries that make the system alwaysright, always wrong, and always select “Jazz.” Table II showsthe results of two ensembles of adversaries, each intent onmaking the system in Fig. 2(b) choose one of every label inGTZAN for the same music with SNR = 15dB, µ = 0.1 andkmax = 100. The adversaries of one ensemble insist upon aclassification confidence of at least Rmin = 0.5; and in theother of at least 0.9. These music recordings are the same 30-second excerpts used in [48]. We see that in all case by one,the ensembles are able to elicit high confidence classificationsfrom the system with minor perturbations of the input. We alsosee that larger perturbations are produced on average when theadversaries insist on a higher minimum confidence: 34.5 dBfor a confidence of at least Rmin = 0.5, and 26.8 dB for aconfidence of at least Rmin = 0.9.These results can be heard here: http://www.eecs.qmul.ac.uk/∼sturm/research/DNN adversaries. We find that the pertur-bations caused by these adversaries are certainly perceptible,unlike those found for image data in [54] and [24]; however,90 2 4 6 8 10Frequency (kHz)050100150200250300350400Time frame0 2 4 6 8 10Frequency (kHz)0501001502002503003504000 2 4 6 8 10Frequency (kHz)0501001502002503003504000 2 4 6 8 10Frequency (kHz)80604020020Magnitude (dB)Fig. 5. Top left: spectrogram excerpt from GTZAN Classical “21” (Mozart, Symphony No. 39 Finale) that the DNN-based system in Fig. 2(b) classifiesas Classical. Top middle: spectrogram of adversarial example classified as Reggae. Top right: spectrogram of the difference of the two. Bottom: magnitudespectrum of one frame (1024 samples) of the original (light blue), adversarial example (black), and difference (orange). Note that all excerpts in GTZAN havea sampling rate of 22050 Hz. The SNR = 21.1dB.Norm. Norm. Acc. SNR (dB)Deep Learning System Acc w/ Adversary mean ± std. dev.DNN-LMD Fig. 3(b) 0.63 0.03 37.8±4.6DNN-LMD+ADV 0.55 0.06 36.5±5.4CDNN-LMD Fig. 3(d) 0.63 0.21 9.62±5.8CDNN-LMD+ADV 0.56 0.21 9.74±6.4TABLE IIIRESULTS OF APPLYING ADVERSARY TO MAKE SYSTEMS IN FIG. 3(B,D)ALWAYS INCORRECT, AND AFTER TRAINING WITH ADVERSARY (ALG. 2).the distortion is very minor, and the music remains exactly thesame, e.g., pitches, rhythm, lyrics, instrumentation, dynamics,and style all remain the same.We now perform an experiment to compare (C)DNNstrained with adversarial examples (as per Alg. 2) to the systemsin Fig. 3(b,d). To do this we test the response of these systemsagainst an adversary aimed at always eliciting an incorrectresponse. (This is different from the adversary used above,which seeks to make the system correct with probabilityp = 0.1.) For this experiment, we set Rmin = 0.5 and SNRto −300 dB in order to allow arbitrarily large perturbations toforce misclassifications. Table III illustrates the results of thisexperiment from which we observe several interesting results.Column 1 shows the normalized accuracy on the original testset (with no adversary present). We see that training againstadversarial examples leads to a slight deflation in accuracy onnew test data. Column 2 shows the normalized accuracy ofthese systems against our adversary intent on forcing a 100%error rate. We see that the CDNN systems are more robust tothis adversary, and that the systems trained against adversarialexamples confer little to no advantage. Column 3 shows theaverage perturbation size of the adversarial examples thatled to misclassifications. We notice that larger perturbations(corresponding to lower SNRs) were required to get theCDNN systems to misclassify test inputs. The minimum SNRproduced was 0.11 dB, while the maximum was 47.6 dB.The results of this experiment point to the conclusions thata) the CDNN systems are more robust to this adversary; andb) training against adversarial examples (contrary to what wehypothesized) does not seem reduce the misclassification rateagainst new adversarial examples. A possible explanation forthe latter results is that, due to the high-dimensional natureof the input space, the set of possible adversarial examples isdensely packed, so that training on a small number of thesepoints is not sufficient to allow the systems to generalize tonew adversarial examples.V. DISCUSSIONReturning to the broadest question motivating our work, weseek to measure the contribution of deep learning to musiccontent analysis. The previous sections describe a series ofexperiments we have conducted using deep learning systemsof a variety of architectures, which we have trained and testedin two different partitions of two benchmark music datasets Wehave evaluated the robustness of these systems to an adversarythat has complete knowledge of the classifiers, and have alsoinvestigated the use of an adversary in the training of deeplearning systems.Our experimental results in Fig. 2 and Table I are essentiallyreproductions of those reported in [44]. Based on the resultsof their experiments with random partitionings of GTZAN,Sigtia et al. [44] claim that their DNN-based systems learnfeatures that “better represent the audio” than standard or“hand-crafted” features, e.g., those referenced in [30] likeMFCCs. Similar conclusions are made about the deep learningsystems in [28], also based on experiments using a randompartitioning of GTZAN. However, we see in Fig. 2 and TableI that when we consider the faults in the GTZAN dataset andpartition it along artist lines, as for the LMD dataset in Fig. 3,our deep learning systems perform significantly worse. This is10(a) GTZAN fault-filtered (b) LMD artist-filteredFig. 6. As in Fig. 2, FoM for majority vote of minimum Mahalanobis distance classification of mean and variances over 5-second “texture” windows ofzero-crossings and the first 13 MFCCs computed from 46 ms windows hopped 50%.an expected outcome [23], [39], [49], but the artist informationin GTZAN was not available until 2012 [46].This motivates the question of whether DNN-based systemsreally do perform better than that of a classifier using standard,low-level and “hand-crafted” features. To examine this, webuild baseline systems that use low-level features, and trainand test them in the same fault-filtered partition of GTZANas in Fig. 2(b), and the artist-filtered partition of LMD as inFig. 3(b,d). Mimicking [28], [44], we compute these featuresbased on a short-time analysis using 46ms frames hoppedby 50%. From each frame we extract the first 13 Mel-frequency cepstral coefficients (MFCCs) and zero-crossings,and compute their mean and variance over five-second texturewindows (which are also hopped by 50%). We combine thefeatures of the training and validation sets of the fault-filteredpartition of GTZAN, and the artist filtered partition of LMD.Both systems use a minimum Mahalanobis distance classifier,and assign a class by majority vote from the classificationsof the individual texture windows. Figure 6 shows the FoMproduced by these baseline systems. We see that for GTZANit actually reproduces more ground truth than the DNN in Fig.2(b) and all but one in Table I. Our simple baseline systemfor LMD reproduces much less ground truth than the (C)DNNin Fig. 3(b,d). Nonetheless, we have no reason to acceptthe conclusion that deep learning features “perform better”than “hand-crafted” features for the particular architecturesconsidered here and those in [28], [44]. Different experimentsare needed to address such a conclusion.A tempting conclusion is that since the normalised classifi-cation accuracies in Figs. 2(b) and 3(d) are extremely unlikelyto arise by chance (p < 10−62 for GTZAN and p < 10−290 forLMD by a Binomial test) it is therefore entirely reasonable toreject the hypothesis that our (C)DNN are choosing outputs atrandom. Hence, one might argue that these (C)DNN must havelearned features that are “relevant” to music genre recognition[28], [31], [44]. This argument appears throughout the MIRresearch discipline [49], and turns on the strong assumptionthat there are only two ways a system can reproduce theground truth of a dataset: by chance or by learning to solvea specific problem thought to be well-posed by a cleanlylabeled dataset [51]. In fact, there is a third way a system canreproduce the ground truth of a music dataset: by learning toexploit characteristics shared between the training and testingdatasets that arise not from a relationship in the real world,but from the curation and partitioning of a dataset in theexperimental design of an evaluation [48], [49], [53]. Sincethe evaluations producing Figs. 2 and 3, as well as all resultsin [28], [44], not to mention a significant number of publishedstudies in MIR [49], do not control for this third way, wecannot validly conclude upon the “relevance” of whatever hasbeen learned by these music content analysis systems.A notion of this problem is given by the significant de-creases in the FoM we measure when partitioning GTZAN andLMD along artist lines. By doing so, we are controlling forsome independent variables that a system might be exploitingto reproduce ground truth, but which arguably have littlerelevance to the high-level labels of the dataset [49]. Moreconcretely, consider that all 100 excerpts labeled Pop inGTZAN come from recordings of music by four artists, 25from each artist. If we train and test a system on a randompartition of GTZAN, we cannot know whether the system isrecognising Pop, recognising the artist, or recognising otheraspects that may or may not be related to Pop. If we train asystem instead with Pop excerpts by three artists, test with thePop excerpts by the fourth artist, then we might be testingsomething closer to Pop recognition. This all depends ondefining what knowledge is relevant to the problem.A common retort to these arguments is that a system shouldbe able to reproduce ground truth “by any means.” One therebydefines “relevant knowledge” as any correlations that helps a11system reproduce an amount of dataset ground truth that isinconsistent with chance. However, this can lead to circularreasoning: system X has learned “relevant knowledge” becauseit reproduces Y amount of ground truth; system X reproducesY amount of ground truth because it has learned “relevantknowledge.” It is also deaf to one of the major aims of researchin music content analysis [14]: “to make music, or informationabout music, easier to find.” If a music content analysis systemis describing music in ways that do not align with those ofits users, then its usability is in jeopardy no matter its FoMin benchmark datasets [42], [56]. Finally, this means that theproblem thought to be well-posed by a cleanly labeled datasetcan be many things simultaneously — which leads to theproblem of how to validly compare apples and oranges [51].In other words, why compare systems when they are solvingdifferent problems? This also applies to the comparisons abovewith the FoM in Fig. 6.While we have no idea whether our (C)DNN systems inFig. 3 are exploiting “irrelevant” characteristics in LMD, ourexperimental results with adversaries in Figs. 4 and 5, andTables II and III, indicate that their decision machinery isincredibly sensitive in very strange ways. Our adversaries areable to fool the high-performing deep learning systems byperturbing their input in minor ways. Auditioning the resultsin Table II show that while the music in each recordingremains exactly the same, and the perturbations are very small,the DNN is nearly always fooled into choosing with highconfidence every class it has supposedly learned. The CDNNis similarly defeated by our adversary; however, it is quitenotable that it requires perturbations of far lower SNR thandoes the DNN. We are currently studying the reasons for this.Our application of adversaries here is close to the “methodof irrelevant transformations” that we apply in [48], [52],[53] to assess the internal models of music content analysissystems, and to test the hypothesis, “the system is usingrelevant criteria to make its decisions.” In [48], we take abrute force approach whereby we apply random but lineartime-invariant and minor filtering to inputs of systems trainedin three different music recording datasets until their FoMbecomes perfect or random. We also make each system applyevery one of its classes to the same music recordings in TableII.4 In [53], we instead apply subtle pitch-preserving time-stretching of music recordings to fool a deep learning systemtrained in the benchmark music dataset BALLROOM [20].We find that through such a transformation we can make thesystem perform perfectly or no better than random by applyingtempo changes of at most 6% to test dataset recordings. Wefind a similar result for the same kind of deep learning systembut trained in LMD [52].Our adversary in Alg. 1 moves instead right to the achillesheel of a deep learning system, coaxing it to behave in arbitraryways for an input simply by making minor perturbations tothe sampled audio waveform that have no effect on the musiccontent it possesses. We observe in Fig. 5 and auditioningTable II that the low- to mid-frequency content of adversarial4These results can be auditioned here: http://www.eecs.qmul.ac.uk/∼sturm/research/TM expt2/index.htmlexamples differs very little from the original recordings, butfind more significant differences in the high-frequency spectra.This suggests that the distribution of energy in the high-frequency spectrum has significant impact on the decisionmachinery of our (C)DNN. The apparent high relevance ofsuch slight characteristics in proportion to that of the actualmusical content of a music recording does not bode wellfor one of the most important aims of machine learning:generalisation.As observed by Goodfellow et al. [24] in their deep learningsystems taught to recognise objects in images, the impressiveFoM we measure of our deep learning systems may be merelya colourful “Potemkin village.” Employing an adversary toscratch a little below the surface reveals the FoM to becuriously hollow. A system that appears to be solving acomplex problem but actually is not is what we term a “horse”[48], which is a nod to the famous horse Clever Hans: a realhorse that appeared to be a capable mathematician but wasmerely responding to involuntary cues that went undetectedbecause his public demonstrations had no validity to attest tosuch an ability. Measuring the number of correct answers Hansgives in an uncontrolled environment does not give reasonto conclude he comprehends what he appears to be doing.It is the same with the experiments we perform above withsystems labelling observations in GTZAN and LMD. In fact,Goodfellow et al. [24] come to the same conclusion: “Theexistence of adversarial examples suggests that ... being ableto correctly label the test data does not imply that our modelstruly understand the tasks we have asked them to perform”[24]. This observation is now well-known in MIR [47]–[50],but deserves to be repeated.VI. CONCLUSIONIn this article, we have shown how to adapt the adversaryof Szegedy et al. [54] to work within the context of musiccontent analysis using deep learning. We have shown how ouradversary is effective at fooling deep learning systems of dif-ferent architectures, trained on different benchmark datasets.We find our convolutional networks are more robust againstthis adversary than our deep neural networks. We have alsosought to employ the adversary as part of the training of thesesystems, but find it results in systems that remain as sensitiveto the same adversary.It is of course not very popular for one to be an “adversary”to research, moving quickly to refute conclusions and breaksystems reported in the literature; however, we insist thatbreaking systems leads ultimately to progress. Considerableinsight can be gained by looking behind the veil of perfor-mance metrics in an attempt to determine the mechanismsby which a system operates, and whether the evaluation isany valid reflection of the qualities we wish to measure. Suchprobing is necessary if we are truly interested in ascertainingwhat a system has learned to do, what its vulnerabilities mightbe, how it compares to competing systems supposedly solvingthe same problem, and how well we can expect it to performwhen used in real-world applications.12ACKNOWLEDGMENTSCK and JL were supported in part by the Danish Councilfor Strategic Research of the Danish Agency for ScienceTechnology and Innovation under the CoSound project, casenumber 11-115328. This publication only reflects the authors’views.REFERENCES[1] J. B. Allen and L. Rabiner. A unified approach to short-time Fourieranalysis and synthesis. Proc. IEEE, 65(11):1558–1564, Nov. 1977.[2] J.-J. Aucouturier and F. Pachet. Scaling up music playlist generation.In Multimedia and Expo, 2002. ICME ’02. Proceedings. 2002 IEEEInternational Conference on, volume 1, pages 105–108 vol.1, 2002.[3] J-.J. Aucouturier and F. Pachet. Improving timbre similarity: How highis the sky? J. of Negative Results in Speech and Audio Sciences, 1(1),2004.[4] Fre´de´ric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra,Ian J. Goodfellow, Arnaud Bergeron, Nicolas Bouchard, and YoshuaBengio. Theano: new features and speed improvements. Deep Learningand Unsupervised Feature Learning NIPS 2012 Workshop, 2012.[5] E. Battenberg and D. Wessel. Analyzing drum patterns using conditionaldeep belief networks. In Proc. ISMIR, 2012.[6] Y. Bengio, I. Goodfellow, and A. Courville. Deep Learning. MIT Press,2015 (in preparation).[7] Yoshua Bengio. Learning deep architectures for AI. Foundations andtrends in Machine Learning, 2(1):1–127, 2009.[8] James Bergstra, Olivier Breuleux, Fre´de´ric Bastien, Pascal Lamblin,Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU math expressioncompiler. In Proceedings of the Python for Scientific ComputingConference (SciPy), June 2010. Oral Presentation.[9] T. Bertin-Mahieux, D. Eck, and M. Mandel. Automatic tagging of audio:The state-of-the-art. In W. Wang, editor, Machine Audition: Principles,Algorithms and Systems. IGI Publishing, 2010.[10] T. Bertin-Mahieux, D. P.W. Ellis, B. Whitman, and P. Lamere. Themillion song dataset. In Proc. ISMIR, 2011.[11] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Audio chordrecognition with recurrent neural networks. In Proc. ISMIR, 2013.[12] C. J. C. Burges, J. C. Platt, and S. Jana. Distortion discriminant analysisfor audio fingerprinting. IEEE Trans. Speech Audio Process., 11(3):165–174, May 2003.[13] M. Casey, C. Rhodes, and M. Slaney. Analysis of minimum distancesin high-dimensional musical spaces. IEEE Trans. Audio, Speech, Lang.Process., 16(5):1015–1028, July 2008.[14] M. Casey, R. Veltkamp, M. Goto, M. Leman, C. Rhodes, and M. Slaney.Content-based music information retrieval: Current directions and futurechallenges. Proc. IEEE, 96(4):668–696, Apr. 2008.[15] Nick Collins. Computational analysis of musical influence: A musico-logical case study using mir tools. In ISMIR, pages 177–182, 2010.[16] N. Dalvi, P. Domingos, Mausam, S. Sanghai, and D. Verma. Adversarialclassification. KDD, 2004.[17] L. Deng and D. Yu. Deep Learning: Methods and Applications. NowPublishers, 2014.[18] S. Dieleman, P. Brakel, and B. Schrauwen. Audio-based music classifi-cation with a pretrained convolutional network. In Proc. ISMIR, 2011.[19] S. Dieleman and B. Schrauwen. End-to-end learning for music audio.In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEEInternational Conference on, pages 6964–6968, May 2014.[20] S. Dixon, F. Gouyon, and G. Widmer. Towards characterisation of musicvia rhythmic patterns. In Proc. ISMIR, pages 509–517, 2004.[21] S. Ewert, B. Pardo, M. Muller, and M.D. Plumbley. Score-informedsource separation for musical audio recordings: An overview. SignalProcessing Magazine, IEEE, 31(3):116–124, May 2014.[22] A. Flexer. A closer look on artist filters for musical genre classification.In Proc. ISMIR, pages 341–344, Sep. 2007.[23] A. Flexer, D. Schnitzer, M. Gasser, and T. Pohle. Combining featuresreduces hubness in audio similarity. In Proc. Int. Symp. Music Info.Retrieval, 2010.[24] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessingadversarial examples. In Proc. ICLR, 2015.[25] Daniel Griffin and Jae S Lim. Signal estimation from modified short-time fourier transform. Acoustics, Speech and Signal Processing, IEEETransactions on, 32(2):236–243, 1984.[26] Niall Griffith and Peter M Todd. Musical networks: Parallel distributedperception and performance. MIT Press, 1999.[27] S. Gu and L. Rigazio. Towards Deep Neural Network ArchitecturesRobust to Adversarial Examples. ArXiv e-prints, December 2014.[28] P. Hamel and D. Eck. Learning features from music audio with deepbelief networks. In Proc. ISMIR, 2010.[29] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of StatisticalLearning: Data Mining, Inference, and Prediction. Springer-Verlag, 2edition, 2009.[30] M. Henaff, K. Jarrett, K. Kavukcuoglu, and Y. LeCun. Unsupervisedlearning of sparse features for scalable audio classification. In Proc. Int.Soc. Music Info. Retrieval, Miami, FL, Oct. 2011.[31] E. J. Humphrey, J. P. Bello, and Y. LeCun. Feature learning and deeparchitectures: New directions for music informatics. J. Intell. Info.Systems, 41(3):461–481, 2013.[32] E.J. Humphrey and J.P. Bello. From music audio to chord tablature:Teaching deep convolutional networks toplay guitar. In Acoustics,Speech and Signal Processing (ICASSP), 2014 IEEE InternationalConference on, pages 6974–6978, May 2014.[33] C. Kereliuk, B. L. Sturm, and J. Larsen. Deep learning, audio adver-saries, and music content analysis. In Proc. WASPAA, 2015.[34] H. Lee, Y. Largman, P. Pham, and A. Y. Ng. Unsupervised feature learn-ing for audio classification using convolutional deep belief networks. InProc. Neural Info. Process. Systems, Vancouver, B.C., Canada, Dec.2009.[35] T. LH. Li, A. B. Chan, and A. HW. Chun. Automatic musical patternfeature extraction using convolutional neural network. In Proc. Int. Conf.Data Mining and Applications, 2010.[36] B. Matityaho and M. Furst. Neural network based model for classifica-tion of music type. In Proc. Conv. Electrical and Elect. Eng. in Israel,pages 1–5, Mar. 1995.[37] G. Montavon, G. B. Orr, and K.-R. Mu¨ller, editors. Neural Networks,Tricks of the Trade, Reloaded. Lecture Notes in Computer Science(LNCS 7700). Springer, 2012.[38] A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easilyfooled: High confidence predictions for unrecognizable images. In Proc.NIPS, 2014.[39] E. Pampalk, A. Flexer, and G. Widmer. Improvements of audio-basedmusic similarity and genre classification. In Proc. Int. Soc. Music Info.Retrieval, pages 628–233, Sep. 2005.[40] G. Papadopoulos and G. Wiggins. Ai methods for algorithmic compo-sition: A survey, a critical view and future prospects. In Proc. AISBSymposim on Musical Creativity, pages 110–117, 1999.[41] A. Pikrakis. A deep learning approach to rhythm modeling withapplications. In Proc. Int. Workshop Machine Learning and Music, 2013.[42] M. Schedl, A. Flexer, and J. Urbano. The neglected user in musicinformation retrieval research. J. Intell. Info. Systems, 41(3):523–539,2013.[43] D. Schwarz. Concatenative sound synthesis: The early years. J. NewMusic Research, 35(1):3–22, Mar. 2006.[44] S. Sigtia and S. Dixon. Improved music feature learning with deepneural networks. In Acoustics, Speech and Signal Processing (ICASSP),2014 IEEE International Conference on, pages 6959–6963, May 2014.[45] C. N. Silla, A. L. Koerich, and C. A. A. Kaestner. The Latin musicdatabase. In Proc. ISMIR, 2008.[46] B. L. Sturm. An analysis of the GTZAN music genre dataset. In Proc.ACM MIRUM Workshop, pages 7–12, Nara, Japan, Nov. 2012.[47] B. L. Sturm. Classification accuracy is not enough: On the evaluation ofmusic genre recognition systems. J. Intell. Info. Systems, 41(3):371–406,2013.[48] B. L. Sturm. A simple method to determine if a music informationretrieval system is a “horse”. IEEE Trans. Multimedia, 16(6):1636–1644, 2014.[49] B. L. Sturm. The state of the art ten years after a state of the art:Future research in music information retrieval. J. New Music Research,43(2):147–172, 2014.[50] B. L. Sturm. A survey of evaluation in music genre recognition.In A. Nu¨rnberger, S. Stober, B. Larsen, and M. Detyniecki, editors,Adaptive Multimedia Retrieval: Semantics, Context, and Adaptation,volume LNCS 8382, pages 29–66, Oct. 2014.[51] B. L. Sturm. “horse” inside: Seeking causes of the behaviours ofmusic content analysis systems. ACM Computers in Entertainment, 2015(submitted).[52] B. L. Sturm, C. Kereliuk, and J. Larsen. ¿ el caballo viejo? latin genrerecognition with deep learning and spectral periodicity. In Proc. Int.Conf. on Mathematics and Computation in Music, 2015.13[53] B. L. Sturm, C. Kereliuk, and A. Pikrakis. A closer look at deep learningneural networks with low-level spectral periodicity features. In Proc. Int.Workshop on Cognitive Info. Process., 2014.[54] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,and R. Fergus. Intriguing properties of neural networks. In Proc. ICLR,2014.[55] G. Tzanetakis and P. Cook. Musical genre classification of audio signals.IEEE Trans. Speech Audio Process., 10(5):293–302, July 2002.[56] J. Urbano, M. Schedl, and X. Serra. Evaluation in music informationretrieval. J. Intell. Info. Systems, 41(3):345–369, Dec. 2013.[57] A. van den Oord, S. Dieleman, and B. Schrauwen. Deep content-basedmusic recommendation. In Proc. NIPS, 2013.[58] N. Vempala and F. Russo. Predicting emotion from music audio featuresusing neural networks. In Proc. CMMR, 2012.[59] A. Wang. An industrial strength audio search algorithm. In Proc. Int.Soc. Music Info. Retrieval, Oct. 2003.[60] F. Weninger, F. Eyben, and B. Schuller. On-line continuous-time musicmood regression with deep recurrent neural networks. In Acoustics,Speech and Signal Processing (ICASSP), 2014 IEEE InternationalConference on, pages 5412–5416, May 2014.[61] B. Whitman, G. Flake, and S. Lawrence. Artist detection in music withminnowmatch. Proc. IEEE Workshop on Neural Networks for SignalProcessing, pages 559–568, 2001.[62] G. A. Wiggins. Semantic gap?? Schemantic schmap!! Methodologicalconsiderations in the scientific study of music. In Proc. IEEE Int. Symp.Mulitmedia, pages 477–482, Dec. 2009.[63] X. Yang, Q. Chen, S. Zhou, and X. Wang. Deep belief networks forautomatic music genre classification. In Proc. INTERSPEECH, pages2433–2436, 2011.[64] Y.-H. Yang and H. H. Chen. Music Emotion Recognition. CRC Press,2011.[65] Chiyuan Zhang, G. Evangelopoulos, S. Voinea, L. Rosasco, and T. Pog-gio. A deep representation for invariance and music classification.In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEEInternational Conference on, pages 6984–6988, May 2014.",
            "id": 8850019,
            "identifiers": [
                {
                    "identifier": "oai:qmro.qmul.ac.uk:123456789/11150",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "10.1109/tmm.2015.2478068",
                    "type": "DOI"
                },
                {
                    "identifier": "30698000",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "285366754",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "29554658",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:pure.atira.dk:publications/524da999-193a-48ac-b6d6-af6bf07ac96e",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "1507.04761",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "43254355",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:arxiv.org:1507.04761",
                    "type": "OAI_ID"
                }
            ],
            "title": "Deep Learning and Music Adversaries",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:pure.atira.dk:publications/524da999-193a-48ac-b6d6-af6bf07ac96e",
                "oai:qmro.qmul.ac.uk:123456789/11150",
                "oai:arxiv.org:1507.04761"
            ],
            "publishedDate": "2015-01-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 43526106,
                    "title": "A closer look at deep learning neural networks with low-level spectral periodicity features.",
                    "authors": [],
                    "date": "2014",
                    "doi": "10.1109/cip.2014.6844511",
                    "raw": "B. L. Sturm, C. Kereliuk, and A. Pikrakis. A closer look at deep learning neural networks with low-level spectral periodicity features. In Proc. Int. Workshop on Cognitive Info. Process., 2014.",
                    "cites": null
                },
                {
                    "id": 43526054,
                    "title": "A closer look on artist filters for musical genre classification.",
                    "authors": [],
                    "date": "2007",
                    "doi": null,
                    "raw": "A. Flexer. A closer look on artist filters for musical genre classification. In Proc. ISMIR, pages 341–344, Sep. 2007.",
                    "cites": null
                },
                {
                    "id": 43526093,
                    "title": "A deep learning approach to rhythm modeling with applications.",
                    "authors": [],
                    "date": "2013",
                    "doi": null,
                    "raw": "A. Pikrakis. A deep learning approach to rhythm modeling with applications. In Proc. Int. Workshop Machine Learning and Music, 2013.",
                    "cites": null
                },
                {
                    "id": 43526121,
                    "title": "A deep representation for invariance and music classification.",
                    "authors": [],
                    "date": "2014",
                    "doi": "10.1109/icassp.2014.6854954",
                    "raw": "Chiyuan Zhang, G. Evangelopoulos, S. Voinea, L. Rosasco, and T. Poggio. A deep representation for invariance and music classification. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 6984–6988, May 2014.",
                    "cites": null
                },
                {
                    "id": 43526101,
                    "title": "A simple method to determine if a music information retrieval system is a “horse”.",
                    "authors": [],
                    "date": "2014",
                    "doi": "10.1109/tmm.2014.2330697",
                    "raw": "B. L. Sturm. A simple method to determine if a music information retrieval system is a “horse”. IEEE Trans. Multimedia, 16(6):1636– 1644, 2014.",
                    "cites": null
                },
                {
                    "id": 43526103,
                    "title": "A survey of evaluation in music genre recognition. In",
                    "authors": [],
                    "date": "2014",
                    "doi": "10.1007/978-3-319-12093-5_2",
                    "raw": "B. L. Sturm. A survey of evaluation in music genre recognition. In A. Nu¨rnberger, S. Stober, B. Larsen, and M. Detyniecki, editors, Adaptive Multimedia Retrieval: Semantics, Context, and Adaptation, volume LNCS 8382, pages 29–66, Oct. 2014.",
                    "cites": null
                },
                {
                    "id": 43526028,
                    "title": "A unified approach to short-time Fourier analysis and synthesis.",
                    "authors": [],
                    "date": "1977",
                    "doi": "10.1109/proc.1977.10770",
                    "raw": "J. B. Allen and L. Rabiner. A unified approach to short-time Fourier analysis and synthesis. Proc. IEEE, 65(11):1558–1564, Nov. 1977.",
                    "cites": null
                },
                {
                    "id": 43526044,
                    "title": "Adversarial classification.",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1145/1014052.1014066",
                    "raw": "N. Dalvi, P. Domingos, Mausam, S. Sanghai, and D. Verma. Adversarial classification. KDD, 2004.",
                    "cites": null
                },
                {
                    "id": 43526092,
                    "title": "Ai methods for algorithmic composition: A survey, a critical view and future prospects.",
                    "authors": [],
                    "date": "1999",
                    "doi": null,
                    "raw": "G. Papadopoulos and G. Wiggins. Ai methods for algorithmic composition: A survey, a critical view and future prospects. In Proc. AISB Symposim on Musical Creativity, pages 110–117, 1999.",
                    "cites": null
                },
                {
                    "id": 43526099,
                    "title": "An analysis of the GTZAN music genre dataset.",
                    "authors": [],
                    "date": "2012",
                    "doi": "10.1145/2390848.2390851",
                    "raw": "B. L. Sturm. An analysis of the GTZAN music genre dataset. In Proc. ACM MIRUM Workshop, pages 7–12, Nara, Japan, Nov. 2012.",
                    "cites": null
                },
                {
                    "id": 43526112,
                    "title": "An industrial strength audio search algorithm.",
                    "authors": [],
                    "date": "2003",
                    "doi": null,
                    "raw": "A. Wang. An industrial strength audio search algorithm. In Proc. Int. Soc. Music Info. Retrieval, Oct. 2003.",
                    "cites": null
                },
                {
                    "id": 43526041,
                    "title": "Analysis of minimum distances in high-dimensional musical spaces.",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1109/tasl.2008.925883",
                    "raw": "M. Casey, C. Rhodes, and M. Slaney. Analysis of minimum distances in high-dimensional musical spaces. IEEE Trans. Audio, Speech, Lang. Process., 16(5):1015–1028, July 2008.",
                    "cites": null
                },
                {
                    "id": 43526032,
                    "title": "Analyzing drum patterns using conditional deep belief networks.",
                    "authors": [],
                    "date": "2012",
                    "doi": null,
                    "raw": "E. Battenberg and D. Wessel. Analyzing drum patterns using conditional deep belief networks. In Proc. ISMIR, 2012.",
                    "cites": null
                },
                {
                    "id": 43526114,
                    "title": "Artist detection in music with minnowmatch.",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1109/nnsp.2001.943160",
                    "raw": "B. Whitman, G. Flake, and S. Lawrence. Artist detection in music with minnowmatch. Proc. IEEE Workshop on Neural Networks for Signal Processing, pages 559–568, 2001.",
                    "cites": null
                },
                {
                    "id": 43526039,
                    "title": "Audio chord recognition with recurrent neural networks.",
                    "authors": [],
                    "date": "2013",
                    "doi": null,
                    "raw": "N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Audio chord recognition with recurrent neural networks. In Proc. ISMIR, 2013.",
                    "cites": null
                },
                {
                    "id": 43526047,
                    "title": "Audio-based music classification with a pretrained convolutional network. In",
                    "authors": [],
                    "date": "2011",
                    "doi": null,
                    "raw": "S. Dieleman, P. Brakel, and B. Schrauwen. Audio-based music classification with a pretrained convolutional network. In Proc. ISMIR, 2011.",
                    "cites": null
                },
                {
                    "id": 43526080,
                    "title": "Automatic musical pattern feature extraction using convolutional neural network. In",
                    "authors": [],
                    "date": "2010",
                    "doi": null,
                    "raw": "T. LH. Li, A. B. Chan, and A. HW. Chun. Automatic musical pattern feature extraction using convolutional neural network. In Proc. Int. Conf. Data Mining and Applications, 2010.",
                    "cites": null
                },
                {
                    "id": 43526036,
                    "title": "Automatic tagging of audio: The state-of-the-art.",
                    "authors": [],
                    "date": "2010",
                    "doi": "10.4018/978-1-61520-919-4.ch014",
                    "raw": "T. Bertin-Mahieux, D. Eck, and M. Mandel. Automatic tagging of audio: The state-of-the-art. In W. Wang, editor, Machine Audition: Principles, Algorithms and Systems. IGI Publishing, 2010.",
                    "cites": null
                },
                {
                    "id": 43526100,
                    "title": "Classification accuracy is not enough: On the evaluation of music genre recognition systems.",
                    "authors": [],
                    "date": "2013",
                    "doi": "10.1007/s10844-013-0250-y",
                    "raw": "B. L. Sturm. Classification accuracy is not enough: On the evaluation of music genre recognition systems. J. Intell. Info. Systems, 41(3):371–406, 2013.",
                    "cites": null
                },
                {
                    "id": 43526056,
                    "title": "Combining features reduces hubness in audio similarity.",
                    "authors": [],
                    "date": "2010",
                    "doi": null,
                    "raw": "A. Flexer, D. Schnitzer, M. Gasser, and T. Pohle. Combining features reduces hubness in audio similarity. In Proc. Int. Symp. Music Info. Retrieval, 2010.",
                    "cites": null
                },
                {
                    "id": 43526043,
                    "title": "Computational analysis of musical influence: A musicological case study using mir tools.",
                    "authors": [],
                    "date": "2010",
                    "doi": null,
                    "raw": "Nick Collins. Computational analysis of musical influence: A musicological case study using mir tools. In ISMIR, pages 177–182, 2010.",
                    "cites": null
                },
                {
                    "id": 43526095,
                    "title": "Concatenative sound synthesis: The early years.",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1080/09298210600696857",
                    "raw": "D. Schwarz. Concatenative sound synthesis: The early years. J. New Music Research, 35(1):3–22, Mar. 2006.",
                    "cites": null
                },
                {
                    "id": 43526042,
                    "title": "Content-based music information retrieval: Current directions and future challenges.",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1109/jproc.2008.916370",
                    "raw": "M. Casey, R. Veltkamp, M. Goto, M. Leman, C. Rhodes, and M. Slaney. Content-based music information retrieval: Current directions and future challenges. Proc. IEEE, 96(4):668–696, Apr. 2008.",
                    "cites": null
                },
                {
                    "id": 43526117,
                    "title": "Deep belief networks for automatic music genre classification.",
                    "authors": [],
                    "date": "2011",
                    "doi": null,
                    "raw": "X. Yang, Q. Chen, S. Zhou, and X. Wang. Deep belief networks for automatic music genre classification. In Proc. INTERSPEECH, pages 2433–2436, 2011.",
                    "cites": null
                },
                {
                    "id": 43526110,
                    "title": "Deep content-based music recommendation.",
                    "authors": [],
                    "date": "2013",
                    "doi": null,
                    "raw": "A. van den Oord, S. Dieleman, and B. Schrauwen. Deep content-based music recommendation. In Proc. NIPS, 2013.",
                    "cites": null
                },
                {
                    "id": 43526076,
                    "title": "Deep learning, audio adversaries, and music content analysis.",
                    "authors": [],
                    "date": "2015",
                    "doi": "10.1109/waspaa.2015.7336950",
                    "raw": "C. Kereliuk, B. L. Sturm, and J. Larsen. Deep learning, audio adversaries, and music content analysis. In Proc. WASPAA, 2015.",
                    "cites": null
                },
                {
                    "id": 43526045,
                    "title": "Deep Learning: Methods and Applications.",
                    "authors": [],
                    "date": "2014",
                    "doi": "10.1561/2000000039",
                    "raw": "L. Deng and D. Yu. Deep Learning: Methods and Applications. Now Publishers, 2014.",
                    "cites": null
                },
                {
                    "id": 43526033,
                    "title": "Deep Learning.",
                    "authors": [],
                    "date": "2015",
                    "doi": "10.1007/978-3-642-36657-4_1",
                    "raw": "Y. Bengio, I. Goodfellow, and A. Courville. Deep Learning. MIT Press, 2015 (in preparation).",
                    "cites": null
                },
                {
                    "id": 43526084,
                    "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images.",
                    "authors": [],
                    "date": "2014",
                    "doi": "10.1109/cvpr.2015.7298640",
                    "raw": "A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proc. NIPS, 2014.",
                    "cites": null
                },
                {
                    "id": 43526040,
                    "title": "Distortion discriminant analysis for audio fingerprinting.",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1109/tsa.2003.811538",
                    "raw": "C. J. C. Burges, J. C. Platt, and S. Jana. Distortion discriminant analysis for audio fingerprinting. IEEE Trans. Speech Audio Process., 11(3):165– 174, May 2003.",
                    "cites": null
                },
                {
                    "id": 43526105,
                    "title": "el caballo viejo? latin genre recognition with deep learning and spectral periodicity.",
                    "authors": [],
                    "date": "2015",
                    "doi": "10.1007/978-3-319-20603-5_34",
                    "raw": "B. L. Sturm, C. Kereliuk, and J. Larsen. ¿ el caballo viejo? latin genre recognition with deep learning and spectral periodicity. In Proc. Int. Conf. on Mathematics and Computation in Music, 2015.",
                    "cites": null
                },
                {
                    "id": 43526049,
                    "title": "End-to-end learning for music audio.",
                    "authors": [],
                    "date": "2014",
                    "doi": "10.1109/icassp.2014.6854950",
                    "raw": "S. Dieleman and B. Schrauwen. End-to-end learning for music audio. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 6964–6968, May 2014.",
                    "cites": null
                },
                {
                    "id": 43526109,
                    "title": "Evaluation in music information retrieval.",
                    "authors": [],
                    "date": "2013",
                    "doi": "10.1007/s10844-013-0249-4",
                    "raw": "J. Urbano, M. Schedl, and X. Serra. Evaluation in music information retrieval. J. Intell. Info. Systems, 41(3):345–369, Dec. 2013.",
                    "cites": null
                },
                {
                    "id": 43526058,
                    "title": "Explaining and harnessing adversarial examples.",
                    "authors": [],
                    "date": "2015",
                    "doi": null,
                    "raw": "I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In Proc. ICLR, 2015.",
                    "cites": null
                },
                {
                    "id": 43526072,
                    "title": "Feature learning and deep architectures: New directions for music informatics.",
                    "authors": [],
                    "date": "2013",
                    "doi": "10.1007/s10844-013-0248-5",
                    "raw": "E. J. Humphrey, J. P. Bello, and Y. LeCun. Feature learning and deep architectures: New directions for music informatics. J. Intell. Info. Systems, 41(3):461–481, 2013.",
                    "cites": null
                },
                {
                    "id": 43526074,
                    "title": "From music audio to chord tablature: Teaching deep convolutional networks toplay guitar.",
                    "authors": [],
                    "date": "2014",
                    "doi": "10.1109/icassp.2014.6854952",
                    "raw": "E.J. Humphrey and J.P. Bello. From music audio to chord tablature: Teaching deep convolutional networks toplay guitar. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 6974–6978, May 2014.",
                    "cites": null
                },
                {
                    "id": 43526104,
                    "title": "horse” inside: Seeking causes of the behaviours of music content analysis systems.",
                    "authors": [],
                    "date": "2015",
                    "doi": null,
                    "raw": "B. L. Sturm. “horse” inside: Seeking causes of the behaviours of music content analysis systems. ACM Computers in Entertainment, 2015 (submitted).",
                    "cites": null
                },
                {
                    "id": 43526096,
                    "title": "Improved music feature learning with deep neural networks.",
                    "authors": [],
                    "date": "2014",
                    "doi": "10.1109/icassp.2014.6854949",
                    "raw": "S. Sigtia and S. Dixon. Improved music feature learning with deep neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 6959–6963, May 2014.",
                    "cites": null
                },
                {
                    "id": 43526091,
                    "title": "Improvements of audio-based music similarity and genre classification.",
                    "authors": [],
                    "date": "2005",
                    "doi": null,
                    "raw": "E. Pampalk, A. Flexer, and G. Widmer. Improvements of audio-based music similarity and genre classification. In Proc. Int. Soc. Music Info. Retrieval, pages 628–233, Sep. 2005.",
                    "cites": null
                },
                {
                    "id": 43526030,
                    "title": "Improving timbre similarity: How high is the sky?",
                    "authors": [],
                    "date": "2004",
                    "doi": null,
                    "raw": "J-.J. Aucouturier and F. Pachet. Improving timbre similarity: How high is the sky? J. of Negative Results in Speech and Audio Sciences, 1(1), 2004.",
                    "cites": null
                },
                {
                    "id": 43526107,
                    "title": "Intriguing properties of neural networks.",
                    "authors": [],
                    "date": "2014",
                    "doi": null,
                    "raw": "C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In Proc. ICLR, 2014.",
                    "cites": null
                },
                {
                    "id": 43526027,
                    "title": "JL were supported in part by the Danish Council for Strategic Research of the Danish Agency for Science Technology and Innovation under the CoSound project, case number 11-115328. This publication only reflects the authors’ views.",
                    "authors": [],
                    "date": null,
                    "doi": null,
                    "raw": "CK and JL were supported in part by the Danish Council for Strategic Research of the Danish Agency for Science Technology and Innovation under the CoSound project, case number 11-115328. This publication only reflects the authors’ views.",
                    "cites": null
                },
                {
                    "id": 43526034,
                    "title": "Learning deep architectures for AI. Foundations and trends",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1561/2200000006",
                    "raw": "Yoshua Bengio. Learning deep architectures for AI. Foundations and trends in Machine Learning, 2(1):1–127, 2009.",
                    "cites": null
                },
                {
                    "id": 43526066,
                    "title": "Learning features from music audio with deep belief networks.",
                    "authors": [],
                    "date": "2010",
                    "doi": null,
                    "raw": "P. Hamel and D. Eck. Learning features from music audio with deep belief networks. In Proc. ISMIR, 2010.",
                    "cites": null
                },
                {
                    "id": 43526119,
                    "title": "Music Emotion Recognition.",
                    "authors": [],
                    "date": "2011",
                    "doi": "10.1109/tasl.2010.2064164",
                    "raw": "Y.-H. Yang and H. H. Chen. Music Emotion Recognition. CRC Press, 2011.",
                    "cites": null
                },
                {
                    "id": 43526108,
                    "title": "Musical genre classification of audio signals.",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1109/tsa.2002.800560",
                    "raw": "G. Tzanetakis and P. Cook. Musical genre classification of audio signals. IEEE Trans. Speech Audio Process., 10(5):293–302, July 2002.",
                    "cites": null
                },
                {
                    "id": 43526062,
                    "title": "Musical networks: Parallel distributed perception and performance.",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.2307/40285896",
                    "raw": "Niall Griffith and Peter M Todd. Musical networks: Parallel distributed perception and performance. MIT Press, 1999.",
                    "cites": null
                },
                {
                    "id": 43526082,
                    "title": "Neural network based model for classification of music type.",
                    "authors": [],
                    "date": "1995",
                    "doi": "10.1109/eeis.1995.514161",
                    "raw": "B. Matityaho and M. Furst. Neural network based model for classification of music type. In Proc. Conv. Electrical and Elect. Eng. in Israel, pages 1–5, Mar. 1995.",
                    "cites": null
                },
                {
                    "id": 43526113,
                    "title": "On-line continuous-time music mood regression with deep recurrent neural networks.",
                    "authors": [],
                    "date": "2014",
                    "doi": "10.1109/icassp.2014.6854637",
                    "raw": "F. Weninger, F. Eyben, and B. Schuller. On-line continuous-time music mood regression with deep recurrent neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 5412–5416, May 2014.",
                    "cites": null
                },
                {
                    "id": 43526111,
                    "title": "Predicting emotion from music audio features using neural networks.",
                    "authors": [],
                    "date": "2012",
                    "doi": null,
                    "raw": "N. Vempala and F. Russo. Predicting emotion from music audio features using neural networks. In Proc. CMMR, 2012.",
                    "cites": null
                },
                {
                    "id": 43526029,
                    "title": "Scaling up music playlist generation.",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1109/icme.2002.1035729",
                    "raw": "J.-J. Aucouturier and F. Pachet. Scaling up music playlist generation. In Multimedia and Expo, 2002. ICME ’02. Proceedings. 2002 IEEE International Conference on, volume 1, pages 105–108 vol.1, 2002.",
                    "cites": null
                },
                {
                    "id": 43526053,
                    "title": "Score-informed source separation for musical audio recordings: An overview.",
                    "authors": [],
                    "date": "2014",
                    "doi": "10.1109/msp.2013.2296076",
                    "raw": "S. Ewert, B. Pardo, M. Muller, and M.D. Plumbley. Score-informed source separation for musical audio recordings: An overview. Signal Processing Magazine, IEEE, 31(3):116–124, May 2014.",
                    "cites": null
                },
                {
                    "id": 43526115,
                    "title": "Semantic gap?? Schemantic schmap!! Methodological considerations in the scientific study of music.",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1109/ism.2009.36",
                    "raw": "G. A. Wiggins. Semantic gap?? Schemantic schmap!! Methodological considerations in the scientific study of music. In Proc. IEEE Int. Symp. Mulitmedia, pages 477–482, Dec. 2009.",
                    "cites": null
                },
                {
                    "id": 43526060,
                    "title": "Signal estimation from modified shorttime fourier transform. Acoustics, Speech and Signal Processing,",
                    "authors": [],
                    "date": "1984",
                    "doi": "10.1109/tassp.1984.1164317",
                    "raw": "Daniel Griffin and Jae S Lim. Signal estimation from modified shorttime fourier transform. Acoustics, Speech and Signal Processing, IEEE Transactions on, 32(2):236–243, 1984.",
                    "cites": null
                },
                {
                    "id": 43526068,
                    "title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction.",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1111/j.1751-5823.2009.00095_18.x",
                    "raw": "T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer-Verlag, 2 edition, 2009.",
                    "cites": null
                },
                {
                    "id": 43526098,
                    "title": "The Latin music database.",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1109/ism.2008.54",
                    "raw": "C. N. Silla, A. L. Koerich, and C. A. A. Kaestner. The Latin music database. In Proc. ISMIR, 2008.",
                    "cites": null
                },
                {
                    "id": 43526037,
                    "title": "The million song dataset.",
                    "authors": [],
                    "date": "2011",
                    "doi": null,
                    "raw": "T. Bertin-Mahieux, D. P.W. Ellis, B. Whitman, and P. Lamere. The million song dataset. In Proc. ISMIR, 2011.",
                    "cites": null
                },
                {
                    "id": 43526094,
                    "title": "The neglected user in music information retrieval research.",
                    "authors": [],
                    "date": "2013",
                    "doi": "10.1007/s10844-013-0247-6",
                    "raw": "M. Schedl, A. Flexer, and J. Urbano. The neglected user in music information retrieval research. J. Intell. Info. Systems, 41(3):523–539, 2013.",
                    "cites": null
                },
                {
                    "id": 43526102,
                    "title": "The state of the art ten years after a state of the art: Future research in music information retrieval.",
                    "authors": [],
                    "date": "2014",
                    "doi": "10.1080/09298215.2014.894533",
                    "raw": "B. L. Sturm. The state of the art ten years after a state of the art: Future research in music information retrieval. J. New Music Research, 43(2):147–172, 2014.",
                    "cites": null
                },
                {
                    "id": 43526035,
                    "title": "Theano: a CPU and GPU math expression compiler.",
                    "authors": [],
                    "date": "2010",
                    "doi": null,
                    "raw": "James Bergstra, Olivier Breuleux, Fre´de´ric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David WardeFarley, and Yoshua Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010. Oral Presentation.",
                    "cites": null
                },
                {
                    "id": 43526031,
                    "title": "Theano: new features and speed improvements.",
                    "authors": [],
                    "date": "2012",
                    "doi": null,
                    "raw": "Fre´de´ric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.",
                    "cites": null
                },
                {
                    "id": 43526051,
                    "title": "Towards characterisation of music via rhythmic patterns.",
                    "authors": [],
                    "date": "2004",
                    "doi": null,
                    "raw": "S. Dixon, F. Gouyon, and G. Widmer. Towards characterisation of music via rhythmic patterns. In Proc. ISMIR, pages 509–517, 2004.",
                    "cites": null
                },
                {
                    "id": 43526064,
                    "title": "Towards Deep Neural Network Architectures Robust to Adversarial Examples. ArXiv e-prints,",
                    "authors": [],
                    "date": "2014",
                    "doi": null,
                    "raw": "S. Gu and L. Rigazio. Towards Deep Neural Network Architectures Robust to Adversarial Examples. ArXiv e-prints, December 2014.",
                    "cites": null
                },
                {
                    "id": 43526078,
                    "title": "Unsupervised feature learning for audio classification using convolutional deep belief networks.",
                    "authors": [],
                    "date": "2009",
                    "doi": null,
                    "raw": "H. Lee, Y. Largman, P. Pham, and A. Y. Ng. Unsupervised feature learning for audio classification using convolutional deep belief networks. In Proc. Neural Info. Process. Systems, Vancouver, B.C., Canada, Dec. 2009.",
                    "cites": null
                },
                {
                    "id": 43526070,
                    "title": "Unsupervised learning of sparse features for scalable audio classification.",
                    "authors": [],
                    "date": "2011",
                    "doi": null,
                    "raw": "M. Henaff, K. Jarrett, K. Kavukcuoglu, and Y. LeCun. Unsupervised learning of sparse features for scalable audio classification. In Proc. Int. Soc. Music Info. Retrieval, Miami, FL, Oct. 2011.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1507.04761",
                "http://orbit.dtu.dk/en/publications/deep-learning-and-music-adversaries(524da999-193a-48ac-b6d6-af6bf07ac96e).pdf?nofollow=true&rendering=long"
            ],
            "updatedDate": "2022-04-24T20:13:45",
            "yearPublished": 2015,
            "journals": [
                {
                    "title": "IEEE Transactions on Multimedia",
                    "identifiers": [
                        "issn:1941-0077",
                        "1941-0077"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/30698000.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/30698000"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/30698000/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/30698000/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/8850019"
                }
            ]
        },
        {
            "acceptedDate": "2005-02-14T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Clarke"
                },
                {
                    "name": "Hartel"
                },
                {
                    "name": "Martin Leucker"
                },
                {
                    "name": "Michael Weber"
                },
                {
                    "name": "Perdita Stevens"
                },
                {
                    "name": "Stevens"
                },
                {
                    "name": "Thomas Noll"
                },
                {
                    "name": "Wadler"
                },
                {
                    "name": "Wadler"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/28969322",
                "https://api.core.ac.uk/v3/outputs/36517940",
                "https://api.core.ac.uk/v3/outputs/238801512",
                "https://api.core.ac.uk/v3/outputs/303208107"
            ],
            "createdDate": "2015-02-08T16:02:11",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 11026,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/11026",
                    "logo": "https://api.core.ac.uk/data-providers/11026/logo"
                },
                {
                    "id": 647,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/647",
                    "logo": "https://api.core.ac.uk/data-providers/647/logo"
                },
                {
                    "id": 904,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/904",
                    "logo": "https://api.core.ac.uk/data-providers/904/logo"
                }
            ],
            "depositedDate": "2005-02-15T00:00:00",
            "abstract": null,
            "documentType": "research",
            "doi": "10.1007/s10009-004-0184-3",
            "downloadUrl": "https://core.ac.uk/download/28969322.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "     Edinburgh Research Explorer                                      Functional programming languages for verification tools: acomparison of Standard ML and HaskellCitation for published version:Leucker, M, Noll, T, Stevens, P & Weber, M 2005, 'Functional programming languages for verification tools:a comparison of Standard ML and Haskell' International Journal on Software Tools for Technology Transfer,vol 7, no. 2, pp. 184-194., 10.1007/s10009-004-0184-3Digital Object Identifier (DOI):10.1007/s10009-004-0184-3Link:Link to publication record in Edinburgh Research ExplorerDocument Version:Author final version (often known as postprint)Published In:International Journal on Software Tools for Technology TransferGeneral rightsCopyright for the publications made accessible via the Edinburgh Research Explorer is retained by the author(s)and / or other copyright owners and it is a condition of accessing these publications that users recognise andabide by the legal requirements associated with these rights.Take down policyThe University of Edinburgh has made every reasonable effort to ensure that Edinburgh Research Explorercontent complies with UK legislation. If you believe that the public display of this file breaches copyright pleasecontact openaccess@ed.ac.uk providing details, and we will remove access to the work immediately andinvestigate your claim.Download date: 20. Feb. 2015Functional programming languages for verification tools: a comparisonof Standard ML and HaskellMartin Leucker1,1, Thomas Noll2, Perdita Stevens3,2, Michael Weber21Department of Computer Systems, Uppsala University, Box 337, 75105 Uppsala, Swedene-mail: leucker@docs.uu.se2Lehrstuhl fu¨r Informatik II, RWTH Aachen, Ahornstr. 55, Aachen, Germanye-mail: {noll,weber}@i2.informatik.rwth-aachen.de3School of Informatics, University of Edinburgh, JCMB, King’s Buildings, Mayfield Road, Edinburgh EH9 3JZ, UKe-mail: Perdita.Stevens@ed.ac.ukKeywords:AbstractWe compare Haskell with Standard ML asprogramming languages for verification tools basedon our experience developing the verification plat-formTruth inHaskell and the Edinburgh Con-currency Workbench (CWB) in Standard ML.We review not only technical language featuresbut also the “worlds” of the languages, for ex-ample, the availability of compilers, tools, andlibraries. We also discuss the merits and diffi-culties of comparing programming languages inthis wide sense and support our view that Truthand the CWB are similar enough to justify theconclusions drawn in this paper.1 IntroductionConcurrent software and hardware systems play an in-creasing role in today’s applications. Due to the largenumber of states and to the high degree of non-determin-ism arising from the dynamic behaviour of such systems,testing is generally not sufficient to ensure the correct-ness of their implementation. Formal specification andverification methods are therefore becoming more andmore popular, aiming to give rigorous support for thesystem design and for establishing its correctness prop-erties, respectively (cf. [1] for an overview).In view of the inherent complexity of formal meth-ods, it is desirable to provide the user with tool sup-port. It is even indispensable for the design of safety-critical concurrent systems where an ad hoc or conven-tional software engineering approach is not justifiable.For example, model checking is a particularly successfulautomated approach to verification in which one employsdecision procedures to prove that (a model of) a systemhas certain properties specified in a suitable logic.One major concern in the development of model-checking tools and other verification tools is correctness.1 Supported by European Research Training Network “Games”2 Supported by EPSRC GR/A01756/011 STTT0184 – January 11, 2005 9:29Since a verification tool is used for verifying hardware,protocols, and software, it would be useless if it were nottrustworthy. Thus, a programming language employedfor developing a verification tool should support the taskof verifying the code, at least in the informal sense ofsatisfying oneself that the code is correct. In general,functional languages are often considered to provide thisfeature, especially if the language disallows side effects.Considering the broad range of functional languagesthat have been designed, it is surprising, and in our viewunfortunate, that there is little literature comparing dif-ferent functional languages. The Pseudoknot benchmarkpaper [3] studies several implementations of functionalprogramming languages (Haskell and Standard MLamong them) with respect to their runtime and mem-ory performance, and [7] compares the module systemsof Haskell and Standard ML. However, the developertrying to choose between the languages needs to be con-cerned about a much wider class of issues, including bothtechnical language features and “environmental” aspectssuch as the availability of libraries, documentation, sup-port, and multiplatform compilers. We did not find goodsources of help for a developer trying to choose betweenthe languages based on a larger collection of relevant as-pects like this. By contrast, many comparisons of Javawith C++ are readily available.It is unsurprising, therefore, that developers (thosewho choose a functional language at all) often choose thelanguage most used in their institution, without seriouslyconsidering alternatives. The difficulty of getting infor-mation to guide an informed choice may also contributeto developers whose workplaces do not have a history offunctional programming language use deciding againstexperimenting with one.Why is there so little material to help developersmake an informed choice? Part of the reason must bethat it is very hard to do convincing comparisons of lan-guages without being vulnerable to the criticism that oneis not comparing like with like. We think that a fair com-parison needs to be based on real experience of peopleusing the languages to build real systems in the samedomain; otherwise it is almost impossible to be sure thatdifferences are not due to differences in the domains ofapplication. The systems themselves need to be broadlycomparable in size and complexity, need to be more thantoys, and should preferably have been developed andmaintained over years (since a language that makes de-velopment easy might nevertheless encourage the devel-opment of code that is unmaintainable). Moreover, thedomain should be one where either of the languages isa reasonable choice, and the comparison should be doneby people with a reasonably typical level of experience inthe languages. A comparison is probably most generallyuseful to developers when it is done neither by novicesin the languages compared nor by people intimately fa-miliar with the compiler internals.2 STTT0184 – January 11, 2005 9:29This paper recounts our experiences in using SMLand Haskell for two broadly comparable applicationsin the domain of verification tools on which the authorshave worked for some years: the Edinburgh ConcurrencyWorkbench (CWB), in SML, and the verification plat-form Truth, in Haskell. The domain of verificationtools is eminently suited to the use of a statically typedfunctional language such as SML and Haskell, andboth languages are popular choices in this domain. Allof the authors have accumulated considerable experiencewith the languages we use, but we are not functional pro-gramming researchers.Thus our primary motivation for writing this paper isthat we believe we are in an unusually good position toproduce a comparison of Haskell with SML that maybe useful to developers choosing between the languages.A secondary motivation is to be helpful to language de-signers and developers who work to support languagesby providing a record of our experiences, good and bad,with SML and Haskell.The rest of this paper is structured as follows. Sec-tion 2 discusses the domain of verification tools and in-troduces the two systems. Section 3 discusses the classof languages from which SML and Haskell are drawnand briefly introduces the two languages for readers whomay not be familiar with them. Sections 4 and 5 arethe main body of the paper; Sect. 4 compares Haskelland SML on the basis of their technical language designfeatures, whereas Sect. 5 considers the equally important“environment” aspects. Finally, Sect. 6 concludes.2 Verification toolsThe domain on which we compare SML and Haskellis that of verification tools. The term “verification tool”covers any tool whose task it is to assist in checking thecorrectness of some artefact. Usually the artefact con-cerned is (an abstraction of) something produced in thesoftware or hardware development process.We introduce Truth and the CWB and briefly sum-marise their histories before discussing the characteristicfeatures of verification tools in general.2.1 The Edinburgh Concurrency Work-bench, in SMLWork on the CWB3 began in 1986. The CWB’s keystrength is its breadth: a variety of different verificationmethods are supported for several different process alge-bras. In particular, it allows users to:• Define behaviours given either in an extended ver-sion of Milner’s CCS (Calculus of CommunicatingSystems [5]) or in its synchronous version, SCCS,and to perform various analyses on these behaviours,3 http://www.dcs.ed.ac.uk/home/cwb/3 STTT0184 – January 11, 2005 9:29such as exploring the state space of a given processor checking various semantic equivalences and pre-orders;• Define propositions in a powerful modal logic andcheck whether a given process satisfies a propertyformulated in this logic;• Play Stirling-style model-checking games to under-stand why a process does or does not satisfy a for-mula;• Derive automatically logical formulae which distin-guish nonequivalent processes;• Interactively simulate the behaviour of an agent,thus guiding it through its state space in a con-trolled fashion.One major focus of the CWB was always research;it was a platform which researchers (especially those atEdinburgh) could use to experiment with new relationsbetween processes and new algorithms. In the earlyyears all of these changes were retained in the main tool,even those which had been added experimentally withoutmuch consideration for the integrity of the CWB over-all. This contributed to the architectural degradation ofthe CWB and its increasing fragility: an important taskfaced by Stevens on taking over the maintenance of theCWB in 1994 was to reverse this process. The currentversion of the CWB consists of around 25 kloc in SML,plus several thousand in other languages for various sup-porting utilities.The CWB was developed in Standard ML, but vari-ations were long maintained for several major ML com-pilers because different compilers provided different ex-tensions to the SML90 standard, and especially becausethey had different system build facilities. We settled onStandard ML of New Jersey (SML/NJ) because mostusers of the CWB used that compiler and the effort inmaintaining build scripts (the major point of difference)for several compilers did not seem well spent. Perhapswe should once again target Poly/ML,4 for example, infuture. We will discuss the history of the family of ML-like languages in Sect. 3. For now it suffices to say thatthis paper inevitably considers SML/NJ more than anyother SML compiler, and that since our experience iswith SML, we do not consider in depth other languagesin the ML family, specifically Caml and O’Caml. Thecontribution made by the SML language to both thearchitectural degradation problem and its solution arediscussed later.4 http://www.polyml.org/4 STTT0184 – January 11, 2005 9:292.2 The verification platform Truth, inHaskellIn terms of features, Truth5 is similar to the CWB.In its current version, it supports tableau-based modelchecking for the full µ-calculus and game-based modelchecking for the alternation-free subcalculus. Both op-erate on finite transition systems, given in terms of CCSprocesses. The latter can be visualised and simulatedin an interactive fashion, to help the user understandTruth’s answers. Current development activities con-centrate on the parallel implementation of model check-ing on a cluster of workstations and on a specificationlanguage compiler generator which, given the definitionof a language, automatically generates a correspondingparser and a semantic evaluator.Truth’s initial version dates back to 1997, and its de-velopment could benefit a lot from the progress made inthe design of verification tools over the years. As a con-sequence, its architecture is quite modular and easy tounderstand, and a deep change of the module structurehas not been necessary so far. It now consists of approx-imately 18 kloc in Haskell. Although there are severalHaskell compilers, Truth is written for the GlasgowHaskell Compiler6 (GHC), and since it uses some non-standard Haskell extensions and libraries only presentin the GHC, we have not tried to port it.MoreoverTruthemploys theparsergeneratorHappy7and many of the available Haskell libraries, and it inte-grates several stand-alone systems such as the daVinci8graph visualisation tool and the GraphViz package.9 Fur-thermore, it uses existing C and Java libraries to providefunctionality such as textual and graphical user inter-faces and network communication, comprising approxi-mately 13 kloc. It is one of the bigger real-world applica-tions that is registered in the official Haskell pages.10It is worth mentioning that Truth is one of the fewtools listed there which was developed using but not forfunctional programming.2.3 Characteristics of verification toolsin generalThe peculiarities of the verification tool domain from thepoint of view of software engineering were considered byStevens in [9]. Here we briefly summarise and then focuson the implications for language choice.Verification tools answer precisely defined questionsabout precisely defined systems. Thus it is compara-tively easy to understand what it means for the tool’sbehaviour to be correct. The downside is that certain5 http://www-i2.informatik.rwth-aachen.de/Research/Truth/6 http://www.haskell.org/ghc/7 http://haskell.cs.yale.edu/happy/8 http://www.informatik.uni-bremen.de/daVinci/9 http://www.graphviz.org/10 http://www.haskell.org/practice.html5 STTT0184 – January 11, 2005 9:29classes of bugs are unacceptable in a verification tool;semantic correctness is vital. Thus any language fea-tures supporting the development of correct programsare highly desirable.A further characteristic is that verification tools tendto be developed in research environments, where it ismore easily recognised for novel theoretical contributions,or new applications of theory, than for the application of“best practice” in software engineering, which is likelyto be discounted because it is not new. Anything thatspeeds up development is an advantage, as it enables thedevelopers to spend a higher proportion of their time onthe work which is most valued. In such environments,it is also difficult to justify spending large amounts ofeffort on academically uninteresting aspects of the tool,such as a GUI, or on “invisible” areas such as testing(!),documentation, and ensuring portability. Nevertheless,the usability and, ultimately, success of the tool dependheavily on such aspects. Therefore, those planning todevelop verification tools will do well to choose a lan-guage in which professional results can be achieved witha minimum of effort.It is perhaps instructive to note that in some cases,the same considerations may apply to those developinglanguages and their associated tools.3 The space of programming lan-guagesClearly Haskell and SML, the languages of Truthand the CWB, have a great deal in common: both arebasically functional languages and both have static typesystems which are strong in the sense that a well-typedprogram will be free of certain classes of runtime errors.Moreover, both are minority languages, with their originsin academia. What is the significance of these featuresfor verification tools?The functional paradigm. Essentially, a functionalprogramming language is one in which the natural pro-gramming style includes treating functions as first-classconcepts. For example, one expects to write higher-orderfunctions; that is, functions which take other functionsas arguments. There is, however, no universally agreeddefinition of what it is to be a functional programminglanguage, although no reasonable definition would ex-clude either SML orHaskell. The difficulty stems fromthe impure nature of most languages, which stems in turnfrom the need to permit the use of whichever paradigm ismost appropriate for a particular problem. It is possible,for example, to write higher-order functions in C; the rea-son why C is not included in definitions of a functionalprogramming language is that this is not the natural,normal way to solve problems in C.The main reason, in our view, for using a functional6 STTT0184 – January 11, 2005 9:29language for a verification tool is that the paradigm isa good match for the domain, as the most importantconcepts in the domain tend to be algorithms. It isoften claimed that programs written in functional lan-guages are easier to reason about, and hence are morelikely to be correct, than those in one of the imperativeparadigms (procedural or object oriented). The theo-retical concept on which the claim rests is referentialtransparency, essentially the fact that identifiers are usedfor values, rather than for references whose values maychange. Where this property holds, it can indeed facili-tate reasoning, at least in small pieces of code. However,we have found that in practice, building a verificationtool in a way which provides reasonable modularity andefficiency necessitates the use of “impure” features of thelanguages, so that referential transparency is lost.Today the most obvious alternative to the functionalparadigm for a verification tool writer is the object-orientedparadigm. The main argument in favour of the func-tional paradigm is that the most important concepts inthe domain tend to be algorithms, not objects. In thisrespect the verification tool domain differs from mostbusiness domains, and the use of a less popular languagemay be justified. However, as we shall see, being out ofthe mainstream carries disadvantages sufficient to giveone pause.Static typing. In a statically typed language, the com-piler carries out certain checks to ensure that the pro-gram is free of certain types of errors which might other-wise cause incorrect behaviour at runtime. This does not,of course, ensure that the program is free of errors, but itcan enable errors to be caught early and easily corrected,thus speeding up the development process. Static typ-ing is often criticised for being inflexible; but when suchcriticisms are investigated, they turn out to be criticismsof the inflexibility of a particular type system. We willgive examples of such inflexibilities in Sect. 4. We arguethat a coherent understanding of a solution to a prob-lem includes an understanding of the types of the entitiesinvolved; if these fit the type system of the language con-cerned, it is hard to see how having errors caught by thecompiler can fail to be a benefit, although one could stillargue about the size of the benefit.It is clear from the successes achieved by certain groupsworking with dynamically typed languages such as Er-lang (in the functional world) and Smalltalk (in the object-oriented world) that it is possible to write complex, cor-rect software without static typing. However, none ofthe authors would willingly give up the benefits of statictyping. We will discuss particular features of the typesystems of SML and Haskell below.7 STTT0184 – January 11, 2005 9:293.1 Standard MLStandard ML ([6]) is an essentially functional languagein the sense discussed above. By “essentially” we meanthat it is not a pure functional language: for example,references are permitted. ML originated at Edinburghin the late 1970s. Research and experimentation con-tinued over the succeeding decades in several centres,spawning a family of ML-like languages. In what is com-monly regarded as the mainstream, a formal languagedefinition was produced; this defined “Standard ML”, orSML90. From early on, there were several reasonablyfaithful implementations of this standard. Later a majorrewrite of the original language definition resulted in thenew definition of Standard ML, sometimes referred toas SML97. Other notable ML-like languages are Camland O’Caml. Although they have enough similarities toSML that many of the same considerations will apply,there are also some significant differences which mightaffect a user’s choice. In particular O’Caml’s supportfor software architecture is radically different, incorpo-rating aspects of object orientation. We do not considerthese languages in this paper, since our experience is withSML.Technically, the revision to SML97 has been a sub-stantial improvement; but it has led, temporarily at least,to difficulties of tools and libraries not all being updatedat once; old SML programs cannot be compiled by newcompilers and vice versa.A variety of compilers is still available for StandardML; by far the most widely used is Standard ML ofNew Jersey (SML/NJ), and this is the only compilersupported by the CWB.The definition of StandardML includes the StandardBasis Library, providing such things as string manipula-tion, operating system interfaces, and basic data struc-tures. SML/NJ comes with a more extensive library.3.2 HaskellHaskell is a purely functional programming language [8].The current standard is Haskell98, which fixes the syn-tax and semantics as well as a large set of standard li-braries.Until recently the embedding of input and outputoperations, which have to be considered as side effects,in purely functional programming languages was gener-ally poor. Monadic I/O is a very elegant approach toovercoming this problem [11]. Haskell supports thisconcept and supplies versatile I/O libraries offering ex-ception handling and file manipulation operations, whichwere of great help in building a user-friendly and reliabletool.8 STTT0184 – January 11, 2005 9:294 Comparison of language designfeaturesWe begin by considering and comparing the more tech-nical aspects of SML and Haskell, before going on toconsider non-technical questions in the next section.4.1 TypingAs semantic correctness is crucial to any verification tool,it is natural to believe that a strong static type system,enabling a large class of errors to be caught at compiletime, is a good thing in a language for verification tools.Our experience supports this; although verification toolshave been written in Lisp, for example, we would notlike to give up the static typing provided by both SMLand Haskell. The type systems of SML and Haskellare actually rather similar. In this subsection we beginour discussion by considering two related features whichHaskell and SML have in common: parametric poly-morphism and type inference. In the following subsec-tions, we shall discuss the major differences between thelanguages’ type systems separately.Extensive type inference is convenient especially infunctional programming where identifiers often have com-plex higher-order types. However, it has serious draw-backs for maintainability of code. The human reader ofcode needs to understand the types involved, and it isfrustrating to know that the compiler has worked out in-formation which is not available to the reader. The nat-ural response is that good programming practice is thento include type annotations; but we have found this hardto put into practice. An annoyance is that the syntax ofHaskell sometimes makes this impossible. For example,in Haskell function types are implicitly all-quantifiedand thus it was not possible to give type annotations forcertain local functions. This has been remedied with so-called scoped type variables, which have been introducedin GHC around version 4.03 (too late for Truth), butare not legal Haskell98.A more serious point which applies even to SMLwhich does permit type annotations is that if type an-notations are included which are descriptive enough tobe helpful, they are too specific to allow reuse throughparametric polymorphism. On the other hand the mostgeneral type is – except for utility functions – often mean-ingless to the programmer and fails to document the trueintention of the function. For example, perhaps the pro-grammer writes a function whose first argument has mostgeneral type α list × β. Maybe there is initially onlyone application of this function, to an argument of typeaction list× string. It may be that the fact that the firstargument has type action list is essential to the natureof the function; for example, perhaps this is reflected inthe name of the function, and using the function on anyother kind of list would be confusing. However, perhaps9 STTT0184 – January 11, 2005 9:29the type of the second argument is less important, andif the code works on another type, the programmer maybe quite happy to see it used on that type. “Morally”,the function’s argument has type action list × α. If theprogrammer thinks this through, it is possible to anno-tate the function accordingly; but it is not natural todo so, since it involves thinking about all possible futurereuse of the code at a time when it is more appropriate toconcentrate on the initial intended use. One could arguethat this is what comments are for, but the advantage oftype annotations is that the compiler can automaticallycheck that they are consistent with actual code.There is a tension between trying to enable code reuseon the one hand and on the other hand trying to makecode understandable and trying to maintain appropri-ate encapsulation barriers. We find that these last two,though different, often go together: one encapsulates thedefinition of an important type together with appropri-ate functions for manipulating it, and then uses the newtype name in type annotations to elucidate the code.However, in doing so one loses the power of parametricpolymorphism for code reuse in clients of this new typebecause clients cannot see the structure of the type.For example, processes in the process calculi we workwith can have restrictions applied to them. A restrictionis conveniently implemented as a list of actions, but cer-tain invariants need to be maintained. If we allow clientsto see that a restriction is a list of actions, then whenthey manipulate processes they can use the standard listfunctions on the restriction, but we cannot easily enforcethe invariants. On the other hand, if we use encapsula-tion to make available only a type restriction so that wecan enforce the invariants, we have to provide all nec-essary functions for manipulating this type. This is notunreasonable: it is the same work, for example, that wewould have to do if we worked in an object-oriented lan-guage and created a class Restriction. However, when wehave a variety of slightly different kinds of restriction, wehave to implement the manipulating functions afresh ev-ery time; to gain encapsulation we have lost parametricpolymorphism as a reuse mechanism, and we do not haveinheritance available to us as an alternative mechanism.This kind of situation arises very frequently in both theCWB and Truth because we write code to deal withvariants of process algebras and logics and with variouslyprocessed versions of them.An additional issue in SML is that it is sometimesdifficult to decide whether a conceptual type should beimplemented at the module level or only at the core level;in the CWB we generally resolve this by using both butnot revealing that decision outside the module where itis made, so that, for example, the signature for processesexports only a type restriction, whereas an implementa-tion of that signature typically builds a structure Restric-tion, exporting a type from the content of that structure.(Note that because of the divide between module and10 STTT0184 – January 11, 2005 9:29core level, we do not have the option of working only atthe module level; in order to write functions that workwith restrictions – which is essential – at some point wehave to decide what type a restriction has. So functorconstruction and multiple applications of functors do notsolve this problem, though they may contribute to a so-lution.)Further, we find that the powerful type systems ofSML and Haskell are a mixed blessing, often leadingto complex type errors which are understandable onlyto people who are familiar with the subtleties of the re-spective type system. This is to some extent inevitablein a language whose standard idioms involve complexhigher-order types, but refinements such as SML’s equal-ity types and weak types add to the problem, since theneed for these is not easy for the non-type-theorist pro-grammer to understand. Recent work on more informa-tive error messages, such as [4], is to be welcomed, buthas yet to make a difference to the compilers. Further-more, an interactive type analyser would be desirable,a tool which, requested by the user, would visualise thetypes of certain subexpressions. In the meantime, ouradvice is that there is little to choose between SML andHaskell in this respect.4.2 Strictness vs. lazinessThe most obvious difference between SML andHaskellis that SML is strict whilst Haskell is lazy. For dis-cussion of the concepts in general see, for example, [10].Basically, laziness means that values are only computedon demand, allowing the implementation of infinite datastructures. In contrast, strictness refers to the fact thate.g. the arguments of a function call have to be evalu-ated before executing the call, no matter whether theyare required or not.In the context of verification tools, laziness seemsto be an appealing feature because one might hope toget “for free” certain “on-the-fly” verification techniquesthat normally have to be worked out in each special case.For example, consider a class of verification questionsconcerning a system, such as the model-checking prob-lem “does this system satisfy this property”. To answersome questions in the class, it will be necessary to calcu-late the entire state space of the system. For others, onlya small part of the state space, perhaps that reachablewithin three transitions from the starting state, will berelevant. A global algorithm is one which always calcu-lates the whole state space; a local one does not. Localalgorithms are generally harder to design and verify thanglobal ones and often have poorer worst-case complex-ity, though in practice they may perform much better.One might hope to be able to get a local algorithm froma global one “for free” using laziness because the code forcalculating certain parts of the state space would simplynever be evaluated if its results were not called for. Inpractice, however, the Truth team found that one has11 STTT0184 – January 11, 2005 9:29to implement the algorithm generating the state spacecarefully in order to guarantee the desired behaviour.For example, the use of monads (cf. Sect. 4.3) or of ac-cumulator techniques can easily destroy the on-the-flyproperty. Since there are no visual clues (program anno-tations) in the source code, it is often not entirely obviouswhy a function is not as lazy as one would have hopedwhen writing the code. Also, code which involves excep-tions or destructive updates of data structures needs tobe crafted quite carefully in a lazy context. Eager evalu-ation, on the other hand, is easier to write, comprehend,and debug because things happen deterministically, inthe order dictated by the source code.Altogether the effort required to preserve the localityof a lazily evaluated, global algorithm often correspondsto the design of an algorithm which is local by nature.Summarising, lazy evaluation is an attractive feature,but the Truth team would have liked a flexible mecha-nism with which to specify parts of the program, whichshould be evaluated eagerly or lazily.4.3 Imperative featuresBoth the Truth and the CWB team have found imper-ative features to be essential. Sometimes the concern isefficiency, but more often it is understandability: manyof the algorithms we wish to implement are conceivedimperatively, and in such cases implementing them func-tionally makes the implementation more difficult to readand hence more likely to contain errors. Prominent ex-amples of algorithms with an imperative character aregraph algorithms, which play an important roˆle in toolssuch as ours which deal with transition systems. Thedata structures we deal with grow too big to keep severalcopies in memory, and the usual way to extract informa-tion from them is to walk them in a given order, collect-ing information and destructively updating the structureon the way, which can be straightforwardly described andefficiently implemented in imperative ways.Here SML scores by providing imperative featuresin the core language in a reasonable and powerful way,although they can be syntactically awkward. I/O is sup-ported by the Standard Basis Library. Haskell usesmonads for destructive updates and I/O; they add a re-stricted form of stateful computation to a pure language,retaining referential transparency ([11]). The disadvan-tage is that programs become more complicated. Also,if imperative elements of a given application were nottaken into account during its design but turn out tobe necessary later on, often major parts have to be re-designed or (at least) reimplemented, especially becausetypes change significantly. A simple but recurring exam-ple is to add printing of status information to an oth-erwise purely functional algorithm. In the worst casethis could result in having to rewrite the algorithm ina monadic style, but also to rewrite its callers (and tran-sitively their callers as well), plus adjusting all type an-12 STTT0184 – January 11, 2005 9:29notations on the way. Even when using opaque accessorsto data structures, the required changes cannot necessar-ily be limited to a single module, but affect large partsof the system. This is clearly undesirable from a soft-ware engineering or economical point of view. Indeedfor certain parts of the Truth system a redesign turnedout to be necessary in the past, mostly in order to imple-ment more efficient versions of algorithms by introducingimperative constructs like destructive updates.The Truth team considers this point as one of thebiggest drawbacks of the purely functional paradigm asfollowed by Haskell.4.4 Architecture supportThe architecture of a system makes a vital contributionto its correctness. We hope to study module systems inthis context, building on [7], in future; in this paper wecan only indicate the main issues.A Haskell module defines a collection of values,datatypes, type synonyms, classes, etc., as well as theirimport/export relationship with other modules. Over-loaded functions are provided in a structured way in theform of type classes, which can be thought of as familiesof types (or more generally as families of tuples of types)whose elements are called instances of the class. In theinstantiation the definitions of the overloaded operationsare given.In Standard ML, structures provide the main name-space management mechanism; they may contain sub-structures, functions, values, types, etc. A structuremay be coded directly or produced by the applicationof a functor, which may be thought of as a generic orparameterised structure. The programmer may definesignatures which act as the types for structures; for ex-ample, a functor may be defined to take, as argument,any structure matching a given signature. The mod-ule system is separate from the core language; one can-not, for example, apply a functor conditionally. Whilstthis keeps the language definition clean, in the CWB ithas often caused problems leading to code duplication.The changes made to SML in SML97 are welcome; theelimination of structure sharing and the introduction of“where” clauses have solved several long-standing prob-lems for the CWB.A basic facility which is desirable in a module sys-tem is that it should be possible to define an interface toa module separately from the module itself. This helpsdevelopers to understand the system, as they can readinterfaces to modules without being distracted by im-plementation information. We also want to be able toapply the same interface to several modules and to pro-vide several interfaces to the same module. In both theCWB and Truth this need arises, for example, becausewe often work with several variants of a process alge-bra, logic, or algorithm which share an interface. Wewant the compiler to do the work of making sure that13 STTT0184 – January 11, 2005 9:29the modules stay consistent, and we want to avoid du-plicating code. SML’s signatures support this way ofworking reasonably well, although not without problems.The Truth team has found that Haskell does not sup-port this situation so well: inside the module export list,entities cannot be annotated with types, so a commonpractice is to add them in comments. However, this iserror prone, since there is no way for the compiler toenforce their correctness or check their consistency withthe implementation in the module body.We feel that SML’s architectural features are bettersuited than Haskell’s to our purposes; neither is ideal,however, and this seems an interesting area for futurestudy, especially as we do not think that the class andpackage systems of C++ or Java would be ideal either.4.5 ExceptionsTheCWB used to make heavy use of exceptions as a con-trol flow mechanism. This led to correctness problemsbecause the compiler could not check whether or not ex-ceptions were always handled. A common class of bugsoccurred when a programmer added a new piece of func-tionality to the CWB by adding a new module declaringan exception; the exception could arise outside the newmodule; but the programmer did not, for whatever rea-son, modify the CWB’s top level module to handle theexception sensibly. To make matters worse, in SML90,although one could write a handler that would catch allexceptions (using a wildcard), so that at least the userwould not see theCWB “crash” in the case of such a bug,one could not tell dynamically which exception was ac-tually being handled. This would result in a message tothe user of the CWB along the lines of “Sorry, an ML ex-ception has been raised. This is a bug: please report it.”The exception mechanism has been improved in SML97compared with SML90: it is now possible to interrogatean exception for its identity, which at least enables theCWB to give a fuller error message, which is useful fordebugging.Still, in a language with type safety as a strength, it isa pity to use a programming style in which the program-mer cannot be certain that all exceptions are handled.From the point of view of the user of a verification tool,it is not very much better for the application to terminatebecause of an unhandled exception than it would havebeen for it to terminate because of a runtime type error.Therefore, the CWB now uses exceptions in a more dis-ciplined way which seems to work well. A small numberof specified exceptions (corresponding to such things as“error in user input”, “assertion violated”, etc.) are al-lowed to rise to the top level and are individually handledthere. All other exceptions are kept within small piecesof code (e.g. within one SML module) and in each casethe programmer verifies by eye that the exception can-not escape. Because the latter is hard work, exceptionsare only used where the alternative is really painful.14 STTT0184 – January 11, 2005 9:29We are not claiming, of course, that SML compil-ers could and should check whether exceptions are al-ways handled; this has been a research topic for sometime. Java’s requirement that functions document (cer-tain kinds of) exceptions that may be raised is an at-tempt to address the problem, but it is clumsy and in-teracts badly with a functional programming style. Ourpoint is that the style of Standard ML programming of-ten seen and encouraged, relying heavily on exceptions,has serious disadvantages which the software engineerneeds to guard against.The Truth team found that exceptions interactedbadly with laziness: as a rather disturbing effect, partialevaluation enables exceptions to escape from an enclos-ing exception handler. To get the exceptions actuallyraised inside the handler, initially the Truth team hadto resort to code like if x==x then x else x to en-force the evaluation of x at the right time. Recently,better ways to trigger full evaluation have been pro-vided (deepSeq $! x), but they are non-standard andof course destroy laziness. We think it would be muchmore natural to avoid situations of this kind by adoptingstrict rather than lazy evaluation as the standard strat-egy in the language. Laziness, which is a very costlyfeature, could then be provided upon request, using an-notations of the function and constructor symbols.We have often seen programming languages comparedon the basis of how many lines of code it takes to imple-ment some piece of functionality. We consider this a poormetric. The length of a piece of code is not well corre-lated either with the time it takes to write it or with thetime it takes to understand it; a short piece of code maywell be harder to write and to maintain than a longerone. This is why we have not tried to compare SML andHaskell on this point.5 Comparison of non-language de-sign features5.1 The available compilers and their char-acteristicsThere are now three main freely available SML97 com-pilers, SML/NJ, Poly/ML, and MoscowML. (HarlequinMLWorks ceased to be available when Harlequin wasbought: there was hope that it might become open source,but this now appears unlikely.) SML/NJ can now pro-duce native code for many platforms, which is importantfor a widely distributed verification tool. However, itneeds a third-party utility to produce stand-alone appli-cations, and even then there is a problem with runningthe application from outside its directory. This is hard toexplain to users of the CWB and causes embarrassment.For Haskell, too, three compilers are available, allof them freely: NHC98, HBC, and GHC. For Truth,15 STTT0184 – January 11, 2005 9:29only GHC was considered feature-complete enough; italso provides some extensions to the Haskell98 lan-guage which have proven helpful, for example multipa-rameter classes and existential types.5.2 Libraries and associated toolsGood libraries and tools can help to ensure correctness(e.g. because well-used libraries have been debugged byothers) and can cut down development time. We con-sider and compare what is available for Haskell andfor SML.General-purpose libraries. BothHaskell and SMLcome along with standard libraries specified alongsidethe language. SML97 defines the Standard Basis Library11(ML97SBL); Haskell98’s libraries are described in theLibrary Report12 (H98LR). Broadly similar, these pro-vide basic data structures, interface to the operating sys-tem, etc. Both GHC and SML/NJ ship with some extra,non-standard libraries.GUI libraries. There is an X Window System toolkit,eXene,13 written in Concurrent ML, though for a longtime this was apparently not usable with SML97 (be-cause of a signal-handling bug, fixed more recently thanthe last major CWB changes). Research projects haveprovided portable GUI library facilities for use with Stan-dard ML, such as sml tk.14 Thus one can implementa GUI in SML; but really good high-level toolkits arestill lacking. The CWB has made no serious attempt todo this. For Haskell, too, some bindings for commonGUI toolkits are available, but at the time GUI sup-port was added to Truth none of them was regarded asstable or feature-complete enough to be usable for whatwas planned. In the end, the process simulation GUIfor Truth was written in Java and was interfaced tothe Haskell part via Unix pipes. (The CWB followeda similar path in a student project, as yet unreleased.)Associated tools. A debugger is invaluable in pro-gram development, especially when experimenting withverification algorithms which may contain bugs. Un-fortunately, writing debuggers for functional languagesturned out to be harder than for imperative languageslike C. This is even more true for a lazily evaluatedlanguage like Haskell, where the inspection of a valuewould sometimes change the evaluation order. Never-theless some attempts have been made in this direction,mostly resulting in so-called tracers (like Freya, Hood, orHat), which can record program runs for later analysis.11 http://www.smlnj.org/doc/basis/12 http://www.haskell.org/definition/13 http://people.cs.uchicago.edu/∼jhr/eXene/14 http://www.informatik.uni-bremen.de/∼cxl/sml tk/16 STTT0184 – January 11, 2005 9:29None of them was used during Truth development be-cause they were either not available at that time or didnot support some of the GHC features used in Truth.There is no debugger available for SML/NJ. There is,however, a debugger for Poly/ML, which is a welcomedevelopment. We have not yet used it.There is a lexer (ML-Lex) and a parser generator(ML-Yacc) for SML. These were long unavailable in SML97versions but do now seem to work (see below re docu-mentation). The CWB uses ML-Lex but does not useML-Yacc. At a very early stage a hand-built parser wasproduced, and by the time the major reengineering workwas done on theCWB its syntax (perhaps unfortunately,but understandably) included features which were notsupported by ML-Yacc, so that to move to ML-Yaccat that point would have involved a user-visible syntaxchange. This is an example of the problems which canarise when a suitable third-party component is not avail-able at the right moment; users may not have the op-tion of adopting it later. As mentioned, Truth uses theHappy parser generator.Overall there is little to choose betweenHaskell andSML in this category, but both suffer from being minor-ity languages. There are few providers of libraries andtools, and key developers are often more concerned withcompilers. This is understandable, but to us librariesand tools are just as important.5.3 Documentation and other sources ofhelpFamously, Standard ML has a formal specification [6],but this is impenetrable to most programmers. Fortu-nately there are also several accessible books and tu-torials available. The official specification of Haskellis given by the Haskell98 Language Report,15 whichdefines the syntax of Haskell programs and gives aninformal abstract semantics. For such a technical doc-ument it contains much plain text, and the general im-pression of local Haskell developers is that it is quitereadable. On the other hand, as was noted elsewhere:16“The informal specification in theHaskell report leavestoo much room for confusion and misinterpretation. Thisleads to genuine discrepancies between implementations,as many subscribers to the Haskell mailing list willhave seen.”Regarding the compiler and associated tool docu-mentation, the overall impression of the authors is thatGHC’s documentation is slightly better than that of SML/NJ.(The ML-Lex documentation has not been updated forSML97, for example.) This has not always been thecase, but the GHC developers have improved the docu-mentation quite a lot in the recent past.15 http://www.haskell.org/onlinereport/16 http://www.cse.ogi.edu/∼mpj/thih/17 STTT0184 – January 11, 2005 9:29In both cases documentation for libraries is patchy,especially in the case of compiler-specific libraries, whereit sometimes happens that the programmer must consultthe source code to get more information than the signa-ture of a function. H98LR and ML97SBL are better doc-umented. From 1997 to 2001 there was no complete andup-to-date documentation of the latter, which was a se-rious problem. Now, however, an updated web page isavailable;17 the documentation appeared in book form [2]in 2002, which is a welcome development. TheCWB andTruth teams each had the impression initially that theother’s language’s libraries were better documented: thismay reflect that one notices faults only on close acquain-tance. A plus for Haskell is that the GHC library doc-umentation has a consistent history of being frequentlyupdated and improved.Moving from documents to people as sources of help,we have found the newsgroups comp.lang.ml and comp.lang.functionaland the GHC mailing lists to be useful. Naturally it iseasier to get help with problems which can be describedbriefly. When we have needed help with, for example,making architectural decisions, local language expertshave been invaluable; this is something that developersshould bear in mind.Last but not least the home pages of Standard MLof New Jersey18 and of Haskell19 provide useful collec-tions of links and references to other resources.5.4 Foreign-function interfacesWe have made no serious attempt to bind C and SMLcode within theCWB, because the StandardML foreign-function interfaces were perceived (and experienced ina student project) as hard to use and inefficient. MatthiasBlume’s new “NLFFI” foreign-function interface may wellchange the situation.The foreign-function interface inHaskell has under-gone a major redesign and is now quite usable. Truthhas been extended by a parallel model-checking algo-rithm, which uses the FFI layer to call C functions fromthe MPICH resp. LAM libraries, both well-known imple-mentations of the Message Passing Interface standard.For this application the marshalling required to convertbetween Haskell and C data formats turned out tobe very inefficient, however. Another problem was theinstability of the FFI interface at the time the Truthteam were using it: it changed rapidly between releases ofGHC. The Truth team made extraordinary use of pre-processor directives and autoconf magic in an attemptto allow Truth to support many compiler versions, but17Unfortunately, it documents the version of the library corre-sponding to the very latest “working”, i.e. experimental, versionof the compiler. There still seems to be no freely available docu-mentation for the version of the library in the latest release-qualityversion of the compiler!18 http://www.smlnj.org/19 http://www.haskell.org/18 STTT0184 – January 11, 2005 9:29they were eventually forced to give up.5.5 Stability of languages and their im-plementationsThe current stable version of Haskell is Haskell98,dating from 1997. This is the fifth major version of thelanguage definition (the next will be Haskell 2 !). Com-pilers, of course, provide the effective definition of thelanguage. There have been many changes in what GHCsupports (e.g. multiparameter classes, implicit parame-ters). Not all changes to extensions have been backwardscompatible, which is inconvenient for programmers whoneed those extensions.Regarding the stability of Haskell implementations,only GHC has been thoroughly examined, since the otherimplementations have been ruled out by other issues, asstated earlier. GHC is under steady development, andthe quality of released versions differs greatly. Some arequite stable, but for others patch-level releases have tobe made quickly to fix the worst bugs. Unsurprisingly,bugs often accompany new features. In fairness, bugsare fixed promptly by the GHC developers once they arereported to the relevant mailing list. However, faced witha show stopper, an application programmer must choosebetween waiting for an official release of GHC includinga fix, or becoming expert in building the compiler itself,which is non-trivial, time consuming, and will not furtherhis/her aims.Standard ML was subject to a major revision, fromSML90 to SML97. The SML/NJ compiler has under-gone many releases, but now seems fairly stable. Asmentioned, tools and libraries tend to lag. The ML2000project intended to develop a future version of ML. Lit-tle has been heard of the project recently, and many ofits early ideas have been incorporated in O’Caml. Webelieve that SML97 will increase, rather than decrease,in stability over the next few years.5.6 PerformanceThis is a controversial topic, but it is an important onefor developers of verification tools. Both speed and spaceusage are important, with space usage often being moreimportant, as the amount of memory used by a verifica-tion tool is normally the limiting factor. Our experiencewith both SML andHaskell suggests that performanceis particularly poor with respect to memory usage. Also,as noted, the key feature of Haskell, lazy evaluation,comes at a high cost.6 ConclusionWe proceed by summarising the features we found (not)helpful in using functional languages in general, and spe-cifically in the use of SML and Haskell.19 STTT0184 – January 11, 2005 9:296.1 Helpful featuresBoth teams found the functional paradigm a good fit forthe domain of verification tools. The automatic mem-ory management of SML and Haskell lets us focus onthe important parts of the algorithms instead of fiddlingwith implementation details. Data structures that arereadily available like lists and their natural use were ap-preciated as well. Also, static types proved to be helpfulin catching certain classes of errors early.However, some of these features turned out to bedouble-edged swords, and other features which were ini-tially considered useful turned out not to be, or to beless helpful than we had hoped.6.2 Mythical silver bulletsThe price of using convenient constructs like higher-orderfunctions and functional languages in general is oftenpaid in uncompetitive runtime and memory performance.While in imperative languages it is possible, after gettingprograms right, to get them fast, we find this harder infunctional languages.It is commonly argued that referential transparencymakes programs easier to understand, easier to reasonabout, and generally more robust. However, we did notfind this feature to be worth its cost. Constraining (inHaskell) the use of e.g. destructive updates when theyare needed turned out to waste a lot of precious devel-oper time. Reasoning about any sufficiently complex al-gorithm on the source code level is intractable as well,even when avoiding impure features.Static types were helpful, as mentioned above, butalso got in our way. Understanding complex type errorsand their origins requires an intimate understanding ofthe type system. Easier means of exploring them andderiving complex type annotations are lacking.6.3 SML vs. HaskellThe most outstanding difference between Haskell andSML is the evaluation order. While Haskell’s lazy ap-proach might have its merits in some cases, it was morea hindrance than a help, causing longer development,having negative impact on the performance, and makingthe source code harder to understand.While the use of monads in Haskell is a nice theo-retical concept to capture side effects and stateful com-putation in a purely functional setting, their disadvan-tages lead us to vote for the impure SML here, especiallygiven that many of our domain’s algorithms are naturallydescribed imperatively.We found the module systems in both SML andHaskelllacking features from a software engineering point of view.For example, in SML the “include” mechanism for sig-natures is not flexible enough to prevent one from havingto duplicate information across several SML signatures20 STTT0184 – January 11, 2005 9:29when different views onto a structure are required (com-parable to “private” and “public” interfaces in languagessuch as Java). The lack of conditional application offunctors, similarly, can lead to code duplication, whichis in turn a maintenance problem. Since we have notsurveyed other programming languages in this respect,it is not clear whether they would meet our demands,though.Both teams would have wished for better support forexceptions in order to enhance runtime error messages(SML) and to better cope with astonishing evaluationorder interactions (Haskell).Confronted with the poor availability of ready-to-use libraries for SML and Haskell compared to main-stream languages, one is often referred to their foreign-function interfaces. In Haskell, the use comes witha performance penalty. We do not have first-hand expe-rience for SML.Aside from the language issues we have investigated,we found the tool support in both languages lacking incomparison to mainstream languages, resulting in thereinvention of wheels other languages can readily use.We regard this an important factor when planning thedevelopment schedule of SML or Haskell programs.We found documentation for Haskell to be better,while stability of the SML implementation was found tobe superior when compared to its Haskell counterpart.6.4 Considerations for future projectsThere have been positive and negative aspects to bothour sets of experiences with Haskell and SML, as therewould doubtless have been with whatever language wehad chosen. Overall, we consider Standard ML to bea slightly better choice for our kind of application thanHaskell, more because of a more stable environmentof supporting tools than because of language features.Of course, there are many alternatives including otherfunctional languages with which we have less experience;O’Caml might be a strong candidate.However, it turned out in our discussions that none ofus were enthusiastic about the idea of using a functionallanguage for a future verification tool because of theirimpoverished environments compared with mainstreamprogramming languages. Our impression was that SMLand Haskell can play out their advantages mainly inthe prototyping stages of a project, an arena where bothwould have to compete with dynamic languages like Lispor Smalltalk, or scripting languages like Python (whichhave faster turn-around cycles due to absence of a com-pilation phase).Our conclusion is that, if/when we develop new ver-ification tools, we would like to conduct a study on theuses of imperative languages for verification tools. Dur-ing our investigations we got the impression that thoseseem to be better equipped for features we need in ourdomain.21 STTT0184 – January 11, 2005 9:29We hope that reporting our experience using majorfunctional languages will help the community to improvesuch languages and their worlds in future.Acknowledgement. We thank the anonymous refereesfor helpful comments and discussions.References[1] Clarke EM, Wing JM (1996) Formal methods: stateof the art and future directions. ACM Comput Surv28(4):626–643[2] Gansner ER, Reppy JH (2002) The Standard MLBasis Library. Cambridge University Press, Cam-bridge, UK[3] Hartel PH et al. (1996) Benchmarking implemen-tations of functional languages with ‘Pseudoknot’,a float-intensive benchmark. J Function Programm6(4):621–655[4] McAdam B (2002) Repairing type errors in func-tional programs. PhD thesis, Division of Informat-ics, University of Edinburgh[5] Milner R (1989) Communication and concurrency.International Series in Computer Science. Prentice-Hall, Upper Saddle River, NJ[6] Milner R, Tofte M, Harper R, MacQueen D (1997)The definition of Standard ML (revised). MIT Press,Cambridge, MA[7] Nicklisch J, Peyton Jones SL (1996) An explorationof modular programs. In: Glasgow workshop onfunctional programming, July 1996[8] Peterson J, Hammond K, et al (1996) Report onthe programming language Haskell, a non-strictpurely-functional programming language, version1.3. Technical report, Yale University, New Yaven,CT, May 1996[9] Stevens P (1999) A verification tool developer’s vademecum. Int J Softw Tools Technol Transfer 2(2):89–94[10] Wadler P (1996) Lazy versus strict. ACM ComputSurv 28(2):318–320[11] Wadler P (1997) How to declare an imperative.ACM Comput Surv 29(3):240–26322 STTT0184 – January 11, 2005 9:29",
            "id": 8979935,
            "identifiers": [
                {
                    "identifier": "2123450267",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "36517940",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:pure.ed.ac.uk:publications/e0c9ca1c-c726-4edb-b03a-5da863d83a03",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "28969322",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "238801512",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "303208107",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1007/s10009-004-0184-3",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:publications.rwth-aachen.de:155632",
                    "type": "OAI_ID"
                }
            ],
            "title": "Functional programming languages for verification tools: a comparison of Standard ML and Haskell",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2123450267",
            "oaiIds": [
                "oai:pure.ed.ac.uk:publications/e0c9ca1c-c726-4edb-b03a-5da863d83a03",
                "oai:publications.rwth-aachen.de:155632"
            ],
            "publishedDate": "2005-01-01T00:00:00",
            "publisher": "'Springer Science and Business Media LLC'",
            "pubmedId": null,
            "references": [
                {
                    "id": 36480484,
                    "title": "A veri¯cation tool developer's vade mecum.",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1007/s100090050019",
                    "raw": "Stevens P (1999) A veri¯cation tool developer's vade mecum. Int J Softw Tools Technol Transfer 2(2):89{",
                    "cites": null
                },
                {
                    "id": 36480482,
                    "title": "An exploration of modular programs. In: Glasgow workshop on functional programming,",
                    "authors": [],
                    "date": "1996",
                    "doi": null,
                    "raw": "Nicklisch J, Peyton Jones SL (1996) An exploration of modular programs. In: Glasgow workshop on functional programming, July 1996",
                    "cites": null
                },
                {
                    "id": 36480478,
                    "title": "Benchmarking implementations of functional languages with `Pseudoknot', a °oat-intensive benchmark.",
                    "authors": [],
                    "date": "1996",
                    "doi": "10.1017/s0956796800001891",
                    "raw": "Hartel PH et al. (1996) Benchmarking implementations of functional languages with `Pseudoknot', a °oat-intensive benchmark. J Function Programm 6(4):621{655",
                    "cites": null
                },
                {
                    "id": 36480480,
                    "title": "Communication and concurrency.",
                    "authors": [],
                    "date": "1989",
                    "doi": null,
                    "raw": "Milner R (1989) Communication and concurrency. International Series in Computer Science. PrenticeHall, Upper Saddle River, NJ",
                    "cites": null
                },
                {
                    "id": 36480476,
                    "title": "Formal methods: state of the art and future directions.",
                    "authors": [],
                    "date": "1996",
                    "doi": "10.1145/242223.242257",
                    "raw": "Clarke EM, Wing JM (1996) Formal methods: state of the art and future directions. ACM Comput Surv 28(4):626{643",
                    "cites": null
                },
                {
                    "id": 36480486,
                    "title": "How to declare an imperative.",
                    "authors": [],
                    "date": "1997",
                    "doi": "10.1145/262009.262011",
                    "raw": "Wadler P (1997) How to declare an imperative. ACM Comput Surv 29(3):240{263 22 STTT0184 { January 11, 2005 9:29",
                    "cites": null
                },
                {
                    "id": 36480485,
                    "title": "Lazy versus strict.",
                    "authors": [],
                    "date": "1996",
                    "doi": "10.1145/234528.234738",
                    "raw": "Wadler P (1996) Lazy versus strict. ACM Comput Surv 28(2):318{320",
                    "cites": null
                },
                {
                    "id": 36480479,
                    "title": "Repairing type errors in functional programs.",
                    "authors": [],
                    "date": "2002",
                    "doi": null,
                    "raw": "McAdam B (2002) Repairing type errors in functional programs. PhD thesis, Division of Informatics, University of Edinburgh",
                    "cites": null
                },
                {
                    "id": 36480483,
                    "title": "Report on the programming language Haskell, a non-strict purely-functional programming language, version 1.3.",
                    "authors": [],
                    "date": "1996",
                    "doi": "10.1145/130697.130699",
                    "raw": "Peterson J, Hammond K, et al (1996) Report on the programming language Haskell, a non-strict purely-functional programming language, version 1.3. Technical report, Yale University, New Yaven, CT, May 1996",
                    "cites": null
                },
                {
                    "id": 36480481,
                    "title": "The de¯nition of Standard ML (revised).",
                    "authors": [],
                    "date": "1997",
                    "doi": null,
                    "raw": "Milner R, Tofte M, Harper R, MacQueen D (1997) The de¯nition of Standard ML (revised). MIT Press, Cambridge, MA",
                    "cites": null
                },
                {
                    "id": 36480477,
                    "title": "The Standard ML Basis Library.",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1017/cbo9780511546846.004",
                    "raw": "Gansner ER, Reppy JH (2002) The Standard ML Basis Library. Cambridge University Press, Cambridge, UK",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [],
            "updatedDate": "2022-04-24T22:13:48",
            "yearPublished": 2005,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "1433-2779"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/28969322.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/28969322"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/28969322/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/28969322/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/8979935"
                }
            ]
        },
        {
            "acceptedDate": "2012-09-07T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Lincoln, M."
                },
                {
                    "name": "Renals, S."
                },
                {
                    "name": "Zwyssig, E."
                }
            ],
            "contributors": [
                "The Pennsylvania State University CiteSeerX Archives"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/28976789",
                "https://api.core.ac.uk/v3/outputs/194715329"
            ],
            "createdDate": "2014-10-22T20:53:17",
            "dataProviders": [
                {
                    "id": 145,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/145",
                    "logo": "https://api.core.ac.uk/data-providers/145/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 647,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/647",
                    "logo": "https://api.core.ac.uk/data-providers/647/logo"
                }
            ],
            "depositedDate": "2012-03-01T00:00:00",
            "abstract": "This paper examines the effect of sensor performance on speaker diarisation in meetings and investigates the use of more advanced beamforming techniques, beyond the typically employed delay-sum beamformer, for mitigating the effects of poorer sensor performance. We present superdirective beamforming and investigate how different time difference of arrival (TDOA) smoothing and beamforming techniques influence the performance of state-of-the-art diarisation systems. We produced and transcribed a new corpus of meetings recorded in the instrumented meeting room using a high SNR analogue and a newly developed low SNR digital MEMS microphone array (DMMA.2). This research demonstrates that TDOA smoothing has a significant effect on the diarisation error rate and that simple noise reduction and beamforming schemes suffice to overcome audio signal degradation due to the lower SNR of modern MEMS microphones. Index Terms — Speaker diarisation in meetings, digital MEMS microphone array, time difference of arrival (TDOA), superdirective beamforming 1",
            "documentType": "research",
            "doi": "10.1109/icassp.2012.6288839",
            "downloadUrl": "https://core.ac.uk/download/28976789.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "     Edinburgh Research Explorer                                      On the effect of SNR and superdirective beamforming in speakerdiarisation in meetingsCitation for published version:Zwyssig, E, Renals, S & Lincoln, M 2012, 'On the effect of SNR and superdirective beamforming in speakerdiarisation in meetings'. in Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE InternationalConference on. pp. 4177-4180, ICASSP 2012, Kyoto, Japan, 26-30 March.,10.1109/ICASSP.2012.6288839Digital Object Identifier (DOI):10.1109/ICASSP.2012.6288839Link:Link to publication record in Edinburgh Research ExplorerDocument Version:Author final version (often known as postprint)Published In:Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference onGeneral rightsCopyright for the publications made accessible via the Edinburgh Research Explorer is retained by the author(s)and / or other copyright owners and it is a condition of accessing these publications that users recognise andabide by the legal requirements associated with these rights.Take down policyThe University of Edinburgh has made every reasonable effort to ensure that Edinburgh Research Explorercontent complies with UK legislation. If you believe that the public display of this file breaches copyright pleasecontact openaccess@ed.ac.uk providing details, and we will remove access to the work immediately andinvestigate your claim.Download date: 20. Feb. 2015ON THE EFFECT OF SNR AND SUPERDIRECTIVE BEAMFORMING IN SPEAKERDIARISATION IN MEETINGSErich Zwyssig1,2, Steve Renals1 and Mike Lincoln11Centre for Speech Technology Research, University of Edinburgh, Edinburgh, EH8 9AB, Scotland UK2EADS IW, Appleton Tower, 6th Floor, Edinburgh, EH8 9LE, Scotland UKABSTRACTThis paper examines the effect of sensor performanceon speaker diarisation in meetings and investigates the useof more advanced beamforming techniques, beyond the typ-ically employed delay-sum beamformer, for mitigating theeffects of poorer sensor performance. We present super-directive beamforming and investigate how different timedifference of arrival (TDOA) smoothing and beamformingtechniques influence the performance of state-of-the-art diar-isation systems. We produced and transcribed a new corpusof meetings recorded in the instrumented meeting room us-ing a high SNR analogue and a newly developed low SNRdigital MEMS microphone array (DMMA.2). This researchdemonstrates that TDOA smoothing has a significant effecton the diarisation error rate and that simple noise reductionand beamforming schemes suffice to overcome audio sig-nal degradation due to the lower SNR of modern MEMSmicrophones.Index Terms— Speaker diarisation in meetings, digitalMEMS microphone array, time difference of arrival (TDOA),superdirective beamforming1. INTRODUCTIONSpeaker diarisation is the process of determining who spokewhen during a conversation. Diarisation systems typicallyidentify both the number of speakers in the recording andthe time intervals during which each individual is speaking.Speaker diarisation has recently been used in the analysis ofmeeting recordings which has shown that accurate diarisationis crucial to the performance of subsequent processes, such asspeaker recognition and transcription [1].Meetings are usually recorded using microphone arraysconsisting of a number of high quality analogue microphoneswhich provide a high signal to noise ratio (SNR). However,microphone arrays have recently been developed using digitalMEMS microphones (so-called silicon microphones). MEMSmicrophone arrays have a number of advantages (e.g. lowerprice, smaller size), however the MEMS sensors have the dis-advantage of significantly lower SNRs than their analoguecounterparts.In this paper we study the effect of the sensor perform-ance on the diarisation task and investigate the use of super-directive beamforming to mitigate the effects of poorer sensorperformance.2. BACKGROUNDTime delay of arrival (TDOA) estimation seeks to identify thetime difference between signals from a given source arrivingat two different sensors in a sensor array and is an essentialfirst step in most beamforming techniques. An establishedmethod for performing TDOA estimation is the generalisedcross correlation with phase transform (GCC-PHAT [2],[3])which can be used to determine the relative delay betweensignals arriving at two microphones in a microphone array.The GCC-PHAT of two signals is defined asGˆPHAT (f) =Xi(f)[Xj(f)]∗|Xi(f)[Xj(f)]∗| (1)where xi(t) and xj(t) are two discrete signals in the timedomain and Xi(f) and Xj(f) their discrete Fourier trans-form. The TDOA dˆPHAT (i, j) of the two signals xi(t)and xj(t) is estimated as the maximum value of the inverseFourier transform RˆPHAT of GˆPHAT :dˆPHAT (i, j) = arg maxd(RˆPHAT (d)) (2)GCC-PHAT does not produce stable delay estimates whenused in acoustically noisy environments (such as a typicalmeeting room), and smoothing techniques, such as Viterbidelay selection, can be used to obtained better estimates [4].In our experiments we compare the performance of smoothedand un-smoothed delay estimates for beamforming in termsof the achieved diarisation error rate.For acoustic beamforming, the delays between eachmicrophone and a reference channel (typically taken as thechannel with the highest energy level) are calculated, andthese can be directly used for delay-sum beamforming. Theoutput of a delay-sum beamformer is the weighted sum ofall the microphone signals, with each channel delayed by itsZwyssig, E., Renals, S., & Lincoln, M. (2012). On the effect of SNR and superdirective beamforming in speaker diarisation in meetings.  In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. (pp. 4177-4180). 10.1109/ICASSP.2012.6288839corresponding delay estimate:y(n) =1MN∑m=1xm(n− τm) (3)where y(n) is the output signal of the beamformer, M thenumber of microphones, xm the input signal at microphonem and τm the delay of that input signal.One commonly used measure of the performance ofbeamforming techniques is the array gain, G, which showsthe improvement of the signal to noise ratio of the arraycompared to an individual sensor:G =SNRarraySNRsensor(4)Delay-sum beamforming achieves a signal amplificationof 3dB for every doubling of the number of microphones. En-hancement is achieved by constructively adding the signalsfrom the look direction and suppressing interference fromother sources. By optimising the array gain, more sophist-icated methods, known as superdirective beamformers, canbe used to improve the beamformers directional selectivity atlower frequencies, further cancelling undesired sources. Anumber of superdirective beamformers have been developed.Examples include filter-sum, differential, eigen, generalisedsidelobe cancelling, and minimum variance distortionless re-sponse (MVDR) beamformers, each being differentiated bythe method employed to optimise G.In this work we employ an MVDR superdirective beam-former [5]. The aim of MVDR beamforming is to minimisethe power of the output signal of the array, while maintain-ing unity gain in the look direction and additional constraints(such as maximum white noise gain). MVDR beamformingis based on filter-sum beamforming and its frequency domainoutput signal, Yb, is defined as:Yb(ejΩ) =M−1∑m=0Wm∗(ejΩ)Xm(ejΩ) = WHX (5)where Wm(ejΩ) denotes the filter coefficients of the beam-former for sensor m at frequency Ω, Xm(ejΩ) are the micro-phone input signals and [·]H denotes the matrix transpose con-jugate.3. DMMA.2Most microphone arrays to date have been composed of highquality, but expensive and relatively bulky, analogue micro-phones. A digital MEMS (micro electro mechanical system)microphone is a microphone on a chip containing a pressuresensitive membrane, a matched pre-amplifier, and integratedanalogue-digital conversion (ADC) and downsampling. Wehave previously constructed a prototype digital MEMS micro-phone array, DMMA.1 [6], and preliminary experiments pro-duced promising results for a task based on the adaptation(a) Microphones on daughter boards (b) Complete microphone arrayFig. 1. The digital MEMS microphone arrayof WSJ acoustic models. DMMA.1 has a number of limit-ations, most significantly the inability to directly record allchannels individually at 48kHz sample rate. In order to over-come this problem, a second microphone array has been con-structed which allows the recording of 8 microphone channelsat sample rates from 8 kHz to 48 kHz.In this work we have designed a new array, DMMA.2(Figure 1), which like DMMA.1 is an 8 channel circularmicrophone array with a diameter of 20 cm. It is built usingADI ADMP441 omnidirectional MEMS microphones1 withbottom port and I2S output and the Rigisystems USBPAL2, aUSB 2.0 multi-channel audio interface for Windows PC andMAC OS X.Digital MEMS microphones have significantly lower in-trinsic signal to noise ratios compared to analogue micro-phones. Initial tests on the microphones used in the DMMA.2suggest that this sensor noise is not white as would be expec-ted. While SNR and THD measurements carried out showthe microphones to be within specification, the MEMS micro-phones output a non-white chirping noise, which we suspectoriginates from the DSP built into the microphones. Furthertests, including sensor measurements in a vacuum enclosureare being conducted.4. AD IMR CORPUSThe DMMA.2 and an array with identical geometry con-structed using high signal to noise ratio analogue micro-phones have been used to simultaneously record six researchmeetings of around one hour in length. The recordings weremade in a typical meeting room at the University of Edin-burgh. The analogue array is identical to that used in theAMI meeting corpus recordings and is fully documented in[1]. From each of the recordings, a continuous ten to fifteenminute segment containing lively discussion has been selec-ted, creating a total of approximately 78 minutes of record-ings. These extracts were transcribed to show speech/non-1http://www.analog.com/en/mems-sensors/microphones/admp441/products/product.html2http://www.rigisystems.net/Zwyssig, E., Renals, S., & Lincoln, M. (2012). On the effect of SNR and superdirective beamforming in speaker diarisation in meetings.  In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. (pp. 4177-4180). 10.1109/ICASSP.2012.6288839Recording Length [s] # of speakersrec14june2011 825 5rec15june2011 804 7rec21june2011 630 4rec22june2011 856 4rec28june2011 607 4rec29june2011 914 6Table 1. Summary of AD IMR recordingsMicrophoneArrayNoiseReductionNoiseReductionTDOAEstimation(Viterbi smoothing)TDOAEstimationDelay-sumBeamformingSuperdirective Beamforming VAD and DiarisationGWmdm toolsqio-fe BeamformItSHOuTICSIFig. 2. Data flow for the experimentsspeech events and for each speech segment the speaker IDwas annotated. Both overlapping speech (where more thanone speaker is talking simultaneously) and back channels(short interjections from listeners, typically indicating agree-ment or disagreement with the main speaker) were includedin the transcription. The transcription was formatted using theRTTM specification, as defined by NIST 3, allowing scoringof automatically generated diarisation annotations using thestandard NIST evaluation tools. Details of the meeting re-cordings contained in the corpus, named AD IMR, are listedin Table 1.5. METHODSExperiments were conducted to investigate the effect on thediarisation task of using the digital array and superdirectivebeamforming. Using two state-of-the-art diarisation systems,we compared the error rates achieved using the low SNRrecordings from the DMMA.2 with recordings of the samemeeting from the analogue array. Using both smoothed andun-smoothed delay estimates, we then compared diarisationerrors using the MVDR beamformer and the currently useddelay-sum beamformer.Figure 2 shows the data flow for the experiments. Initially,Wiener-filter-based noise reduction is applied to the analogueand digital microphone signals [7] and both smoothed andunsmoothed TDOA values for each of the channels calcu-lated [4]. Enhanced signals are then generated using threetechniques: (1) Delay-sum beamforming using smootheddelay estimates; (2) Superdirective beamforming using un-smoothed delay estimates; (3) Superdirective beamforming3http://www.itl.nist.gov/iad/mig/tests/rt/using smoothed delay estimates. We used the open sourceBeamformIt toolkit4 [8] and the AMI project beamformingtools [9].Speaker diarisation is then performed on the three en-hanced signals using two diarisation systems—SHOUTspeech recognition toolkit [10] and the ICSI speaker diar-isation system [11]5.6. RESULTSTwo metrics are used to verify the performance of speakerdiarisation systems—the voice activity detection error rate(VER) and the diarisation error rate (DER). The VER is cal-culated from missed speech and false alarms—that is, speechsegments that are not detected as speech and non-speech seg-ments that are identified as speech. In addition to missedspeech and false alarms, DER (see [8], chapter 6.1.3, page162ff) also takes into account the speaker to whom eachsegment is assigned, and penalises segments assigned to thewrong speaker. In order to account for errors in the refer-ence labels and slight variations in automatic processing, atolerance of ±250ms is permitted at the edge of each speechsegment.The VER and DER results for the six meetings in theAD IMR corpus are given in Table 2. The results show that,for diarisation, the new digital microphone array compareswell with the analogue array despite the reduced SNR, pro-ducing only marginally increased error rates. This result sug-gests that MEMS microphone technology provides a viablealternative to expensive analogue devices for speech data cap-ture, and further experiments will be conducted on a varietyof speech processing tasks using the DMMA.2Table 2 also shows that Viterbi smoothing of the TDOAcoefficients and delay-sum beamforming provide betterresults than superdirective beamforming using either smoothedor un-smoothed delays. This may in part be due to the fact thatthe TDOA smoothing method was optimised for diarisationperformance using a delay-sum beamformer, and alternativeoptimisation may be required in the superdirective case. Also,it is possible that the superdirective beamformer actually re-moves vital acoustic information from the sidelobes, leadingto an increased DER due to the diarisation tools being tunedto acoustic output from a delay-sum beamformer. Analysingthe effect of the superdirective beamformer white noise gainconstraint GW on the diarisation error rate, it was found that,by tuning GW , the performance gap between the digital andanalogue arrays could be reduced. In general, reducing theGW leads to a decrease of the difference in the DER betweenthe analogue and digital arrays, as shown in Figure 3, with4http://www.xavieranguera.com/beamformit/5The implementation of the ICSI system evaluated here only uses acousticfeatures, in contrast to the system used in the ICSI submission to the NISTRT09 evaluation which incorporates TDOA features directly as an input tothe diarisation system.Zwyssig, E., Renals, S., & Lincoln, M. (2012). On the effect of SNR and superdirective beamforming in speaker diarisation in meetings.  In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. (pp. 4177-4180). 10.1109/ICASSP.2012.6288839Table 2. % VER and DER for delay-sum (DSB) and superdirective (SDB) beamforming using the ICSI and SHOUT diarisationsystems, for analogue and digital arrays. FA denotes false alarms, MS denotes missed speech.SHOUT ICSIDER VER FA MS DER VER FA MSDSB analogue 20.54 2.3 1.3 1 22.49 2.2 1.3 0.9(TDOA smoothing) digital 21.89 3 1.5 1.5 22.81 2.9 1.5 1.4SDB analogue 29.21 4.8 3.5 1.3 28.17 4.7 3.5 1.2GW=0.6 digital 35.16 4.9 3 1.9 30.31 4.8 3.1 1.7modified SDB analogue 23.11 3.6 1.9 1.7 21.58 3.5 1.9 1.6GW=0.6 (TDOA smoothing) digital 25.45 3.7 1.6 2.1 28.82 3.7 1.7 2Fig. 3. Effect of white noise gain constraint GW on DERbest performance achieved by setting GW < 0.15.7. CONCLUSIONS AND FUTURE WORKIn this paper we have presented the development of a newdigital MEMS microphone array. We have recorded a newcorpus of 6 meetings using both the digital array and an ana-logue array, and annotated 78 minutes of data extracted fromthe recordings for speech/non-speech and speaker identifica-tion. We have compared the performance of two state-of-the-art diarisation systems using both the analogue and digital re-cordings, and a number of delay estimation and beamformingtechniques.We found that the digital MEMS microphone array ap-proaches the performance of the analogue array when usingsuperdirective beamforming, if the white noise gain constraintof the beamformer is correctly adjusted. In addition, we foundthat superdirective beamforming, even when using delay es-timation smoothing, is unable to match the diarisation per-formance of delay-sum beamforming and believe this may becaused by mismatch between the beamformer output and thediarisation systems used.Future work will investigate optimising the TDOA estim-ation and diarisation system for such a beamformer in order toincrease performance. We also plan to record more meetingswith the DMMA.2, some with improved speaker tracking andothers in an anechoic chamber, to investigate the effects ofSNR and reverberation on diarisation performance.8. REFERENCES[1] T. Hain, L. Burget, J. Dines, P.N. Garner, A.E. Hannani,M. Huijbregts, M. Karafiat, M. Lincoln, and V. Wan, “TheAMIDA 2009 meeting transcription system,” in Interspeech,2010.[2] C. Knapp and G. Carter, “The generalized correlation methodfor estimation of time delay,” IEEE Transactions on Acoustics,Speech and Signal Processing, 1976.[3] M. S. Brandstein and H. F. Silverman, “A robust method forspeech signal time-delay estimation in reverberant rooms,” inICASSP, 1997.[4] X. Anguera, C. Wooters, and J. Hernando, “Acoustic beam-forming for speaker diarization of meetings,” IEEE Transac-tions on Audio, Speech, and Language Processing, 2007.[5] J. Bitzer and K. Uwe Simmer, “Superdirective microphonearrays,” in Microphone arrays: signal processing techniquesand applications, M. Brandstein and E Ward, Eds. SpringerVerlag, 2001.[6] E.P. Zwyssig, M. Lincoln, and S. Renals, “A digital micro-phone array for distant speech recognition,” in ICASSP, 2010.[7] A. Adami, L. Burget, S. Dupont, H. Garudadri, F. Grezl,H. Hermansky, P. Jain, S. Kajarekar, N. Morgan, and S. Siva-das, “Qualcomm-ICSI-OGI front end archive,” http://www.icsi.berkeley.edu/Speech/papers/qio/,2002.[8] X. Anguera, Robust Speaker Diarization for Meetings, Ph.D.thesis, Universitat Polite`cnica de Catalunya, 2006.[9] G. Lathoud, I. McCowan, and D. Moore, “Segmentingmultiple concurrent speakers using microphone arrays,” inEurospeech, 2003.[10] M. Huijbregts, “SHOuT Speech Recognition Toolkit,” http://shout-toolkit.sourceforge.net/, 2006.[11] G. Friedland, A. Janin, D. Imseng, X. Anguera, L. Gottlieb,M. Huijbregts, M. Knox, and O. Vinyals, “The ICSI RT-09 speaker diarization system,” IEEE Transactions on Audio,Speech, and Language Processing, 2012.Zwyssig, E., Renals, S., & Lincoln, M. (2012). On the effect of SNR and superdirective beamforming in speaker diarisation in meetings.  In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. (pp. 4177-4180). 10.1109/ICASSP.2012.6288839",
            "id": 8985009,
            "identifiers": [
                {
                    "identifier": "194715329",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1109/icassp.2012.6288839",
                    "type": "DOI"
                },
                {
                    "identifier": "22543687",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:citeseerx.psu:10.1.1.294.3992",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "2115449049",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "oai:pure.ed.ac.uk:publications/e99ef659-e055-4225-a3a2-7106e196b210",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "28976789",
                    "type": "CORE_ID"
                }
            ],
            "title": "On the effect of SNR and superdirective beamforming in speaker diarisation in meetings",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2115449049",
            "oaiIds": [
                "oai:pure.ed.ac.uk:publications/e99ef659-e055-4225-a3a2-7106e196b210",
                "oai:citeseerx.psu:10.1.1.294.3992"
            ],
            "publishedDate": "2012-01-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 36566078,
                    "title": "A digital microphone array for distant speech recognition,” in ICASSP,",
                    "authors": [],
                    "date": "2010",
                    "doi": "10.1109/icassp.2010.5495040",
                    "raw": "E.P. Zwyssig, M. Lincoln, and S. Renals, “A digital microphone array for distant speech recognition,” in ICASSP, 2010.",
                    "cites": null
                },
                {
                    "id": 36566075,
                    "title": "A robust method for speech signal time-delay estimation in reverberant rooms,” in ICASSP,",
                    "authors": [],
                    "date": "1997",
                    "doi": "10.1109/icassp.1997.599651",
                    "raw": "M. S. Brandstein and H. F. Silverman, “A robust method for speech signal time-delay estimation in reverberant rooms,” in ICASSP, 1997.",
                    "cites": null
                },
                {
                    "id": 36566076,
                    "title": "Acoustic beamforming for speaker diarization of meetings,”",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1109/tasl.2007.902460",
                    "raw": "X. Anguera, C. Wooters, and J. Hernando, “Acoustic beamforming for speaker diarization of meetings,” IEEE Transactions on Audio, Speech, and Language Processing, 2007.",
                    "cites": null
                },
                {
                    "id": 36566073,
                    "title": "meeting transcription system,” in Interspeech,",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1109/tasl.2011.2163395",
                    "raw": "T. Hain, L. Burget, J. Dines, P.N. Garner, A.E. Hannani, M. Huijbregts, M. Karaﬁat, M. Lincoln, and V. Wan, “The AMIDA 2009 meeting transcription system,” in Interspeech, 2010.",
                    "cites": null
                },
                {
                    "id": 36566083,
                    "title": "On the effect of SNR and superdirective beamforming in speaker diarisation in meetings.",
                    "authors": [],
                    "date": "2012",
                    "doi": "10.1109/tasl.2011.2158419",
                    "raw": "G. Friedland, A. Janin, D. Imseng, X. Anguera, L. Gottlieb, M. Huijbregts, M. Knox, and O. Vinyals, “The ICSI RT09 speaker diarization system,” IEEE Transactions on Audio, Speech, and Language Processing, 2012. Zwyssig, E., Renals, S., & Lincoln, M. (2012). On the effect of SNR and superdirective beamforming in speaker diarisation in meetings. In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. (pp. 4177-4180). 10.1109/ICASSP.2012.6288839",
                    "cites": null
                },
                {
                    "id": 36566079,
                    "title": "Qualcomm-ICSI-OGI front end archive,” http:// www.icsi.berkeley.edu/Speech/papers/qio/,",
                    "authors": [],
                    "date": "2002",
                    "doi": null,
                    "raw": "A. Adami, L. Burget, S. Dupont, H. Garudadri, F. Grezl, H. Hermansky, P. Jain, S. Kajarekar, N. Morgan, and S. Sivadas, “Qualcomm-ICSI-OGI front end archive,” http:// www.icsi.berkeley.edu/Speech/papers/qio/, 2002.",
                    "cites": null
                },
                {
                    "id": 36566080,
                    "title": "Robust Speaker Diarization for Meetings, Ph.D. thesis, Universitat Polit` ecnica de Catalunya,",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "X. Anguera, Robust Speaker Diarization for Meetings, Ph.D. thesis, Universitat Polit` ecnica de Catalunya, 2006.",
                    "cites": null
                },
                {
                    "id": 36566081,
                    "title": "Segmenting multiple concurrent speakers using microphone arrays,” in Eurospeech,",
                    "authors": [],
                    "date": "2003",
                    "doi": null,
                    "raw": "G. Lathoud, I. McCowan, and D. Moore, “Segmenting multiple concurrent speakers using microphone arrays,” in Eurospeech, 2003.",
                    "cites": null
                },
                {
                    "id": 36566082,
                    "title": "SHOuT Speech Recognition Toolkit,” http: //shout-toolkit.sourceforge.net/,",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "M. Huijbregts, “SHOuT Speech Recognition Toolkit,” http: //shout-toolkit.sourceforge.net/, 2006.",
                    "cites": null
                },
                {
                    "id": 36566077,
                    "title": "Superdirective microphone arrays,” in Microphone arrays: signal processing techniques",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1007/978-3-662-04619-7_2",
                    "raw": "J. Bitzer and K. Uwe Simmer, “Superdirective microphone arrays,” in Microphone arrays: signal processing techniques and applications, M. Brandstein and E Ward, Eds. Springer Verlag, 2001.",
                    "cites": null
                },
                {
                    "id": 36566074,
                    "title": "The generalized correlation method for estimation of time delay,”",
                    "authors": [],
                    "date": "1976",
                    "doi": "10.1109/tassp.1976.1162830",
                    "raw": "C. Knapp and G. Carter, “The generalized correlation method for estimation of time delay,” IEEE Transactions on Acoustics, Speech and Signal Processing, 1976.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "http://www.cstr.inf.ed.ac.uk/downloads/publications/2012/zwyssig-dmma2-icassp12.pdf",
                "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.294.3992"
            ],
            "updatedDate": "2022-04-24T22:14:57",
            "yearPublished": 2012,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/28976789.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/28976789"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/28976789/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/28976789/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/8985009"
                }
            ]
        },
        {
            "acceptedDate": "2010-09-30T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Abosi"
                },
                {
                    "name": "Barbara Martini"
                },
                {
                    "name": "Cao"
                },
                {
                    "name": "Dimitra Simeonidou"
                },
                {
                    "name": "Eduard Escalona"
                },
                {
                    "name": "Fabio Baroncelli"
                },
                {
                    "name": "Georgios S. Zervas"
                },
                {
                    "name": "Grasa"
                },
                {
                    "name": "Ho"
                },
                {
                    "name": "Karim Torkmen"
                },
                {
                    "name": "Martini"
                },
                {
                    "name": "Nejabati"
                },
                {
                    "name": "Ovadia"
                },
                {
                    "name": "Piero Castoldi"
                },
                {
                    "name": "Politi"
                },
                {
                    "name": "Qiao"
                },
                {
                    "name": "Reza Nejabati"
                },
                {
                    "name": "Sun"
                },
                {
                    "name": "Valerio Martini"
                },
                {
                    "name": "Yixuan Qin"
                },
                {
                    "name": "Zervas"
                }
            ],
            "contributors": [
                ""
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/54930037",
                "https://api.core.ac.uk/v3/outputs/223332015",
                "https://api.core.ac.uk/v3/outputs/188878979",
                "https://api.core.ac.uk/v3/outputs/9318119"
            ],
            "createdDate": "2012-08-31T04:52:28",
            "dataProviders": [
                {
                    "id": 176,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/176",
                    "logo": "https://api.core.ac.uk/data-providers/176/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 1082,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/1082",
                    "logo": "https://api.core.ac.uk/data-providers/1082/logo"
                },
                {
                    "id": 286,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/286",
                    "logo": "https://api.core.ac.uk/data-providers/286/logo"
                }
            ],
            "depositedDate": "2010-09-30T00:00:00",
            "abstract": "This paper presents a novel service-oriented network architecture to bridge the informational gap between user applications and optical networks providing technology-agnostic multigranular optical network services for clouds. A mediation layer (service plane) between user applications and network control is proposed to facilitate a mapping process between user application requests and the network services. At the network level, a multigranular optical network (MGON) is proposed and implemented to support dynamic wavelength and subwavelength granularities with different transport formats [optical burst switched (OBS), optical burst transport (OBT)], reservation protocols (one-way, two-way), and different quality-of-service (QoS) levels per service type. The service-oriented multigranular optical network has been designed, implemented, and demonstrated on an experimental testbed. The testbed consists of service and network resource provisioning, service abstraction, and network resource virtualization. The service-to-network interoperation is provided by means of a gateway that maps service requests to technology-specific parameters and a common signaling channel for both service and network resource provisioning",
            "documentType": "research",
            "doi": "10.1364/jocn.2.000883",
            "downloadUrl": "https://core.ac.uk/download/9318119.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "Zervas et al. VOL. 2, NO. 10 /OCTOBER 2010/J. OPT. COMMUN. NETW. 883Service-Oriented Multigranular OpticalNetwork Architecture for CloudsGeorgios S. Zervas, Valerio Martini, Yixuan Qin, Eduard Escalona, Reza Nejabati,Dimitra Simeonidou, Fabio Baroncelli, Barbara Martini, Karim Torkmen, and Piero Castoldicptdtpigmtntmtvwb(wtiamnwQoOflwawIlctioghibcddaAbstract—This paper presents a novel service-orientednetwork architecture to bridge the informational gap be-tween user applications and optical networks providingtechnology-agnostic multigranular optical network servicesfor clouds. A mediation layer (service plane) between userapplications and network control is proposed to facilitate amapping process between user application requests and thenetwork services. At the network level, a multigranular op-tical network (MGON) is proposed and implemented to sup-port dynamic wavelength and subwavelength granularitieswith different transport formats [optical burst switched(OBS), optical burst transport (OBT)], reservation protocols(one-way, two-way), and different quality-of-service (QoS)levels per service type. The service-oriented multigranularoptical network has been designed, implemented, and dem-onstrated on an experimental testbed. The testbed consistsof service and network resource provisioning, service ab-straction, and network resource virtualization. The service-to-network interoperation is provided by means of a gate-way that maps service requests to technology-specificparameters and a common signaling channel for both ser-vice and network resource provisioning.Index Terms—Optical networks; Service-orientedarchitecture; Cloud computing.I. INTRODUCTIONC loud computing is a service approach that provides ITservices such as storage or computational capabilitiesenabled by distributed resources, such as data centers andclusters, to applications while hiding the relevant technol-ogy and implementation-specific details [1]. Cloud comput-ing is different from grid computing, since the latter is ori-ented to distributed computation and it requires grid-specific applications.Research on clouds has mainly focused on the IT infra-structure and technologies for both research and commer-cial purposes. Companies conducting clouds are deployingdifferent cloud platforms (Amazon EC2 [1], Google App En-gine [2], Microsoft Live Mesh [3], Sun Grid [4]), giving spe-Manuscript received August 14, 2009; revised July 2, 2010; accepted August20, 2010; published September 30, 2010 \u0001Doc. ID 115339\u0002.G. S. Zervas (e-mail: gzerva@essex.ac.uk), Y. Qin, E. Escalona, R. Nejabati,and D. Simeonidou are with the High Performance Networks Group, Schoolof Computer Science and Electronic Engineering, University of Essex,Colchester, CO4 3SQ, UK.V. Martini, K. Torkmen, and P. Castoldi are with the CEIICP—ScuolaSuperiore Sant’Anna, Via G. Moruzzi 1, 56124 Pisa, Italy.F. Baroncelli and B. Martini are with CNIT—National Laboratory ofPhotonic Networks, Via G. Moruzzi 1, 56124 Pisa, Italy.Digital Object Identifier 10.1364/JOCN.2.0008831943-0620/10/100883-9/$15.00 ©ial attention to factors such as CPU, memory, and storagerovisioning. However, the success of cloud computing andhe overall performance of the applications developed on itepends also on the physical network infrastructure that in-erconnects the distributed resources. In fact, clouds needroper consideration for network dynamics, such as capac-ty, quality of service (QoS), reservation, and complete inte-ration and federation of the clouds. In such an environ-ent, the transport infrastructure could be virtualized andreated as a type and part of the service [5,6]. The opticaletwork infrastructure that follows a traditional architec-ural approach to creating and delivering network servicesay not be able to be part of the cloud’s architecture andhus meet its requirements in a seamless way. Hence, it isital to understand and redefine the role of optical net-orks, to not just to carry information created by the cloud,ut also to be an active and integral element of it.The principle of multigranular optical cross-connectsMG-OXCs) that can switch traffic at fiber, waveband, andavelength granularities [7–9] has been proposed to reducehe cost and complexity of traditional OXCs. The MG-OXCs a key element for routing high-speed WDM data traffic inmultigranular optical network. The authors have comple-ented these studies by introducing a multigranular opticaletwork (MGON) [10–12] that is able to support dynamicavelength and subwavelength granularities with differentoS levels. By combining the two extensions, the full rangef bandwidth granularities can be supported by a singleXC design. Moreover, it allows fast reconfigurability andexibility on the electronic control of switching technologieshile offering good cost–performance balance.A MGON can support both optical circuit switched (OCS)nd optical packet/burst switched (OPS/OBS) technologiesith different transport formats and reservation protocols.n OCS networks, bandwidth granularity is at the wave-ength level since one or more wavelengths are allocated to aonnection, while connectivity between the source and des-ination is established using a two-way reservation, which isn general a time-consuming procedure. In OPS networks,ne or more IP packets with similar attributes are aggre-ated in an optical packet and tagged with an opticaleader. This scheme does not need advance reservation, andt can provide on-demand one-way connectivity by imitatingest-effort IP packet routing in the optical domain. OBSombines the advantages of OCS and OPS [13,14]. The fun-amental premise is the separation of the control plane andata plane and the segregation of functionality within theppropriate domain (electronic or optical) [9].2010 Optical Society of AmericatdtttpttabMQitAAtpaS884 J. OPT. COMMUN. NETW./VOL. 2, NO. 10 /OCTOBER 2010 Zervas et al.A cloud may benefit from a dynamic MGON able to pro-vide on-demand and fine-grained connectivity to applica-tions with a suitable level of network resource virtualiza-tion. The service-oriented optical network (SOON)framework [14] has been proposed to bridge the informa-tional gap between applications and optical networks by in-troducing a mediation layer that facilitates a mapping pro-cess between user application requests and the networkservices. SOON aims at disjoining the parameters perceivedby an end user from the technology-specific directivesneeded by network devices. This approach exists on cloudsbut only for the abstraction of IT infrastructure. SOON alsoenables automatic network configuration for establishingon-demand connection with different classes of service. Assuch, the SOON framework can be used as part of the infra-structure layer of the cloud computing stack [15] in order tooversee and deliver the network infrastructure as a service.The SOON framework can then enhance the agility of thecloud to reprovision technological infrastructures not only inthe IT environment but also on the network too.To address these challenges this work proposes a MGONable to deliver different levels of granularity as a service toclouds using the SOON framework. The SOON-enabledMGON allows dynamic interaction between application re-quests (network service) and network module configuration(edge node) by mapping application requests expressed interms of QoS parameters (e.g., latency, bandwidth) totechnology-specific attributes (e.g., burst size, wavelength,path, resource reservation method). A list of the benefits ofproposed interoperable architectural and implementationsolutions across the service plane, control plane, and dataplane are shown below:\u0001 Service-Oriented Optical Network (SOON)\u0002 Bridge the informational gap between the user appli-cation and the optical network\u0002 Directly offer resources (network and data) as ser-vice\u0002 Provide a mediation layer that facilitates a mappingprocess between user application requests and thenetwork services• Disjoin the parameters perceived by an end userfrom the technology-specific directives needed bynetwork devices\u0002 To facilitate intelligent discovery, reservation, andco-allocation of distributed resources across thenetwork\u0001 MGON\u0002 Bandwidth provisioning and switching capability atwavelength and subwavelength granularities\u0002 Agility and scalability of switching granularities pro-viding a dynamic network infrastructure solution\u0002 Fast reconfigurability and flexibility on the elec-tronic control of switching technologies\u0002 Cost–performance efficiency by offering an optimalbalance between slow and fast switch fabrictechnologies\u0001 SOON-enabled MGON advantages\u0002 Dynamic interaction between user request (networkservice) and network module configuration (edgenode)\u0002 Network elements are able to recognize and processapplication demands/requests• Mapping of application requests to technology-specific attributes (burst size, latency, wavelength,path, resource reservation method)\u0002 Service-aware connection establishment and QoSprovisioning\u0002 Fast provisioning and recoveryThe remainder of this paper is organized as follows. First,he service-oriented multigranular network architecture isescribed and analyzed in Section II. This section describeshe proposed SOON-MGON architecture and provides de-ailed information regarding the SOON internal signaling,he SOON-MGON integration, and the control and datalane packet format required for such integration. In Sec-ion III, the feasibility of MGON using different physical op-ical switches [microelectromechanical systems (MEMS)nd acousto-optic] is demonstrated on an experimental test-ed. In addition, we report on the functionalities of SOON-GON able to provide different network services for theuadHD video-on-demand (VoD) application and also showts performance in terms of latency and jitter. Finally, Sec-ion IV concludes the paper.II. SERVICE-ORIENTED MULTIGRANULAR NETWORKARCHITECTURE. SOON-Enabled Multigranular NetworkrchitectureThe SOON architecture, depicted in Fig. 1 introduces aechnology-independent mediation layer, called the servicelane (SP), that acts as a middleware between thepplication-layer and the network-layer. In particular, theP composes and orchestrates the connectivity service pro-Fig. 1. (Color online) SOON architecture functional blocks.Zervas et al. VOL. 2, NO. 10 /OCTOBER 2010/J. OPT. COMMUN. NETW. 885vided by the control plane (CP) at the boundary of the trans-port network, thus providing technology-independent net-work services with a level of abstraction suitable for beinginvoked by an application. It also decouples the networktechnologies from existing and emerging network servicesand allows the CP to focus on connectivity provisioning ser-vices.The SP [14] architecture is based on functional entities,called the centralized service element (CSE) and distributedservice elements (DSEs). The CSE supports the SP function-alities regarding the identity of a customer application andthe relevant contractual obligation with the network serviceprovider. The SP does not require intervention of the net-work management system for service provisioning (i.e., on-request fashion), allowing the direct invocation of networkservices to the network (i.e., on-demand fashion). In thiswork, the application that is able to request connectivityservices from the SP is represented by the service controlfunction (SCF). A SCF interacts with the SP via the XML-based user-to-service interface (USI).The SOON also has a compulsory technology-dependentsublayer called the broker network service (BNS) that inter-acts with specific devices and maps the SP command intospecific technology directives. The BNS module works onone side with the SP entities and on the other side with thenetwork via the control plane management interface (CMI).The CMI structure depends on the type of network trans-port service supported by SP operations. Usually, the CMIimplementation is based on configuration protocols such asNETCONF [14]. In this work, the CMI is a proprietary in-terface suitable for the communication between the DSEsand the OBS devices. The SOON approach is as generic aspossible and can be applied to other types of transport net-work systems by only changing the BNS module.B. SOON Internal SignalingTo realize the above-mentioned functionalities, differentsignaling types and interfaces have been implemented toperform different functions. An automatic provisioning sys-tem has been implemented using two types of signalingamong the DSEs called service provisioning signaling andbackground signaling.1) Service Provisioning Signaling: The service provision-ing signaling configures the network devices upon a networkservice-specific request (i.e., different message set per net-work service type) issued by an SCF. The service provision-ing signaling may be subdivided into three phases: the clientrequest phase, the service correlation phase, and the networkconfiguration phase. Each phase involves a specific addressspace and QoS parameters with different granularity.Figure 2 summarizes the parameter mapping from theSCF request through the SP and the set of network direc-tives down to the MG edge node.a) Client request phase: This phase starts when anSCF sends a network-services client request messageto a DSE to establish network service among a set ofclient networks. This message contains the followingset of parameters:• SCF identity parameters are used to check the SCFidentity and allow the establishment of a session be-tween the SCF and the designed DSEs (e.g., theSCF-IP address, the SCF-TCP port used by sendmessages and receive response messages).• Perceived QoS parameters are the QoS attributesthat a customer is able to perceive and to monitor,using software or hardware probes, such as thebandwidth, the packet delay, or the packet jitter.• Client address space parameters represent the clientaddress space and the CEs (customer edge nodes) in-volved in a network service and viewed by theprovider.b\u0002 Service correlation phase: The DSE discovers theset of provider edges (PEs) involved in the network ser-vice provisioning from the list of CEs contained in themessage provided during the client request phase.During the service correlation phase, service abstrac-tion and network resource virtualization are per-formed.• The service abstraction maps a set of parametersthat an application-layer module specifies for a par-ticular service request into a set of specific param-eters used by the network-layer for the identifica-tion and provisioning of that service.• The network resource virtualization aims at hidingthe network technology details from the applica-tions. The resource virtualization allows the applica-tion to request new services without dealing withnetwork technology details. At the same time, virtu-alization improves network scalability and allowsheterogeneous technologies to be used without theneed to change the interaction with its customerapplications.c\u0002 Network configuration phase: The DSE communi-cates the parameters received during the service corre-lation phase to the DSEs, under the control of whichthe PE falls. In turn, each DSE uses these parametersto issue a set of node-specific network configuration re-quest messages to the actual network element via theCMI. In this work the CMI is a proprietary protocolFig. 2. (Color online) Three phases of provisioning signaling.ppgcuatedtptFtnumTqpdsnn“itiTraDFeapbtda886 J. OPT. COMMUN. NETW./VOL. 2, NO. 10 /OCTOBER 2010 Zervas et al.transported by TCP messages.2) Background Signaling: The background signaling isservice independent and allows collecting and abstracting ofthe network status information (e.g., network technologyand topology information) needed by the service provision-ing signaling to perform dynamic correlation of the networkservice parameters among the network devices.The background signaling is used to update the networkresource database (NR-DB) within the DSEs in order to per-form the service correlation phase previously described. Thebackground signaling operation is periodically executed atregular intervals (typically 10 minutes or after the setup ofa new network service).As shown in Fig. 3, the background signaling consists ofthree phases:a) Network resource discovery phase (arrows 1, 2):Each DSE gathers at regular intervals the informationregarding the identity of the PE and CEs attached tothe controlled PE and the information related to thenetwork service already established.b) Network resource abstraction phase: Each DSEmaps the technology-specific information obtainedduring the resource discovery phase into thetechnology-independent information used by the DSEsto perform the service abstraction and network re-source virtualization tasks. The information is storedin the NR-DB.c) Information distribution phase (arrows 3, 4): EachDSE distributes the information contained in itsNR-DB to the CSE module that acts as a communica-tion switch maintaining all the DSEs updated with theknowledge of the overall network information at theboundary of the optical burst switching and transport(OBST) nodes network.C. SOON–MGON Interoperation for On-DemandNetwork Service ProvisioningTo enable on-demand service provisioning within theSOON framework, some configuration operations must beFig. 3. (Color online) Background signaling.erformed between the different involved processes. Thehysical OBS information needed to populate the NR-BD isathered via the CMI interface at regular intervals. Thisonfiguration allows the discovery of the physical interfacesed as well as the possible paths. Each DSE then associ-tes that information with its address (Service_address) andhe address of the relevant DSE.The service-oriented multigranular network consists ofdge and core nodes. The architecture of the edge node isisplayed in Fig. 4. The main element of the architecture ishe FPGA prototype (Virtex II-Pro), which is programmed toarse SOON messages and also perform a number of func-ionalities that will be discussed below.As shown in Fig. 4, the DSE has 1GE connectivity to thePGA board. Initially, the DSE sends a network configura-ion request (as part of phase 3 of service provisioning sig-aling) to the FGPA board via the CMI. The FPGA is thensing a packet parser to identify and interpret allessages—SOON signaling messages and data packets.he SOON message called the “network services client re-uest” is encapsulated within a burst control header (BCH)acket and forwarded over the MGON control plane to theestination DSE attached to a remote edge node.The network configuration request message (SOON mes-age) is used to trigger both the configuration of the edgeode and the reservation of the end-to-end multigranularetwork service. This message is parsed and passed to theburst scheduler parameter mapping” module. This modules responsible for mapping the SOON configuration requesto different parameters on multigranular scheduling.The SOON message is encapsulated on a TCP header us-ng specific ports to provide technology-specific information.he integration of the SOON framework with MGON is car-ied out using a matrix that maps some type of services intopool of parameters suitable from the edge node. Thus, theSE module is in charge of the mapping process, and thePGA interprets the matrix number received with the TCPncapsulated SOON message and configures the burst size,ggregation time (delay time), wavelength/path, and trans-ort service used as a function of the TCP source port num-er received from the DSE (see Fig. 4). Figure 5 shows aimeline of the entire signaling flow from the service planeown to the OBS network burst signaling (BCH) up to thectual burst transmission.Fig. 4. (Color online) Edge node architecture.ittlnoatcsssFZervas et al. VOL. 2, NO. 10 /OCTOBER 2010/J. OPT. COMMUN. NETW. 887Table I provides detailed information on how client ser-vices are mapped to technology-specific parameters. Fourdifferent services are considered in this experiment: 1)QuadHD—VoD, 2) video-over-IP (VoIP), 3) video streaming,and 4) raw data transfer. Each of these has been mapped todifferent functions provided by the OBST testbed. The com-bination of the OBST functionalities depends on the appli-cation’s QoS requirements. Parameters such as the offeredtraffic, network status, and availability of network resourcescan also be included in the selection process. The burst sizeis affected by the limited internal RAM of the FPGA. Thewavelength/path selection determines the number of hopsand type of network resources used, which will affect theend-to-end delay and burst loss probability. Nevertheless,the testbed consists of two edge nodes, and thus there is nocause for contention at the core network and as a result noburst loss.Two transport services are available for selection of theoptical burst switching and the optical burst transport. Thefirst one is based on the traditional OBS using one-way just-in-time (JIT) signaling per burst basis, whereas the secondapproach reserves a circuit for the duration of the service,allowing bursts to be transported over it.D. Control and Data Plane Packet Formats TowardService OrientationTo implement the functional modules of the SOON-enabled OBST network [16,17] and in particular the OBSTedge router, new control and data plane packet/burst for-mats are proposed and defined as shown in Fig. 6. Regard-Fig. 5. (Color online) End-to-end signaling.TABLE ICLIENT SERVICE MAPPED ON TECHNOLOGY-SPECIFICPARAMETERSng the control plane, signaling information is divided intowo main categories, the generic control header (GCH) andhe control payload.The generic control header is constructed to encapsu-ate any type of control payload required for different OBSTetwork approaches (e.g., SOON-enabled OBS). It consistsf 64 bits with the following fields:• VN: version number required to identify the format ofthe control payload• CPT: control payload type used to describe the encap-sulated control payload• PL: payload length• SGN: the type of signaling used to represent in-band orout-of-band• CN: the connectivity that indicates whether the controlsignal is sent to one or multiple destinations (point-to-point, multicast, anycast)• Source• DestinationThe control payload format depends on the capabilitiesnd enhancement of the OBST control plane. Here, twoypes are presented, namely, the BCH and the applicationontrol header (ACH).The BCH is used on traditional OBST networks and isent prior to the burst data in order to reserve network re-ources and establish end-to-end OBST connectivity. It con-ists of 128 bits with the following fields:• VN: version number.• CoS: The class of service is actually generated from theedge router classifier and is used to maintain the re-quired CoS throughout the OBS network.• Resv: The reservation block identifies the type of signal-ing mechanism [e.g., JIT, just-enough-time (JET), two-way] utilized to reserve the network resources.• BL: Burst lambda identifies the lambda that the burstis carried on.ig. 6. (Color online) Control and data plane packet/burst formats.tblncmesdtitdswtqmpbqSsacaaaeebaccltfdsamMsgmcap888 J. OPT. COMMUN. NETW./VOL. 2, NO. 10 /OCTOBER 2010 Zervas et al.• The burst length and offset time are then required ateach OBS core node in order to reserve and configurethe switch for the appropriate time period.The other type of control payload is the ACH, which isused to carry upper-layer messages and enrich the opticalburst control plane with service information. The ACH hasvariable size and consists of the following fields:• VN: It defines the format of the payload.• CoS: same functionality as in BCH.• Message connectivity (MC) is equivalent to the BC pa-rameter of BCH.• SLA: Service level agreement identifies the interactionbetween the service plane and the control plane. It isalso used to encapsulate service-related messages gen-erated by SOON and can be used to propagate/piggyback them over an OBST optical network.After the detailed analysis of the SOON-OBS integration,signaling protocols, and packet format definition, Section IIIdescribes the testbed architecture/configuration, the experi-ments conducted, and the results measured.III. NETWORK ARCHITECTURE AND TESTBED SETUPThe proposed service-oriented network architecture isbased on SOON elements and OBST technologies utilizingservice-aware edge and core routers interconnected by JITSOON signaling. The architectural standpoint of the testbedis shown in Fig. 7, while the practical setup is shown inFig. 8.The SOON translates applications’ service requests ex-pressed in terms of perceived QoS and resources to atechnology-specific pool at the edge of the OBST network.Through this capability of decoupling network technologiesfrom services, the CP is unburdened of service-orientedfunctionalities and it can focus on the provisioning of con-nectivity services.The SOON supports service abstraction and resource vir-tualization capabilities that allow a set of application-specific parameters to be mapped into a set of parametersused by the network for the actual configuration of a servicewhile avoiding the exposition to applications of thetechnology-related details of network resources, such as theburst dimension, assembly time, network transport service(OBS, OBT), and end-to-end lightpath.The SOON framework also has the ability to coordinateFig. 7. (Color online) Architectural block diagram of the SOON-enabled OBST testbed.he different OBS edge devices to create unidirectional andidirectional end-to-end wavelength (OBT) and subwave-ength (OBS) paths. This is obtained through an ad-hoc sig-aling protocol among DSEs, which can exchange the edgeharacteristic and reachability information. The DSE alsoanages the periodical update of information regarding thedge node devices used to solve the reachability of theource/destination through the network resource internalatabase (NR-DB). To enable the framework to interact withhe OBS network a specific technology-dependent modulenstalled in each DSE has been conceived, which translateshe information received from the DSE into a set of specificirectives comprehensible by the Ethernet–optical burstwitching–transport (E-OBS-T) devices.The service-aware edge OBS router, which utilizes a net-ork processor and FPGA devices, is able to differentiate be-ween service-layer messages (based on SOON), network re-uests, and data packets. In the case of SOON signalingessages, the edge router forwards them to the controllane. In the case of incoming SOON network requests, theurst aggregation scheduler is triggered to reflect service re-uirements into buffer size and the time thresholds. TheOON message is also used to decide on the network provi-ioning system of either OBS or OBT and then the appropri-te lightpath (one out of a maximum of four). The OBS ac-ess control and provisioning system is created byggregating bursts and in turn generating and transmittingBCH ahead of time \u00015 \u0001s\u0002 in order to configure thecousto-optic switch on a per-burst basis. OBT supports annd-to-end lightpath for the duration of the service by gen-rating a BCH upon reception of a SOON message. The com-ination of lambda selection by controlling a SG-DBR tun-ble laser (at Edge 1) connected to a MEMS switch (foronnectivity with all core nodes) together with the BCH pro-essed at the core FPGA provides the end-to-end dynamicightpath. Finally the data packets are buffered on aggrega-ion first in, first out (FIFO) and then transmitted over dif-erent wavelength or subwavelength lightpaths. The OBSTata plane transport mechanism is based on keep-alive mes-ages in between burst transmission.The service-aware core OBS router comprises three nodesll controlled by a centralized FPGA-based control planeodule creating a meshed topology. The two nodes consist ofEMS switches \u000110 ms\u0002 and the third one of both a MEMSwitch and an acousto-optic switch \u00014 \u0001s\u0002 to form a multi-ranular optical cross-connect (MG-OXC). The control planeodule utilizes a network processor and an FPGA, and itan process and forward service-layer information on the flys well as allocate switch resources for either OBS or OBTrovisioning.Fig. 8. (Color online) SOON-enabled OBST testbed setup.omwOo4raeisds1Umc1ebdei2tv(drtFpZervas et al. VOL. 2, NO. 10 /OCTOBER 2010/J. OPT. COMMUN. NETW. 889The OBS traffic is transported using a specific lightpaththrough the MG-OXC node and switched at the acousto-optic switch supporting service 1 (see Table I). The rest ofthe traffic is being transported over three different light-paths having different numbers of hops based on OBT forthe duration of the service.Unlike the existing service-oriented architectures, whichuse the legacy IP network for carrying SOON signals andmessages, in the proposed architecture SOON messages arecarried in the optical domain with JIT signaling and withinthe GCH. The SCF application issues a request to the SOONframework for a specific network service (four network ser-vices). Then, the DSE element uses proprietary signaling totrigger the other involved DSE and configures the OBS de-vices. The SOON signaling is service specific and has differ-ent message sets for each type of provided service.Figure 9 shows the overall time elapsed during the SOONnetwork service provisioning process.The first section (from the left) of the figure representsthe SOON processing time of a service request before thestart of the signaling (blue peak), while the second blockrepresent the processing for the building of specific direc-tives for the edge node configuration (pink peak) inferredfrom the user request. After the edge configuration theSOON gathers the ACK message from the DSE module andsends a service-provided ACK to the SCF service application(red line).The SOON JIT control protocol performance has beenevaluated by measuring the end-to-end service time, whichincludes the edge and core node parsing and forwardingtime. This value is mostly dependent on the end host perfor-mance used for SOON elements and not the actual E-OBS-Ttestbed.A. Experimental Results and DiscussionIn this experiment, SOON service and connection estab-lishment as well as high-definition video over OBST trans-mission are demonstrated. The SOON JIT messages encap-sulated in GCHs are sent over the OBST control plane andthe generated variable optical bursts are sent over anEthernet-type data plane.Figure 10 shows the delay and jitter of receiving 46 Mbpshigh-definition video over the E-OBS-T testbed, which has200 Mbps background TCP traffic. Service 1 is defined andimplemented in the edge node (FPGA) as best-effort class.Service 2 is defined and implemented in the edge node(FPGA) as timing critical class. In order to study the effectFig. 9. (Color online) SOON edge-to-edge overall provisioningtime.f OBST on the real-time transmission of high-performanceedia, four prerecorded videos, with different qualities,ere used in different streaming media scenarios across theBST testbed network.These videos varied from high definition with resolutionsf 1280\u0002720 and 1440\u00021080 and bit rates of 27 and6 Mbps, respectively, to QuadHD of 2560\u00021600 and bitates of 106 and 156 Mbps. TCP background traffic ofround 200 Mbps was also generated through a traffic gen-rator in order to emulate the current Internet traffic behav-or (TCP and UDP data).The aggregation developed is hybrid and combines bothize and time threshold. Size and time thresholds are alsoynamically changed per SOON service with a maximumize threshold of 5000 bytes and a time limit of 2 ms. Figure0(a) shows that for service 1 (E-OBT) more than 95% of theDP packets have a delay of less than 3 ms with a maxi-um delay of less than 4 ms, which is well within the ac-eptable level, and for service 2 the value is less than.8 ms. The horizontal lines that appear mean that no pack-ts have delay less than a certain value due to the minimumurst aggregation time-out threshold. There are also severalumping dots close to zero. This is because when some pack-ts enter the burst aggregation buffer, the aggregation timers just about to time out. Figure 10(b) shows that for service(OBS) the jitter also remains below 1.4 ms for 100% of theraffic and below 0.9 ms for service 1, also a well-acceptedalue.Finally, Fig. 11 illustrates the physical layer performanceFER) of three different paths using different lambdas andifferent numbers and types of optical switches. Thus, B2Bepresents the back-to-back performance of the end-to-endestbed. Then, all different paths using different lambdasFig. 10. (Color online) (a) Delay and (b) jitter for service 1 and 2.ig. 11. (Color online) Frame error rate of all different networkaths.aatcsvtgc890 J. OPT. COMMUN. NETW./VOL. 2, NO. 10 /OCTOBER 2010 Zervas et al.(e.g., Lambda I/Path I, Lambda II/Path II, Lambda III/PathIII, all shown on both Figs. 8 and 11) experience a powerpenalty of 1.5–2.5 dB, which is well within acceptable levelsconsidering the fact that the whole testbed is optimized oncefor all possible paths and the fact that each path is using adifferent number of MEMS switches (two up to five) or evena combination of MEMS switches and acousto-opticswitches.IV. CONCLUSIONSA novel service-oriented multigranular optical networkarchitecture has been presented as a candidate for clouds.Key architectural issues, protocols, packet formats, andtechniques for a service-oriented MGON architecture wereinvestigated. In particular, this work has explored a serviceplane that is able to facilitate a mapping process betweenuser application requests and the network services throughservice abstraction and network resource virtualization.Then, the electronic gateway that has been developed in theedge node to translate user application requests totechnology-dependent information (burst size, assemblytime, wavelength/path, transport service) has been intro-duced. Also, a detailed description of the generic packet for-mat required to carry service plane and control plane infor-mation over the same control channel (lambda) for cross-layer interoperation reasons has been provided. A testbedhas been designed and implemented to support all above-mentioned functionalities, and experiments were conductedto show both the service plane performance (SOON provi-sioning time) in addition to application performance(QuadHD VoD) under different traffic conditions. Finally,the frame error rate (FER) has been experimentally mea-sured for three different paths using different lambdas anddifferent numbers and types of optical switches (MEMs andacousto-optic).ACKNOWLEDGMENTSThe work described in this paper was carried out with thesupport of the BONE project (“Building the Future OpticalNetwork in Europe”), a Network of Excellence funded by theEuropean Commission through the 7th ICT-FrameworkProgramme.REFERENCES[1] Amazon Elastic Compute Cloud (EC2), http://www.amazon.com/ec2/, 2008.[2] Google App Engine, http://appengine.google.com, 2008.[3] Microsoft Live Mesh, http://www.mesh./com, 2008.[4] Sun network.com (Sun Grid), http://www.network./com, 2008.[5] C. E. Abosi, R. Nejabati, and D. Simeonidou, “A novel servicecomposition mechanism for the future optical internet,” J. Opt.Commun. Netw., vol. 1, no. 2, pp. A106–A120, July 2009.[6] E. Grasa, A. Lopez, S. Figuerola, G. Junyent, and M. Savoie,“UCLPv2: a network virtualization framework built on webservices,” IEEE Commun. Mag., vol. 46, no. 3, pp. 126–134,Mar. 2008.[7] X. Cao, Y. Xiong, V. Anand, and C. Qiao, “Wavelength bandswitching in multi-granular all-optical networks,” Proc. SPIE,vol. 4874, pp. 198–210, July 2002.[8] P. H. Ho and H. T. Mouftah, “Routing and wavelength assign-ment with multi-granularity traffic in optical networks,” J.Lightwave Technol., vol. 20, no. 8, pp. 1292–1303, Aug. 2002.[9] C. T. Politi, C. Matrakidis, A. Stavdas, D. Gavalas, and M. J.O’Mahony, “Single layer multigranular OXCs architecturewith conversion capability and enhanced flexibility,” J. Opt.Netw., vol. 5, no. 12, pp. 1002–1012, Dec. 2006.[10] G. S. Zervas, M. De Leenheer, L. Sadeghioon, D. Klonidis, Y.Qin, R. Nejabati, D. Simeonidou, C. Develder, B. Dhoert, and P.Demeester, “Multi-granular optical cross-connect: design,analysis and demonstration,” J. Opt. Commun. Netw., vol. 1,no. 1, pp. 69–84, June 2009.[11] R. Nejabati, G. Zervas, G. Zarris, Y. Qin, E. Escalona, M.O’Mahony, and D. Simeonidou, “A multi-granular opticalrouter for future networks,” J. Opt. Netw., vol. 7, no. 11, pp.914–927, Nov. 2008.[12] M. Savi, G. Zervas, Y. Qin, V. Martini, C. Raffaelli, F. Baron-celli, B. Martini, P. Castoldi, R. Nejabati, and D. Simeonidou,“Data-plane architectures for multi-granular OBS network,” inOptical Fiber Communication Conf., San Diego, 2009, paperOML5.[13] C. Qiao and M. Yoo, “Optical burst switching (OBS)—a newparadigm for an optical Internet,” J. High Speed Netw., vol. 8,no. 1, pp. 69–84, 1999[14] B. Martini, V. Martini, F. Baroncelli, K. Torkman, and P. Cas-toldi, “Application-driven control of resources in multiserviceoptical networks,” J. Opt. Commun. Netw., vol. 1, no. 2, pp.A270–A283, July 2009.[15] S. Johnston, “Taxonomy: the 6 layer cloud computing stack,”http://samj.net/2008/09/taxonomy-6-layer-cloud-computing-stack.html.[16] Y. Sun, T. Hashiguchi, V. Q. Minh, X. Wang, H. Morikawa, andT. Aoyama, “A burst-switched photonic network testbed: its ar-chitectures, protocols and experiments,” IEICE Trans. Com-mun., vol. E88-B, no. 10, pp. 3864–3873, Oct. 2005.[17] S. Ovadia, C. Maciocco, M. Paniccia, and R. Rajaduray, “Pho-tonic burst switching (PBS) architecture for hop and span-constrained optical networks,” IEEE Commun. Mag., vol. 41,no. 11, pp. S24–S32, Nov. 2003Georgios S. Zervas was awarded theM.Eng. degree in electronic and telecommu-nication systems engineering with distinc-tion and the Ph.D. degree at the Universityof Essex (UK) in 2003 and 2008, respec-tively. He is a Research Fellow in the HighPerformance Networks Group at the Uni-versity of Essex involved in the current andpast EC-funded projects MAINS,STRONGEST, GEANT3, BONE, Phos-phorus MUFINS, and e-Photon/One. He isuthor and co-author of over 60 papers in international journalsnd conferences. His research interests include high-speed optoelec-ronic router design, subwavelength networks (e.g., OBS), GMPLS,ognitive networks, and grid/cloud networks. He is also involved intandardization activities in the Open Grid Forum (OGF).Valerio Martini received his master’s de-gree (M.Sc.) in telecommunication engineer-ing in March 2005 from Pisa University(Italy) defending his thesis on QoS optimi-zation in virtual LAN (VLAN) protocol(IEEE 802.1Q) using Opnet Network simu-lator in the Network Laboratory of the In-formation Technology Department. He hasbeen a Research Fellow Academy Visitor atthe KTH, Royal Institute of Technology, inStockholm (Sweden) in 2007 and at the Uni-ersity of Essex, Colchester, London (UK), in 2008. His research in-erests include network virtualization, L2/L3 VPN service technolo-ies, service virtualization in (G)MPLS networks, and cloudomputing. He received the Ph.D. degree in telecommunicationsiptPmaMpPPpnJr“fsbpncjZervas et al. VOL. 2, NO. 10 /OCTOBER 2010/J. OPT. COMMUN. NETW. 891from “Scuola Superiore Sant’Anna” in March 2009 working at theIntegrated Research Center for Photonic Networks and Technology(CEIIC), Pisa (Italy). At the moment he holds the position of SystemEngineer at Juniper Networks Amsterdam.Yixuan Qin (Ph.D. in electronic system en-gineering) is a Senior Research Officer inthe High Performance Networks Group atthe University of Essex involved in the EC-funded projects MUFINS, e-Photon/One\u0003,BONE, DICONET, MAINS, andSTRONGEST since 2007. His research in-terests include high-speed digital systemdesign, embedded system design, hardwareaccelerated computing/networking, flexiblenetworks, passive optical networks, opticalburst switching, impairment-aware-based routing, and GMPLSnetworks. He has over 20 publications including journals, confer-ence papers, and a book chapter.Eduard Escalona has been a ResearchMember within the High Performance Net-works Group at the University of Essexsince 2007. He obtained his M.Sc. degree intelecommunications engineering (2003) atthe Universitat Politecnica de Catalunya(UPC). He has participated in several Euro-pean projects such as IST LION, NOBEL,CARISMA, and PHOSPHORUS. He is cur-rently involved in the GÉANT3 and FP7GEYSERS projects. His main research ar-eas are management and control of optical networks, focused oncontrol plane issues and service-oriented architectures.Reza Nejabati is currently an AcademicMember of the High Performance NetworksGroup in the School of Computer Scienceand Electronic Engineering, University ofEssex. He has over ten years of academicand industrial experience in the field oftelecommunication and computer science.The main current areas of his research in-terests are the application of ultra-high-speed network technologies for the futureInternet, design and control issues forservice-oriented and application-aware networks, network virtual-ization as well as high-performance network architecture and tech-nologies for e-science applications. He has more than 80 publica-tions in these areas.Dimitra Simeonidou is the Head of theHigh Performance Networks Group, whichincludes the Photonics Networks Labora-tory and the newly established NetworkMedia Laboratory, at the University of Es-sex, UK. She joined Essex in 1998 (previ-ously with Alcatel Submarine Networks).While at Alcatel, she held the post of SeniorPrinciple Engineer and contributed to theintroduction of WDM in long-haul subma-rine links and pioneered the design and de-ployment of optical add–drop multiplexers. At Essex, she is leadinga group of 30 researchers and Ph.D. students and she is involved innumerous national and international research projects. Her re-search is focusing in the fields of optical networks, grid and cloudcomputing, and the future Internet. She is author and coauthor ofover 350 papers, 11 patents, and several standardization docu-ments.Fabio Baroncelli (M’05) received the mas-ter’s degree in physics in 1997 from theUniversity of Pisa (Italy). He worked forSynapsis as a Web Software Engineer andfor Marconi Communications as a SoftwareEngineer in DWDM technology and net-work management. Since 2003 he has beena Researcher at the \u0001Centro di Eccellenzaper l’Ingegneria dell’Informazione, della Co-municazione e della Percezione\u0001 (CEIICP)in Pisa (Italy). His main research interestsnclude optical control and management plane design, cloud com-uting architectures, and hybrid MANET.Barbara Martini received the master’s de-gree in electronic engineering in 1999 fromthe University of Florence (Italy). Shejoined Italtel from university working as aHardware Engineer on network devicedriver design and TCP/IP stack protocolsand then joined Marconi Communicationsin the summer of 2000 as a Software Engi-neer involved in network management soft-ware design in DWDM equipment. Since2003 she has been a Research Engineer athe CNIT National Laboratory of Photonics Networks located inisa (Italy). Her main research interests include network manage-ent system design, GMPLS optical control plane architectures,nd service platform architectures in next-generation networks.oreover, she also carries out researches in advanced security sup-orts for multidomain networks.Karim Torkmen was born in Tunis (Tuni-sia) in 1982. He received the National Di-ploma of Engineer in telecommunicationsfrom the High School of Communications ofTunis (Sup’Com), Tunis (Tunisia), in 2006.He achieved the degree of InternationalMaster on Communication Networks Engi-neering (IMCNE) at Scuola SuperioreSant’Anna, Pisa (Italy), in 2007. Currentlyhe is a Ph.D. student at the Centre of Excel-lence for Information, Communication anderception Engineering (CEIICP) of Scuola Superiore Sant’Anna,isa (Italy). His main research interests include network servicerovisioning to multimedia applications and mobility support in IPetworks.Piero Castoldi (Ph.D. in information tech-nology) has been an Associate Professor atScuola Superiore Sant’Anna, Pisa, Italy,since 2001. He spent abroad at PrincetonUniversity (USA) overall about two years in1996, 1997, 1999, and 2000, and in 2001 hevisited for two months the University ofTexas at Dallas, USA. He has also served asa Project Manager of many projects of theInteruniversity National Consortium forTelecommunications (CNIT), and sinceanuary 2005 he has been the Director of the CNIT National Labo-atory of Photonic Networks. He is also currently the Leader of theNetworks and Services” research area at the Center of Excellenceor Networks Engineering at Scuola Superiore Sant’Anna. His re-earch interests cover telecommunications networks and systems,oth wired and wireless, and more recently reliability, switchingaradigms, and control of optical networks, including application-etwork cooperation mechanisms, in particular for grids andlouds. He is author of more than 200 publications in internationalournals and conference proceedings.",
            "id": 8725583,
            "identifiers": [
                {
                    "identifier": "54930037",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:research-information.bris.ac.uk:publications/b7390911-fae0-4b09-a96e-0c671d0d4a0d",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "oai:repository.essex.ac.uk:3757",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "10.1364/jocn.2.000883",
                    "type": "DOI"
                },
                {
                    "identifier": "188878979",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "223332015",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:www.iris.sssup.it:11382/305749",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "9318119",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "385711109",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:research-information.bris.ac.uk:openaire_cris_publications/b7390911-fae0-4b09-a96e-0c671d0d4a0d",
                    "type": "OAI_ID"
                }
            ],
            "title": "Service-Oriented Multigranular Optical Network Architecture for Clouds",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:repository.essex.ac.uk:3757",
                "oai:research-information.bris.ac.uk:openaire_cris_publications/b7390911-fae0-4b09-a96e-0c671d0d4a0d",
                "oai:www.iris.sssup.it:11382/305749",
                "oai:research-information.bris.ac.uk:publications/b7390911-fae0-4b09-a96e-0c671d0d4a0d"
            ],
            "publishedDate": "2010-01-01T00:00:00",
            "publisher": "'The Optical Society'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://repository.essex.ac.uk/3757/1/05594024.pdf"
            ],
            "updatedDate": "2022-04-24T09:39:43",
            "yearPublished": 2010,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "1943-0620"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/9318119.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/9318119"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/9318119/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/9318119/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/8725583"
                }
            ]
        },
        {
            "acceptedDate": "2013-01-31T00:00:00",
            "arxivId": "1210.5424",
            "authors": [
                {
                    "name": "Balasubramanian, Shantharam"
                },
                {
                    "name": "Islam, Muhammad Nazmul"
                },
                {
                    "name": "Kompella, Sastry"
                },
                {
                    "name": "Mandayam, Narayan B."
                },
                {
                    "name": "Seskar, Ivan"
                }
            ],
            "contributors": [
                "Muhammad Nazmul"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/192384701"
            ],
            "createdDate": "2014-10-23T17:19:53",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2012-10-01T00:00:00",
            "abstract": "In this paper, we design and implement time exchange (TE) based cooperative\nforwarding where nodes use transmission time slots as incentives for relaying.\nWe focus on distributed joint time slot exchange and relay selection in the sum\ngoodput maximization of the overall network. We formulate the design objective\nas a mixed integer nonlinear programming (MINLP) problem and provide a\npolynomial time distributed solution of the MINLP. We implement the designed\nalgorithm in the software defined radio enabled USRP nodes of the ORBIT indoor\nwireless testbed. The ORBIT grid is used as a global control plane for exchange\nof control information between the USRP nodes. Experimental results suggest\nthat TE can significantly increase the sum goodput of the network. We also\ndemonstrate the performance of a goodput optimization algorithm that is\nproportionally fair.Comment: Accepted in 2012 Military Communications Conferenc",
            "documentType": "research",
            "doi": "10.1109/milcom.2012.6415742",
            "downloadUrl": "http://arxiv.org/abs/1210.5424",
            "fieldOfStudy": "computer science",
            "fullText": "1Implementation of Distributed Time Exchange\nBased Cooperative Forwarding\nMuhammad Nazmul Islam, Shantharam Balasubramanian, Narayan B. Mandayam, Ivan Seskar and Sastry\nKompella∗\nWireless Information & Networking Laboratory, Rutgers University\nmnislam@winlab.rutgers.edu, shantharampsg@gmail.com, narayan@winlab.rutgers.edu,\nseskar@winlab.rutgers.edu, ∗ Information Technology Division, Naval Research Laboratory, Email: sk@ieee.org\nAbstract—In this paper, we design and implement time ex-\nchange (TE) based cooperative forwarding where nodes use\ntransmission time slots as incentives for relaying. We focus\non distributed joint time slot exchange and relay selection in\nthe sum goodput maximization of the overall network. We\nformulate the design objective as a mixed integer nonlinear\nprogramming (MINLP) problem and provide a polynomial time\ndistributed solution of the MINLP. We implement the designed\nalgorithm in the software defined radio enabled USRP nodes of\nthe ORBIT indoor wireless testbed. The ORBIT grid is used\nas a global control plane for exchange of control information\nbetween the USRP nodes. Experimental results suggest that TE\ncan significantly increase the sum goodput of the network. We\nalso demonstrate the performance of a goodput optimization\nalgorithm that is proportionally fair.\nIndex Terms—Resource Delegation, Cooperative Forwarding,\nGlobal Control Plane, Testbed Implementation, GNUradio.\nI. INTRODUCTION\nCooperative forwarding improves the connectivity and\nthroughput in wireless networks [1]. However, forwarding\nalways incurs some costs, e.g., delay/power, at the forwarder\nnode. Hence, there have been recent studies on resource\ndelegation based incentivized forwarding [2]–[7]. Resource\ndelegation based forwarding allows the sender node to delegate\na portion of its allotted resource to the forwarder node as an\nimmediate incentive for relaying. Previous works on incen-\ntivized forwarding have focused on resource exchange from\na theoretical perspective and used numerical simulations to\njustify the effectiveness of the approach. The main contribution\nof our work is to demonstrate the advantages of incentivized\nforwarding using the ORBIT indoor wireless testbed [8].\nWe specifically focus on the uplink of an N node time\ndivision multiple access (TDMA) network where each node\nreceives an initial number of time slots and transmits data to\nthe base station (BS) through the direct path. In this context,\nwe focus on a two hop time exchange (TE) based forwarding\nscheme where the sender node transfers a portion of its allotted\ntime slots to the forwarder node as an incentive for relaying.\nThe basic idea of TE is illustrated in Fig. 1. In this example,\nnode 1 and 2 initially receive 4 time slots and transmit through\nthe direct path. In TE, node 1 performs a half duplex decode\nand forward (DF) relaying of node 2’s data and node 2\ndelegates one time slot to node 1 as an incentive for relaying.\nNode 2 may attain higher data rate through this cooperation\nFig. 1. The motivating scenario for time exchange based cooperative\nforwarding\nsince its data goes to the BS through two different paths. Node\n1 may also get higher data rate since it has more transmission\ntime slots. The optimal time slot delegation is an important\nquestion in this context.\nA sender node can delegate time slots to multiple forwarder\nnodes and transmit data to the BS through multiple paths.\nHowever, the authors of [9] have shown that for a source-\ndestination pair, in the presence of multiple relay nodes, it\nis sufficient to select the “best relay node” to achieve full\ndiversity order. On the other hand, the assumption of one\nsender node for one forwarder node reduces the relay selection\ncomplexity. Therefore, we assume one forwarder node for\none sender node and vice versa in this work. The optimal\ndistributed sender-forwarder pair selection problem becomes\nanother important question in this context.\nIn this work, we address the joint time slot delegation and\nsender-forwarder pair selection question in the context of sum\ngoodput maximization of the overall network. We formulate\nthe joint optimization problem as a mixed integer nonlinear\nprogramming (MINLP) problem. Using our relay selection\nwork of [5], we show that the distributed solution of the\nMINLP requires O(ktot) computational complexity and at\nmost N2 message passing where ktot and N denote the total\ntime slots and nodes in the network. The designed algorithm\nmaximizes the sum goodput of the network while preserving\nthe local goodput of the individual nodes.\nWe implement the designed algorithm in the ORBIT in-\ndoor wireless testbed. We use software defined radio enabled\nUSRP [10] nodes of the ORBIT [8] testbed. The ORBIT grid\nserves as the global coordination plane [11] to exchange the\nar\nX\niv\n:1\n21\n0.\n54\n24\nv1\n  [\ncs\n.IT\n]  \n19\n O\nct \n20\n12\n2Fig. 2. Direct Transmission and Time Exchange System Model\nprotocol information between the nodes. The data transmission\nthrough the air is processed using the GNUradio codes [12].\nPrevious works in resource exchange [2]–[7] have focused\non developing information theoretical algorithms. We design\nthe theoretical framework in accordance with the testbed\nconstraints and then implement the framework in the ORBIT\ntestbed. Our theoretical analysis has similarities to the clas-\nsical maximum weighted scheduling study of [13]. However,\nunlike [13], we consider a network where each node starts with\na fixed number of time slots and then tries to find the optimal\nnumber of time slot transfers. Our proposed framework can\nbe applied to TDMA based commerical (GSM & Edge [14],\n802.16 Wireless MAN [15]) and tactical (Joint Tactical Radio\nSystem [16] and Link16 [17]) networks.\nThis paper is organized as follows. Sec. II and III illustrate\nthe system model and design objective respectively. We solve\nthe optimization problem in Sec. IV. After describing the\nexperimental setup in Sec. V, we demonstrate the testbed\nresults in Sec. VI. We conclude the work in VII.\nII. SYSTEM MODEL\nWe consider the uplink of an N node single cell TDMA\nnetwork. Let V = {1, 2, · · · , N} denote the set of N nodes\nthat transmit data to the BS (node 0). Each node uses the\nsame bandwidth. Node i ∈ V is initially allotted kini time\nslots per second. Without loss of generality, assume one\npacket is transmitted per time slot. Each node is assumed to\nemploy a fixed modulation scheme. This assumption is based\non the testbed implementation constraint and will be further\nexplained in the experimental setup section. Due to the fixed\nmodulation and total bandwidth usage, the data transmission\nrate of each node depends on the number of allotted time slots.\nWe use packet loss probability as the channel strength\nindicator. Let Peij denote the packet loss probability in the ij\npath. Define goodput of node i by the number of packets of\nnode i that successfully reach the BS. Node i initially transmits\nthrough the direct link i0. Hence, the initial goodput of node\ni, Rini , can be found as:\nRini = Ri0 = k\nin\ni ∗ (1− Pei0) (1)\nA. Goodput Analysis in TE\nIn TE, nodes perform two hop half duplex decode and\nforward (DF) relaying. Let, SF = {SF1, · · · ,SFk} =\n{(s1, f1), (s2, f2), · · · , (sK , fK)} denote the sender-forwarder\npairing sets, i.e., the forwarder node fi relays the sender\nTABLE I\nSUMMARY OF USED NOTATIONS\nNotation Meaning\nPeij Packer error probability of the ij link\nkini Initial transmission time slot of node i\nktei Node i’s btransmission time slot in BE\nRini Initial goodput of node i\nRtei Node i’s goodput in TE\nRij Goodput in the ij link\nV Set of N nodes\nD Set of nodes that transmit without cooperation\nSF Set of sender-forwarder pairs\nSF i ith sender-forwarder pair (si, fi)\ns Sender node\nf Forwarder Node\nktot Total time slots in 1 second\nni Number of neighbouring nodes of node i\nnode si’s data, along with transmitting fi’s own data. Let\nD = d1, d2, · · · , dL denotes the direct set, i.e., the set of\nremaining nodes that transmit data without cooperation.\nThe left and right hand side of Fig. 2 shows the direct\ntransmission and TE model of an arbitrary sender-forwarder\npair (s, f). Node s and f initially receive kins and k\nin\nf time\nslots and obtain Rins and R\nin\nf goodput respectively.\nIn TE, node s transmits for ktes time slots. During k\nte\ns time\nslots, node s transmits ktes packets. Among these packets,\nthe BS and node f receive Rs0 and Rsf ‘error-free’ packets\nrespectively. Following the analysis of (1),\nRsf = k\nte\ns ∗ (1− Pesf ) , Rs0 = ktes ∗ (1− Pes0) (2)\nAssume Pesf ≤ Pes0. Hence, Rsf ≥ Rf0. Node f acts as a\nforwarder of node s and transmits for ktef time slots. During\nthese time slots, ktef ∗ (1−Pef0) packets ‘successfully’ reach\nthe BS. Assume,\nRtef +Rc = k\nte\nf ∗ (1− Pef0) (3)\nHere, Rtef denotes the number of ’error-free’ packets that\ncontain node f ’s own data. Rc represents the number of ‘error-\nfree’ packets that contain node s’s data and are forwarded by\nnode f .\nLet Rtes denote the goodput of node s in TE. Also assume\nthat when node s transmits, the packets that get ‘lost’ at node\nf (the closer node), also get ‘lost’ at node 0 (the far node).\nBased on this assumption and using the max-flow-min-cut\ntheorem [18],\nRtes ≤ min\n(\nRsf , Rs0 +Rc\n)\n(4)\nBased on this goodput analysis, we focus on the distributed\njoint optimal time slot exchange and relay selection in the sum\ngoodput maximization of a TE network.\nIII. DESIGN OBJECTIVE\nProblem I\nmax .\n∑\nd∈D\nRted +\n∑\n(s, f)∈SF\n(\nRtef +R\nte\ns ) (5a)\n3s.t. (Rtef , R\nte\ns ) ∈ conv(ktef , ktes ) ∀ (s, f) ∈ SF (5b)\nRtef ≥ Rinf , Rtes ≥ Rins ∀ (s, f) ∈ SF (5c)\nktef +k\nte\ns ≤ kinf +kins , (ktef , ktes ) ∈ Z+, ∀ (s, f) ∈ SF (5d)\nRted = k\nte\nd ∗ (1− Ped0), kted = kind , ∀d ∈ D (5e)\nD ⊆ V , SF ∈ V × V , SF i ∩ SF j = ∅ ∀i 6= j (5f)\nSF i ∩ D = ∅ ∀ i ∈ [1,K] (5g)\nSF1 ∪ SF2 · · · ∪ SFK ∪ D = V (5h)\nV ariables D,SF , Rtes , Rtef , ktes , ktef\nEquation (5e) and (1) suggest that Rted = R\nin\nd . Therefore,\nthe goodputs of the nodes in the direct set D are not optimiza-\ntion variables. Equation (5b) denotes that the goodputs of the\nforwarder and sender remain in the convex hull of the allotted\ntime slots, ktes and k\nte\nf . This convex hull is governed by (2)\nand (3). Equation (5c) ensures that the goodputs of the sender\nand the forwarder through TE don’t drop below their initial\ngoodputs. Equation (5d) shows that the total time slots used\nby the sender and forwarder are constrained by the summation\nof the initial time slots allotted to those nodes. Equation (5f)-\n(5h) denote that the direct node set and the sender-forwarder\npairs cannot have any common node and together, they form\nthe overall set V .\nProblem I is similar to the design objective of our earlier\nwork on bandwidth exchange [5]. However, we focused on\ninformation theoretic capacity [19] based resource allocation\nand relay selection in [5]. In this work, we focus on packet\nerror probability based goodput maximization, which is a\ntangible objective in an indoor wireless testbed.\nThe convex constraint of (3), the discrete time slot alloca-\ntion and the sender-forwarder pair selection objectives make\nproblem I a mixed integer nonlinear programming (MINLP)\nproblem. The solution of this MINLP involves an exponential\nnumber of variables and constraints. In the next section, we\nfocus on designing a polynomial time distributed solution of\nproblem I.\nIV. OPTIMIZATION PROBLEM SOLUTION\nLet Rtot =\n∑\ni∈V R\nin\ni denote the summation of the initial\ngoodputs of the nodes. For a fixed SF , Rtot can be expressed\nin the following form:\nRtot =\n∑\ni∈V\nRini\n=\n∑\nd∈D\nRind +\n∑\n(s, f)∈SF\n(\nRinf +R\nin\ns\n)\n(6)\n=\n∑\nd∈D\nRted +\n∑\n(s, f)∈SF\n(\nRinf +R\nin\ns\n)\n(7)\nEquation (7) uses the fact that Rted = R\nin\nd ∀ d ∈ D. Subtracting\nRtot from the objective function of I, we find the following\noptimization problem:\nProblem II\nmax .\n∑\n(s, f)∈SF\n(\nRtef +R\nte\ns −Rinf −Rins ) (8a)\ns.t. (Rtef , R\nte\ns ) ∈ conv(ktef , ktes ) ∀ (s, f) ∈ SF (8b)\nRtef ≥ Rinf , Rtes ≥ Rins ∀ (s, f) ∈ SF (8c)\nktef +k\nte\ns ≤ kinf +kins , (ktef , ktes ) ∈ Z+, ∀ (s, f) ∈ SF (8d)\nD ⊆ V , SF ∈ V × V , SF i ∩ SF j = ∅ ∀i 6= j (8e)\nSF i ∩ D = ∅ ∀ i ∈ [1,K] (8f)\nSF1 ∪ SF2 · · · ∪ SFK ∪ D = V (8g)\nV ariables D,SF , Rtes , Rtef , ktes , ktef\nThe inclusion of constant terms does not change the optimal\nvariables of an optimization problem [20]. Therefore, the\noptimal variables of both problem I and II are same. We focus\non solving problem II in the subsequent analysis and use the\noptimal variables to find the solution of problem I.\nProblem II involves both sender-forwarder pair selection and\ndiscrete time slot allocation features. For a fixed set of sender-\nforwarder pairs, the constraints in (8b)- (8d) ensure that the\ndiscrete time slot exchange in one pair does not affect the other\npairs. Hence, we now focus on an arbitrary sender-forwarder\npair (s, f) and find the discrete time slot exchange in this pair.\nA. Time Slot Allocation for a Fixed Sender Forwarder Pair\nProblem III\nmax\n(\nRtes −Rins\n)\n+\n(\nRtef −Rinf\n)\n(9a)\nRsf = k\nte\ns ∗ (1− Pesf ) , Rs0 = ktes ∗ (1− Pes0) (9b)\nRtef +Rc = k\nte\nf ∗ (1− Pef0) , Rtes ≤ min\n(\nRsf , Rs0 +Rc\n)\n(9c)\n4Rtef ≥ Rinf , Rtes ≥ Rins (9d)\nktef + k\nte\ns ≤ kinf + kins , (ktef , ktes ) ∈ Z+ , (9e)\nvariables Rc, Rsf , Rs0, R\nte\ns , R\nte\nf , k\nte\ns , k\nte\nf (9f)\nLemma 2: Problem III is concave if (ktef , k\nte\ns ) ∈ R+.\nProof: The objective function in (9a) and the constraints\nin (9b), (9c) and (9e) are linear. Minimum of linear (concave)\nfunctions is concave [20]. Hence, the constraint in (9c) is\nconvex. Thus, problem III is a concave maximization problem\nif (ktef , k\nte\ns ) ∈ R+. \u0004\nThe internal concave structure of problem III allows us to\ngenerate an upper and lower bound of problem III.\n1) Upper Bound: Let us modify problem III by relaxing\nthe integer time slot constraint, i.e., let us assume (ktef , k\nte\ns ) ∈\nR+. Let’s call it problem IV. Now, the feasible region of the\nmodified problem is a superset of that of problem III. Hence,\nthe optimal solution of the modified problem is an upper bound\nof problem III. Denote this upper bound by u0.\n2) Lower Bound: Let kte,∗s and k\nte,∗\nf denote the optimal\ntime slot solutions of problem IV. Now, if kte,∗s and k\nte,∗\nf are\nintegers, they are also the optimal solutions of problem III.\nOtherwise, convert kte,∗s and k\nte,∗\nf to the nearest integers and\nfind the corresponding goodputs. If the corresponding solution\nis feasible for problem III, it can serve as a lower bound to\nthe optimal solution of problem III. Denote this lower bound\nby l0.\n3) Computational Complexity of Problem III: Probelm III\ncan be optimally solved by searching over kins time slot\ntransfers. However, one can further reduce the complexity\nby solving the convex programming based upper and lower\nbounds of problem III. Let \u000f denote the tolerance of the\noptimal solution. If (u0− l0) ≤ \u000f, we can use the lower bound\nas the solution of problem III.\nB. Optimal Sender Forwarder Pair Selection\nThe optimal solution of problem III denotes the sender-\nforwarder pair’s goodput gain through cooperation, over non-\ncooperation. The optimal relay selection part of problem II\nis to find the set of sender-forwarder pairs that maximizes\nthe summation of the goodput gain. Now, consider a graph\nG = (V, E) where the vertices V represent the set of N\nnodes under consideration and E denotes the edges between\nthese nodes. Define the edge weight of any (i, j) pair by\nRtei +R\nte\nj −Rini −Rinj , i.e., the difference, in terms of good-\nput, between the cooperation and non-cooperation scenario.\nUsing the interference-free scheduling algorithm of [13], the\noptimal sender-forwarder set selection problem can be shown\nto be equivalent to solving the maximum weighted matching\n(MWM) algorithm [21] in the above graph. A detailed proof\nof the equivalence between the relay selection problem and the\nMWM algorithm can be found in [5]. The next section will\nillustrate the use of MWM in the optimal sender-forwarder\npair selection among 3 testbed nodes.\nFig. 3. Orbit Testbed\nThe MWM algorithm can be distributively solved using the\ndistributed local greedy MWM [22]. Distributed MWM finds\nthe pairs by selecting the locally heaviest edges and guarantees\nat least 50% performance of the optimal solution of problem\nI [22]. Our distributed TE implementation protocol is based\non the distributed MWM and will be described in the next\nsection.\nC. Distributed TE Protocol\n1) Node i ∈ V has kini transmission time slots and knows\nthe packet error probability of its direct path, Pei0,\n2) Let J be the set of neighbours of i. In a wireless\nenvironment, neighbouring nodes can hear each other.\nNode j ∈ J receives node i’s packets and can calculate\nthe inter-node packet error probability, Peij .\n3) Node i sends an omnidirectional message containing\nPei0 and kini to its neighbours.\n4) Node i solves problem III for all j ∈ J and finds the\ngoodput gain for each of its neighbours. Thus, node i\nknows its adjacent link weights.\n5) Node i picks the “candidate” node j, based on the\nheaviest adjacent link weight and sends “add” request.\n6) If node i receives an “add” request from node j, i and\nj form a cooperative pair. Node i sends “drops” request\nto its other neighbours.\n7) If node i receives a “drop” request from j, node i\nremoves the (i, j) link from its adjacent edge set. Node\ni returns to step 5.\n8) The pair selection process converges after at most N2\nmessage passings. The ‘matched’ nodes form the set of\ncooperative pairs SF . The ‘unmatched’ nodes form D\nand transmit data without cooperation.\n9) The sender node s of a cooperative pair transmits its\nown data during ktes time slots.\n10) The BS sends ACK of the ‘correctly’ received packets\nto the sender node. The forwarder node also hears these\nACK messages. Based on this information, node f finds\nthe packets of s that got ‘lost’ at the BS.\n11) Node f transmits for ktef time slots. During these slots,\nnode f forwards the ‘lost’ packets of s and then trans-\nmits its own data packets to the BS.\nD. Computational Complexity of the Distributed Algorithm\nEach node i ∈ V solves problem III for each of its\nneighbours. Let ni be the number of neighbours of node\n5Fig. 4. USRP Daughterboards\ni. Since problem III can be optimally solved using O(kini )\nsearches, each node performs O(nikini ) computations. The\ntotal number of computations in the N node network is\nO(nik\nin\ni N).\nThe number of time slots allotted to node i, kini , can be\napproximated as, kini ≈ k\ntot\nN . Since ni << k\ntot, the overall\ncomplexity is, O(ktot).\nThus, the proposed TE protocol solves an approximated\ndistributed version of problem I with O(ktot) complexity and\nat most N2 message passings.\nV. EXPERIMENTAL SETUP\nA. ORBIT Testbed & USRP Nodes\nWe implement the proposed TE based incentivized algo-\nrithm among the USRP nodes of ORBIT, an indoor wireless\ntestbed of Wireless Information Network Laboratory (WIN-\nLAB), Rutgers University. ORBIT has 400 nodes, overall, in\na 20m × 20m square grid. Fig. 3 shows a snapshot of the\nORBIT testbed.\nORBIT has 15 USRP nodes that can be used in software\ndefined radio based experiments. Fig. 4 shows the snapshots\nof two USRP daughter boards. We use the GNU radio soft-\nware toolkil [12] to run experiments in these USRP nodes.\nSpecifically, we use the benchmark-tx.py and benchmark-\nrx.py codes to transmit and receive packets between two\nUSRP nodes [12]. The flexibility of GNUradio allows us to\nchange the transmission power level and packet sizes through\nsoftware. This variable power capability of GNUradio, along\nwith the spatial separation among the nodes, allow us to create\nlinks with different strengths between different node pairs.\nAs shown in Fig. 5, we use four USRP nodes of the ORBIT\ntestbed to conduct the TE based cooperative forwarding exper-\niments. Fig. 5 also shows the spatial separation of the selected\nnodes. Here, node 1, 2 and 3 constitute the user set V and node\n0 serves as the BS. The ORBIT grid is used as a global control\nplane to exchange the control information between the nodes.\nB. Selection of Parameters\nThe benchmark-tx.py and benchmark-rx.py codes of GNU-\nradio allow the following four modulation schemes: a) GMSK,\nb) differential binary phase shift keying (DBPSK), c) differen-\ntial quadrature phase shift keying (DQPSK) and d) differential\n8 phase shift keying (D8PSK). DBPSK, DQPSK and D8PSK\nare found to be very sensitive to peak power clippings due to\ntheir variable envelope waveform. Therefore, we use a fixed\nmodulation scheme, GMSK, in our experiments.\nFig. 5. Spatial Separation of Selected Nodes\nFig. 6. Illustration of MWM in sender-forwarder pair selection\nEach node transmits at 1 Mbps and each packet contains\n1500 bytes. As a result, it takes (1500 ∗ 8)/(1 ∗ 106), i.e.,\n0.012 second to transmit one packet. We assume each time\nslot to be 0.012 second long, i.e., one packet is transmitted in\neach slot.\nThe total transmission time is assumed to be 3 second.\nEach node is initially allotted 1 second transmission time, i.e.,\n1/0.012 or 83 time slots. We approximate the number of time\nslots since fractional packet transmission is not considered.\nWe also add 32 bit CRC sequence in each packet and make\nit similar to the Ethernet packet structure [23]. Note that, we do\nnot use error control coding in these experiments. Therefore,\nthe presence of a single bit error leads to the ‘loss’ of the\nwhole packet due to CRC.\nVI. EXPERIMENTAL EVALUATION\nA. Illustration of MWM in Relay Selection\nFig. 6 shows the use of MWM in the optimal sender-\nforwarder pair selection among 3 testbed nodes. The left figure\nof the top row shows the packet loss probability between the\ninter-node pairs. These packet error probabilities were based\non 1500 byte packet length, GMSK modulation, some fixed\npower level and CRC checking. The middle figure of the top\nrow focuses on the direct transmission scenario and shows the\ngoodput (in packet/3 second) of each node. Each node initially\nreceives 83 time slots and transmits one packet at each slot\nthrough the direct path. The packet error probability in link 30\nis 0%. Therefore, all transmitted packets of node 3 reach the\nBS. Node 1 and 2’s goodputs are considerably lower due to\nthe high packet error probability in link 10 and 20 respectively.\n6Node 1 Node 2 Node 3 Total\n0\n100\n200\n300\n400\n500\n600\n700\n800\nG\noo\ndp\nut\n (k\nbp\ns)\n \n \nDirect\nTE\nFig. 7. Sum Goodput Maximization in 3 node (Packet length = 1500 bytes,\nCRC checking, GMSK modulation)\nThe top right, the bottom left and the bottom middle figure\nshow the goodput (in packet/3 second) of different sender-\nforwarder cooperation scenarios. The top right figure focuses\non the TE based cooperation between node 1 and 3. Here, node\n1 and 3 solve the two node time slot allocation optimization\nof problem III. The cooperation allows node 3 to achieve a\ngoodput of 132 packets and ensures that node 1’s goodput\ndoes not drop below 18 packets, its initial value. Therefore,\nthe overall goodput gain obtained through the cooperation of\nnode 1 and 3 is 49 packets. As a result, the 13 link of the\nMWM graph, shown in the bottom right figure, is assigned a\nweight of 49.\nThe bottom left and bottom middle figures demonstrate the\ncooperation scenario in node 1–2 and 2–3 respectively. The\nbottom right figure shows the link weight of the corresponding\ncooperation pairs. The distributed local greedy MWM selects\nlink 13. Therefore, node 1 and 3 cooperate using TE, whereas,\nnode 2 transmits without cooperation.\nB. Sum Goodput Maximization\nFig. 7 compares the sum goodput (in kilo bit per second\n(kbps)) of TE and direct path transmission. Fig. 6 shows that\nnode 1 and 3 get selected as the cooperative pair due to the\nMWM algorithm. Therefore, node 1 and 3 solve problem III\nto find the optimal time slot transfers. Due to the sum goodput\nmaximization objective, the benefits of cooperation go to node\n3, i.e., the node with the better channel. Node 3’s goodput\nincreases by 70%. The constraint of (9d) ensures that node\n1 gets its initial goodput, at least. On the other hand, node 2\ntransmits without cooperation and its goodput does not change\nfrom the initial value.\nC. Proportional Fair Maximization of Goodput\nFig. 8 compares the proportional fair maximization per-\nformance of direct transmission and TE. Here, the selected\ncooperative nodes, s and f , solve a modified version of\nproblem III. In this modified problem, s and f maximize\n(Rtes −Rins )∗ (Rtef −Rinf ) instead of maximizing (Rtes +Rtef ).\nHence, the goodput of both nodes increase due to cooperation.\nFig. 8 shows that the goodputs of node 1 and 3 increase by\n70% and 30% respectively.\nNode 1 Node 2 Node 3 Total\n0\n100\n200\n300\n400\n500\n600\n700\n800\nG\noo\ndp\nut\n (k\nbp\ns)\n \n \nDirect\nTE\nFig. 8. Proportional Fair Maximization of Goodput (Packet length = 1500\nbytes, CRC checking, GMSK modulation)\nVII. CONCLUSION\nWe designed and implemented TE based cooperative for-\nwarding among the USRP nodes in the ORBIT indoor wireless\ntestbed. We solved the joint time slot allocation and sender-\nforwarder pair selection problem in this setup. Our proposed\nalgorithm maximizes the global goodput of the network while\nensuring that no node’s goodput drops below its initial value.\nThe ORBIT grid is used as a global control plane to exchange\nthe control information between the USRP nodes. Experimen-\ntal results suggest that resource delegation based cooperative\nforwarding can significantly improve the sum goodput and\nproportional fair goodput performance of the network.\nThe use of adaptive modulation and signal to noise ratio\nbased resource allocation in testbed implementation remains\nan area of future research.\nVIII. ACKNOWLEDGEMENTS\nThis work is supported by the Office of Naval Research\nunder grant N00014-11-1-0132. We thank Kush Patel, Sid\nParadkar and Hakim Ergaibi for their assistance in GNUradio\ncoding and testbed implementation.\nREFERENCES\n[1] J. N. Laneman, D. N. C. Tse, and G. Wornell, “Cooperative diversity\nin wireless networks : efficient protocols and outage behavior,” IEEE\nTrans. Info. Theory, vol. 50(12), pp. 3062–3080, Dec. 2004.\n[2] D. Zhang, R. Shinkuma, and N. B. Mandayam, “Bandwidth exchange:\nAn energy conserving incentive mechanism for cooperation,” IEEE\nTrans. Wireless Comm, vol. 9(6), pp. 2055–2065, June 2010.\n[3] H. Xu and B. Li, “Efficient resource allocation with flexible channel\ncooperation in OFDMA cognitive radio networks,” in Proc. IEEE\nINFOCOM’2010, Mar. 2010, pp. 1–9.\n[4] Z. Han, T. Himson, W. P. Siriwongpairat, and K. J. Ray Liu, “Resource\nallocation for multiuser cooperative ofdm networks: Who helps whom\nand how to cooperate,” IEEE Trans. Vehicular Technology, vol. 58(5),\npp. 2378–2391, June 2009.\n[5] M. N. Islam, N. B. Mandayam, and S. Kompella, “Optimal re-\nsource allocation and relay selection in bandwidth exchange based\ncooperative forwarding,” To Appear in WiOpt 2012, Jan 2012,\nhttp://arxiv.org/abs/1112.5767.\n[6] J. Zhang and Q. Zhang, “Stackelberg game for utility-based cooperative\ncognitive radio networks,” in Proc. ACM MOBIHOC’2009, May 2009,\npp. 23–31.\n[7] M. Lindstorm and P. Lungaro, “Resource delegation and rewards to\nstimulate forwarding in multihop cellular networks,” in Proc. IEEE\nVTC’2005, June 2005.\n[8] “ORBIT: Open access research testbed for next-generation wireless\nnetworks,” accessed April 2012, http://www.orbit-lab.org.\n7[9] Y. Zhao, R. Adve, and T. J. Lim, “Improving amplify-and-forward\nrelay networks: Optimal power allocation versus selection,” IEEE Trans.\nWireless Comm, vol. 6(8), pp. 3114–3123, Aug. 2007.\n[10] “ETTUS research,” accessed April 2012, http://www.ettus.com.\n[11] D. Raychaudhuri, N. B. Mandayam, J. B. Evans, B. J. Ewy, S. Seshan,\nand P. Steenkiste, “CogNet: an architectural foundation for experi-\nmental cognitive radio networks within the future internet,” in Proc.\nMobiArch’2006, Dec. 2006.\n[12] “GNU Radio Website,” accessed April 2012, http://www.gnuradio.org.\n[13] L. Tassiulas and A. Ephremides, “Stability properties of constrained\nqueing systems and scheduling for maximum throughput in multihop\nradio networks,” IEEE Trans. Automatic Control, vol. 37(12), pp. 1936–\n1949, Dec. 1992.\n[14] A. Goldsmith, Wireless Communications, Cambridge University Press,\nNew York, NY, 2005.\n[15] C. Eklund, R. B. Marks, K. L. Stanwood, and S. Wang, “IEEE standard\n802.16: a technical overview of the WirelessMAN/sup TM/ air interface\nfor broadband wireless access,” IEEE Communications Magazine, vol.\n40, pp. 98–107, June 2002.\n[16] C. D. Young, “The mobile data link (MDL) of the joint tactical radio\nsystem wideband networking waveform,” in Proc. IEEE MILCOM 2007,\nOct. 2006.\n[17] Air Land Sea Application Center, Fortmonroe, VA, Introduction to\nTactical Digital Information Link J and Quick Reference Guide, 2000.\n[18] T. Cover and H. El Gamal, “Capacity theorems for the relay channel,”\nIEEE Trans. Info. Theory, vol. 25(5), pp. 572–584, Sept. 1979.\n[19] T. M. Cover and J. A. Thomas, Elements of Information Theory, John\nWiley and Sons, Hoboken, NJ, 2005.\n[20] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge\nUniversity, Cambridge, UK, 2004.\n[21] J. Edmonds, “Paths, trees and flowers,” Canadian Journal of Mathe-\nmatics, vol. 17, pp. 449–467, 1965.\n[22] R. Preis, “Linear time 1/2-approximation algorithm for maximum\nweighted matching in general graphs,” in General Graphs, Symposium\non Theoretical Aspects of Computer Science, STACS 99. 1998, pp. 259–\n269, Springer.\n[23] “Ethernet frame,” accessed April 2012,\nhttp://www.infocellar.com/networks/ethernet/frame.htmg.\n",
            "id": 17018909,
            "identifiers": [
                {
                    "identifier": "10.1109/milcom.2012.6415742",
                    "type": "DOI"
                },
                {
                    "identifier": "2040223704",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "24773538",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "1210.5424",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "oai:arxiv.org:1210.5424",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "192384701",
                    "type": "CORE_ID"
                }
            ],
            "title": "Implementation of Distributed Time Exchange Based Cooperative Forwarding",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2040223704",
            "oaiIds": [
                "oai:arxiv.org:1210.5424"
            ],
            "publishedDate": "2012-10-19T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://www.winlab.rutgers.edu/%7Enarayan/PAPERS/MILCOM_2012_Bandwidth_Exchange_Implementation.pdf",
                "http://arxiv.org/abs/1210.5424"
            ],
            "updatedDate": "2021-07-22T07:59:09",
            "yearPublished": 2012,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1210.5424"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/17018909"
                }
            ]
        },
        {
            "acceptedDate": "2012-11-13T00:00:00",
            "arxivId": "1211.4090",
            "authors": [
                {
                    "name": "Kleijn, Jetty"
                },
                {
                    "name": "Koutny, Maciej"
                },
                {
                    "name": "Pietkiewicz-Koutny, Marta"
                },
                {
                    "name": "Rozenberg, Grzegorz"
                }
            ],
            "contributors": [],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/26276310"
            ],
            "createdDate": "2014-10-23T17:20:30",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 645,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/645",
                    "logo": "https://api.core.ac.uk/data-providers/645/logo"
                }
            ],
            "depositedDate": "2012-11-15T00:00:00",
            "abstract": "Automated synthesis from behavioural specifications is an attractive and\npowerful way of constructing concurrent systems. Here we focus on the problem\nof synthesising a membrane system from a behavioural specification given in the\nform of a transition system which specifies the desired state space of the\nsystem to be constructed. We demonstrate how a Petri net solution to this\nproblem, based on the notion of region of a transition system, yields a method\nof automated synthesis of membrane systems from state spaces.Comment: In Proceedings MeCBIC 2012, arXiv:1211.347",
            "documentType": "research",
            "doi": "10.4204/eptcs.100.1",
            "downloadUrl": "http://arxiv.org/abs/1211.4090",
            "fieldOfStudy": "computer science",
            "fullText": "Gabriel Ciobanu (Ed.): Membrane Computing and\nBiologically Inspired Process Calculi (MeCBIC 2012)\nEPTCS 100, 2012, pp. 1–13, doi:10.4204/EPTCS.100.1\nc© J.Kleijn, M.Koutny, M.Pietkiewicz-Koutny & G.Rozenberg\nThis work is licensed under the\nCreative Commons Attribution License.\nMembrane Systems and Petri Net Synthesis\n(Invited Paper)\nJetty Kleijn\nLIACS\nLeiden University\nLeiden, The Netherlands\nkleijn@liacs.nl\nMaciej Koutny\nSchool of Computing Science\nNewcastle University\nNewcastle upon Tyne, UK\nmaciej.koutny@ncl.ac.uk\nMarta Pietkiewicz-Koutny\nSchool of Computing Science\nNewcastle University\nNewcastle upon Tyne, UK\nmarta.koutny@ncl.ac.uk\nGrzegorz Rozenberg\nLIACS\nLeiden University\nLeiden, The Netherlands\nand\nDepartment of Computer Science\nUniversity of Colorado at Boulder\nBoulder, Colorado, USA\nrozenber@liacs.nl\nAutomated synthesis from behavioural specifications is an attractive and powerful way of construct-\ning concurrent systems. Here we focus on the problem of synthesising a membrane system from a\nbehavioural specification given in the form of a transition system which specifies the desired state\nspace of the system to be constructed. We demonstrate how a Petri net solution to this problem, based\non the notion of region of a transition system, yields a method of automated synthesis of membrane\nsystems from state spaces.\n1 Introduction\nMembrane systems ([19, 20, 21, 22]) are a computational model inspired by the functioning of living\ncells and their architecture and in particular, the way chemical reactions take place in cells divided by\nmembranes into compartments. The reactions are abstracted to rules that specify which and how many\nmolecules can be produced from given molecules of a certain kind and quantity. As a result, membrane\nsystems are essentially multiset rewriting systems. The dynamic aspects of the membrane system model\nincluding potential behaviour (computations), derive from such evolution rules.\nPetri nets (see, e.g., [5, 23, 25]) are a well-established general model for distributed computation with\nan extensive range of tools and methods for construction, analysis, and verification of concurrent systems.\nTheir diverse applications areas include computational and operational foundations for problems and\nissues arising in biology; see for example, [15], for a recent comprehensive overview of applications of\nPetri nets in systems biology.\nThere are intrinsic similarities between Petri nets and membrane systems. In particular, there exists\na canonical way of translating membrane systems into Petri nets. This translation is faithful in the\nsense that it relates computation steps at the lowest level and induces in a natural way (sometimes new)\nextensions and interpretations of Petri net structure and behaviour (e.g., inhibitor arcs, localities, and\nmaximal concurrency). More details on the relationship between Petri nets and membrane systems can\nbe found in, e.g., [9, 14].\n2 Membrane Systems and Petri Net Synthesis\nThe strong semantical link between the two models invites to extend where necessary and possible\nexisting Petri net techniques and bring them to the domain of membrane systems. An example is the\nprocess semantics of Petri nets that can help to understand the dynamics and causality in the biological\nevolutions represented by membrane systems [8, 12]. In this paper, we focus on the synthesis problem,\nthat is, the problem of automated construction of a system from a specification of its (observed or desired)\nbehaviour.\nAutomated synthesis from behavioural specifications is an attractive and powerful way of construct-\ning correct concurrent systems [1, 2, 4, 6, 7, 18, 24]. Here we will re-visit the problem of synthesising a\nPetri net from a behavioural specification given in the form of a transition system. The latter specifies the\ndesired state space of the Petri net to be constructed. We will recall a solution to this problem based on\nthe notion of region of a transition system. We will then demonstrate how this solution leads to a method\nof automated synthesis of basic membrane systems from state spaces. We also discuss how the proposed\nmethod could be extended to cope with more complicated kinds of membrane systems.\n2 Preliminaries\nMultisets. A multiset over a finite set X is a function θ : X → N= {0,1,2, . . .}. θ may be represented\nby listing its elements with repetitions, e.g., θ = {y,y,z} is such that θ(y) = 2, θ(z) = 1, and θ(x) = 0\notherwise. θ is said to be empty (and denoted by ∅) if there are no x such that x ∈ θ by which we mean\nthat x ∈ X and θ(x) ≥ 1.\nFor two multisets θ and θ ′ over X , the sum θ +θ ′ is the multiset given by (θ +θ ′)(x) = θ(x)+θ ′(x)\nfor all x ∈ X , and for k ∈N the multiset k ·θ is given by (k ·θ)(x) = k ·θ(x) for all x ∈ X . The difference\nθ − θ ′ is given by (θ − θ ′)(x) = max{θ(x)− θ ′(x),0} for all x ∈ X . We denote θ ≤ θ ′ whenever\nθ(x) ≤ θ ′(x) for all x ∈ X , and θ < θ ′ whenever θ ≤ θ ′ and θ 6= θ ′. The restriction θ |Z of θ to a subset\nZ ⊆ X is given by θ |Z(x) = θ(x) for x ∈ Z, and θ |Z(x) = 0 otherwise. The size |θ | of θ is given by\n∑x∈X θ(x). If f : X →Y is a function then f (θ) is the multiset over Y such that f (θ)(y) = ∑x∈ f−1(y) θ(x),\nfor every y ∈Y .\nStep transition systems. A step transition system over a finite set (of actions) A is a triple TS =\n(Q,A ,q0), where: Q is a set of nodes called states; A is the set of arcs, each arc being a triple (q,α ,q′)\nsuch that q,q′ ∈ Q are states and α is a multiset over A; and q0 ∈ Q is the initial state. We may write\nq α−→ q′ whenever (q,α ,q′) is an arc, and denote by\ntsStepsq = {α | α 6=∅ ∧ ∃q′ : q\nα\n−→ q′}\nthe set of nonempty steps enabled at a state q in TS. We additionally assume that:\n• if q α−→ q′ and q α−→ q′′ then q′ = q′′ (i.e., TS is deterministic);\n• for every state q ∈ Q, there is a path from q0 leading to q;\n• for every action a ∈ A, there is an arc q α−→ q′ in TS such that a ∈ α ; and\n• for every state q ∈ Q, we have q ∅−→ q′ iff q = q′.\nLet TS = (Q,A ,q0) be a step transition system over a set of actions A, and TS′ = (Q′,A ′,q′0) be\na step transition system over a set of actions A′. TS and TS′ are isomorphic if there are two bijections,\nφ : A → A′ and ν : Q → Q′, such that ν(q0) = q′0 and, for all states q,q′ ∈ Q and multisets α over A:\n(q,α ,q′) ∈A ⇐⇒ (ν(q),φ(α),ν(q′)) ∈A ′ .\nJ.Kleijn, M.Koutny, M.Pietkiewicz-Koutny & G.Rozenberg 3\nWe denote this by TS ∼φ ,ν TS′ or TS∼ TS′.\nPetri nets. A Place/Transition net (or PT-net) is specified as a tuple PT = (P,T,W,M0), where: P and\nT are finite disjoint sets of respectively places and transitions; W : (T ×P)∪ (P× T )→ N is the arc\nweight function; and M0 : P → N is the initial marking (in general, any multiset of places is a marking).\nWe assume that, for each transition t, there is at least one place p such that W (p, t) > 0. In diagrams,\nsuch as that in Figure 1, places are drawn as circles, and transitions as boxes. If W (x,y)≥ 1, then (x,y) is\nan arc leading from x to y. An arc is annotated with its weight if the latter is greater than one. A marking\nM is represented by drawing in each place p exactly M(p) tokens (small black dots).\npa1 p\nb\n1 p\nc\n1\nt\nr13\n1\nt\nr11\n1\nt\nr12\n1\npa2 p\nb\n2\npc2\nt\nr21\n2\nt\nr22\n2\npa3\npb3 p\nc\n3\nt\nr31\n3\n2\nFigure 1: A PT-net.\nA step U of PT is a multiset of transitions. Its pre-multiset and post-multiset of places, •U and U•,\nare respectively given by\n•U(p) = ∑\nt∈U\nU(t) ·W (p, t) and U•(p) = ∑\nt∈U\nU(t) ·W (t, p) ,\nfor each place p. For the PT-net in Figure 1 we have:\n•{tr111 ,t\nr11\n1 ,t\nr31\n3 }= {p\nb\n1,p\nb\n1,p\na\n3} and {t\nr11\n1 ,t\nr11\n1 ,t\nr31\n3 }\n•\n= {pa1,p\na\n1,p\nc\n1,p\na\n3,p\na\n3,p\nc\n3}.\nWe distinguish two basic modes of execution of PT-nets. To start with, a step of transitions U is\nfree-enabled at a marking M if •U ≤ M. We denote this by M[U〉free, and then say that a free-enabled U\nis max-enabled at M if U cannot be extended by a transition to yield a step which is free-enabled at M,\ni.e., there is no t ∈ T such that M[U + {t}〉free. We denote this by M[U〉max. In other words, U is free-\nenabled at M if in each place there are sufficiently many tokens for the specified multiple occurrence of\neach of its transitions. Maximal concurrency (max-enabledness) means that extending U would demand\nmore tokens than M supplies. For the PT-net in Figure 1 we have that, at the given marking M0, the step\n{tr121 ,t\nr21\n2 } is free-enabled but not max-enabled, and {t\nr11\n1 ,t\nr12\n1 ,t\nr21\n2 ,t\nr22\n2 } is max-enabled.\nFor each mode of execution m ∈ {free,max}, a step U which is m-enabled at a marking M can be\nm-executed leading to the marking M′ given by M′ = M− •U +U•. We denote this by M[U〉mM′. For\nthe PT-net in Figure 1 we have\nM0[{tr121 ,t\nr21\n2 }〉free{p\nb\n1,p\nb\n1,p\nb\n2,p\nb\n2,p\nc\n2,p\nc\n2,p\na\n3} .\n4 Membrane Systems and Petri Net Synthesis\nPetri nets with localities. PT-nets are a general model of concurrent computation. To capture the com-\npartmentisation of membrane systems, [12] adds explicit localities to transitions. Though not necessary\nfrom a modelling point of view, we associate in this paper — only for notational convenience — also\neach place with a locality.\nA PT-net with localities (or PTL-net) is a tuple PTL = (P,T,W, ℓ,M0) such that (P,T,W,M0) is a PT-\nnet, and ℓ is a location mapping for the transitions and places. Whenever ℓ(x) = ℓ(z), we call x and z\nco-located. In diagrams, nodes representing co-located transitions and/or places will be shaded in the\nsame way, as shown in Figure 2.\npa1 p\nb\n1 p\nc\n1\nt\nr13\n1\nt\nr11\n1\nt\nr12\n1\npa2 p\nb\n2\npc2\ntr212\nt\nr22\n2\npa3\npb3 p\nc\n3\nt\nr31\n3\n2\nFigure 2: A PTL-net corresponding to a basic membrane system, where ℓ(xzi ) = i, for each node of the\nform xzi . Note that, e.g., transitions t\nr11\n1 , t\nr12\n1 and t\nr13\n1 are co-located.\nCo-locating transitions leads to one more way of enabling for steps of transitions. We say that a\nstep U of PTL is lmax-enabled at a marking M if M[U〉free and U cannot be extended by a transition\nco-located with a transition in U to yield a step which is free-enabled at M; i.e., there is no t ∈ T such\nthat ℓ(t) ∈ ℓ(U) and M[U +{t}〉free. We denote this by M[U〉lmax, and then denote the lmax-execution of\nU by M[U〉lmaxM′, where M′ = M− •U +U•. Note that locally maximal (lmax) concurrency is similar to\nmaximal concurrency, but now only active localities1 cannot execute further transitions. For the PTL-net\nin Figure 2 we have that {tr111 ,t\nr12\n1 } is lmax-enabled at the given marking, but {t\nr11\n1 } is not.\nLet m ∈ {free,max, lmax} be a mode of execution of a PTL-net PTL. Then an m-step sequence is\na finite sequence of m-executions starting from the initial marking, and an m-reachable marking is any\nmarking resulting from the execution of such a sequence. Moreover, the m-concurrent reachability graph\nof PTL is the step transition system:\nCRGm(PTL) =\n(\n[M0〉m ,\n{\n(M,U,M′) | M ∈ [M0〉m ∧ M[U〉mM′\n}\n, M0\n)\n,\nwhere [M0〉m is the set of all m-reachable markings which are the nodes of the graph; M0 is the initial\nnode; and the arcs between the nodes are labelled by m-executed steps of transitions. Concurrent reacha-\nbility graphs provide complete representations of the dynamic behaviour of PTL-nets evolving according\nto the chosen mode of execution.\n1 By active localities of a step U we mean the localities of transitions present in U .\nJ.Kleijn, M.Koutny, M.Pietkiewicz-Koutny & G.Rozenberg 5\n1\n2 3\n1\n2 3\nFigure 3: A membrane structure (m = 3) and its compartments with 1 being the root node, (1,2) ∈ µ\nand 1 = parent(3).\nMembrane structures. A membrane structure µ (of degree m ≥ 1) is given by a rooted tree with m\nnodes identified with the integers 1, . . . ,m. We will write (i, j) ∈ µ or i = parent( j) to indicate that there\nis an edge from i (parent) to j (child) in the tree of µ , and i ∈ µ means that i is a node of µ . The nodes of\na membrane structure represent nested membranes which in turn determine compartments (compartment\ni is enclosed by membrane i and lies in-between i and its children, if any), as shown in Figure 3.\nWe will say that a PTL-net PTL = (P,T,W, ℓ,M0) is spanned over the membrane structure µ if ℓ :\nP∪T → µ and the following hold, for all p ∈ P and t ∈ T :\n• if W (p, t) > 0 then ℓ(p) = ℓ(t); and\n• if W (t, p) > 0 then ℓ(p) = ℓ(t) or (ℓ(p), ℓ(t)) ∈ µ or (ℓ(t), ℓ(p)) ∈ µ .\nThe PTL-net of Figure 2 is spanned over the membrane structure depicted in Figure 3.\nBasic membrane systems. Let V be a finite alphabet of names of objects (or molecules) and let µ be\na membrane structure of degree m. A basic membrane system (over V and µ) is a tuple\nBMS = (V,µ ,w01, . . . ,w0m,R1, . . . ,Rm)\nsuch that, for every membrane i, w0i is a multiset of objects from V , and Ri is a finite set of evolution\nrules associated with membrane (compartment) i. Each evolution rule r ∈Ri is of the form r : lhsr → rhsr,\nwhere lhsr (the left hand side of r) is a nonempty multiset over V , and rhsr (the right hand side of r) is a\nmultiset over\nV ∪{aout | a ∈V}∪{ain j | a ∈V and (i, j) ∈ µ} .\nHere a symbol ain j represents an object a that is sent to a child node (compartment) j and aout means that\na is sent to the parent node. If i is the root of µ then no indexed object of the form aout belongs to rhsr.\nA configuration of BMS is a tuple\nC = (w1, . . . ,wm)\nof multisets of objects, and C0 = (w01, . . . ,w0m) is the initial configuration. Figure 4 shows a basic mem-\nbrane system over the membrane structure depicted in Figure 3.\nA membrane system evolves from configuration to configuration as a consequence of the application\nof evolution rules. There are different execution modes ranging from fully synchronous — as many\napplications of rules as possible — to sequential — a single application of a rule at a time. Here, similarly\nas in the case of PTL-nets, we distinguish three modes, all based on the notion of a vector multi-rule.\n6 Membrane Systems and Petri Net Synthesis\n1\n2 3\n{a,b}\nr11 : {b}→ {a}\nr12 : {a}→ {b,cin2 ,ain3}\nr13 : {b}→ {c,ain3}\n{a,b,c,c}\nr21 : {a,c}→ {b}\nr22 : {b}→ {a}\n∅\nr31 : {a}→ {a,a,c,cout}\nFigure 4: Basic membrane system BMS0.\nA vector multi-rule of BMS is a tuple r = 〈r1, . . . ,rm〉 where, for each membrane i of µ , ri is a\nmultiset of rules from Ri. For such a vector multi-rule, we denote by lhsri the multiset\n∑\nr∈Ri\nri(r) · lhsr\nin which all objects in the left hand sides of the rules in ri are accumulated, and by rhsri the multiset\n∑\nr∈Ri\nri(r) · rhsr\nof all (indexed) objects in the right hand sides. The first multiset specifies how many objects are needed\nin each compartment for the simultaneous execution of all the instances of evolution rules in r.\nA vector multi-rule r of BMS is\n• free-enabled at a configuration C if lhsri ≤ wi, for each i.\nMoreover, a free-enabled vector multi-rule r = 〈r1, . . . ,rm〉 is:\n• max-enabled if no ri can be extended to a vector multi-rule which is free-enabled at C; and\n• lmax-enabled if no nonempty ri can be extended to a vector multi-rule which is free-enabled at C.\nFor example, in Figure 4,\n• 〈∅,∅,{r31}〉 is not free-enabled;\n• 〈{r11,r12},∅,∅〉 is lmax-enabled but not max-enabled; and\n• 〈{r11,r12},{r21,r22},∅〉 is max-enabled.\nIf r is free-enabled (free) at a configuration C, then C has in each membrane i enough copies of\nobjects for the application of the multiset of evolution rules ri. Maximal concurrency (max) requires that\nadding any extra rule makes r demand more objects than C can provide. Locally maximal concurrency\n(lmax) is similar but in this case only those compartments which have rules in r cannot enable any more\nrules; in other words, each compartment either uses no rule, or uses a maximal multiset of rules.\nJ.Kleijn, M.Koutny, M.Pietkiewicz-Koutny & G.Rozenberg 7\nThe effect of the rules is independent of the mode of execution m ∈ {free,max, lmax}. A vector\nmulti-rule r which is m-enabled at C can m-evolve to a configuration C′ = (w′1, . . .w′m) such that, for each\ni and object a:\nw′i(a) = wi(a)− lhsri (a)+ rhsri (a)+ rhsrparent(i)(aini)+ ∑\ni=parent( j)\nrhsrj(aout)\nwhere rhsrparent(i) =∅ if i is the root of µ . We denote this by C\nr\n−→m C′. Moreover, an m-computation is\na finite sequence of m-evolutions starting from the initial configuration; any configuration which can be\nobtained through such a computation is called m-reachable. For the basic membrane system depicted in\nFigure 4 we have, for example:\nC0\n〈{r11,r12},∅,∅〉\n−−−−−−−−−→lmax ({a,b},{a,b,c,c,c},{a})\n〈∅,{r21,r22},∅〉\n−−−−−−−−−→lmax ({a,b},{a,b,c,c},{a}) .\nLet m ∈ {free,max, lmax} be a mode of execution of a basic membrane system BMS. Then the\nm-concurrent reachability graph of BMS is given by:\nCRGm(BMS) =\n(\n[C0〉m ,\n{\n(C,r1 + . . .+ rm,C′) |C ∈ [C0〉m ∧ C\n〈r1,...,rm〉\n−−−−−→m C′\n}\n, C0\n)\n,\nwhere [C0〉m is the set of all m-reachable configurations which are the nodes of the graph; C0 is the initial\nnode; and the arcs between the nodes are labelled by multisets of evolution rules involved in the m-\nexecuted vector multi-rules.2 Similarly as in the case of PTL-nets, concurrent reachability graphs capture\ncompletely the dynamic behaviour of basic membrane system evolving according to the chosen mode of\nexecution.\n3 Membrane Systems and Petri Nets\nThere is a natural way of translating a basic membrane system BMS = (V,µ ,w01, . . . ,w0m,R1, . . . ,Rm) over\na membrane structure µ into a behaviourally equivalent PTL-net PTL(BMS) = (P,T,W, ℓ,M0) spanned\nover the same membrane structure. In the constructed net, places represent objects present inside com-\npartments, and transitions represent evolution rules. Both places and transitions are associated with\nmembranes and this information is represented by the location mapping.\nThe constructed PTL-net PTL(BMS) has a separate place paj with ℓ(paj) = j, for each object a and\nmembrane j, and a separate transition tri with ℓ(tri ) = i, for each rule r in compartment i.\nThe initial marking inserts w0j(a) tokens into each place paj . The connectivity between transition\nt = tri and place p = paj is given by:\nW (p, t) =\n{\nlhsr(a) if i = j\n0 otherwise ,\nas well as:\nW (t, p) =\n\n\nrhsr(a) if i = j\nrhsr(aout) if j = parent(i)\nrhsr(ain j) if i = parent( j)\n0 otherwise .\n2 Though it may be that rules from different membranes are the same in terms of the multisets defining their left hand and\nright hand sides, we assume here that evolution rules associated with different membranes can be distinguished, e.g., by giving\nthem each their own name (an injective label).\n8 Membrane Systems and Petri Net Synthesis\nFigure 2 shows the result of the above translation for the basic membrane system in Figure 4. Note that\nit immediately follows from the construction that the PTL-net PTL(BMS) is spanned over µ .\nThe PTL-net PTL(BMS) provides a faithful representation of the behaviour of the basic membrane\nsystem BMS. To capture this very close relationship, we define two bijective mappings, ν and ρ , which\nallow us to move between BMS and PTL(BMS):\n• for every marking M of PTL(BMS), ν(M) = (w1, . . . ,wm) is the configuration of BMS given by\nwi(a) = M(pai ), for every object a and every i.\n• for every step U of PTL(BMS), ρ(U) = 〈r1, . . . ,rm〉 is the vector multi-rule of BMS given by\nri(r) =U(tri ), for every rule r ∈ Ri and every i.\nIt is then possible to establish a direct relationship between (the operation of) the original membrane\nsystem and the PTL-net resulting from the above translation at the system level:\nC r−→m C′ =⇒ ν−1(C) [ρ−1(r)〉m ν−1(C′)\nM[U〉mM′ =⇒ ν(M)\nρ(U)\n−→m ν(M′)\n(1)\nfor all modes of execution m∈{free,max, lmax}, configurations C of BMS and markings M of PTL(BMS).\nTogether with ν(M0) =C0, this result means that the m-step sequences of PTL(BMS) faithfully represent\nm-computations of BMS, and the same applies to markings and configurations. Crucially, we obtain\nTheorem 1 For each m ∈ {free,max, lmax},\nCRGm(PTL(BMS))∼φ ,ν CRGm(BMS) ,\nwhere the mapping ν is defined as above, and φ(tri ) = r, for every transition tri of PTL(BMS).\nThe above theorem captures the very tight behavioural correspondence between BMS and PTL(BMS),\nallowing to apply analytical techniques developed for Petri nets in the analysis of membrane systems.\nFor example, one can employ the invariant analysis based on linear algebra [26], or use the causality se-\nmantics approach of Petri nets based on occurrence nets, as first outlined in [12]. In this paper, we show\nhow techniques used to synthesise Petri nets could be employed in order to construct basic membrane\nsystems from their intended behaviours as represented by step transition systems. First, however, we\nprovide a translation from PTL-nets spanned over membrane structures to basic membrane systems.\nLet PTL = (P,T,W, ℓ,M0) be a PTL-net spanned over a membrane structure µ . For such a PTL-net,\nwe construct the corresponding basic membrane system BMS(PTL) over µ in the following way:\n• P is the set of objects;\n• the initial configuration is ν ′(M0) where, for every marking M of PTL,\nν ′(M) = (M|ℓ−1(1)∩P, . . . ,M|ℓ−1(m)∩P) ;\n• each transition t ∈ T with t• = {p1, . . . , pk} has a corresponding evolution rule φ ′(t) of the form\nt : •t →{a1, . . . ,ak} where, for i = 1, . . . ,k,\nai =\n\n\npi if ℓ(pi) = ℓ(t)\npiout if ℓ(pi) = parent(ℓ(t))\npiinℓ(pi) if ℓ(t) = parent(ℓ(p\ni))\nJ.Kleijn, M.Koutny, M.Pietkiewicz-Koutny & G.Rozenberg 9\n• for each membrane i ∈ µ , the set of evolution rules is given by Ri = {φ ′(t) | ℓ(t) = i}.\nAgain, the translation results in a very close behavioural correspondence.\nTheorem 2 For each m ∈ {free,max, lmax},\nCRGm(PTL)∼φ ′,ν ′ CRGm(BMS(PTL)) ,\nwhere the mappings φ ′ and ν ′ are defined as above.\nIt follows from Theorems 1 and 2 that the problem of synthesis of basic membrane systems from\nstep transition systems is equivalent to the problem of synthesis of PTL-nets spanned over membrane\nstructures. It therefore suffices to solve the latter, and in the next section we describe a solution based on\nthe notion of a region of a step transition system.\n4 Synthesising nets corresponding to membrane systems\nThe Petri net synthesis problem we consider is formulated as follows.\nProblem 1 Given are a finite set T , a membrane structure µ , a mapping ℓ : T → µ , m∈{free,max, lmax},\nand TS = (Q,A ,q0) which is a finite step transition system over T .\nConstruct a PTL-net PTL = (P,T, ℓ,M0) spanned over µ such that CRGm(PTL)∼ TS, and ℓ is an exten-\nsion of the mapping defined for T .\nAs demonstrated in [4], synthesis problems like Problem 1 can be solved using techniques coming\nfrom the theory of regions of transition systems (see, e.g., [1, 7, 18]). Intuitively, a region represents a\nsingle place in a hypothetical net generating the given transition system. Regions are used both to check\nwhether a net satisfying the conditions can be constructed and, if the answer turns out to be positive, to\nconstruct such net.\nIn this particular case, a region of the step transition system TS consists of three mappings\nreg =\n(\nσ : Q → N , ı : T → N , ω : T → N ) (2)\nsuch that, for every arc q α−→ q′ of TS,\nσ(q)≥ ω(α) and σ(q′) = σ(q)−ω(α)+ ı(α) . (3)\nHere ω(α) = ∑t∈T α(t) ·ω(t) and similarly ı(α) = ∑t∈T α(t) · ı(t). In a region of the form (2) repre-\nsenting a place p, σ(q) is the number of tokens in p in the marking corresponding to the node q, ω(t)\nrepresents the weight of the arc from p to transition t, and ı(t) represents the weight of the arc from t\nto p. It is then natural to require in (3) that p contains enough tokens not to block a step α executed at q,\nand also to ensure that the number of tokens in p before and after executing α is consistent with the total\narc weight of the step α in relation to p.\nIn the case of Problem 1, one also needs to take into account the fact that the target PTL-net must\nbe spanned over µ . This imposes additional constraints on allowed regions (places) and the location\nmapping ℓ. We call a region reg as in (2) with a location ℓ(reg) ∈ µ compatible with the membrane\nstructure µ if the following hold, for every t ∈ T :\n• if ω(t)> 0 then ℓ(t) = ℓ(reg); and\n• if ı(t)> 0 then ℓ(t) = ℓ(reg) or (ℓ(t), ℓ(reg)) ∈ µ or (ℓ(reg), ℓ(t)) ∈ µ .\n10 Membrane Systems and Petri Net Synthesis\nThe set of all such regions will be denoted by Pµ . Note that if reg is such that ω(t) > 0, for at least one\nt ∈ T , then ℓ(reg) is uniquely determined; otherwise we always choose ℓ(reg) to be the membrane which\nis higher up in the tree structure of µ than any other suitable candidate. As a result, we can leave ℓ(reg)\nimplicit.\nFinally, Problem 1 should be feasible in the sense that the transition system TS can be realised by a\nsuitable net. There are two necessary and sufficient conditions for realisability (see [4, 17]):\n• state separation: for every pair of distinct states of the transition system there is a region (a marked\nplace) distinguishing between them; and\n• forward closure: there are sufficiently many places defined by regions of the transition system to\ndisallow steps not present in the transition system.\nFirst we describe how all places can be found that potentially provide a solution to Problem 1; in other\nwords, all the regions (2) of the transition system TS which are compatible with µ .\nFinding compatible regions. Let T , µ , ℓ : T → µ and TS = (Q,A ,q0) be as in Problem 1. Assume\nthat Q = {q0, . . . ,qh} and T = {t1, . . . , tn}. We use three vectors of non-negative variables:\nx = x0 . . .xh y = y1 . . .yn z = z1 . . . zn .\nWe also denote p = xyz and define a homogeneous linear system\nP :\n{\nxi ≥ α · z\nx j = xi +α · (y− z)\nfor all qi\nα\n−→ q j in TS\nwhere α · z denotes α(t1) · z1 + · · ·+α(tn) · zn and similarly for α · (y− z).\nThe regions (2) of TS are then determined by the integer solutions p of the system P assuming that,\nfor 0 ≤ i ≤ h and 1 ≤ j ≤ n,\nσ(qi) = xi ı(t j) = y j ω(t j) = z j\nThe set of rational solutions of P forms a polyhedral cone in Qh+2n+1. As described in [3], one\ncan effectively compute finitely many integer generating rays p1, . . . ,pk of this cone such that any in-\nteger solution p of P can be expressed as a linear combination of the rays with non-negative rational\ncoefficients:\np =\nk\n∑\nl=1\ncl ·pl .\nSuch rays pl are fixed and (some of them) turned into net places if Problem 1 has a solution. More\nprecisely, if pl is included in the constructed net, then\nM0(pl) = xl0 W (pl, ti) = zli W (ti,pl) = yli , (4)\nwhere M0 is the initial marking of the target net, and ti ∈ T .\nClearly, not all such rays can be considered for the inclusion in the net being constructed, as the\ncorresponding regions have to be compatible with µ . We therefore ensure through a simple check that\nthe generating rays p1, . . . ,pk are compatible with µ , deleting in the process those which are not. Note\nthat any p ∈ Pµ is a non-negative linear combination of rays compatible with µ .\nHaving found the generating rays compatible with µ , we proceed to check whether Problem 1 has\nany solutions at all.\nJ.Kleijn, M.Koutny, M.Pietkiewicz-Koutny & G.Rozenberg 11\nChecking state separation. Let TS = (Q,A ,q0) be as in Problem 1. We take in turn each pair of dis-\ntinct states, qi and q j, of Q and decide whether there exists p = (σ , ı,ω) ∈ Pµ with coefficients c1, . . . ,ck\nsuch that σ(qi) = xi 6= x j = σ(q j). Since the latter is equivalent to\nk\n∑\nl=1\ncl · x\nl\ni 6=\nk\n∑\nl=1\ncl · x\nl\nj ,\none can simply check whether there exists at least one pl (called a witness [6]) such that xli 6= xlj.\nChecking forward closure. Again, let TS = (Q,A ,q0) be as in Problem 1, and m ∈ {free,max, lmax}.\nFirst, we take in turn each state qi of Q, and calculate the set of region enabled steps, denoted by\nregStepsqi . Intuitively, region enabled steps are those that cannot be disabled (or blocked) by compatible\nregions.\nTo build regStepsqi one only needs to consider nonempty steps α with |α | ≤ m ·Max, where Max is\nthe maximum size of steps labelling arcs in TS, and m is the number of membranes of µ . The reason is\nthat, for each membrane i ∈ µ there exists a compatible region (σ , ı,ω) ∈ Pµ (called a witness) such that\nσ(Q) = {Max} and, for every t ∈ T ,\nω(t) = ι(t) =\n{\n1 if ℓ(t) = i\n0 otherwise .\nTaken together, all such regions block any step α with |α |> m ·Max.\nFor each nonempty step α with |α | ≤ m ·Max it is the case that α /∈ regStepsqi iff for some p ∈ Pµ\nwith coefficients c1, . . . ,ck we have xi < α · z. Since the latter is equivalent to\nk\n∑\nl=1\ncl · (x\nl\ni −α · z\nl)< 0 ,\none simply checks whether there exists at least one pl (again called a witness) such that xli −α · zl < 0.\nHaving determined the region enabled steps, in order to establish forward closure we need to verify\nthat, for every state q ∈ Q,\ntsStepsq =\n\n\nregStepsq if m= free\n{α ∈ regStepsq | ¬∃t ∈ T : α +{t} ∈ regStepsq} if m= max\n{α ∈ regStepsq | ¬∃t ∈ T : α +{t} ∈ regStepsq ∧ ℓ(t) ∈ ℓ(α)} if m= lmax .\nConstructing the solution net. If the above checks for the feasibility of Problem 1 are successful, one\ncan construct a solution PTL-net spanned over µ by taking all the witness rays and regions, and treating\nthem as places in the way indicated in (4). The resulting net PTL satisfies\nCRGm(PTL)∼ TS .\n5 Concluding remarks\nWe have described how one can adapt a solution to the Petri net synthesis problem based on regions of\nstep transition systems, so that the resulting method can be used to construct basic membrane systems\n12 Membrane Systems and Petri Net Synthesis\nwith a specific behaviour. Moreover, there are other synthesis results developed for Petri nets which can\nbe employed to extend the proposed solution in several directions, two of which are briefly mentioned\nbelow.\nIn Problem 1 it is assumed that the association of transitions with membranes is given. This can\nbe relaxed and one can aim at synthesising membrane systems without such an association, or even\nwithout being given a membrane structure (in such a case, the synthesis procedure should construct a\nmembrane structure as well). For such a modification, there already exist results which can be used to\ndevelop a solution. More precisely, the method of ‘discovering’ localities in [17] works for PTL-nets\nwith localised conflicts (where transitions which share an input place are co-located). Since all PTL-nets\nspanned over membrane structures have localised conflicts, the result in [17] can be adapted to work for\nbasic membrane systems.\nEvolution rules of membrane systems are often equipped with promoters and inhibitors. Both fea-\ntures have direct counterparts in Petri nets in the form of activator and inhibitor arcs, and suitable transla-\ntions between membrane systems and Petri nets can be developed as described in [9, 14]. Moreover, the\nsynthesis technique based on regions of step transition systems works also for PTL-nets extended with\nactivator and inhibitor arcs [16]. In fact, there is a general setting of so-called τ-nets and corresponding\nτ-regions [1, 4]. Here the parameter τ is a general and convenient way of capturing different types of\nconnections (arcs and their combinations) between places and transitions, removing the need to re-state\nand re-prove the key results every time a new kind arcs is introduced. Note that the recently introduced\nSET-nets [13, 14] (with qualitative rather than quantitative resource management) and set membrane\nsystems [10] can be treated within the general theory of τ-net synthesis based on regions of transition\nsystems [11].\nAcknowledgements\nThis paper is based on an invited talk presented at the 6th Workshop on Membrane Computing and\nBiologically Inspired Process Calculi (MECBIC), 8th September 2012, Newcastle upon Tyne, United\nKingdom. The reported research was supported by the EPSRC GAELS project.\nReferences\n[1] E.Badouel and Ph.Darondeau: Theory of Regions. In: Reisig, W., Rozenberg, G. (eds.): Lectures on Petri\nNets I: Basic Models, Advances in Petri Nets. Lecture Notes in Computer Science 1491. Springer-Verlag,\nBerlin Heidelberg New York (1998) 529–586, doi:10.1007/3-540-65306-6_22\n[2] L.Bernardinello: Synthesis of Net Systems In: Marsan, M.A. (ed.): Application and Theory of Petri Nets\n1993. Lecture Notes in Computer Science 691. Springer-Verlag, Berlin Heidelberg New York (1993) 89–\n105, doi:10.1007/3-540-56863-8_42\n[3] N.Chernikova: Algorithm for Finding a General Formula for the Non-negative Solutions of a System of\nLinear Inequalities. USSR Computational Mathematics and Mathematical Physics 5 (1965) 228–233\n[4] P.Darondeau, M.Koutny, M.Pietkiewicz-Koutny and A.Yakovlev: Synthesis of Nets with Step Firing Policies.\nFundamenta Informaticae 94 (2009) 275–303\n[5] J.Desel and G.Juhas: What Is a Petri Net? Lecture Notes in Computer Science 2128, Springer-Verlag (2001)\n1–25, doi:10.1007/3-540-45541-8_1\n[6] J.Desel and W.Reisig: The Synthesis Problem of Petri Nets. Acta Informatica 33 (1996) 297–315, doi:10.\n1007/s002360050046\nJ.Kleijn, M.Koutny, M.Pietkiewicz-Koutny & G.Rozenberg 13\n[7] A.Ehrenfeucht and G.Rozenberg: Partial 2-structures; Part I: Basic Notions and the Representation Prob-\nlem, and Part II: State Spaces of Concurrent Systems. Acta Informatica 27 (1990) 315–368, doi:10.1007/\nBF00264611, doi:10.1007/BF00264612\n[8] J.Kleijn and M.Koutny: Processes of Membrane Systems with Promoters and Inhibitors. Theoretical Com-\nputer Science 404 (2008) 112–126, doi:10.1016/j.tcs.2008.04.006\n[9] J.Kleijn and M.Koutny: Petri Nets and Membrane Computing. In: [22] (2010) 389–412\n[10] J.Kleijn and M.Koutny: Membrane Systems with Qualitative Evolution Rules. Fundamenta Informaticae 110\n(2011) 217–230, doi:10.3233/FI-2011-539\n[11] J.Kleijn, M.Koutny, M.Pietkiewicz-Koutny and G.Rozenberg: Step Semantics of Boolean Nets. Acta Infor-\nmatica (2012), doi:10.1007/s00236-012-0170-2\n[12] J.Kleijn, M.Koutny and G.Rozenberg: Process Semantics for Membrane Systems. Journal of Automata,\nLanguages and Combinatorics 11 (2006) 321–340\n[13] J.Kleijn, M.Koutny and G.Rozenberg: Modelling Reaction Systems with Petri Nets. BioPPN 2011, CEUR\nWorkshop Proceedings, 724 2011 36–52\n[14] J.Kleijn, M.Koutny and G.Rozenberg: Petri Nets for Biologically Motivated Computing. Scientific Annals\nof Computer Science 21 (2011) 199–225\n[15] I.Koch, W.Reisig and F.Schreiber: Modeling in Systems Biology — The Petri Net Approach. Springer Verlag\n(2010)\n[16] M.Koutny and M.Pietkiewicz-Koutny: Synthesis of Elementary Net Systems with Context Arcs and Locali-\nties. Fundamenta Informaticae 88 (2008) 307–328\n[17] M.Koutny and M.Pietkiewicz-Koutny: Synthesis of Petri Nets with Localities. Scientific Annals of Computer\nScience 19 (2009) 1–23\n[18] M.Mukund: Petri Nets and Step Transition Systems. International Journal of Foundations of Computer Sci-\nence 3 (1992) 443–478, doi:10.1142/S0129054192000231\n[19] G.Pa˘un: Computing with Membranes. J. Comput. Syst. Sci. 61 (2000) 108–143, doi:10.1006/jcss.1999.\n1693\n[20] G.Pa˘un: Membrane Computing, An Introduction. Springer-Verlag, Berlin Heidelberg New York (2002)\n[21] G.Pa˘un and G.Rozenberg: A Guide to Membrane Computing. Theoretical Computer Science 287 (2002)\n73–100, doi:10.1016/S0304-3975(02)00136-6\n[22] G.Pa˘un, G.Rozenberg and A.Salomaa: The Oxford Handbook of Membrane Computing. Oxford University\nPress (2010)\n[23] C.A.Petri: Kommunikation mit Automaten. PhD Thesis (1962)\n[24] M.Pietkiewicz-Koutny: The Synthesis Problem for Elementary Net Systems with Inhibitor Arcs. Fundamenta\nInformaticae 40 (1999) 251–283\n[25] W.Reisig and G.Rozenberg (eds.): Lectures on Petri Nets. Lecture Notes in Computer Science 1491,1492\n(1998), doi:10.1007/3-540-65306-6_19\n[26] M.Silva, E.Teruel and J.M.Colom: Linear Algebraic and Linear Programming Techniques for the Analysis\nof Place/Transition Net Systems. Lecture Notes in Computer Science, Springer-Verlag 1491 (1998) 309–373,\ndoi:10.1007/3-540-65306-6_19\n",
            "id": 17024506,
            "identifiers": [
                {
                    "identifier": "10.4204/eptcs.100.1",
                    "type": "DOI"
                },
                {
                    "identifier": "2114365801",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "1211.4090",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "26276310",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:arxiv.org:1211.4090",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "24780656",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:doaj.org/article:a337ca5476a74311b1ec1bf855320ef2",
                    "type": "OAI_ID"
                }
            ],
            "title": "Membrane Systems and Petri Net Synthesis",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2114365801",
            "oaiIds": [
                "oai:doaj.org/article:a337ca5476a74311b1ec1bf855320ef2",
                "oai:arxiv.org:1211.4090"
            ],
            "publishedDate": "2012-11-01T00:00:00",
            "publisher": "'Open Publishing Association'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://doaj.org/search?source=%7B%22query%22%3A%7B%22bool%22%3A%7B%22must%22%3A%5B%7B%22term%22%3A%7B%22id%22%3A%22a337ca5476a74311b1ec1bf855320ef2%22%7D%7D%5D%7D%7D%7D",
                "http://arxiv.org/abs/1211.4090"
            ],
            "updatedDate": "2021-06-27T02:26:33",
            "yearPublished": 2012,
            "journals": [
                {
                    "title": "Electronic Proceedings in Theoretical Computer Science",
                    "identifiers": [
                        "2075-2180"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1211.4090"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/17024506"
                }
            ]
        },
        {
            "acceptedDate": "2013-10-10T00:00:00",
            "arxivId": "1312.4477",
            "authors": [
                {
                    "name": "Al-Naymat, Ghazi"
                }
            ],
            "contributors": [
                "Ghazi"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/210360150"
            ],
            "createdDate": "2014-10-24T19:23:08",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2013-05-01T00:00:00",
            "abstract": "Recent research on pattern discovery has progressed from mining frequent\npatterns and sequences to mining structured patterns, such as trees and graphs.\nGraphs as general data structure can model complex relations among data with\nwide applications in web exploration and social networks. However, the process\nof mining large graph patterns is a challenge due to the existence of large\nnumber of subgraphs. In this paper, we aim to mine only frequent complete graph\npatterns. A graph g in a database is complete if every pair of distinct\nvertices is connected by a unique edge. Grid Complete Graph (GCG) is a mining\nalgorithm developed to explore interesting pruning techniques to extract\nmaximal complete graphs from large spatial dataset existing in Sloan Digital\nSky Survey (SDSS) data. Using a divide and conquer strategy, GCG shows high\nefficiency especially in the presence of large number of patterns. In this\npaper, we describe GCG that can mine not only simple co-location spatial\npatterns but also complex ones. To the best of our knowledge, this is the first\nalgorithm used to exploit the extraction of maximal complete graphs in the\nprocess of mining complex co-location patterns in large spatial dataset.Comment: 1",
            "documentType": "research",
            "doi": "10.1109/aiccsa.2013.6616417",
            "downloadUrl": "http://arxiv.org/abs/1312.4477",
            "fieldOfStudy": "computer science",
            "fullText": "1GCG: Mining Maximal Complete Graph Patterns\nfrom Large Spatial Data\nGhazi Al-Naymat College of Computer Science and Information Technology\nUniversity of Dammam, KSA\nghalnaymat@ud.edu.sa\nAbstract\nRecent research on pattern discovery has progressed from mining frequent patterns and sequences to mining structured patterns,\nsuch as trees and graphs. Graphs as general data structure can model complex relations among data with wide applications in web\nexploration and social networks. However, the process of mining large graph patterns is a challenge due to the existence of large\nnumber of subgraphs. In this paper, we aim to mine only frequent complete graph patterns. A graph g in a database is complete\nif every pair of distinct vertices is connected by a unique edge. Grid Complete Graph (GCG) is a mining algorithm developed to\nexplore interesting pruning techniques to extract maximal complete graphs from large spatial dataset existing in Sloan Digital Sky\nSurvey (SDSS) data. Using a divide and conquer strategy, GCG shows high efficiency especially in the presence of large number\nof patterns. In this paper, we describe GCG that can mine not only simple co-location spatial patterns but also complex ones. To\nthe best of our knowledge, this is the first algorithm used to exploit the extraction of maximal complete graphs in the process of\nmining complex co-location patterns in large spatial dataset.\nI. INTRODUCTION\nWith the rapid invention of advanced technology, researchers have been collecting large amounts of data on a continuous or\nperiodic basis in many fields. This data becomes the potential for researchers to discover useful information and knowledge that\nhas not been seen before. In order to process this data and extract useful information, the data needs to be organised in a suitable\nformat. Hence data preparation plays a very important role in the data mining process.\nThe focus of this study is mainly on extracting interesting complex patterns from Sloan Digital Sky Survey (SDSS) astronomy\ndataset [1]. The SDSS is the most motivated astronomical survey project ever undertaken. The survey maps in detail one-quarter\nof the entire sky, determining the positions and absolute brightness of more than 100 million celestial objects. The first official\nData Release (DR1) of SDSS was in June 2003. Since then there have been many new releases including the ninth major release\n(DR9) in August 2012 that provides images, imaging catalogs, spectra, and redshift. Release DR9 contains more than 5TB of\ndata, which includes measures of 500 million unique celestial objects.\nAvailability of such large amount of useful data is an obvious opportunity for application of data mining techniques to extract\ninteresting information. However, while much research has been done by the astronomical researchers, a feeble effort has been\nmade to apply data mining techniques on SDSS data. That is because the SDSS data format is not suitable for mining purposes,\nthat is the main motivation of this paper.\nAs mentioned in [2] spatial databases store spatial attributes about objects, and hence, SDSS is a large spatial dataset as it\ncontains many attributes for each object. One of the most significant problems in spatial data mining is to find object types\nthat frequently co-locate with each other in large databases. Co-location means objects that are found in the neighborhood of\neach other. The proposed approach mines co-location patterns in SDSS data and uses these patterns to generate interesting\ninformation about different types of galaxies. In this work only the galaxies existing in SDSS is used. However, this approach\ncould be generalised to be used with any other celestial objects.\nThe data preparation plays a vital rolein the mining process, hence it is done in two folds. First, extracting the galaxies in\nSDSS data and categorising them into “Early” and “Late” type galaxies. Second, our proposed algorithm GCG is utilised to\ngenerate co-location patterns (maximal complete graphs) from the data. A complete graph is any set of spatial objects such that\nall objects in the set co-locate. A maximal complete graph is a complete graph which is not a subset of any other complete graph.\nFig. 1 depicts some examples of spatial co-locations, the line between the vertices (objects) indicates that they are co-located.\nThe second column of Table I displays the maximal complete graph patterns, which are presented in Fig. 1.\nThe full general problem of extracting maximal complete graphs from a graph is known as NP-Hard. GCG efficiently extracts\nmaximal complete graphs in a given spatial database with capability to divide the space into grid structure based on a predefined\ndistance. A divide and conquer strategy is applied via a grid structure to reduce the search space.\nIn this paper we focus on using maximal complete graphs to allow us to mine interesting complex spatial relationships\nbetween the object types. A complex spatial relationship includes not only whether an object type, say A, is present in a\n(maximal) complete graph, but also:\n• Whether more than one object of its type is present in the maximal complete graph. This is called a positive type and is\ndenoted by A+.\nar\nX\niv\n:1\n31\n2.\n44\n77\nv1\n  [\ncs\n.D\nB]\n  1\n3 D\nec\n 20\n13\nFig. 1. Graphs in a plane as examples of the co-location patterns\nTABLE I. MAXIMAL COMPLETE GRAPH PATTERNS\nID Maximal Complete Graphs Transactions\n1 {C1,D1} {C,D}\n2 {C5,D7} {C,D}\n3 {B1,C5} {B,C}\n4 {A4,D2,D5} {A,D+}\n5 {A5,C3,D3} {A,C,D}\n6 {A5,D3,D6} {A,D+}\n7 {A2,B2,D4} {A,B,D}\n8 {A1,A3,C2,C4,C6} {A+,C+}\n• Whether objects of a particular type are not present in a maximal complete graph – that is, the absence of types. This is\ncalled a negative type and is denoted by −A.\nThe inclusion of positive and / or negative types makes a relationship complex. This allows us to mine patterns that say, for\nexample, that A occurs with multiple B’s but not with a C. That is, the presence of A may imply the presence of multiple B’s\nand the absence of C. This is interesting in the astronomy domain. The last column of Table I shows examples of (maximal)\ncomplex relationships.\nMaximal complete graphs generated by GCG can be represented as transactions as given in Column 3 of Table I. These\ntransactions can be used by ANY association rule mining technique. Association rule mining techniques that proposed by [3],\n[4] can generate useful rules, which will be interpreted as relationships between objects.\nWe are not interested in maximal complex patterns (relationships) in themselves, as they provide only local information (that\nis, about a maximal complete graph). We are however interested in sets of object types (including complex types), that appear\nacross the entire dataset (that is, amongst many maximal complete graphs). In other words, we are interested in mining interesting\ncomplex spatial relationships (sets), where “interesting” is defined by a global measure. We use a variation of the minPI [12]\nmeasure to define interestingness.\nA. Problem Statement\nGiven the set of maximal complete graphs, find all interesting complex patterns that occur amongst the set of maximal\ncomplete graphs. More specifically, find all sets of object types, including positive and negative (that is, complex) types\nthat are interesting as defined by their Support being above a threshold.\nThis problem therefore becomes an itemset mining task. In order to do this very quickly, we use interesting itemset mining\nalgorithm, that is GLIMIT [5].\nIncluding negative types makes the problem much more difficult, as it is typical for spatial data to be sparse. This means that\nthe absence of a type can be very common.\nB. Contributions\nIn this paper we make the following contributions:\n• An efficient algorithm called GCG is proposed to generate all maximal complete graph patterns that exist in large spatial\ndatasets.\n• We introduce the concept of maximal complete graph. We demonstrate how the use of maximal complete graphs makes\nmore sense than using complete graphs, and we showed that they allow the use of negative patterns.\nGlobal: Mines patterns that are  globally \ninteresting (considers all complete graphs)  \n \nMine All \nMaximal \nComplete \nGraph \nSpatial \nData \n(SDSS) \n \n \n \n \nExtract \nComplex \n(Relationships) \n \n \n \nMine \nInteresting  \n \nA,B,B,B \nB,C \nA,A,B \n… \nA,B,B+,-C \n-A,B,C \nA,A+,B,-C \n… \nA1  (5,2) \nA2  (4,0) \nB4  (6,0) \n… \nLocal: only members of the maximal \ncomplete graphs are considered. \nAny \nFrequent \nPattern \nMining \nAlgorithm \n \n \nFrequent \nComplete \nGraphs \nGCG \nAlgorithm \nPrepare  \nthe  \nExtracted \nData \nFig. 2. Framework showing the complete mining process.\n• We show that complex and interesting co-location patterns can be efficiently extracted from huge and sparse spatial datasets.\nThe rest of the paper is organized as follows: Section II gives further details of our approach. Section III contains our\nexperiments and an analysis of the results. Section IV puts our contributions in context of related work, then we conclude in\nSection V.\nII. MAXIMAL COMPLETE GRAPH MINING\nFigure 2 shows the overall process flow of our method. The follwoing subsections elaborate more about our approach.\nA. Data Extraction and Categorisation\nRaw data needs most of the time to be prepared to suit data mining algorithms. This section illustrates the method of extracting\nthe important attributes from the SDSS database. These attributes used to categorise galaxy objects. A view called SpecPhoto\nwhich is derived from a table called SpecPhotoAll is used. The latter is a joined table between the PhotoObjAll and SpecObjAll\ntables. In other words, SpecPhoto is view of joined Spectro and PhotoObjects that have the clean spectra1.\nThe concern was to extract only the galaxy objects from the SDSS using parameter (object type=0). The total number of\ngalaxy-type objects stored in the SDSS catalog is more thant 507,594. However, to ensure the accuracy for calculating the\ndistance between objects and the earth which leads to calculate the X,Y, and Z coordinates for each object, some parameters\nare used, such as zConf < 0.95 (the rigid objects) and zWarning = 0 (correct RedShift). Therefore, the number of objects is\nreduced to (442,923).\nSDSS release 9 provides a table called Neighbors. This table contains all objects that are located within 0.5 arcmins, this makes\nit not useful in this study because there is no ability to choose any distance that would form the neighborhood relation between\nobjects. For example, in our experiments (1, · · · , 5) mega-parsec (distances) are used as the thresholds to check whether objects\nare close to each other or not. Table II discloses the extracted fields from the SDSS (DR9) that used during the preparation\nprocess.\nData extraction: The data was obtained from SDSS (DR9) [6]. This data is extracted from the online catalog services using\nseveral SQL statements and tools, which offered by the catalog. These tools are accessible from the SDSS site2.\n1http://www.sdss.org/\n2http://cas.sdss.org/dr5/en/tools/search/sql.asp\nTABLE II. THE SDSS SCHEMA\nNo Field name Field description\n1. specObjID Unique ID\n2. z Final RedShift\n3. ra Right ascention\n4. dec Declination\n5. cx x of Normal unit vector\n6. cy y of Normal unit vector\n7. cz z of Normal unit vector\n8. primTarget prime target categories\n9. objType object type : Galaxy =0\n10. modelMag u Ultraviolet magniutde\n11. modelMag r Red Light magnitude\nData transformation: The data obtained from the previous step is transformed to identify the categories of the glaxies and\nto represent the data in the right format.\nNew attributes creation: With all necessary fields, this step is to calculate the exact value of the X,Y, and Z coordinates\nwhich are not explicitly shown in the SDSS data. First , the distance D between objects and the earth is calculated using\nHubble’s law and the value of z for each object as in Equation ( 1 ) . Second, by considering the unit vectors cx, cy, and cz,\nand multiplying them by the D, the value of X,Y and Z coordinates are calculated by Equations 2, 3, and 4, respectively.\nD ≈ c× z\nHo\n(1)\nwhere c is the speed of light, z is the object RedShift, and Ho is Hubbles’ constant. Currently the best estimate for this\nconstant is 71 kms−1Mpc−1 [7], [8].\nX = D × cx (2)\nY = D × cy (3)\nZ = D × cz (4)\nGalaxies Categorisation: Different parameters were used to categorise galaxy types. Based on the difference between\nUltraviolet U and Red light magnitude R, galaxies are categorised as either “Early”’ or “Late”’. If the difference is greater\nthan or equal to 2.22 the galaxy is “Early”’, otherwise it is “Late”’. The value of the r-band Petrosian magnitude indicates\nwhether the galaxy is “Main”’ (close to the earth) or “Luminous Red Galaxies” (LRG). That is by checking the value of r-band.\nIf r-band ≤ 17.77, that indicates that the object is “Main” galaxy otherwise it is “LRG” [9]. The four galaxy types that found\nare Main-Late, Main-Early, LRG-Late, and LRG-Early.\nB. Basic Definitions and Concepts\nThis section briefly defines the concepts that are used in this paper.\nConsider a set of objects O with fixed locations. Given an appropriate distance measure d : O×O → R we can define a graph\nG as follows; let O be the vertices and construct an edge between two objects o1 ∈ O and o2 ∈ O if d(o1, o2) ≤ τ , where τ is\na chosen distance. A co-location pattern is a connected subgraph.\nDefinition 1 (Complete Graph): A Complete Graph g ∈ O is any fully connected subgraph of G. That is, d(o1, o2) ≤\nτ ∀{o1, o2} ∈ g × g.\nFor example, in Fig. 1, {A4,D2,D5} form a complete graph as each object co-locates with each other. Similarly {C5,D7}\nform another complete graph.\nAs we have mentioned in Section I we use maximal complete graphs so that we can define and use complex patterns\nmeaningfully and to avoid double counting.\nDefinition 2 (Maximal Complete Graph): A maximal complete graph GM is a complete graph that is not a subset (sub-graph)\nof any other complete graph.\nIn Fig. 1, {A4,D2,D5} form a maximal complete graph as it is not a subset of another complete graph. However, {A2,D4}\nis not a maximal complete graph since it is a subset of the complete graph {A2,B2,D4}.\nThe mining of maximal complete graphs is done directly – it does not require mining all sub-complete graphs first.\nDefinition 3 (Complete Graph’s Cardinality): It is the number of vertices in a complete graph, that is |O|. In other words, it\nis the size of the complete graph. This value can be used to find the total number of edges E (Equation 5) that the complete\ngraph can have. The below equation shows that.\nE =\n|O|(|O| − 1)\n2\n, (5)\nwhere |O| is the number of vertices.\nC. Mining Maximal Complete Graphs\nFirst, data preperation process starts a maximal complete graph mining algorithm to extract all maximal complete graphs, and\nstrips them of the object identifiers (producing raw maximal complete craphs as shown in Table VI. One pass is then made\nover the raw maximal complete graphs in order to extract complex relationships. We describe this in Section II-F. This produces\ncomplex maximal complete graphs. Each of these complex maximal complete graphs is then considered as a transaction T , and\nan interesting itemset mining algorithm, using minPI as the interestingness measure, is used to extract the interesting complex\nrelationships.\nIn itemset mining, the dataset consists of a set of transactions T , where each transaction t ∈ T is a subset of a set of items\nI; that is, t ⊆ I . In our work, the set of complex maximal complete graphs (relationships) becomes the set of transactions T\n(third column in Table I). The items are the object types – including the complex types such as A+ and −A. For example,\nif the object types are {A,B,C}, and each of these types is present and absent in at least one maximal complete graph, then\nI = {A,A+,−A,B,B+,−B}. An interesting itemset mining algorithm mines T for interesting itemsets. The support of an\nitemset I ′ ⊆ I is the number of transactions containing the itemset: support(I ′) = |{t ∈ T : I ′ ⊆ t}|. So called frequent itemset\nmining uses the support as the measure of interestingness. For reasons described in Section I we use minPI [10] which, under\nthe mapping described above, is equivalent to\nminPI(I ′) = min\ni∈I′\n{support(I ′)/support({i})}\nSince minPI is anti-monotonic, we can easily prune the search space for interesting patterns. We adopted the method used in\nGLIMIT ([5], [11]) to mine the interesting patterns from maximal complete graphs. GLIMIT is a very fast and efficient itemset\nmining algorithm that has been shown to outperform Apriori like algorithms [12] and FP-Growth [13].\nAs shown in Fig. 2, the complete graph generation and complex relationship extraction are local procedures, in the sense that\nthey deal only with individual maximal complete graphs. In contrast, the interesting pattern mining is global – it finds patterns\nthat occur across the entire space. Secondly, we consider subsets of maximal complete graphs only in the last step – after the\ncomplex patterns have been extracted.\nTABLE III. EXAMPLE: DATASET OF TWO DIMENSIONS.\nObject type X-Coordinate Y-Coordinate\nA1 2.5 4.5\nA2 6 4\nA3 2 9\nB1 1.5 3.5\nB2 5 3\nB3 5 4\nC1 2.5 3\nC2 6 3\nD1 3 9\nD2 7 1.5\nD. GCG algorithm\nAlgorithm 1 reveals the pseudocode of the GCG algorithm. This section shows how the algorithm works through an example.\nBy assuming that all objects are spatial, we use Fig. 3 to depict some example items and their locations. These objects and their\ncoordinates are given in Table III. It should be noted that SDSS is three dimensional dataset, but in the example two dimensions\nare used for the sake of simplicity.\nEdges E in each subgraph are formed by calculating the distance between adjacent objects. In other word, if the distance\nbetween them ≤ τ , the edge will be created. Each subgraph, in this context, forms a co-location pattern. Therefore, results of\nthis algorithm are patterns containing objects that are co-located. GCG algorithm (1) functionality is described as follows:\n1) Lines 1 - 12: Dividing the space into a grid structure and concurrently placing each point into its particular grid cell\nbased on its coordinates (Fig. 3). The size of the grid cell is d× d, where d = τ . The value of τ is given as one of the\ninputs for the GCG algorithm.\nAlgorithm 1 Grid Complete Graph algorithm.\nInput: Set of points (P1, · · · , Pn), Threshold τ\nOutput: A list of maximal Complete Graph patterns. {Generating grid structure.}\n1: GridMap← φ\n2: PointList← {P1, · · · , Pn}\n3: for all Pi ∈ PointList do\n4: Get the coordinates of each point Pkx, Pky, Pkz\n5: Generate the composite key (GridKey=(Pkx, Pky, Pkz)).\n6: if GridKey ∈ GridMap then\n7: GridMap← Pi\n8: else\n9: GridMap← new GridKey\n10: GridMap.GridKey ← Pi\n11: end if\n12: end for\n{Obtaining the neighborhood lists.}\n13: for all pi ∈ GridMap do\n14: pi.list← φ\n15: NeighborGrids← (the 27 neighbor cells of pi)\n16: NeighborList← φ\n17: if NeighborGridsi.size() > 1 then\n18: for all pj ∈ NeighborGridsj do\n19: if EucDist (pi, pj) ≤ τ then\n20: pi.list← pj (pi, pj are neighbors)\n21: end if\n22: end for\n23: end if\n24: NeighborList← pi.list\n25: end for\n{Pruning neighborhood list if at least one of its items violates the maximal Complete Graph definition.}\n26: TempList← φ\n27: MCompleteGraphList← φ\n28: for all Recordi ∈ NeighborList do\n29: RecordItems← Recordi\n30: for all pi ∈ RecordItems do\n31: for all pj ∈ RecordItems do\n32: if EucDist(pi, pj) ≤ τ then\n33: Templist← pj (pi, pj are neighbors)\n34: end if\n35: end for\n36: end for\n37: MCompleteGraphList← Templist\n38: end for\nTABLE IV. NEIGHBOR LISTS.\nLists Members\n1 {A1, B1, C1}\n2* {B1, A1, C1}\n3* {C1, A1, B1}\n4 {D1, A3}\n5 {A2, B2, C2, B3}\n6* {B2, A2, B3, C2}\n7** {C2, A2, B2, B3, D2}\n8 {D2, C2}\n9* {A3, D1}\n10* {B3, C2, B2, A2}\nTABLE V. NEIGHBOR LISTS AFTER THE PRUNING STEP.\nLists 1 4 5 8\nMembers {A1,B1,C1} {D1,A3} {A2,B2,C2,B3} {D2,C2}\n2) Lines 13 - 25: Finding each object’s neighborhood lists. This step is the most important one, and it is the most crucial step\nfor the complexity issue. It uses the Euclidean distance technique to check the neighborhood relationship between objects.\n02\n4\n6\n8\n10\n0 2 4 6 8\nA3 D1\nA2 B3\nB2 C2\nD2\nA1\nB1\nC1\nC1 Grid\nFig. 3. Spatial objects in a 2D (x and y coordinates) grid structure.\nHowever, the number of checked spatial objects depends on the density of the grid and the content of the neighbor cells3.\nAccording to the example in Fig. 3, also because the sample data contains 10 objects, a list for each object is created\nexcept for those objects that are located lonely. Our concern is to find co-location patterns that have number of members\n≥ 2 (i.e. |O| ≥ 2); because one object does not form any type of relationship. Consequently, no need to count objects\nthat do not have connections (i.e. relationship) with at least one another object. However, in our example all objects share\nrelationships. For example, object {A1} has a relationship with {B1,C1} and object {A2} with {B2, B3, C2}. It can be\nseen that these objects share the same location, this means {A1, B1, C1} are co-located because the distance between\nthem is ≤ τ . Table IV shows some redundant lists – marked by * – (same objects in different order); this gives us the\nchance to prune the complete list without losing the objects as they present in another list.\n3) Lines 26 - 38: Pruning any neighbor list that contains at least one object violating the co-location condition. For example,\nlist 7 is pruned because two of its members {A2,D2} are not close to each other as given in Table IV (lists marked by\n**).\nAs a result of the previous steps, list of maximal complete graphs will be formed. For example, {A1, B1, C1} forms a maximal\ncomplete graph and so forth for lists (4, 5, 8) as shown in Table V.\nE. GCG algorithm analysis\nThis section discusses the GCG algorithm completeness, correctness, and complexity.\nCompleteness: All objects in neighbor lists appear as set or subset in maximal complete graph lists. After acquiring the entire\nneighbors for each point, another check among these neighbors is done to assure that all points are neighbors to each other.\nIntuitively, doing that results to have repeated neighbor lists. Therefore, this ensures finding all maximal complete graphs in any\ngiven graph.\n\t\r  \nA\n\t\r  \n\t\r  \nB\n\t\r  \n\t\r  \nD\n\t\r  \n\t\r  \nC\n\t\r  \nUndirected\t\r  Graph\t\r   Neighborhood\t\r  \t\r  \nList\t\r  \nMaximal\t\r  Complete\t\r  \nGraph\t\r  \nFig. 4. Example of two maximal complete graphs used to show the correctness of the proposed algorithm.\n3Number of neighbor cells is 9 or 27 if the data is 2D or 3D, respectively.\nCorrectness: Every subset of a maximal complete graph appears in the neighbors list. Thus, all maximal complete graphs\nthat appear in maximal complete graph’s list will not be found as a subset in another maximal complete graph. That is, the\ndefinition of maximal complete graph. Fig. 4 displays an undirect graph and the neighborhood list and the existed maximal\ncomplete graph patterns. It is very clear that the pair {A,D} does not appear in the neighborhood list, because the distance\nbetween d(A,D) > τ (i.e. no edge between them). As a result, the pair {A,D} will not be included in the maximal complete\ngraphs’ list. In other words, any subset of any maximal complete graph appears in the neighborhood list and it will not appear\nas an independent maximal complete graph. By this, the correctness of the proposed algorithm is shown.\nComplexity: Assume there is N points and c cells in a gird, and assume that all points are uniformly distributed. Hence, on\naverage there is N/c points per cell. Also, assume each cell has l neighbors. Then to create the neighborhood list of one point\nl(N/c) points need to be examined to check if they are within distance τ . Since the total number of points is N , thus the cost\nis O(N2l/c). And since c >> l, an assumption, that this part of the algorithm is sub-quadratic, can be stated. Second, pruning\nneighborhood lists assuming that on average the length of each neighborhood list is k. Then for each neighborhood list, k other\nlists have to be examined to check if a point is in others neighborhood list or not. Therefore, for each point, k other neighborhood\nlists are examined as well as within each one, up to k points will be checked. Consequently, the cost is O(N(k2)). Finally, the\ntotal cost is the cost to put the points in cell (O (N)), the cost to create the neighborhood lists O(N2l/c), and the cost to prune\nthe lists O(N(k2)). The total complexity of the algorithm is O(N(Nl/c+ k2 + 1)).\nFig. 5. Complete graph example used in explaining the process of extracting the complex relationships.\nTABLE VI. REPRESENTING MAXIMAL COMPLETE GRAPHS OF FIG. 5 AS COMPLEX RELATIONSHIPS\nID Maximal Raw Maximal Complex\nComplete Graphs Complete Graphs Relationships\n1 {A3, B1, B2, B3} {A, B, B, B} {A, B, B+, -C}\n2 {B1, C1} {B, C} {-A, B, C}\n3 {A1, A2, B} {A, A, B} {A, A+, B, -C}\nF. Extracting Complex Relationships\nA relationship is called complex if it consists of complex types as defined in Section I.\nExtracting a complex relationship R from a maximal complete graph GM is straightforward – we simply use the following\nrules for every type t:\n1) First, remove the object identifiers. This produces a “raw” maximal complete graph GM .\n2) If GM contains an object with type t, R = R ∪ t.\n3) If GM contains more than one object of type t, R = R ∪ t+.\n4) If GM does not contain an object of type t, R = R ∪ −t.\nNote that if R includes a positive type A+, it will also always include the basic type A. This is necessary to that maximal\ncomplete graphs that contain A+ will also be counted as containing A when we mine for interesting patterns.\nRecall that the negative type only makes sense if we use maximal complete graphs. The last column of Table VI shows the\nresult of applying all four rules.\nIII. EXPERIMENTS AND RESULTS DISCUSSION\nExperiments are carried out to confirm the achieved results when using the proposed algorithm on the SDSS data. All\nexperiments were carried out on a Mac OS X 10.7 operated laptop (2.53 GHz) Intel Core Duo processor and 4 GB main\nmemory. The data structures and algorithm were implemented in Java and compiled with the GNU compiler.\nA. Scalability of GCG algorithm\nFig. 6 demonstrates the runtime of the GCG algorithm with various numbers of objects (galaxies) and distances. It illustrates\nthat the runtime increases slightly as the number of objects and distance increase. The distance is increased by 1 Mpc every\ntime, whereas the number of objects is increased by 50K objects. The maximum number of records was 350000. To explain\nfurther, when the distance increases the grid size increases. Also by increasing number of objects at the same time, it allows\nmore objects to appear in the same gird’s cell or in the neighbor grid areas. Therefore, the two factors (distance, number of\nobjects) affect the runtime of the GCG algorithm.\nB. Galaxy types in large complete graphs\nWe applied the GCG algorithm on the “Main” galaxies extracted from SDSS to generate maximal complete graphs with\nneighborhood distance (4 Mpc). We selected the complete graphs with the largest cardinality (|O| = 22). Fig. 7 shows the\ndistribution of “Early” and “Late” type galaxies in the reported complete graphs. These results show that large complete graphs\nconsist of more “Early” type galaxies (Elliptic) than “Late” type galaxies (Spiral). This conforms to the patterns given by [1]\nthat say “Early” type galaxies tend to stay away from “Late” type galaxies.\nGridClique Performance\n0\n50\n100\n150\n200\n250\n300\n350\n400\n50000 100000 150000 200000 250000 300000 350000\nNumber of Objects\nR u\nn T\ni m\ne  \n( S e\nc s\n)  \n1\n2\n3\n4\n5\nFig. 6. GCG’s runtime using 5 different distances.\n−5 0 5 10 15 20 25\n0\n50\n100\n150\n200\n250\n300\nNo. of Late Galaxies in a Clique\nFr\neq\nue\nnc\ny\nMean =7.426\nStd.Dev =3.8726\n(a) Number of “Main-Late” galaxies in complete graphs\n−5 0 5 10 15 20 25\n0\n50\n100\n150\n200\n250\n300\nNo. of Early Galaxies in a Clique\nFr\neq\nue\nnc\ny\nMean   =11.61\nStd.Dev = 3.4269\n(b) Number of “Main-Early” galaxies in complete graphs\nFig. 7. The existence of galaxies in the universe.\n0 10 20 30 40 50 60 70 80\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\nx 104\nClique Cardinality \nFre\nque\nncy\nMain Galaxy Cliques\nFig. 8. Complete Graphs cardinalities for Main galaxies using threshold = 4 Mpc. Frequency X 104\nC. Complete Graphs Cardinalities\nFigure 8 shows the complete graphs cardinalities in “Main” galaxies. It shows that complete graphs with cardinality between\n2 and 5, small complete graphs, are more frequent than large complete graphs.\nIV. RELATED WORK\nHuang et al. [14] defined the co-location pattern as the presence of a spatial feature in the neighborhood of instances of other\nspatial features. They developed an algorithm for mining valid rules in spatial databases using an Apriori based approach. Their\nalgorithm does not separate the co-location mining and interesting pattern mining steps like our approach does. Also, they did\nnot consider complex relationships or patterns.\nMonroe et al. [15] used cliques as a co-location pattern (subgraphs), but in our research we used complete graphs instead.\nSimilar to our approach, they separated the clique mining from the pattern mining stages. However, they did not use maximal\ncomplete graph. They treated each clique as a transaction and used an Apriori based technique for mining association rules.\nSince they used cliques (rather than maximal complete graphs) as their transactions, the counting of pattern instances is very\ndifferent. They considered complex relationships within the pattern mining stage. However, their definition of negative patterns is\nvery different – they used infrequent types while we base our definition on the concept of absence in maximal complete graphs.\nThey also used a different measure, namely, maxPI.\nArunasalam et al. [4] used a similar approach to [15]. They proposed an algorithm called NP maxPI which also used the\nMaxPI measure. The proposed algorithm prunes the candidate itemsets using a property of maxPI. They also used an Apriori\nbased technique to mine complex patterns. A primary goal of their work was to mine patterns which have low support and high\nconfidence. As with the work of [15], they did not use maximal complete graphs.\nZhang et al. [16] enhanced the algorithm proposed in [14] and used it to mine special types of co-location relationships in\naddition to cliques, namely; the spatial star, and generic patterns. This means they didn’t use maximal complete graphs.\nMost of the previous research and to the best of our knowledge, previous work has used Apriori type algorithms for mining\ninteresting co-location patterns. However, we embedded GLIMIT [5] as the underlying pattern mining algorithm as already\ndiscussed in Section II-C. To the best of our knowledge, no previous work has used the concept of maximal complete graph to\nmine comoplex co-location patterns in large spatial data.\nV. CONCLUSION\nIn this paper, we presented a framework, which incorporates our proposed algorithm GCG to mine complex co-location patterns\nexist in large spatial dataset (SDSS). Most of the previous research conducted in this area used Apriori type algorithms to mine\nonly normal co-location patterns. However, we showed the importance of using complex co-location patterns, which are extracted\nfrom maximal complete graphs. We also presented how our proposed algorithms strips efficiently all maximal complete graphs in\nlarge spatial dataset (SDSS) using divide and conquer strategy. We have shown that the idea of mining maximal complete graphs\nis very important in our work since complex patterns only makes sense when using maximal complete graphs. Future work\nwould be to extend this framework to extract interesting relationships using different types of spatial objects in the astronomy\ndomain.\nREFERENCES\n[1] J. Gray, D. Slutz, A. S. Szalay, A. R. Thakar, J. vandenBerg, P. Z. Kunszt, and C. Stoughton, “Data mining the sdss skyserver database,” Microsoft\nResearch, Tech. Rep. MSR-TR-2002-01, 2002.\n[2] S. Sekhar and S. Chawla, Spatial Databases:A Tour. Prentice Hall, 2003.\n[3] R. Agrawal, T. Imielinsk, and A. Swami, “Mining association rules between sets of items in large databases,” in SIGMOD ’93: Proceedings of the 1993\nACM SIGMOD international conference on Management of data. New York, NY, USA: ACM Press, 1993, pp. 207–216.\n[4] B. Arunasalam, S. Chawla, and P. Sun, “Striking two birds with one stone: Simultaneous mining of positive and negative spatial patterns,” in Proceedings\nof the Fifth SIAM International Conference on Data Mining, 2005, pp. 173–182.\n[5] F. Verhein and S. Chawla, “Geometrically inspired itemset mining,” in ICDM. IEEE Computer Society, 2006, pp. 655–666. [Online]. Available:\nhttp://doi.ieeecomputersociety.org/10.1109/ICDM.2006.75\n[6] S. D. S. Survey, “Sdss - sloan digital sky survey. retrieved august 5, 2005 from http://cas.sdss.org/dr5/en/help/download/,” 2006.\n[7] D.N.Spergel, M.Bolte, and W.Freedman, “The age of the universe,” Proceedings of the National Academy of Science, vol. 94, pp. 6579–6584, 1997.\n[8] H. M. and S. Churchman, “Hubble’s law. retrieved march 12, 2005, from from http://map.gsfc.nasa.gov/,” 1999.\n[9] V.J.Martin and E.Saar, Statistics of the Galaxy Distribution. Chapman and Hall/CRC, 2002.\n[10] S. Shekhar, Y. Huang, and H. Xiong, “Discovering spatial co-location patterns from spatial data sets:a general approach,” vol. 16, 2004, pp. 1472–1485.\n[11] F. Verhein and G. Al-Naymat, “Fast mining of complex spatial co-location patterns using glimit,” in The 2007 International Workshop on Spatial and\nSpatio-temporal Data Mining (SSTDM) in cooperation with The 2007 IEEE International Conference on Data Mining (ICDM). Los Alamitos, CA, USA:\nIEEE Computer Society, 2007, pp. 679–684.\n[12] R. Agrawal and R. Srikant, “Fast algorithms for mining association rules,” in Proceedings of 20th International Conference on Very Large Data Bases\nVLDB. Morgan Kaufmann, 1994, pp. 487–499.\n[13] J. Han, J. Pei, and Y. Yin, “Mining frequent patterns without candidate generation,” in 2000 ACM SIGMOD Intl. Conference on Management of Data.\nACM Press, May 2000, pp. 1–12. [Online]. Available: citeseer.ist.psu.edu/han99mining.html\n[14] Y. Huang, H. Xiong, S. Shekhar, and J. Pei, “Mining confident co-location rules without a support threshold,” in Proceedings of the 18th ACM Symposium\non Applied Computing ACM SAC. ACM Press, New York, 2003.\n[15] R. Munro, S. Chawla, and P. Sun, “Complex spatial relationships,” in Proceedings of the 3rd IEEE International Conference on Data Mining, ICDM\n2003. IEEE Computer Society, 2003, pp. 227–234.\n[16] X. Zhang, N. Mamoulis, D. W. Cheung, and Y. Shou, “Fast mining of spatial collocations,” in Proceedings of the tenth ACM SIGKDD international\nconference on Knowledge discovery and data mining. ACM Press-New York, 2004, pp. 384 – 393.\n",
            "id": 17167787,
            "identifiers": [
                {
                    "identifier": "oai:arxiv.org:1312.4477",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "210360150",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "1312.4477",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "10.1109/aiccsa.2013.6616417",
                    "type": "DOI"
                },
                {
                    "identifier": "24987917",
                    "type": "CORE_ID"
                }
            ],
            "title": "GCG: Mining Maximal Complete Graph Patterns from Large Spatial Data",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:arxiv.org:1312.4477"
            ],
            "publishedDate": "2013-12-13T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1312.4477"
            ],
            "updatedDate": "2021-07-22T13:10:01",
            "yearPublished": 2013,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1312.4477"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/17167787"
                }
            ]
        },
        {
            "acceptedDate": "2014-09-03T00:00:00",
            "arxivId": "1402.1607",
            "authors": [
                {
                    "name": "Liu, Kangqi"
                },
                {
                    "name": "Long, Xin"
                },
                {
                    "name": "Tao, Meixia"
                },
                {
                    "name": "Xiang, Zhengzheng"
                }
            ],
            "contributors": [
                "Kangqi"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/188671258"
            ],
            "createdDate": "2014-10-24T19:24:14",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2014-06-01T00:00:00",
            "abstract": "We study the degrees of freedom (DoF) of MIMO two-way X relay channels.\nPrevious work studied the case $N < 2M$, where $N$ and $M$ denote the number of\nantennas at the relay and each source, respectively, and showed that the\nmaximum DoF of $2N$ is achievable when $N \\leq \\lfloor\\frac{8M}{5}\\rfloor$ by\napplying signal alignment (SA) for network coding and interference cancelation.\nThis work considers the case $N>2M$ where the performance is limited by the\nnumber of antennas at each source node and conventional SA is not feasible. We\npropose a \\textit{generalized signal alignment} (GSA) based transmission\nscheme. The key is to let the signals to be exchanged between every source node\nalign in a transformed subspace, rather than the direct subspace, at the relay\nso as to form network-coded signals. This is realized by jointly designing the\nprecoding matrices at all source nodes and the processing matrix at the relay.\nMoreover, the aligned subspaces are orthogonal to each other. By applying the\nGSA, we show that the DoF upper bound $4M$ is achievable when $M \\leq\n\\lfloor\\frac{2N}{5}\\rfloor$ ($M$ is even) or $M \\leq\n\\lfloor\\frac{2N-1}{5}\\rfloor$ ($M$ is odd). Numerical results also demonstrate\nthat our proposed transmission scheme is feasible and effective.Comment: 6 pages, 6 figures, to appear in IEEE ICC 201",
            "documentType": "research",
            "doi": "10.1109/icc.2014.6884019",
            "downloadUrl": "http://arxiv.org/abs/1402.1607",
            "fieldOfStudy": "computer science",
            "fullText": "ar\nX\niv\n:1\n40\n2.\n16\n07\nv1\n  [\ncs\n.IT\n]  \n7 F\neb\n 20\n14\nGeneralized Signal Alignment For MIMO Two-Way\nX Relay Channels\nKangqi Liu, Meixia Tao, Zhengzheng Xiang and Xin Long\nDept. of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China\nEmails: {forever229272129, mxtao, 7222838, lx yokumen}@sjtu.edu.cn\nAbstract—We study the degrees of freedom (DoF) of MIMO\ntwo-way X relay channels. Previous work studied the case N <\n2M , where N and M denote the number of antennas at the relay\nand each source, respectively, and showed that the maximum\nDoF of 2N is achievable when N ≤ ⌊ 8M\n5\n⌋ by applying signal\nalignment (SA) for network coding and interference cancelation.\nThis work considers the case N > 2M where the performance\nis limited by the number of antennas at each source node and\nconventional SA is not feasible. We propose a generalized signal\nalignment (GSA) based transmission scheme. The key is to let\nthe signals to be exchanged between every source node align\nin a transformed subspace, rather than the direct subspace, at\nthe relay so as to form network-coded signals. This is realized\nby jointly designing the precoding matrices at all source nodes\nand the processing matrix at the relay. Moreover, the aligned\nsubspaces are orthogonal to each other. By applying the GSA,\nwe show that the DoF upper bound 4M is achievable when\nM ≤ ⌊ 2N\n5\n⌋ (M is even) or M ≤ ⌊ 2N−1\n5\n⌋ (M is odd). Numerical\nresults also demonstrate that our proposed transmission scheme\nis feasible and effective.\nI. INTRODUCTION\nWireless relay has been an important ingredient in both ad\nhoc and infrastructure-based wireless networks. It shows great\npromises in power reduction, coverage extension and through-\nput enhancement. In the simplest scenario, a relay node only\nserves a single user. This forms the classic one-way relaying\nand the relay strategies are maturing. With the rapid expansion\nof multi-user communications, a relay has become very much\nlike a wireless gateway where multiple users communicate\nwith each other via a common relay. A fundamental question\nthat arises is what is the maximum number of data streams\nthat can be transmitted and how to achieve it. This leads to\nthe analysis of degrees of freedom (DoF) and also drives the\ndevelopment of more advanced relay strategies for efficient\nrelay-assisted multi-user communication.\nThe recently proposed two-way relaying is such an advanced\nrelay method that offers high spectral efficiency in a system\nwhere two users exchange information with each other through\na relay [1]. The key idea is to apply physical layer network\ncoding (PLNC) at the relay end [2] [3]. With PLNC, the max-\nimum achievable DoF of the MIMO two-way relay channel\nis 2min{M,N} [4], where M and N denote the number of\nantennas at each source node and the relay, respectively. When\nThis work is supported by the National 973 project under grant\n2012CB316100 and by the NSF of China under grants 61322102 and\n61329101.\nthere are three or more users exchanging information with each\nother via a common relay, PLNC is not enough to achieve the\nDoF of the network.\nBased on the idea of interference alignment [5] [6], another\npromising technique, signal alignment (SA) is firstly proposed\nin [7] to analyze the maximum achievable DoF for MIMO Y\nchannel, where three users exchange independent messages\nwith each other via a relay. By jointly designing the precoders\nat each source node, SA is able to align the signals from two\ndifferent source nodes in a same subspace of the relay node.\nBy doing so, the two data streams to be exchanged between a\npair of source codes are combined into one network-coded\nsymbol and thus the relay can forward more data streams\nsimultaneously. It is proved that with SA for network-coding\nand network-coding aware interference nulling the theoretical\nupper bound 3M of DoF is achievable when N ≥ ⌈ 3M2 ⌉ [8].\nHere, again, M and N denote the number of antennas at each\nsource node and the relay node, respectively. The extension to\nK-user MIMO Y Channels is considered in [9].\nIn [10], SA is applied in MIMO two-way X relay channel,\nwhere there are two groups of source nodes and one relay\nnode, and each of the two source nodes in one group exchange\nindependent messages with the two source nodes in the other\ngroup via the relay node. It is shown that the DoF upper bound\nis 2min{2M,N}, and the upper bound 2N is achievable when\nN ≤ ⌊ 8M5 ⌋ by applying SA and interference cancelation.\nIn this paper, we are interested in the ability of the DoF\nupper bound 4M for MIMO two-way X relay channel in the\ncase N > 2M , where the performance is limited only by the\nnumber of antennas at each source. It is worth mentioning that\nSA is not feasible under the antenna configuration N ≥ 2M .\nThe reason is as follows. Recall that SA condition is\nH1,rv1 = H2,rv2, (1)\nwhere Hi is an N ×M matrix (corresponding to the channel\nmatrix from source i to relay) and vi is an M × 1 vector\n(corresponding to the beamforming vector of source i). The\nabove alignment condition can be rewritten as\n[H1,r −H2,r]\n[\nv1\nv2\n]\n= 0. (2)\nClearly, for (2) to hold, one must N < 2M , or equivalently\nM > N2 .\nTo achieve the maximum DoF at M ≤ N2 , we propose a\nnew transmission scheme, named generalized signal alignment\n(GSA). Compared with the existing SA, the proposed GSA has\nthe following major difference. The signals to be exchanged\ndo not align directly in the subspace observed by the relay.\nInstead, they are aligned in a transformed subspace after\ncertain processing at the relay, which is orthogonal to each\nother. This is done by jointly designing the precoding matrices\nat the source nodes and the processing matrix at the relay node.\nWith the proposed GSA, we show that the total DoF upper\nbound 4M of MIMO two-way X relay channel is achievable\nwhen M ≤ ⌊ 2N5 ⌋(M is even) or M ≤ ⌊ 2N−15 ⌋(M is odd).\nThe remainder of the paper is organized as follows. In\nSection II, we introduce the system model of the MIMO two-\nway X relay channel. In Section III, we introduce the GSA\ntransmission scheme with a motivate example. In Section IV,\nwe analyze the achievability of the DoF upper bound when\nM < N2 . In Section V, we show our numerical results. Section\nVI presents concluding remarks.\nNotations: (·)T , (·)H and (·)† denote the transpose, Her-\nmitian transpose and the Moore-Penrose pseudoinverse, re-\nspectively. ε[·] stands for expectation. |x| means 2-norm of\nvector x. span(H) and null(H) stand for the column space\nand the null space of the matrix H, respectively. dim(H)\ndenotes the dimension of the column space of H. 〈x〉 denotes\nnormalization operation on vector x, i.e.〈x〉 = x|x| . ⌊x⌋ denotes\nthe largest integer no greater than x. ⌈x⌉ denotes the smallest\ninteger no less than x. I is the identity matrix.\nII. SYSTEM MODEL\nWe consider the same MIMO two-way X relay channel as\nin [10] and shown in Fig. 1. It consists of two groups of\nsource nodes, each equipped with M antennas, and one relay\nnode, equipped with N antennas. Each source node exchanges\nindependent messages with each source node in the other\ngroup with the help of the relay. The independent message\ntransmitted from source i to source j is denoted as Wij . At\neach time slot, the message is encoded into a dij × 1 symbol\nvector sij , where dij denotes the number of independent data\nflows from source i to source j.\nTaking source node 1 for example, the transmitted signal\nvector x1 from source node 1 is given by\nx1 = V13s13 +V14s14 = V1s1, (3)\nwhere s1 = [s13, s14]T , V1 = [V13,V14], V13 and V14 are the\nM × d13 and M × d14 precoding matrices for the information\nsymbols to be sent to source node 3 and 4, respectively.\nThe communication of the total messages takes place in two\nphases: the multiple access (MAC) phase and the broadcast\n(BC) phase. In the MAC phase, all four source nodes transmit\ntheir signals to the relay. The received signal yr at the relay\nis given by\nyr =\n4∑\ni=1\nHi,rxi + nr, (4)\nFig. 1. MIMO Two-way X Relay Channel.\nwhere Hi,r denotes the frequency-flat quasi-static N × M\ncomplex channel matrix from source node i to the relay and\nnr denotes the N × 1 additive white Gaussian noise (AWGN)\nwith variance σ2n. The entries of the channel matrix Hi,r and\nthose of the noise vector nr, are independent and identically\ndistributed (i.i.d.) zero-mean complex Gaussian random vari-\nables with unit variance. Thus, each channel matrix is of full\nrank with probability 1.\nUpon receiving yr in (4), the relay processes these messages\nto obtain a mixed signal xr, and broadcasts to all the users.\nThe received signal at source node i can be written as\nyi = Gr,ixr + ni, (5)\nwhere Gr,i denotes the frequency-flat quasi-static M × N\ncomplex channel matrix from relay to the source node i, and ni\ndenotes the AWGN at the node i. Each user tries to obtain its\ndesirable signal from its received signal using its own transmit\nsignal as side information.\nIII. GENERALIZED SIGNAL ALIGNMENT\nIn this section, we shall introduce the GSA-based trans-\nmission scheme for MIMO two-way X relay channel when\nM < N2 . Note that for this case, the corresponding DoF upper\nbound is 4M [10].\nAs a motivating example, we consider a system where each\nsource has M = 2 antennas and the relay has N = 5 antennas.\nFor this system, the proposed GSA-based transmission scheme\nachieves d13 = d14 = d23 = d24 = d31 = d32 = d41 = d42 =\n1, yielding a total DoF of 8.\nA. MAC phase\nIn the MAC phase, each source node transmits the precoded\nsignals to the relay simultaneously. Let source node 1 transmits\nthe data streams s13 and s14 by using beamforming vectors\nv13 and v14, respectively to source nodes 3 and 4. Similar\nnotations are used for the other three nodes. Thus, there are\ntotally 8 data streams arriving at the relay. We rewrite the\nreceived signal (4) as\nyr\n= [H1,r H2,r H3,r H4,r]\n\n\nV1 0 0 0\n0 V2 0 0\n0 0 V3 0\n0 0 0 V4\n\n\n\n\ns1\ns2\ns3\ns4\n\n+ nr\n= HVs+ nr, (6)\nwhere H is the 5 × 8 overall channel matrix, V is the 8 × 8\nblock-diagonal overall precoding matrix and s is the 8 × 1\ntransmitted signal vector for all the source nodes, given by\ns = [s13 s14 s23 s24 s31 s32 s41 s42]\nT\n. (7)\nSince the relay has only 5 antennas, it is impossible for it\nto decode all the 8 data streams. However, based on the idea\nof physical layer network coding, we only need to obtain the\nfollowing network-coded symbol vector at the relay\ns⊕ = [s13 + s31, s14 + s41, s23 + s32, s24 + s42]\nT\n. (8)\nNext, we show that there exist a precoding matrix V for the\nsource nodes and a processing matrix A for the relay node such\nthat the network-coded symbol vector in (7) can be obtained.\nTo state it formally, we introduce the following theorem.\nTheorem 1: Given the received signal model in (6), there\nexists an 8×8 block-diagonal precoding matrix V and a 4×5\nrelay processing matrix A such that\nyˆr = Ayr\n= AHVs +Anr\n= s⊕ +Anr. (9)\nProof: Let ai denote the i-th row of A, for i = 1, · · · , 4.\nEach ai can be thought as a combining vector for the trans-\nmitted signals of a source node pair. Specifically, we take a1\nfor elaboration. We aim to design a1 to align the transmitted\nsignals from source pair (1,3) and cancel the undesired signals\nfrom source nodes 2 and 4. Thus, we design a1 such that it\nfalls into the null space of H2,r and H4,r\naT1 ⊆ Null\n[\nH2,r H4,r\n]T\n. (10)\nSince\n[\nH2,r H4,r\n]T is a 4 × 5 matrix, we can always find\nsuch a1. Similarly, the other rows of A can be obtained as:\naT2 ⊆ Null\n[\nH2,r H3,r\n]T\n. (11)\naT3 ⊆ Null\n[\nH1,r H4,r\n]T\n. (12)\naT4 ⊆ Null\n[\nH1,r H3,r\n]T\n. (13)\nwhere a2 is for source pair (1,4), a3 is for source pair (2,3)\nand a4 is for source pair (2,4).\nGiven the processing matrix A at the relay, the effective\nchannel in the MAC phase can be written as\nAH =\n\n\nc11 c12 0 0 c15 c16 0 0\nc21 c22 0 0 0 0 c27 c28\n0 0 c33 c34 c35 c36 0 0\n0 0 c43 c44 0 0 c47 c48.\n\n\n(14)\nFig. 2. The construction the processing matrix at relay.\nDefine Ci, i = 1, 2, 3, 4 as\nC1 =\n[\nc11 c12\nc21 c22\n]\n, C2 =\n[\nc33 c34\nc43 c44\n]\n,\nC3 =\n[\nc15 c16\nc35 c36\n]\n, C4 =\n[\nc27 c28\nc47 c48\n]\n. (15)\nWe can construct the precoding matrix for each source node\nas\nVi = C\n−1\ni , i = 1, 2, 3, 4. (16)\nThe above precoding matrix Vi can be seen as a zero-\nforcing based precoder to cancel inter-stream interference\nbetween those signals not to be exchanged for each source\nnode pair. For example, V1 helps cancel the unwanted signal\ns14 for source node pair (1, 3) and s13 for source node pair\n(1, 4). Substituting (10) - (13) and (16) into (6), we can obtain\n(9). The theorem is thus proved.\nBy jointly designing the precoding matrices at the source\nnodes and the processing matrix at the relay, our proposed\nGSA has successfully aligned the transmitted signals for each\nsource node pair at the relay. The key steps of GSA are\nillustrated in Fig. 2 and Fig. 3. Fig. 2 shows that aT1 falls\ninto the null space of H2,r and H4,r. It is similar to other\nai (i=2, 3, 4) by (10) - (13). Fig. 3 shows the whole signal\nprocessing procedure to obtain the network-coded messages.\nHere, the signals received at relay after the effective channel\nAH is firstly given. Then the signals are rotated by V to be\naligned in four orthogonal directions.\nB. BC Phase\nDuring the BC phase, the relay broadcasts an estimate of\nthe four network coded symbols using the precoding matrix\nU = [u1, ...,u4]. More specifically, u1, u2, u3, u4 are for\nsymbols s13+s31, s14+s41, s23+s32, s24+s42, respectively.\nEach beamformer is designed as below\nu1 ⊆ Null\n[\nGr,2\nGr,4\n]\n, u2 ⊆ Null\n[\nGr,2\nGr,3\n]\nu3 ⊆ Null\n[\nGr,1\nGr,4\n]\n, u4 ⊆ Null\n[\nGr,1\nGr,3\n]\n. (17)\nFig. 3. Generalized signal alignment procedure.\nSince\n[\nGTr,j , GTr,k\n]T\nis a 4 × 5 matrix, we can always find\nthese beamforming vectors to satisfy the above conditions.\nPlugging (17) into (5), we can obtain the received signal\nfor source node 1 as below\ny1 = Gr,1Usˆ⊕ + n1\n= Gr,1\n[\nu1(s13 + s31) + u2(s14 + s41)\n]\n+ n1. (18)\nSince source node 1 knows s13 and s14, it can decode\nthe desired signal from (18) after applying self-interference\ncancellation. In the same manner, the other source nodes can\nalso obtain the signals intended for themselves. Thus, the total\nDoF of 8 is achieved, which is also the upper bound for the\nnetwork when M = 2, N = 5.\nC. Extension to M = 2, N > 5\nWhen M = 2, N > 5, we can always find the null space\nfor ai (i=1,2,3,4) by (10)-(13). The precoding matrix V then\ncan be calculated by (16). It can be seen that ui exists by (17).\nThus, we can apply our proposed GSA-based transmission\nscheme for MIMO two-way X relay channel to achieve the\ntotal DoF of 8. We omit the proof here.\nIV. ACHIEVABILITY OF THE UPPER BOUND\nIn this section, we will generalize the method in the previous\nsection to arbitrary N , M with M < N2 and we will show that\nit can achieve the DoF upper bound 4M when M ≤ ⌊ 2N5 ⌋\nfor even M , and M ≤ ⌊ 2N−15 ⌋ for odd M .\nWe first consider the case when M is even. The proposed\nGSA-based transmission scheme achieves total DoF upper\nbound d13 = d14 = d23 = d24 = d31 = d32 =\nd41 = d42 =\nM\n2 , yielding a total DoF of 4M . Denote\ns⊕ = [s\n1\n13+s\n1\n31, s\n2\n13+s\n2\n31, · · · , s\nM\n2\n13 +s\nM\n2\n31 , s\n1\n14+s\n1\n41, · · · , s\nM\n2\n14 +\ns\nM\n2\n41 , s\n1\n23 + s\n1\n32, · · · , s\nM\n2\n23 + s\nM\n2\n32 , s\n1\n24 + s\n1\n42, · · · , s\nM\n2\n24 + s\nM\n2\n42 ]\nT as\nthe network-coded messages expected to obtain at the relay,\nwhere skij denotes the k-th data streams from source node i\nto source node j.\nDenoting Ai as the ( (i−1)M2 + 1)-th to the (\niM\n2 )-th row\nvectors of A, for i = 1, · · · , 4. Each Ai can be thought as\na combining matrix for the transmitted signals of a source\nnode pair. Thus, we design Ai similar to (10) and (13) as the\nfollowing:\nAT1 ⊆ Null\n[\nH2,r H4,r\n]T\nAT2 ⊆ Null\n[\nH2,r H3,r\n]T\nAT3 ⊆ Null\n[\nH1,r H4,r\n]T\nAT4 ⊆ Null\n[\nH1,r H3,r\n]T\n. (19)\nHere, A1 is for source pair (1,3), A2 is for source pair (1,4),\nA3 is for source pair (2,3) and A4 is for source pair (2,4).\nWe can see that ATi is an N × M2 matrix and [Hj,r Hk,r ]\nT\nis a 2M × N matrix. The matrix ATi exists if and only if\nN − 2M ≥ M2 , which is equivalent to M ≤ ⌊\n2N\n5 ⌋.\nAfter obtaining the matrix A, we can get the matrix V using\nthe same method as (16) in Section III. Then we show the\nexistence of the precoding matrix U (N × 2M ). We can write\nU as\nU =\n[\nU1 U2 U3 U4\n]\n, (20)\nwhere each Ui is an N × M2 matrix and\nU1 ⊆ Null\n[\nGr,2\nGr,4\n]\n, U2 ⊆ Null\n[\nGr,2\nGr,3\n]\nU3 ⊆ Null\n[\nGr,1\nGr,4\n]\n, U4 ⊆ Null\n[\nGr,1\nGr,3\n]\n. (21)\nWe can see that [Gr,j Gr,k]T is a 2M × N matrix. The\nmatrix Ui exists if and only if N − 2M ≥ M2 , which is\nequivalent to M ≤ ⌊ 2N5 ⌋. Hence, we can apply GSA-based\ntransmission scheme when M is even and M ≤ ⌊ 2N5 ⌋.\nSimilarly, we can achieve the upper bound of the DoF 4M\nby d13 = d24 = d31 = d42 = M+12 and d14 = d23 = d32 =\nd41 =\nM−1\n2 when M is odd. The corresponding matrices A\nand U exist if and only if N−2M ≥ M+12 , which is equivalent\nto M ≤ ⌊ 2N−15 ⌋.\nNote that the proposed GSA based transmission scheme\ncan be applied to align signal pairs even when N > 2M .\nIn this case, A is an identity matrix, and GSA reduces to the\nconventional SA.\nFinally, we summarize the generalized signal alignment\nprocedure in the following chart.\nOutline of Generalized Signal Alignment\n• Step 1. construct the matrix H using the channel matrices\nH1,r, H2,r, H3,r, H4,r according to (6).\n• Step 2. Design the relay processing matrix A according\nto (19).\n• Step 3. Compute the effective channel in the MAC\nphase AH and construct the source precoding matrix V\naccording to (16).\n• Step 4. Design the BC precoding matrix U with matrix\nGr,1, Gr,2, Gr,3, Gr,4 according to (21).\nV. NUMERICAL RESULTS\nIn this section, we provide numerical results to show the\nsum rate performance of the proposed scheme for the MIMO\ntwo-way X relay channel. The channel between each source\nnode and the relay node is modeled as Rayleigh distribution\nwith unit variance and it is independent for different node.\nThe numerical results are illustrated with the ratio of the total\ntransmitted signal power to the noise variance at each receive\nantenna and the total throughput of the channel. Each result\nis averaged over 10000 independent channel realizations.\nWe now explain how we compute the sum rate for the\nMIMO two-way X relay channel when applying the GSA\ntransmission scheme.\nGSA transmission scheme can be used in both amplify-and-\nforward (AF) and decode-and-forward (DF) strategy. From (6)\nand (18), when we apply AF strategy, we can obtain\nyi = Gr,iU(s⊕ + Anr) + ni. (22)\nLet Ri denote the sum rate of the source node i. We\ncalculate R1 as a representative. First, we write the received\nsignal of source node 1 with AF strategy as\ny1 = Gr,1Us⊕+Gr,1UAnr +n1\n= Gr,1\n[\nU1(s13 + s31) +U2(s14 + s41)\n]\n︸ ︷︷ ︸\nSignal\n+Gr,1UAnr + n1︸ ︷︷ ︸\nNoise\n= G˜r,1s˜1 + n˜1. (23)\nThen we can calculate the sum rate R1 in bits per channel use\nfrom source node 3 and source node 4 to source node 1 by\n(V)\nR1 = log2[det(I+ (ε(n˜1n˜\nH\n1 ))\n−1G˜r,1ε[˜s1s˜H1 ]G˜\nH\nr,1)] (24)\nSimilarly, we can calculate R2, R3, R4 with the same\nmethod. Then total sum rate is given by\nR =\n4∑\ni=1\nRi. (25)\nIn Fig. 4, we plot the sum rate performance of the proposed\ngeneralized signal alignment transmission scheme at fixed N\nbut varying M (M < N/2). In the figure, SNR denotes the\ntotal transmitted signal power from all the four source nodes to\nthe noise variance at relay. We can observe that the increasing\nspeed of sum-rate (the increase in bps/Hz for every 3dB in\nSNR) matches with the theoretical DoF 4M very well when\nSNR is high enough.\nIn Fig. 5 and Fig. 6, we plot the sum rate performance\nof the proposed GSA transmission scheme at the antenna\nconfigurations of M = ⌊ 2N5 ⌋ for even M and M = ⌊\n2N−1\n5 ⌋\nfor odd M , respectively. The upper bound of the DoF of 4M\nis also achieved. These results indicate that our proposed GSA\ntransmission scheme is feasible and effective.\nVI. CONCLUSION\nIn this paper, we have analyzed the achievability of the DoF\nupper bound for the MIMO two-way X relay channal when\nM ≤ N2 . In the newly-proposed GSA transmission scheme,\nthe processing matrix at the relay and the precoding matrix\nat the source nodes are designed jointly so that the signals to\nbe exchanged between each source node pair are aligned at\nthe relay. We showed that when M ≤ ⌊ 2N5 ⌋(M is even) or\nM ≤ ⌊ 2N−15 ⌋(M is odd), the upper bound of the total DoF\n4M is achieved. Theoretical analysis and numerical results\nboth show that the transmission scheme proposed is feasible\nand effective.\nFig. 4. Total DoF for the MIMO two-way X relay channel under generalized\nsignal alignment transmission scheme.\nFig. 5. Total DoF for the MIMO two-way X relay channel when M = 2N\n5\n.\nREFERENCES\n[1] B. Rankov and A.Wittneben, “Spectral efficient protocols for half-\nduplex fading relay channels,” IEEE Journal on Selected Areas in\nCommunications, vol. 25, no. 2, Feb. 2007.\n[2] S. Zhang, S. Liew, and P. Lam, “Physical layer network coding,” in\nProc. ACM MobiCom, Sep. 2006, pp. 63–68.\n[3] S. Katti, S. Gollakota, and D. Katabi, “Embracing wireless interference:\nAnalog network coding,” in Proc. ACM SIGCOMM, Sep. 2007, pp. 397–\n408.\n[4] R. R. Vaze and R. W. Heath, “Capacity scaling for MIMO two-way\nrelaying,” in IEEE International Symposium on Information Theory, Jun.\n2007, pp. 1451 – 1455.\n[5] S. Jafar and S. Shamai, “Degrees of freedom region of the MIMO X\nchannel,” IEEE Transactions on Information Theory, vol. 54, no. 1, pp.\n151–170, Jan. 2008.\n[6] M. Maddah-Ali, A. Motahari, and A. Khandani, “Communication over\nMIMO X channels: Interference alignment, decomposition, and perfor-\nmance analysis,” IEEE Transactions on Information Theory, vol. 54,\nno. 8, pp. 3457–3470, Aug. 2008.\n[7] N. Lee and J.-B. Lim, “A novel signaling for communication on\nMIMO Y channel: Signal space alignment for network coding,” IEEE\nInternational Symposium on Information Theory, pp. 2892 – 2896, 2009.\nFig. 6. Total DoF for the MIMO two-way X relay channel when M =\n2N−1\n5\n.\n[8] N. Lee, J. Lee, and J. Chun, “Degrees of freedom on the MIMO Y\nchannel: signal space alignment for network coding,” IEEE Transactions\non Information Theory, vol. 56, no. 7, pp. 3332 – 3342, Jul. 2010.\n[9] K. Lee, N. Lee, and I. Lee, “Achievable degrees of freedom on K-user\nY channels,” IEEE Transactions on Wireless Communications, vol. 11,\nno. 3, pp. 1210 – 1219, Mar. 2012.\n[10] Z. Xiang, M. Tao, J. Mo, and X. Wang, “Degrees of freedom for MIMO\ntwo-way X relay channel,” IEEE Transactions on Signal Processing,\nvol. 61, pp. 1711 – 1720, 2013.\n",
            "id": 17175536,
            "identifiers": [
                {
                    "identifier": "1402.1607",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "oai:arxiv.org:1402.1607",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "188671258",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1109/icc.2014.6884019",
                    "type": "DOI"
                },
                {
                    "identifier": "2087289301",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "25001217",
                    "type": "CORE_ID"
                }
            ],
            "title": "Generalized Signal Alignment For MIMO Two-Way X Relay Channels",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2087289301",
            "oaiIds": [
                "oai:arxiv.org:1402.1607"
            ],
            "publishedDate": "2014-02-07T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1402.1607",
                "http://iwct.sjtu.edu.cn/Personal/mxtao/paper/Generalized_signal_alignment.pdf"
            ],
            "updatedDate": "2021-07-22T18:15:23",
            "yearPublished": 2014,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1402.1607"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/17175536"
                }
            ]
        },
        {
            "acceptedDate": "2014-05-27T00:00:00",
            "arxivId": "1403.6246",
            "authors": [
                {
                    "name": "Chakraborty, Supratik"
                },
                {
                    "name": "Meel, Kuldeep S."
                },
                {
                    "name": "Vardi, Moshe Y."
                }
            ],
            "contributors": [
                "Unknown",
                "Supratik"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/192676946"
            ],
            "createdDate": "2014-10-24T19:25:16",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2014-01-01T00:00:00",
            "abstract": "Constrained-random simulation is the predominant approach used in the\nindustry for functional verification of complex digital designs. The\neffectiveness of this approach depends on two key factors: the quality of\nconstraints used to generate test vectors, and the randomness of solutions\ngenerated from a given set of constraints. In this paper, we focus on the\nsecond problem, and present an algorithm that significantly improves the\nstate-of-the-art of (almost-)uniform generation of solutions of large Boolean\nconstraints. Our algorithm provides strong theoretical guarantees on the\nuniformity of generated solutions and scales to problems involving hundreds of\nthousands of variables.Comment: This is a full version of DAC 2014 pape",
            "documentType": "research",
            "doi": "10.1145/2593069.2593097",
            "downloadUrl": "http://arxiv.org/abs/1403.6246",
            "fieldOfStudy": "computer science",
            "fullText": "ar\nX\niv\n:1\n40\n3.\n62\n46\nv1\n  [\ncs\n.L\nO]\n  2\n5 M\nar \n20\n14\nBalancing Scalability and Uniformity in SAT Witness\nGenerator ∗\nSupratik Chakraborty\nIndian Institute of Technology, Bombay\nsupratik@cse.iitb.ac.in\nKuldeep S. Meel, Moshe Y. Vardi\nRice University\nkuldeep@rice.edu,vardi@cs.rice.edu\nABSTRACT\nConstrained-random simulation is the predominant approach\nused in the industry for functional verification of complex\ndigital designs. The effectiveness of this approach depends\non two key factors: the quality of constraints used to gener-\nate test vectors, and the randomness of solutions generated\nfrom a given set of constraints. In this paper, we focus on the\nsecond problem, and present an algorithm that significantly\nimproves the state-of-the-art of (almost-)uniform generation\nof solutions of large Boolean constraints. Our algorithm pro-\nvides strong theoretical guarantees on the uniformity of gen-\nerated solutions and scales to problems involving hundreds\nof thousands of variables.\n1. INTRODUCTION\nFunctional verification constitutes one of the most chal-\nlenging and time-consuming steps in the design of modern\ndigital systems. The primary objective of functional veri-\nfication is to expose design bugs early in the design cy-\ncle. Among various techniques available for this purpose,\nthose based on simulation overwhelmingly dominate indus-\ntrial practice. In a typical simulation-based functional ver-\nification exercise, a gate-level or RTL model of the circuit\nis simulated for a large number of cycles with specific in-\nput patterns. The values at observable outputs, as computed\nby the simulator, are then compared against their expected\nvalues, and any discrepancy is flagged as manifestaton of\na bug. The state of simulation technology today is mature\nenough to allow simulation of large designs within reason-\nable time using modest computational resources. Generating\ninput patterns that exercise diverse corners of the design’s\nbehavior space, however, remains a challenging problem [4].\n∗\nThis work was supported in part by NSF grants CNS 1049862 and\nCCF-1139011, by NSF Expeditions in Computing project ”ExCAPE:\nExpeditions in Computer Augmented Program Engineering”, by BSF\ngrant 9800096, by gift from Intel, by a grant from Board of Research\nin Nuclear Sciences, India, and by the Shared University Grid at Rice\nfunded by NSF under Grant EIA-0216467, and a partnership between\nRice University, Sun Microsystems, and Sigma Solutions, Inc.\nPermission to make digital or hard copies of all or part of this work for per-\nsonal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear\nthis notice and the full citation on the first page. Copyrights for components\nof this work owned by others than the author(s) must be honored. Abstract-\ning with credit is permitted. To copy otherwise, or republish, to post on\nservers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from Permissions@acm.org.\nDAC ’14, June 01-05 2014, San Francisco, CA, USA\nCopyright is held by the owner/author(s). Publication rights licensed to\nACM.\nACM 978-1-4503-2370-5/14/06$15.00.\nhttp://dx.doi.org/10.1145/2593069.2593097.\nIn recent years, constrained-random simulation (also called\nconstrained-random verification, or CRV) [21] has emerged\nas a practical approach to address the problem of simulat-\ning designs with “random enough” input patterns. In CRV,\nthe verification engineer declaratively specifies a set of con-\nstraints on the values of circuit inputs. Typically, these con-\nstraints are obtained from usage requirements, environmen-\ntal constraints, constraints on operating conditions and the\nlike. A constraint solver is then used to generate random val-\nues for the circuit inputs satisfying the constraints. Since the\ndistribution of errors in the design’s behavior space is not\nknown a priori, every solution to the set of constraints is as\nlikely to discover a bug as any other solution. It is therefore\nimportant to sample the space of all solutions uniformly or\nalmost-uniformly (defined formally below) at random. Un-\nfortunately, guaranteeing uniformity poses significant tech-\nnical challenges when scaling to large problem sizes. This has\nbeen repeatedly noted in the literature (see, for example, [8,\n22, 16]) and also confirmed by industry practitioners1. The\ndifficulties of generating solutions with guarantees of unifor-\nmity have even prompted researchers to propose alternative\ntechniques for generating input patterns [8, 22]. This paper\ntakes a step towards remedying this situation. Specifically,\nwe describe an algorithm for generating solutions to a set of\nBoolean constraints, with stronger guarantees on uniformity\nand with higher scalability in practice than that achieved\nearlier.\nSince constraints that arise in CRV of digital circuits are\nencodable as Boolean formulae, we focus on uniform gener-\nation of solutions of Boolean formulae. Henceforth, we call\nsuch solutions SAT witnesses. Besides its usefulness in CRV\nand in other applications [2, 23], uniform generation of SAT\nwitnesses has had strong theoretical interest as well [14].\nMost prior approaches to solving this problem belong to\none of two categories: those that focus on strong guaran-\ntees of uniformity but scale poorly in practice (examples be-\ning [27, 3, 14]), and those that provide practical heuristics\nto scale to large problem instances with weak or no guar-\nantees of uniformity (examples being [7, 16, 25])). In [5],\nChakraborty, Meel and Vardi attempted to bridge these ex-\ntremes through an algorithm called UniWit. More recently,\nErmon, Gomes, Sabharwal and Selman [9] proposed an algo-\nrithm called PAWS for sampling witnesses from discrete dis-\ntributions over large dimensions. While PAWS is designed\nto work with any discrete distribution specified through a\ngraphical model, for purposes of this paper, we focus only\non distributions that assign equal weight to every assign-\n1Private communication: R. Kurshan\nment. For such distributions, both PAWS and UniWit repre-\nsent alternative (albeit related) approaches to solve the same\nproblem – that of uniform generation of SAT witnesses. Un-\nfortunately, both algorithms suffer from inherent limitations\nthat make it difficult to scale them to Boolean constraints\nwith tens of thousands of variables and beyond. In addition,\nthe guarantees provided by these algorithms (in the context\nof uniform generation of SAT witnesses) are weaker than\nwhat one would desire in practice.\nIn this paper, we propose an algorithm called UniGen that\naddresses some of the deficiencies of UniWit and PAWS. This\nenables us to improve both the theoretical guarantees and\npractical performance vis-a-vis earlier algorithms in the con-\ntext of uniform generation of SAT witnesses. UniGen is the\nfirst algorithm to provide strong two-sided guarantees of\nalmost-uniformity, while scaling to problems involving hun-\ndreds of thousands of variables. We also improve upon the\nsuccess probability of the earlier algorithms significantly,\nboth in theory and as evidenced by our experiments.\n2. NOTATION AND PRELIMINARIES\nLet F be a Boolean formula in conjunctive normal form\n(CNF), and let X be the set of variables appearing in F . The\nset X is called the support of F . A satisfying assignment or\nwitness of F is an assignment of truth values to variables in\nits support such that F evaluates to true. We denote the set\nof all witnesses of F as RF . Let D ⊆ X be a subset of the\nsupport such that there are no two satisfying assignments of\nF that differ only in the truth values of variables in D. In\nother words, in every satisfying assignment of F , the truth\nvalues of variables in X \\ D uniquely determine the truth\nvalue of every variable in D. The set D is called a dependent\nsupport of F , and X \\ D is called an independent support\nof F . Note that there may be more than one independent\nsupports of F . For example, (a ∨ ¬b) ∧ (¬a ∨ b) has three\nindependent supports: {a}, {b} and {a, b}. Clearly, if I is\nan independent support of F , so is every superset of I. For\nnotational convenience, whenever the formula F is clear from\nthe context, we omit mentioning it.\nWe use Pr [X : P ] to denote the probability of outcome X\nwhen sampling from a probability space P . For notational\nclarity, we omit P when it is clear from the context. The\nexpected value of the outcome X is denoted E [X]. Given a\nBoolean formula F , a probabilistic generator of witnesses of\nF is a probabilistic algorithm that generates a random wit-\nness in RF . A uniform generator G\nu(·) is a probabilistic gen-\nerator that guarantees Pr [Gu(F ) = y] = 1/|RF |, for every\ny ∈ RF . An almost-uniform generator G\nau(·, ·) ensures that\nfor every y ∈ RF , we have\n1\n(1+ε)|RF |\n≤ Pr [Gau(F, ε) = y] ≤\n1+ε\n|RF |\n, where ε > 0 is the specified tolerance. A near-uniform\ngenerator Gnu(·) further relaxes the guarantee of unifor-\nmity, and ensures that Pr [Gnu(F ) = y] ≥ c/|RF | for a con-\nstant c, where 0 < c ≤ 1. Probabilistic generators are al-\nlowed to occasionally “fail” in the sense that no witness may\nbe returned even if RF is non-empty. The failure proba-\nbility for such generators must be bounded by a constant\nstrictly less than 1. The algorithm presented in this pa-\nper falls in the category of almost-uniform generators. An\nidea closely related to that of almost-uniform generation,\nand used in a key manner in our algorithm, is approxi-\nmate model counting. Given a CNF formula F , an exact\nmodel counter returns the size of RF . An approximate model\ncounter ApproxMC(·, ·, ·) relaxes this requirement to some\nextent. Given a CNF formula F , a tolerance ε > 0 and a con-\nfidence 1−δ ∈ (0, 1], and approximate model counter ensures\nthat Pr[ |RF |\n1+ε\n≤ ApproxMC(F, ε, 1− δ) ≤ (1+ε)|RF |] ≥ 1− δ.\nA special class of hash functions, called r-wise indepen-\ndent hash functions, play a crucial role in our work. Let n,m\nand r be positive integers, and let H(n,m, r) denote a fam-\nily of r-wise independent hash functions mapping {0, 1}n\nto {0, 1}m. We use h\nR\n←− H(n,m, r) to denote the prob-\nability space obtained by choosing a hash function h uni-\nformly at random from H(n,m, r). The property of r-wise\nindependence guarantees that for all α1, . . . αr ∈ {0, 1}\nm\nand for all distinct y1, . . . yr ∈ {0, 1}\nn, Pr\n[∧r\ni=1 h(yi) = αi\n: h\nR\n←− H(n,m, r)\n]\n= 2−mr . For every α ∈ {0, 1}m and\nh ∈ H(n,m, r), let h−1(α) denote the set {y ∈ {0, 1}n |\nh(y) = α}. Given RF ⊆ {0, 1}\nn and h ∈ H(n,m, r), we use\nRF,h,α to denote the set RF ∩h\n−1(α). If we keep h fixed and\nlet α range over {0, 1}m, the sets RF,h,α form a partition of\nRF .\n3. RELATED WORK\nMarrying scalability with strong guarantees of uniformity\nhas been the holy grail of algorithms that sample from solu-\ntions of constraint systems. The literature bears testimony\nto the significant tension between these objectives when de-\nsigning random generators of SAT witnesses. Earlier work\nin this area either provide strong theoretical guarantees at\nthe cost of scalability, or remedy the scalability problem at\nthe cost of guarantees of uniformity. More recently, however,\nthere have been efforts to bridge these two extremes.\nBellare, Goldreich and Petrank [3] showed that a provably\nuniform generator of SAT witnesses can be designed in the-\nory to run in probabilistic polynomial time relative to an NP\noracle. Unfortunately, it was shown in [5] that this algorithm\ndoes not scale beyond formulae with few tens of variables in\npractice. Weighted binary decision diagrams (BDD) have\nbeen used in [27] to sample uniformly from SAT witnesses.\nHowever, BDD-based techniques are known to suffer from\nscalability problems [16]. Adapted BDD-based techniques\nwith improved performance were proposed in [18]; however,\nthe scalability was achieved at the cost of guarantees of uni-\nformity. Random seeding of DPLL SAT solvers [20] has been\nshown to offer performance, although the generated distri-\nbutions of witnesses can be highly skewed [16].\nMarkov Chain Monte Carlo methods (also called MCMC\nmethods) [16, 26] are widely considered to be a practical way\nto sample from a distribution of solutions. Several MCMC al-\ngorithms, such as those based on simulated annealing, Metropolis-\nHastings algorithm and the like, have been studied exten-\nsively in the literature [15, 19]. While MCMC methods guar-\nantee eventual convergence to a target distribution under\nmild requirements, convergence is often impractically slow in\npractice. The work of [26, 16] proposed several such adapta-\ntions for MCMC-based sampling in the context of constrained-\nrandom verification. Unfortunately, most of these adapta-\ntions are heuristic in nature, and do not preserve theoret-\nical guarantees of uniformity. constraints, thereby increas-\ning constraint-solving time. Sampling techniques based on\ninterval-propagation and belief networks have been proposed\nin [7, 10, 13]. The simplicity of these approaches lend scala-\nbility to the techniques, but the generated distributions can\ndeviate significantly from the uniform distribution, as shown\nin [17].\nSampling techniques based on hashing were originally pi-\noneered by Sipser [24], and have been used subsequently by\nseveral researchers [3, 11, 5]. The core idea in hashing-based\nsampling is to use r-wise independent hash functions (for\na suitable value of r) to randomly partition the space of\nwitnesses into “small cells” of roughly equal size, and then\nrandomly pick a solution from a randomly chosen cell. The\nalgorithm of Bellare et al. referred to above uses this idea\nwith n-wise independent algebraic hash functions (where n\ndenotes the size of the support of F ). As noted above, their\nalgorithm scales very poorly in practice. Gomes, Sabhar-\nwal and Selman used 3-wise independent linear hash func-\ntions in [11] to design XORSample′, a near-uniform gener-\nator of SAT witnesses. Nevertheless, to realize the guar-\nantee of near-uniformity, their algorithm requires the user\nto provide difficult-to-estimate input parameters. Although\nXORSample′ has been shown to scale to constraints involv-\ning a few thousand variables, Gomes et al. acknowledge the\ndifficulty of scaling their algorithm to much larger problem\nsizes without sacrificing theoretical guarantees [11].\nRecently, Chakraborty, Meel and Vardi [5] proposed a new\nhashing-based SAT witness generator, called UniWit, that\nrepresents a small but significant step towards marrying the\nconflicting goals of scalability and guarantees of uniformity.\nLike XORSample′, the UniWit algorithm uses 3-wise indepen-\ndent linear hashing functions. Unlike XORSample′, however,\nthe guarantee of near-uniformity of witnesses generated by\nUniWit does not depend on difficult-to-estimate input pa-\nrameters. In [5], UniWit has been shown to scale to formulas\nwith several thousand variables. In addition, Chakraborty\net al proposed a heuristic called “leap-frogging” that allows\nUniWit to scale even further – to tens of thousands of vari-\nables [5]. Unfortunately, the guarantees of near-uniformity\ncan no longer be established for UniWit with “leap-frogging”.\nMore recently, Ermon et al. [9] proposed a hashing-based\nalgorithm called PAWS for sampling from a distribution de-\nfined over a discrete set using a graphical model. While the\nalgorithm presented in this paper has some similarities with\nPAWS, there are significant differences as well. Specifically,\nour algorithm provides much stronger theoretical guaran-\ntees vis-a-vis those offered by PAWS in the context of uni-\nform generation of SAT witness. In addition, our algorithm\nscales to hundreds of thousands of variables while preserv-\ning the theoretical guarantees. PAWS faces the same scala-\nbility hurdles as UniWit, and is unlikely to scale beyond a\nfew thousand variables without heuristic adapatations that\ncompromise its guarantees.\n4. THE UNIGEN ALGORITHM\nThe new algorithm, called UniGen, falls in the category\nof hashing-based almost-uniform generators. UniGen shares\nsome features with earlier hashing-based algorithms such as\nXORSample′ [11], UniWit [5] and PAWS [9], but there are\nkey differences that allow UniGen to significantly outper-\nform these earlier algorithms, both in terms of theoretical\nguarantees and measured performance.\nGiven a CNF formula F , we use a family of 3-independent\nhash functions to randomly partition the set, RF , of wit-\nnesses of F . Let h : {0, 1}n → {0, 1}m be a hash function in\nthe family, and let y be a vector in {0, 1}n. Let h(y)[i] de-\nnote the ith component of the vector obtained by applying\nh to y. The family of hash functions of interest is defined as\n{h(y) | h(y)[i] = ai,0 ⊕ (\n⊕n\nk=1 ai,k · y[k]), ai,j ∈ {0, 1}, 1 ≤\ni ≤ m, 0 ≤ j ≤ n}, where ⊕ denotes the xor operation. By\nchoosing values of ai,j randomly and independently, we can\neffectively choose a random hash function from the family. It\nhas been shown in [11] that this family of hash functions is\n3-independent. Following notation introduced in Section 2,\nwe call this family Hxor(n,m, 3).\nWhileHxor(n,m, 3) was used earlier in XORSample\n′, PAWS,\nand (in a variant of) UniWit, there is a fundamental differ-\nence in the way we use it in UniGen. LetX = {x1, x2, . . . x|X|}\nbe the set of variables of F . Given m > 0, the algorithms\nXORSample′, PAWS and UniWit partition RF by randomly\nchoosing h ∈ Hxor(|X|, m, 3) and α ∈ {0, 1}\nm, and by seek-\ning witnesses of F conjoined with\n∧m\ni=1\n(\nh(x1, . . . x|X|)[i]↔ α[i]\n)\n.\nBy choosing a random h(x1, . . . x|X|) ∈ Hxor(|X|, m, 3), the\nset of all assignments to variables inX (regardless of whether\nthey are witnesses of F ) is partitioned randomly. This, in\nturn, ensures that the set of satisfying assignments of F is\nalso partitioned randomly. Each conjunctive constraint of\nthe form (h(x1 . . . x|X|)[i] ↔ α[i]) is an xor of a subset of\nvariables of X and α[i], and is called an xor-clause. Ob-\nserve that the expected number of variables in each such\nxor-clause is approximately |X|/2. It is well-known (see, for\nexample [12]) that the difficulty of checking satisfiability of\na CNF formula with xor-clauses grows significantly with the\nnumber of variables per xor-clause. It is therefore extremely\ndifficult to scale XORSample′, PAWS or UniWit to problems\ninvolving hundreds of thousands of variables. In [5], an alter-\nnative family of linear hash functions is proposed to be used\nwith UniWit. Unfortunately, this also uses |X|/2 variables\nper xor-clause on average, and suffers from the same prob-\nlem. In [12], a variant of Hxor(|X|, m, 3) is used, wherein\neach variable in X is chosen to be in an xor-clause with a\nsmall probability q (< 0.5). This mitigates the performace\nbottleneck significantly, but theoretical guarantees of (near-\n)uniformity are lost.\nWe address the above problem in UniGen by making two\nimportant observations: (i) an independent support I of F\nis often far smaller (sometimes by a few orders of magni-\ntude) than X, and (ii) since the value of every variable in\nX \\I in a satisfying assignment of F is uniquely determined\nby the values of variables in I, the set RF can be randomly\npartitioned by randomly partitioning its projection on I.\nThis motivates us to design an almost-uniform generator\nthat accepts a subset S of the support of F as an additional\ninput. We call S the set of sampling variables of F , and in-\ntend to use an independent support of F (not necessarily\na minimal one) as the value of S in any invocation of the\ngenerator. Without loss of generality, let S = {x1, . . . x|S|},\nwhere |S| ≤ |X|. The set RF can now be partitioned by ran-\ndomly choosing h ∈ Hxor(|S|, m, 3) and α ∈ {0, 1}\nm, and\nby seeking solutions of F ∧\n∧m\ni=1\n(\nh(x1, . . . x|S|)[i]↔ α[i]\n)\n.\nIf |S| ≪ |X| (as is often the case in our experience), the\nexpected number of variables per xor-clause is significantly\nreduced. This makes satisfiability checking easier, and allows\nscaling to much larger problem sizes than otherwise possi-\nble. It is natural to ask if finding an independent support of\na CNF formula F is computationally easy. While an algo-\nrithmic solution to this problem is beyond the scope of this\npaper, our experience indicates that a small, not necessar-\nily minimal, independent support can often be easily deter-\nmined from the source domain from which the CNF formula\nF is derived. For example, when a non-CNF formula G is\nconverted to an equisatisfiable CNF formula F using Tseitin\nencoding, the variables introduced by the encoding form a\ndependent support of F .\nThe effectiveness of a hashing-based probabilistic gener-\nator depends on its ability to quickly partition the set RF\ninto “small” and “roughly equal” sized random cells. This, in\nturn, depends on the parameter m used in the choice of the\nhash function family H(n,m, r). A high value of m leads to\nskewed distributions of sizes of cells, while a low value of m\nleads to cells that are not small enough. The best choice of\nm depends on |RF |, which is not known a priori. Different\nalgorithms therefore use different techniques to estimate a\nvalue of m. In XORSample′, this is achieved by requiring the\nuser to provide some difficult-to-estimate input parameters.\nIn UniWit, the algorithm sequentially iterates over values\nof m until a good enough value is found. The approach of\nPAWS comes closest to our, although there are crucial dif-\nferences. In both PAWS and UniGen, an approximate model\ncounter is first used to estimate |RF | within a specified tol-\nerance and with a specified confidence. This estimate, along\nwith a user-provided parameter, is then used to determine\na unique value of m in PAWS. Unfortunately, this does not\nfacilitate proving that PAWS is an almost-uniform genera-\ntor. Instead, Ermon, et al. show that PAWS behaves like\nan almost-uniform generator with probability greater than\n1 − δ, for a suitable δ that depends on difficult-to-estimate\ninput parameters. In contrast, we use the estimate of |RF |\nto determine a small range of candidate values of m. This\nallows us to prove that UniGen is almost-uniform generator\nwith confidence 1.\nAlgorithm 1 UniGen(F, ε, S)\n/*Assume S = {x1, . . . x|S|} is an independent support of F ,\nand ε > 1.71 */\n1: (κ,pivot)← ComputeKappaPivot(ε);\n2: hiThresh← 1 + (1 + κ)pivot;\n3: loThresh← 1\n1+κ\npivot;\n4: Y ← BSAT(F,hiThresh);\n5: if (|Y | ≤ hiThresh) then\n6: Let y1, . . . y|Y | be the elements of Y ;\n7: Choose j at random from {1, . . . |Y |}; return yj ;\n8: else\n9: C ← ApproxModelCounter(F, 0.8, 0.8);\n10: q ← ⌈logC + log 1.8− log pivot⌉;\n11: i← q − 4;\n12: repeat\n13: i← i+ 1;\n14: Choose h at random from Hxor(|S|, i, 3);\n15: Choose α at random from {0, 1}i;\n16: Y ← BSAT(F ∧ (h(x1, . . . x|S|) = α),hiThresh);\n17: until (loThresh ≤ |Y | ≤ hiThresh) or (i = q)\n18: if (|Y | > hiThresh) or (|Y | < loThresh) then\n19: return ⊥\n20: else\n21: Let y1, . . . y|Y | be the elements of Y ;\n22: Choose j at random from [|Y |] and return yj ;\nThe pseudocode for UniGen is shown in Algorithm 1. UniGen\ntakes as inputs a Boolean CNF formula F , a tolerance ε\n(> 1.71, for teachnical reasons explained in the Appendix)\nand a set S of sampling variables. It either returns a ran-\nAlgorithm 2 ComputeKappaPivot(tε)\nFind κ ∈ [0, 1) such that ε = (1 + κ)(2.23 + 0.48\n(1−κ)2\n)− 1 ;\npivot← ⌈3e1/2(1 + 1\nκ\n)2⌉;\nreturn (κ, pivot)\ndom witness of F or ⊥ (indicating failure). The algorithm\nassumes access to a source of random binary numbers, and\nto two subroutines: (i) BSAT(F,N), which, for every N > 0,\nreturns min(|RF |, N) distinct witnesses of F , and (ii) an ap-\nproximate model counter ApproxModelCounter(F, ε′, 1− δ′).\nUniGen first computes two quantities, “pivot” and κ, that\nrepresent the expected size of a“small” cell and the tolerance\nof this size, respectively. The specific choices of expressions\nused to compute κ and “pivot” in ComputeKappaPivot are\nmotivated by technical reasons explained in the Appendix.\nThe values of κ and “pivot” are used to determine high and\nlow thresholds (denoted “hiThresh” and “loThresh” respec-\ntively) for the size of each cell. Lines 5–7 handle the easy case\nwhen F has no more than “hiThresh” witnesses. Otherwise,\nUniGen invokes ApproxModelCounter to obtain an estimate,\nC, of |RF | to within a tolerance of 0.8 and with a confi-\ndence of 0.8. Once again, the specific choices of the tolerance\nand confidence parameters used in computing C are moti-\nvated by technical reasons explained in the Appendix. The\nestimate C is then used to determine a range of candidate\nvalues for m. Specifically, this range is {q − 4, . . . q}, where\nq is determined in line 10 of the pseudocode. The loop in\nlines 12–17 checks whether some value in this range is good\nenough for m, i.e., whether the number of witnesses in a cell\nchosen randomly after partitioning RF usingHxor(|S|, m, 3),\nlies within “hiThresh” and “loThresh”. If so, lines 21–22 re-\nturn a random witness from the chosen cell. Otherwise, the\nalgorithm reports a failure in line 19.\nAn probabilistic generator is likely to be invoked multi-\nple times with the same input constraint in constrained-\nrandom verification. Towards this end, note than lines 1–11\nof the pseudocode need to executed only once for every for-\nmula F . Generating a new random witness requires execut-\ning afresh only lines 12–22. While this optimization appears\nsimilar to “leapfrogging” [5, 6], it is fundamentally different\nsince it does not sacrifice any theoretical guarantees, unlike\n“leapfrogging”.\nImplementation issues: In our implementation of UniGen,\nBSAT is implemented using CryptoMiniSAT [1] – a SAT\nsolver that handles xor clauses efficiently. CryptoMiniSAT\nuses blocking clauses to prevent already generated witnesses\nfrom being generated again. Since the independent support\nof F determines every satisfying assignment of F , blocking\nclauses can be restricted to only variables in the set S. We\nimplemented this optimization in CryptoMiniSAT, leading\nto significant improvements in performance. ApproxModelCounter\nis implemented using ApproxMC [6]. Although the authors\nof [6] used “leapfrogging” in their experiments, we disable\nthis optimization since it nullifies the theoretical guarantees\nof [6]. We use “random device” implemented in C++ as the\nsource of pseudo-random numbers in lines 7, 14, 15 and 22 of\nthe pseudocode, and also as the source of random numbers\nin ApproxMC.\nGuarantees: The following theorem shows that UniGen is\nan almost-uniform generator with a high success probability.\nTheorem 1. If S is an independent support of F and if\nε > 1.71, then for every y ∈ RF , we have\n1\n(1 + ε)(|RF | − 1)\n≤ Pr [UniGen(F, ε, S) = y] ≤ (1+ε)\n1\n|RF | − 1\n.\nIn addition, Pr [UniGen(F, ε, S) 6= ⊥] ≥ 0.62.\nFor lack of space, we defer the proof to the Appendix. It\ncan be shown that UniGen runs in time polynomial in ε−1\nand in the size of F , relative to an NP-oracle.\nThe guarantees provided by Theorem 1 are significantly\nstronger than those provided by earlier generators that scale\nto large problem instances. Specifically, neither XORSample′ [11]\nnor UniWit [5] provide strong upper bounds for the probabil-\nity of generation of a witness. PAWS [9] offers a probabilistic\nguarantee that the probability of generation of a witness lies\nwithin a tolerance factor of the uniform probability, while\nthe guarantee of Theorem 1 is not prbabilistic. The success\nprobability of PAWS, like that of XORSample′, is bounded\nbelow by an expression that depends on difficult-to-estimate\ninput parameters. Interestingly, the same parameters also\ndirectly affect the tolerance of distribution of the generated\nwitnesses. The success probability of UniWit is bounded be-\nlow by 0.125, which is significantly smaller than the lower\nbound of 0.62 guaranteed by Theorem 1.\nTrading scalability with uniformity: The tolerance pa-\nrameter ε provides a knob to balance scalability and unifor-\nmity in UniGen. Smaller values of ε lead to stronger guaran-\ntees of uniformity (by Theorem 1). Note, however, that the\nvalue of “hiThresh” increases with decreasing values of ε, re-\nquiring BSAT to find more witnesses. Thus, each invocation\nof BSAT is likely to take longer as ε is reduced.\n5. EXPERIMENTAL RESULTS\nTo evaluate the performance of UniGen, we built a proto-\ntype implementation and conducted an extensive set of ex-\nperiments. Industrial constrained-random verification prob-\nlem instances are typically proprietary and unavailable for\npublished research. Therefore, we conducted experiments on\nCNF SAT constraints arising from several problems avail-\nable in the public-domain. These included bit-blasted ver-\nsions of constraints arising in bounded model checking of cir-\ncuits and used in [5], bit-blasted versions of SMTLib bench-\nmarks, constraints arising from automated program synthe-\nsis, and constraints arising from ISCAS89 circuits with par-\nity conditions on randomly chosen subsets of outputs and\nnext-state variables.\nTo facilitate running multiple experiments in parallel, we\nused a high-performance cluster and ran each experiment\non a node of the cluster. Each node had two quad-core In-\ntel Xeon processors with 4 GB of main memory. Recalling\nthe terminology used in the pseudocode of UniGen (see Sec-\ntion 4), we set the tolerance ε to 6, and the sampling set\nS to an independent support of F in all our experiments.\nIndependent supports (not necessarily minimal ones) for all\nbenchmarks were easily obtained from the providers of the\nbenchmarks on request. We used 2, 500 seconds as the time-\nout for each invocation of BSAT and 20 hours as the overall\ntimeout for UniGen, for each problem instance. If an invo-\ncation of BSAT timed out in line 16 of the pseudocode of\nUniGen, we repeated the execution of lines 14–16 without\nincrementing i. With this set-up, UniGen was able to suc-\ncessfully generate random witnesses for formulas having up\nto 486, 193 variables.\nFor performance comparisons, we also implemented and\nconducted experiments with UniWit – a state-of-art near-\nuniform generator [5]. Our choice of UniWit as a reference\nfor comparison is motivated by several factors. First, UniGen\nand UniWit share some commonalities, and UniGen can be\nviewed as an improvement of UniWit. Second, XORSample′\nis known to perform poorly vis-a-vis UniWit [5]; hence, com-\nparing with XORSample′ is not meaningful. Third, the im-\nplementation of PAWS made available by the authors of [9]\ncurrently does not accept CNF formulae as inputs. It ac-\ncepts only a graphical model of a discrete distribution as in-\nput, making a direct comparison with UniGen difficult. Since\nPAWS and UniWit share the same scalability problem re-\nlated to large random xor-clauses, we chose to focus only on\nUniWit. Since the“leapfrogging” heuristic used in [5] nullifies\nthe guarantees of UniWit, we disabled this optimization. For\nfairness of comparison, we used the same timeouts in UniWit\nas used in UniGen, i.e. 2, 500 seconds for every invocation of\nBSAT, and 20 hours overall for every invocation of UniWit.\nTable 1 presents the results of our performance-comparison\nexperiments. Column 1 lists the CNF benchmark, and columns\n2 and 3 give the count of variables and size of independent\nsupport used, respectively. The results of experiments with\nUniGen are presented in the next 3 columns. Column 4 gives\nthe observed probability of success of UniGen when gener-\nating 1, 000 random witnesses. Column 5 gives the average\ntime taken by UniGen to generate one witness (averaged over\na large number of runs), while column 6 gives the average\nnumber of variables per xor-clause used for randomly parti-\ntioning RF . The next two columns give results of our exper-\niments with UniWit. Column 7 lists the average time taken\nby UniWit to generate a random witness, and column 8 gives\nthe average number of variables per xor-clause used to parti-\ntion RF . A “−” in any column means that the corresponding\nexperiment failed to generate any witness in 20 hours.\nIt is clear from Table 1 that the average run-time for gen-\nerating a random witness by UniWit can be two to three or-\nders of magnitude larger than the corresponding run-time for\nUniGen. This is attributable to two reasons. The first stems\nfrom fewer variables in xor-clauses and blocking clauses when\nsmall independent supports are used. Benchmark“tutorial3”\nexemplifies this case. Here, UniWit failed to generate any wit-\nness because all calls to BSAT in UniWit, with xor-clauses\nand blocking clauses containing numbers of variables, timed\nout. In contrast, the calls to BSAT in UniGen took much\nless time, due to short xor-clauses and blocking clauses us-\ning only variables from the independent support. The other\nreason for UniGen’s improved efficiency is that the compu-\ntationally expensive step of identifying a a good range of\nvalues for m (see Section 4 for details) needs to be executed\nonly once per benchmark. Subsequently, whenever a random\nwitness is needed, UniGen simply iterates over this narrow\nrange of m. In contrast, generating every witness in UniWit\n(without leapfrogging) requires sequentially searching over\nall values afresh to find a good choice for m. Referring to\nTable 1, UniWit requires more than 20, 000 seconds on av-\nerage to find a good value for m and generate a random\nwitness for benchmark “s953a 3 2”. Unlike in UniGen, there\nis no way to amortize this large time over multiple runs in\nUniWit, while preserving the guarantee of near-uniformity.\nTable 1 also shows that the observed success probabil-\nity of UniGen is almost always 1, much higher than what\nTheorem 1 guarantees and better than those from UniWit.\nIt is clear from our experiments that UniGen can scale to\nproblems involving almost 500K variables, while preserv-\ning guarantees of almost uniformity. This goes much beyond\nthe reach of any other random-witness generator that gives\nstrong guarantees on the distribution of witnesses.\n 0\n 50\n 100\n 150\n 200\n 250\n 300\n 350\n 400\n 450\n 500\n 160  180  200  220  240  260  280  300  320\n#\n o\nf \nS\no\nlu\nti\no\nn\ns\nCount\nUS\nUniGen\nFigure 1: Uniformity comparison for case110\nTheorem 1 guarantees that the probability of generation\nof every witness lies within a specified tolerance of the uni-\nform probability. In practice, however, the distribution of\nwitnesses generated by UniGen is much more closer to a\nuniform distribution. To illustrate this, we implemented a\nuniform sampler, henceforth called US, and compared the\ndistributions of witnesses generated by UniGen and by US\nfor some representative benchmarks. Given a CNF formula\nF , US first determines |RF | using an exact model counter\n(such as sharpSAT). To mimic generating a random witness,\nUS simply generates a random number i in {1 . . . |RF |}. To\nensure fair comparison, we used the same source of random-\nness in both UniGen and US. For every problem instance\non which the comparison was done, we generated a large\nnumber N (= 4 × 106) of sample witnesses using each of\nUS and UniGen. In each case, the number of times various\nwitnesses were generated was recorded, yielding a distribu-\ntion of the counts. Figure 1 shows the distributions of counts\ngenerated by UniGen and by US for one of our benchmarks\n(case110) with 16, 384 witnesses. The horizontal axis rep-\nresents counts and the vertical axis represents the number\nof witnesses appearing a specified number of times. Thus,\nthe point (242, 450) represents the fact that each of 450 dis-\ntinct witnesses were generated 242 times in 4 × 106 runs.\nObserve that the distributions resulting from UniGen and\nUS can hardly be distinguished in practice. This holds not\nonly for this benchmark, but for all other benchmarks we\nexperimented with.\nOverall, our experiments confirm that UniGen is two to\nthree orders of magnitude more efficient than state-of-the-\nart random witness generators, has probability of success\nalmost 1, and preserves strong guarantees about the unifor-\nmity of generated witnesses. Furthermore, the distribution\nof generated witnesses can hardly be distinguished from that\nof a uniform sampler in practice.\n6. CONCLUSION\nStriking a balance between scalability and uniformity is\na difficult challenge when designing random witness genera-\ntors for constrained-random verification. UniGen is the first\nsuch generator for Boolean CNF formulae that scales to hun-\ndreds of thousands of variables and still preserves strong\nguarantees of uniformity. In future, we wish to investigate\nthe design of scalable generators with similar guarantees for\nSMT constraints, leveraging recent progress in satisfiability\nmodulo theories.\nAcknowledgments\nWe profusely thank Mate Soos for implementing required\nAPIs in CryptoMiniSAT without which the experimentation\nsection would have been incomplete. Mate was generous with\nhis suggestions to improve our implementation. We thank\nAjith John for his help in experimental setup.\n7. REFERENCES\n[1] CryptoMiniSAT.\nhttp://www.msoos.org/cryptominisat2/.\n[2] F. Bacchus, S. Dalmao, and T. Pitassi. Algorithms\nand complexity results for #SAT and Bayesian\ninference. In Proc. of FOCS, pages 340–351, 2003.\n[3] M. Bellare, O. Goldreich, and E. Petrank. Uniform\ngeneration of NP-witnesses using an NP-oracle.\nInformation and Computation, 163(2):510–526, 1998.\n[4] L. Bening and H. Foster. Principles of verifiable RTL\ndesign – a functional coding style supporting\nverification processes. Springer, 2001.\n[5] S. Chakraborty, K. Meel, and M. Vardi. A scalable\nand nearly uniform generator of SAT witnesses. In\nProc. of CAV, 2013.\n[6] S. Chakraborty, K. S. Meel, and M. Y. Vardi. A\nscalable approximate model counter. In Proc. of CP,\n2013.\n[7] R. Dechter, K. Kask, E. Bin, and R. Emek.\nGenerating random solutions for constraint\nsatisfaction problems. In AAAI, 2002.\n[8] S. Deng, Z. Kong, J. Bian, and Y. Zhao. Self-adjusting\nconstrained random stimulus generation using\nsplitting evenness evaluation and xor constraints. In\nProc. of ASP-DAC, pages 769–774. IEEE, 2009.\n[9] S. Ermon, C. P. Gomes, A. Sabharwal, and B. Selman.\nEmbed and project: Discrete sampling with universal\nhashing. In Proc. of NIPS, 2013.\n[10] V. Gogate and R. Dechter. A new algorithm for\nsampling csp solutions uniformly at random. In CP,\npages 711–715, 2006.\n[11] C. Gomes, A. Sabharwal, and B. Selman. Near\nuniform sampling of combinatorial spaces using XOR\nconstraints. In Proc. of NIPS, pages 670–676, 2007.\n[12] C. P. Gomes, J. Hoffmann, A. Sabharwal, and\nB. Selman. Short XORs for model counting; from\ntheory to practice. In SAT, pages 100–106, 2007.\n[13] M. A. Iyer. Race: A word-level atpg-based constraints\nsolver system for smart random simulation. In ITC,\npages 299–308. Citeseer, 2003.\n[14] M. Jerrum, L. Valiant, and V. Vazirani. Random\ngeneration of combinatorial structures from a uniform\ndistribution. TCS, 43(2-3):169–188, 1986.\n[15] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi.\nOptimization by simulated annealing. Science,\n220(4598):671–680, 1983.\n[16] N. Kitchen. Markov Chain Monte Carlo Stimulus\nGeneration for Constrained Random Simulation. PhD\nthesis, University of California, Berkeley, 2010.\n[17] N. Kitchen and A. Kuehlmann. Stimulus generation\nfor constrained random simulation. In Proc. of\nTable 1: Runtime performance comparison of UniGen and UniWit\nUniGen UniWit\nBenchmark |X| |S|\nSucc\nProb\nAvg\nRun Time (s)\nAvg\nXOR leng\nAvg\nRun Time (s)\nAvg\nXOR len\nSucc\nProb\nSquaring7 1628 72 1.0 2.44 36 2937.5 813 0.87\nsquaring8 1101 72 1.0 1.77 36 5212.19 550 1.0\nSquaring10 1099 72 1.0 1.83 36 4521.11 550 0.5\ns1196a 7 4 708 32 1.0 6.9 16 833.1 353 0.37\ns1238a 7 4 704 32 1.0 7.26 16 1570.27 352 0.35\ns953a 3 2 515 45 0.99 12.48 23 22414.86 257 *\nEnqueueSeqSK 16466 42 1.0 32.39 21 – – –\nLoginService2 11511 36 0.98 6.14 18 – – –\nLLReverse 63797 25 1.0 33.92 13 3460.58 31888 0.63\nSort 12125 52 0.99 79.44 26 – – –\nKaratsuba 19594 41 1.0 85.64 21 – – –\ntutorial3 486193 31 0.98 782.85 16 – – –\nA “*” entry indicates insufficient data for estimating success probability\nICCAD, pages 258–265, 2007.\n[18] J. H. Kukula and T. R. Shiple. Building circuits from\nrelations. In Proc. of CAV, pages 113–123, 2000.\n[19] N. Madras. Lectures on monte carlo methods, fields\ninstitute monographs 16. AMS, 2002.\n[20] M. W. Moskewicz, C. F. Madigan, Y. Zhao, L. Zhang,\nand S. Malik. Chaff: Engineering an efficient sat\nsolver. In Proc. of DAC, pages 530–535, 2001.\n[21] Y. Naveh, M. Rimon, I. Jaeger, Y. Katz, M. Vinov,\nE. Marcus, and G. Shurek. Constraint-based random\nstimuli generation for hardware verification. In Proc of\nIAAI, pages 1720–1727, 2006.\n[22] S. M. Plaza, I. L. Markov, and V. Bertacco. Random\nstimulus generation using entropy and xor constraints.\nIn Proc. of DAC, pages 664–669, 2008.\n[23] D. Roth. On the hardness of approximate reasoning.\nArtificial Intelligence, 82(1):273–302, 1996.\n[24] M. Sipser. A complexity theoretic approach to\nrandomness. In Proc. of STOC, pages 330–335, 1983.\n[25] W. Wei, J. Erenrich, and B. Selman. Towards efficient\nsampling: Exploiting random walk strategies. In Proc.\nof AAAI, pages 670–676, 2004.\n[26] W. Wei and B. Selman. A new approach to model\ncounting. In Proc. of SAT, pages 2293–2299, 2005.\n[27] J. Yuan, A. Aziz, C. Pixley, and K. Albin. Simplifying\nboolean constraint solving for random simulation\nvector generation. TCAD, 23(3):412–420, 2004.\nAPPENDIX\nIn this section, we present a proof of Theorem 1, originally\nstated in Section 4, and also present an extended table of\nperformance comparison results.\nRecall that UniGen is a probabilistic algorithm that takes\nas inputs a Boolean CNF formula F , a tolerance ε and a\nsubset S of the support of F . We first show that if X is the\nsupport of F , and if S ( X is an independent support of F ,\nthen UniGen(F , ε, S) behaves identically (in a probabilis-\ntic sense) to UniGen(F , ε, X). Once this is established, the\nremainder of the proof proceeds by making the simplifying\nassumption S = X.\nClearly, the above claim holds trivially if X = S. There-\nfore, we focus only on the case when S ( X. For nota-\ntional convenience, we assume X = {x1, . . . xn}, 0 ≤ k < n,\nS = {x1, . . . xk} and D = {xk+1, . . . xn} in all the state-\nments and proofs in this section. We also use ~X to denote\nthe vector (x1, . . . xn), and similarly for ~S and ~D.\nLemma 1. Let F ( ~X) be a Boolean function with support\nX, and let S be an independent support of F . Then there\nexist Boolean functions g0, g1, . . . gn−k, each with support S\nsuch that\nF ( ~X)↔\n(\ng0(~S) ∧\nn−k∧\nj=1\n(xk+j ↔ gj(~S))\n)\nProof. Since S is an independent support of F , we have\nD = X \\ S is a dependent support of F . From the def-\ninition of a dependent support, there exist Boolean func-\ntions g1, . . . gk, each with support S, such that F ( ~X) →∧n−k\nj=1 (xk+j ↔ gj(\n~S)).\nLet g0(~S) be the characteristic function of the projection\nof RF on S. More formally, g0(~S) ≡\n∨\n(xk+1,...xn)∈{0,1}\nn−k F ( ~X).\nIt follows that F ( ~X) → g0(~S). Combining this with the re-\nsult from the previous paragraph, we get the implication\nF ( ~X) →\n(\ng0(~S) ∧\n∧n−k\nj=1 (xk+j ↔ gj(\n~S))\n)\nFrom the definition of g0(~S) given above, we have g0(~S)→\nF (~S, xk+1, . . . xn), for some values of xk+1, . . . xn. However,\nwe also know that F ( ~X)→\n∧n−k\nj=1 (xk+j ↔ gj(\n~S)). It follows\nthat\n(\ng(~S) ∧\n∧n−k\nj=1 (xk+j ↔ gj(\n~S))\n)\n→ F ( ~X).\nReferring to the pseudocode of UniGen in Section 4, we ob-\nserve that the only steps that depend directly on S are those\nin line 14, where h is chosen randomly from Hxor(|S|, i, 3),\nand line 16, where the set Y is computed by calling BSAT(F∧\n(h(x1, . . . x|S|) = α),hiThresh). Since all subsequent steps of\nthe algorithm depend only on Y , it suffices to show that if S\nis an independent support of F , the probability distribution\nof Y obtained at line 16 is identical to what we would obtain\nif S was set equal to the entire support, X, of F .\nThe following lemma formalizes the above statement. As\nbefore, we assume X = {x1, . . . xn} and S = {x1, . . . xk}.\nLemma 2. Let S be an independent support of F ( ~X). Let h\nand h′ be hash functions chosen uniformly at random from\nHxor(k, i, 3) and Hxor(n, i, 3), respectively. Let α and α\n′ be\ntuples chosen uniformly at random from {0, 1}i. Then, for\nevery Y ∈ {0, 1}n and for every t > 0, we have\nPr\n[\nBSAT\n(\nF ( ~X) ∧ (h(~S) = α), t\n)\n= Y\n]\n=\nPr\n[\nBSAT\n(\nF ( ~X) ∧ (h′( ~X) = α′), t\n)\n= Y\n]\nProof. Since h′ is chosen uniformly at random fromHxor(n, i, 3),\nrecalling the definition ofHxor(n, i, 3), we have F ( ~X)∧(h\n′( ~X) =\nα′) ≡ F ( ~X)∧\n∧i\nl=1\n(\n(al,0 ⊕\n⊕n\nj=1 al,j · x[j])↔ α\n′[l]\n)\n, where\nthe al,js are chosen independently and identically randomly\nfrom {0, 1}.\nSince S is an independent support of F , from Lemma 1,\nthere exist Boolean functions g1, . . . gn−k, each with sup-\nport S, such that F ( ~X) →\n∧n−k\nj=1 (xk+j ↔ gj(\n~S)). There-\nfore, F ( ~X) ∧ (h′( ~X) = α′) is semantically equivalent to\nF ( ~X) ∧\n∧i\nl=1\n(\n(al,0 ⊕\n⊕k\nj=1 al,j · x[j]⊕B)↔ α\n′[l]\n)\n, where\nB ≡\n⊕n\nj=k+1 al,j ·gj−k(\n~S). Rearranging terms, we get F ( ~X)∧∧i\nl=1\n(\n(al,0 ⊕\n⊕k\nj=1 al,j · x[j])↔ (α\n′[l]⊕B)\n)\n.\nSince α′ is chosen uniformly at random from {0, 1}i and\nsince B is independent of α′, it is easy to see that α′[l]⊕B is a\nrandom binary variable with equal probability of being 0 and\n1. It follows that Pr\n[\nBSAT(F ( ~X) ∧ (h′( ~X) = α′), t) = Y\n]\n=\nPr\n[\nBSAT(F ( ~X) ∧ (h(~S) = α), t) = Y\n]\n.\nLemma 2 allows us to continue with the remainder of the\nproof assuming S = X. It has already been shown in [11]\nthat Hxor(n,m, 3) is a 3-independent family of hash func-\ntions. We use this fact in a key way in the remainder of\nour analysis. The following result about Chernoff-Hoeffding\nbounds, proved in [6], plays an important role in our discus-\nsion.\nTheorem 1. Let Γ be the sum of r-wise independent ran-\ndom variables, each of which is confined to the interval [0, 1],\nand suppose E[Γ] = µ. For 0 < β ≤ 1, if 2 ≤ r ≤\n⌊\nβ2µe−1/2\n⌋\n≤\n4 , then Pr [ |Γ− µ| ≥ βµ ] ≤ e−r/2.\nUsing notation introduced in Section 2, let RF denote the\nset of witnesses of the Boolean formula F . For convenience\nof analysis, we assume that log(|RF | − 1) − log pivot is an\ninteger, where pivot is the quantity computed by algorithm\nComputeKappaPivot (see Section 4). A more careful analy-\nsis removes this assumption by scaling the probabilities by\nconstant factors. Let us denote log(|RF | − 1) − log pivot by\nm. The expression used for computing pivot in algorithm\nComputeKappaPivot ensures that pivot ≥ 17. Therefore, if\nan invocation of UniGen does not return from line 7 of the\npseudocode, then |RF | ≥ 18. Note also that the expression\nfor computing κ in algorithm ComputeKappaPivot requires\nε ≥ 1.71 in order to ensure that κ ∈ [0, 1) can always be\nfound.\nThe following lemma shows that q, computed in line 10 of\nthe pseudocode, is a good estimator of m.\nLemma 3. Pr[q − 3 ≤ m ≤ q] ≥ 0.8\nProof. Recall that in line 9 of the pseudocode, an approx-\nimate model counter is invoked to obtain an estimate, C,\nof |RF | with tolerance 0.8 and confidence 0.8. By the def-\ninition of approximate model counting, we have Pr[ C\n1.8\n≤\n|RF | ≤ (1.8)C] ≥ 0.8. Thus, Pr[logC− log(1.8) ≤ log |RF | ≤\nlogC + log(1.8)] ≥ 0.8. It follows that Pr[logC − log(1.8) −\nlog pivot−log( 1\n1−1/|RF |\n) ≤ log(|RF |−1)−log pivot ≤ logC−\nlog pivot+ log(1.8)− log( 1\n1−1/|RF |\n)] ≥ 0.8. Substituting q =\n⌈logC + log 1.8 − log pivot⌉, m = log(|RF | − 1) − log pivot ,\nlog(1.8) = 0.85 and log( 1\n1−1/|RF |\n) ≤ 0.12 (since |RF | ≥ 18\non reaching line 10 of the pseudocode), we get Pr[q − 3 ≤\nm ≤ q] ≥ 0.8.\nThe next lemma provides a lower bound on the probabil-\nity of generation of a witness. Let wi,y,α denote the proba-\nbility Pr\n[\npivot\n1+κ\n≤ |RF,h,α| ≤ 1 + (1 + κ)pivot and h(y) = α\n: h\nR\n←− Hxor(n, i, 3)\n]\n. The proof of the lemma also provides\na lower bound on wm,y,α.\nLemma 4. For every witness y of F , Pr[y is output] ≥\n0.8(1−e−1)\n(1.06+κ)(|RF |−1)\nProof. If |RF | ≤ 1 + (1 + κ)pivot, the lemma holds trivially\n(see lines 5–7 of the pseudocode). Suppose |RF | ≥ 1 + (1 +\nκ)pivot and let U denote the event that witness y ∈ RF is\noutput by UniGen on inputs F , ε and X. Let pi,y denote the\nprobability that we return from line 17 for a particular value\nof i with y in RF,h,α, where α ∈ {0, 1}\ni is the value chosen\nin line 15. Then, Pr[U ] =\n∑q\ni=q−3\n1\n|Y |\npi,y\n∏i−1\nj=q−3(1 − pj,y),\nwhere Y is the set of witnesses returned by BSAT in line\n16 of the pseudocode. Let fm = Pr[q − 3 ≤ m ≤ q]. From\nLemma 3, we know that fm ≥ 0.8. From the design of the\nalgorithm, we also know that 1\n1+κ\npivot ≤ |Y | ≤ 1 + (1 +\nκ)pivot. Therefore, Pr[U ] ≥ 1\n1+(1+κ)pivot\n·pm,y ·fm. The proof\nis now completed by showing pm,y ≥\n1\n2m\n(1 − e−1). This\ngives Pr[U ] ≥ 0.8(1−e\n−1)\n(1+(1+κ)pivot)2m\n≥ 0.8(1−e\n−1)\n(1.06+κ)(|RF |−1)\n. The last\ninequality uses the observation that 1/pivot ≤ 0.06.\nTo calculate pm,y , we first note that since y ∈ RF , the\nrequirement “y ∈ RF,h,α” reduces to “y ∈ h\n−1(α)”. For α ∈\n{0, 1}n, we definewm,y,α as Pr\n[\npivot\n1+κ\n≤ |RF,h,α| ≤ 1 + (1 + κ)\npivot and h(y) = α : h\nR\n←− Hxor(n,m, 3)\n]\n. Therefore, pm,y =\nΣα∈{0,1}m\n(\nwm,y,α.2\n−m\n)\n. The proof is now completed by\nshowing that wm,y,α ≥ (1− e\n−1)/2m for every α ∈ {0, 1}m\nand y ∈ {0, 1}n.\nTowards this end, let us first fix a random y. Now we de-\nfine an indicator variable γz,α for every z ∈ RF \\ {y} such\nthat γz,α = 1 if h(z) = α, and γz,α = 0 otherwise. Let us\nfix α and choose h uniformly at random from Hxor(n,m, 3).\nThe random choice of h induces a probability distribution on\nγz,α such that E[γz,α] = Pr[γz,α = 1] = 2\n−m. Since we have\nfixed y, and since hash functions chosen from Hxor(n,m, 3)\nare 3-wise independent, it follows that for every distinct\nza, zb ∈ RF \\{y}, the random variables γza,α, γzb,α are 2-wise\nindependent. Let Γα =\n∑\nz∈RF \\{y}\nγz,α and µα = E[Γα].\nClearly, Γα = |RF,h,α| − 1 and µα =\n∑\nz∈RF \\{y}\nE[γz,α]\n= |RF |−1\n2m\n. Also, Pr[pivot\n1+κ\n≤ |RF,h,α| ≤ 1 + (1 + κ)pivot]\n= Pr[pivot\n1+κ\n− 1 ≤ |RF,h,α| − 1 ≤ (1 + κ)pivot] ≥ Pr[\npivot\n1+κ\n≤\n|RF,h,α| − 1 ≤ (1 + κ)pivot]. Using the expression for pivot,\nwe get 2 ≤ ⌊e−1/2(1 + 1/ǫ)2 · |RF |−1\n2m\n⌋. Therefore using The-\norem 1 and substituting pivot = (|RF | − 1)/2\nm, we get\nPr[pivot\n1+κ\n≤ |RF,h,α| − 1 ≤ (1+ κ)pivot] ≥ 1− e\n−1. Therefore,\nPr[pivot\n1+κ\n≤ |RF,h,α| ≤ 1 + (1 + κ)pivot] ≥ 1− e\n−1 Since h is\nchosen at random fromHxor(n,m, 3), we also have Pr[h(y) =\nα] = 1/2m. It follows that wm,y,α ≥ (1− e\n−1)/2m.\nThe next lemma provides an upper bound of wi,y,α and\npi,y.\nLemma 5. For i < m, both wi,y,α and pi,y are bounded\nabove by 1\n|RF |−1\n1(\n1− 1+κ\n2m−i\n)2 .\nProof. We will use the terminology introduced in the proof\nof Lemma 4. Clearly, µα =\n|RF |−1\n2i\n. Since each γz,α is a 0-1\nvariable, V [γz,α] ≤ E [γz,α]. Therefore, σ\n2\nz,α ≤\n∑\nz 6=y,z∈RF\nE [γz,α]\n≤\n∑\nz∈RF\nE [γz,α] = E [Γα] = 2\n−m(|RF | − 1). So Pr[\npivot\n1+κ\n≤\n|RF,h,α| ≤ 1+(1+κ)pivot] ≤ Pr[|RF,h,α|−1 ≤ (1+κ)pivot].\nFrom Chebyshev’s inequality, we know that Pr [|Γα − µz,α| ≥\nκσz,α] ≤ 1/κ\n2 for every κ > 0. By choosing κ = (1 −\n1+κ\n2m−i\n)\nµz,α\nσz,α\n, we have Pr[|RF,h,α| − 1 ≤ (1 + κ)pivot] ≤ Pr[\n|(|RF,h,α| − 1)−\n|RF |−1\n2i\n| ≥ (1− 1+κ\n2m−i\n) |RF |−1\n2i\n]\n≤ 1(\n1−\n(1+κ)\n2m−i\n)2 ·\n2i\n|RF |−1\n. Since h is chosen at random from Hxor(n,m, 3), we\nalso have Pr[h(y) = α] = 1/2i. It follows that wi,y,α ≤\n1\n|RF |−1\n1(\n1− 1+κ\n2m−i\n)2 . The bound for pi,y is easily obtained by\nnoting that pi,y = Σα∈{0,1}i\n(\nwi,y,α.2\n−i\n)\n.\nLemma 6. For every witness y of F , Pr[y is output] ≤\n1+κ\n|RF |−1\n(2.23 + 0.48\n(1−κ)2\n)\nProof. We will use the terminology introduced in the proof\nof Lemma 4. Pr[U ] =\n∑q\ni=q−3\n1\n|Y |\npi,y\n∏i\nj=q−3(1 − pj,y) ≤\n1+κ\npivot\n∑q\ni=q−3 pi,y. We can sub-divide the calculation of Pr[U ]\ninto three cases based on the range of the values m can take.\nCase 1 : q − 3 ≤ m ≤ q.\nNow there are four values that m can take.\n1. m = q − 3. We know that pi,y ≤ Pr[h(y) = α] =\n1\n2i\n.\nPr[U |m = q−3] ≤ 1+κ\npivot\n· 1\n2q−3\n15\n8\n. Substituting the value\nof pivot and m, we get Pr[U |m = q − 3] ≤ 15(1+κ)\n8(|RF |−1)\n.\n2. m = q − 2. For i ∈ [q − 2, q] pi,y ≤ Pr[h(y) = α] =\n1\n2i\nUsing Lemma 5, we get pq−3,y ≤\n1\n|RF |−1\n1\n(1− 1+κ2 )\n2 .\nTherefore, Pr[U |m = q − 2] ≤ 1+κ\npivot\n1\n|RF |−1\n( 1\n1− 1+κ\n2\n) +\n1+κ\npivot\n1\n2q−2\n7\n4\n. Noting that pivot = |RF |−1\n2m\n> 10, Pr[U |m =\nq − 2] ≤ 1+κ\n|RF |−1\n( 7\n4\n+ 0.4\n(1−κ)2\n)\n3. m = q − 1. For i ∈ [q − 1, q], pi,y ≤ Pr[h(y) = α] =\n1\n2i\n.\nUsing Lemma 5, we get pq−3,y+pq−2,y ≤\n1\n|RF |−1\n(\n1(\n1− 1+κ\n22\n) + 1\n(1− 1+κ2 )\n2\n)\n.\nTherefore, Pr[U |m = q−1] ≤ 1+κ\npivot\n(\n1\n|RF |−1\n(\n1(\n1− 1+κ\n22\n)2 + 1(1− 1+κ2 )\n2\n)\n+ 1\n2q−1\n3\n2\n)\n.\nNoting that pivot = |RF |−1\n2m\n> 10 and κ ≤ 1, Pr[U |m =\nq − 1] ≤ 1+κ\n|RF |−1\n(1.9 + 0.4\n(1−κ)2\n).\n4. m = q, pq,y ≤ Pr[h(y) = α] =\n1\n2q\n. Using Lemma 5, we\nget pq−3,y+pq−2,y+pq−1,y ≤\n1\n|RF |−1\n(\n1(\n1− 1+κ\n23\n)2 1(\n1− 1+κ\n22\n)2\n+ 1\n(1− 1+κ2 )\n2\n)\n. Therefore, Pr[U |m = q] ≤ 1+κ\npivot\n(\n1\n|RF |−1(\n1(\n1− 1+κ\n23\n)2 + 1(\n1− 1+κ\n22\n)2 + 1(1− 1+κ2 )\n2\n)\n+ 1\n)\n. Noting that\npivot = |RF |−1\n2m\n> 10, Pr[U |m = q] ≤ 1+κ\n|RF |−1\n(1.58 +\n0.4\n(1−κ)2\n).\nPr[U |q − 3 ≤ m ≤ q] ≤ maxi(Pr[U |m = i]). Therefore,\nPr[U |q − 3 ≤ m ≤ q] ≤ Pr[U |m = q − 1] ≤ 1+κ\n|RF |−1\n(1.9 +\n0.4\n(1−κ)2\n).\nCase 2 : m < q − 3. Pr[U |m < q − 3] ≤ 1+κ\npivot\n· 1\n2q−3\n15\n8\n.\nSubstituting the value of pivot and maximizing m − q + 3,\nwe get Pr[U |m < q − 3] ≤ 15(1+κ)\n16(|RF |−1)\n.\nCase 3 : m > q. Using Lemma 5, we know that Pr[U |m >\nq] ≤ 1+κ\n|RF |−1\n2m\n|RF |−1\n∑q\ni=q−3\n1\n1− 1+κ\n2m−i\n. The R.H.S. is maxi-\nmized when m = q + 1. Hence Pr[U |m > q] ≤ 1+κ\n|RF |−1\n2m\n|RF |−1\n∑q\ni=q−3\n1\n1− 1+κ\n2q+1−i\n. Noting that pivot = |RF |−1\n2m\n>\n10 and expanding the above summation Pr[U |m > q] ≤\n1+κ\n|RF |−1\n1\n10\n(\n1\n(1− 1+κ\n24\n)2\n+ 1\n(1− 1+κ\n23\n)2\n+ 1\n(1− 1+κ\n22\n)2\n+ 1\n(1− 1+κ\n21\n)2\n)\n.\nUsing κ ≤ 1 for the first two summation terms, Pr[U |m >\nq] ≤ 1+κ\n|RF |−1\n· 1\n10\n· (7.1 + 4\n(1−κ)2\n)\nSumming up all the above cases, Pr[U ] = Pr[U |m < q −\n3]× Pr[m < q − 3] + Pr[U |q − 3 ≤ m ≤ q]× Pr[q − 3 ≤ m ≤\nq] + Pr[U |m > q] × Pr[m > q]. Using Pr[m < q − 1] ≤ 0.2,\nPr[m > q] ≤ 0.2 and Pr[q − 3 ≤ m ≤ q] ≤ 1. Therefore,\nPr[U ] ≤ 1+κ\n|RF |−1\n(2.23 + 0.48\n(1−κ)2\n)\nCombining Lemma 4 and 6, the following theorem is\nobtained.\nTheorem 2. For every witness y of F , if ε > 1.71,\n1\n(1 + ε)(|RF | − 1)\n≤ Pr [UniGen(F, ε,X) = y] ≤ (1+ε)\n1\n|RF | − 1\n.\nProof. The proof is completed by using Lemmas 4 and 6 and\nsubstituting (1 + ε) = (1 + κ)(2.23 + 0.48\n(1−κ)2\n). To arrive at\nthe results, we use the inequality 1.06+κ\n0.8(1−e−1)\n≤ (1+κ)(2.23+\n0.48\n(1−κ)2\n).\nTheorem 3. Algorithm UniGen succeeds (i.e. does not re-\nturn ⊥) with probability at least 0.62.\nProof. If |RF | ≤ 1 + (1 + κ)pivot, the theorem holds triv-\nially. Suppose |RF | > 1 + (1 + κ)pivot and let Psucc de-\nnote the probability that a run of the algorithm UniGen suc-\nceeds. Let pi, such that (q − 3 ≤ i ≤ q) denote the condi-\ntional probability that UniGen (F , ε, X) terminates in it-\neration i of the repeat-until loop (line 11-16) with pivot\n1+κ\n≤\n|RF,h,α| ≤ 1 + (1 + κ)pivot, given |RF | > 1 + (1 + κ)pivot.\nTherefore, Psucc =\n∑q\ni=q−3 pi\n∏i\nj=q−3(1 − pj). Let fm =\nPr[q − 3 ≤ m ≤ q]. Therefore, Psucc ≥ pmfm ≥ 0.8pm. The\ntheorem is now proved by using Theorem 1 to show that\npm ≥ 1− e\n−3/2 ≥ 0.77.\nFor every y ∈ {0, 1}n and for every α ∈ {0, 1}m, define an\nindicator variable νy,α as follows: νy,α = 1 if h(y) = α, and\nνy,α = 0 otherwise. Let us fix α and y and choose h uni-\nformly at random from Hxor(n,m, 3). The random choice\nof h induces a probability distribution on νy,α, such that\nPr[νy,α = 1] = Pr[h(y) = α] = 2\n−m and E[νy,α] = Pr[νy,α =\n1] = 2−m. In addition 3-wise independence of hash func-\ntions chosen from Hxor(n,m, 3) implies that for every dis-\ntinct ya, yb, yc ∈ RF , the random variables νya,α, νyb,α and\nνyc,α are 3-wise independent.\nLet Γα =\n∑\ny∈RF\nνy,α and µα = E [Γα]. Clearly, Γα =\n|RF,h,α| and µα =\n∑\ny∈RF\nE [νy,α] = 2\n−m|RF |. Since |RF | >\npivot and i − l > 0, using the expression for pivot , we get\n3 ≤\n⌊\ne−1/2(1 + 1\nε\n)−2 · |RF |\n2m\n⌋\n. Therefore, using Theorem 1,\nPr\n[\n|RF |\n2m\n.\n(\n1− κ\n1+κ\n)\n≤ |RF,h,α| ≤ (1 + κ)\n|RF |\n2m\n]\n> 1−e−3/2.\nSimplifying and noting that κ\n1+κ\n< κ for all κ > 0, we\nobtain Pr\n[\n(1 + κ)−1 · |RF |\n2m\n≤ |RF,h,α| ≤ (1 + κ) ·\n|RF |\n2m\n]\n>\n1− e−3/2. Also, pivot\n1+κ\n= 1\n1+κ\n|RF |−1\n2m\n≤ |RF |\n(1+κ)2m\nand 1 + (1 +\nκ)pivot = 1 + (1+κ)(|RF |−1)\n2m\n≥ (1+κ)|RF |\n2m\n. Therefore, pm =\nPr[pivot\n1+κ\n≤ |RF,h,α| ≤ 1+(1+κ)pivot] ≥ Pr\n[\n(1 + κ)−1 · |RF |\n2m\n≤ |RF,h,α| ≤ (1 + κ) ·\n|RF |\n2m\n]\n≥ 1− e−3/2.\nTable 2 presents an extended version of Table 1. We\nobserve that UniGen is two to three orders of magnitude\nmore efficient than state-of-the-art random witness genera-\ntors, has probability of success almost 1 over a large set of\nbenchmarks arising from different domains.\nTable 2: Extended Table of Runtime performance comparison of UniGen and UniWit\nUniGen UniWit\nBenchmark #Variables |S|\nSucc\nProb\nAvg\nRun Time (s)\nAvg\nXOR len\nAvg\nRun Time (s)\nAvg\nXOR len\nCase121 291 48 1.0 0.19 24 56.09 145\nCase1 b11 1 340 48 1.0 0.2 24 755.97 170\nCase2 b12 2 827 45 1.0 0.33 22 – –\nCase35 400 46 0.99 11.23 23 666.14 199\nSquaring1 891 72 1.0 0.38 36 – –\nSquaring8 1101 72 1.0 1.77 36 5212.19 550\nSquaring10 1099 72 1.0 1.83 36 4521.11 550\nSquaring7 1628 72 1.0 2.44 36 2937.5 813\nSquaring9 1434 72 1.0 4.43 36 4054.42 718\nSquaring14 1458 72 1.0 24.34 36 2697.42 728\nSquaring12 1507 72 1.0 31.88 36 3421.83 752\nSquaring16 1627 72 1.0 41.08 36 2852.17 812\ns526 3 2 365 24 0.98 0.68 12 51.77 181\ns526a 3 2 366 24 1.0 0.97 12 84.04 182\ns526 15 7 452 24 0.99 1.68 12 23.04 225\ns1196a 7 4 708 32 1.0 6.9 16 833.1 353\ns1196a 3 2 690 32 1.0 7.12 16 451.03 345\ns1238a 7 4 704 32 1.0 7.26 16 1570.27 352\ns1238a 15 7 773 32 1.0 7.94 16 136.7 385\ns1196a 15 7 777 32 0.97 8.98 16 133.45 388\ns1238a 3 2 686 32 0.99 10.85 16 1416.28 342\ns953a 3 2 515 45 0.99 12.48 23 22414.86 257\nTreeMax 24859 19 1.0 0.52 10 49.78 12423\nLLReverse 63797 25 1.0 33.92 13 3460.58 31888\nLoginService2 11511 36 0.98 6.14 18 – –\nEnqueueSeqSK 16466 42 1.0 32.39 21 – –\nProjectService3 3175 55 1.0 71.74 28 – –\nSort 12125 52 0.99 79.44 26 – –\nKaratsuba 19594 41 1.0 85.64 21 – –\nProcessBean 4768 64 0.98 123.52 32 – –\ntutorial3 4 31 486193 31 0.98 782.85 16 – –\n",
            "id": 17187701,
            "identifiers": [
                {
                    "identifier": "oai:arxiv.org:1403.6246",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "2149902097",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "10.1145/2593069.2593097",
                    "type": "DOI"
                },
                {
                    "identifier": "25013230",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "1403.6246",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "192676946",
                    "type": "CORE_ID"
                }
            ],
            "title": "Balancing Scalability and Uniformity in SAT Witness Generator",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:arxiv.org:1403.6246"
            ],
            "publishedDate": "2014-01-01T00:00:00",
            "publisher": "'Association for Computing Machinery (ACM)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1403.6246"
            ],
            "updatedDate": "2021-07-22T16:39:20",
            "yearPublished": 2014,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1403.6246"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/17187701"
                }
            ]
        },
        {
            "acceptedDate": "2015-06-25T00:00:00",
            "arxivId": "1404.1685",
            "authors": [
                {
                    "name": "Calardo E."
                },
                {
                    "name": "Carmo J."
                },
                {
                    "name": "Governatori G."
                },
                {
                    "name": "Piolle G."
                },
                {
                    "name": "van Benthem J."
                },
                {
                    "name": "Vardi M. Y."
                },
                {
                    "name": "Åqvist L."
                }
            ],
            "contributors": [
                "Ted",
                "Guido"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/357332908",
                "https://api.core.ac.uk/v3/outputs/260147259",
                "https://api.core.ac.uk/v3/outputs/25016821"
            ],
            "createdDate": "2014-10-24T19:25:38",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 145,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/145",
                    "logo": "https://api.core.ac.uk/data-providers/145/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2015-01-01T00:00:00",
            "abstract": "In this paper we discuss some reasons why temporal logic might not be\nsuitable to model real life norms. To show this, we present a novel deontic\nlogic contrary-to-duty/derived permission paradox based on the interaction of\nobligations, permissions and contrary-to-duty obligations. The paradox is\ninspired by real life norms",
            "documentType": "research",
            "doi": "10.1145/2746090.2746105",
            "downloadUrl": "http://arxiv.org/abs/1404.1685",
            "fieldOfStudy": "computer science",
            "fullText": "Thou Shalt is not You Will\nGuido Governatori\nAbstract: In this paper we discuss some reasons why temporal logic might not be suitable to\nmodel real life norms. To show this, we present a novel deontic logic contrary-to-duty/derived\npermission paradox based on the interaction of obligations, permissions and contrary-to-duty\nobligations. The paradox is inspired by real life norms.\nKeywords: Linear Temporal Logic, Compliance, Deontic Logic, Deontic Paradox\nSoftware Systems Research Group, NICTA, Queensland Research Lab, Brisbane, Australia\nQueensland University of Technology, Brisbane, Australia\nemail: guido.governatori@nicta.com.au\nCopyright c© 2018 NICTA\nISSN: 1833-9646-8026\nar\nX\niv\n:1\n40\n4.\n16\n85\nv3\n  [\ncs\n.A\nI] \n 25\n Ja\nn 2\n01\n5\nPublication History:\n2014-04-07 (Version 2, Revision 6)\n2014-05-13 (Version 3, Revision 16)\n2014-09-28 (Version 4, Revision 22)\n2015-01-19 (Version 5, Revision 30)\n2\nNICTA Technical Report TR-8026 3\n1 Introduction\nThe aim of this note is to discuss the reasons why temporal logic, speciVcally Linear Temporal\nLogic [7] might not be suitable to check whether the speciVcations of a system comply with a\nset of normative requirements.\nThe debate whether it is possible to use temporal logic for the representation of norms\nis not a novel one (see for example [9]), and while the argument had settled for a while,\nthe past decade saw a resurgence of the topic with many works in the Velds of normative\nmulti-agents and business process compliance advocating temporal logic as the formalism to\nexpress normative constraints on agent behaviours and process executions. One of the reasons\nbehind this could be the success of model checking for temporal logic in verifying large scale\nindustry applications1.\nThe problem in normative multi-agents systems and business process compliance is to\ndetermine whether the actions an agent is going to perform (encoded as a plan, corresponding\nto a sequence of actions) or the tasks to be executed by a business process conform with a\nset of normative constraints regulating their possible (legal) behaviours. In both cases we\nhave sequences of actions/tasks leading to sequences of states and constrains over what states\nand sequences of states are deemed legal according to a set of normative constrains. From a\nformal point of view both the behaviours and the constraints are represented by temporal logic\nformulas and an agent or process are compliant if the set of formulas is consistent. Temporal\nlogic is deVnitely capable to model the sequences of states corresponding the behaviours of\nagents and processes, but the issue whether it is able to represents normative constraints (i.e.,\nobligations and prohibition) in a conceptually sound way has been neglected. We believe that\nthis is crucial issue to be addressed before these techniques can be proposed for practical real\nlife cases. Without a positive answer the work based on temporal logic for the representation\nof norms remains a futile formal exercise.\nThe short discussion above boils down to the following question:\nAre normative constrains (i.e., obligations and prohibitions) regulating the be-\nhaviours of their subjects diUerent from other types of constraints?\nIn case of a negative answer we have to identify what are the diUerences, and how to model\nthem in temporal logic. Furthermore, we have to identify what are the issues with the resulting\nmodelling.\nObligations and prohibitions are constraints that limit the scope of actions of the bearer\nsubject to them. However, there is a very important diUerence between obligations and prohi-\nbitions and other types of constraints: violations do not result in inconsistencies. This means\nthat they can be violated without breaking the systems in which they appear. Accordingly,\na better understanding of obligations and prohibitions is that they deVne what is legal (in a\nparticular system) and what is illegal. Based on this reading a violation simply indicates that\nwe ended up in an illegal situation or state. A further aspect we have to consider, and that\nhas been by large neglected by investigations on how to formalise and reason with deontic\nconcepts, is that violations can be compensated for, and a situation where there is a violation\n1The fathers of model checking for temporal logic, i.e., Edmund Clarke, E. Allen Emerson and Joseph Sifakis,\nwere the recipient of the Turing award in 2007 for their role in developing Model-Checking into a highly eUective\nveriVcation technology that is widely adopted in the hardware and software industries.\n4 Thou Shalt is not You Will\nbut there is a compensation for the violation is still deemed legal (even if, from a legal point of\nview, less ideal than the situation where the violation does not occur).\nThe paper is organised as follows: in the next section we introduce a legal scenario (a\nfragment of an hypothetical privacy act) illustrating some of the aspects diUerentiating norms\nform other types of constraints, and we shortly discuss what the outcomes of cases related to\nthis scenario should be. Then in Section 3 we brieWy recall the basics of Linear Temporal Logic\n(LTL). In Section 4 we discuss how to formalise the scenario in LTL. We point out various\nshortcomings for the representation of norms in LTL, and we show that LTL captures only\nsome of the aspects of the scenario (suggesting that it is not able to model real life norms), or it\nleads to paradoxical results.2\n2 Legal Motivation\nSuppose that a Privacy Act contains the following norms:3\nSection 1. The collection of personal information is forbidden, unless acting on a court order\nauthorising it.\nSection 2. The destruction of illegally collected personal information before accessing it is a\ndefence against the illegal collection of the personal information.\nSection 3. The collection of medical information is forbidden, unless the entity collecting the\nmedical information is permitted to collect personal information.\nIn addition the Act speciVes what personal information and medical information are, and they\nturn out to be disjoint.\nSuppose an entity, subject to the Act, collects some personal information without being\npermitted to do so; at the same time they collect medical information. The entity recognises\nthat they illegally collected personal information (i.e., they collected the information without\nbeing authorised to do so by a Court Order) and decides to remediate the illegal collection\nby destroying the information before accessing it. Is the entity compliant with the Privacy\nAct above? Given that the personal information was destroyed the entity was excused from\nthe violation of the Vrst section (illegal collection of personal information). However, even if\nthe entity was excused from the illegal collection, they were never entitled (i.e., permitted)\nto collect personal information4, consequently they were not permitted to collect medical\ninformation; thus the prohibition of collecting medical information was in force. Accordingly,\nthe collection of medical information violates the norm forbidding such an activity.\nLet us examine the structure of the act:\nSection 1 establishes two conditions:\ni. Typically the collection of personal information is forbidden; and\n2Following Åqvist’s [10] presentation, a paradox arises in a deontic logic ∆ either when there is a formula φ\nderivable in ∆ but for which the translation does not seem derivable within the natural normative language, or\nthere is a formula φ which is not derivable in ∆ but for which the translation seems derivable within our natural\nnormative language.\n3The Privacy Act presented here, though realistic, is a Vctional one. However, (i) it is based on the novel\nAustralian Privacy Principles (APP), Privacy Amendment (Enhancing Privacy Protection) Act 2012, and (ii)\nsections with the same logical structure as the clauses of this Vctional act are present in the APP Act.\n4If they were permitted to collect personal information, then the collection would have not been illegal, and\nthey did not have to destroy it.\nNICTA Technical Report TR-8026 5\nii. The collection of personal information is permitted, if there is a court order authorizing\nthe collection of personal information.\nSection 2 can be paraphrased as follows:\niii. The destruction of personal information collected illegally before accessing it excuses\nthe illegal collection.\nSimilarly to Section 2, Section 3 states two conditions:\niv. Typically the collection of medical information is forbidden; and\nv. The collection of medical information is permitted provided that the collection of per-\nsonal information is permitted.\nBased on the above discussion, if we abstract from the actual content of the norms, the structure\nof the act can be represented by the following set of norms (extended form):\nE1. A is forbidden.\nE2. A is permitted givenC (alternatively: ifC, then A is permitted).\nE3. The violation of A is compensated by B\nE4. D is forbidden.\nE5. If A is permitted, so is D.\nTo compensate a violation we have to have a violation the compensation compensates. More-\nover, to have a violation we have to have an obligation or prohibition, the violation violates.\nAccordingly, it makes sense to combine E1 and E3 in a single norm, obtaining thus the following\nset of norms (condensed form):\nC1. A is forbidden; its violation is compensated by B.\nC2. A is permitted givenC (alternatively: ifC, then A is permitted).\nC3. D is forbidden.\nC4. If A is permitted, so is D.\nBased on the discussion so far the logical structure of the act is (logical form):\nL1. ForbiddenA; if ForbiddenA and A, then ObligatoryB.\nL2. ifC, then PermittedA.\nL3. ForbiddenD.\nL4. If PermittedA, then PermittedD.\nNotice the way we modelled the violation of the prohibition of A in L1, namely as the conjunc-\ntion of A and the prohibition of A.5 Then we model that B is the compensation of the violation\nof A as an implication from the violation of A to the obligation of B.\nLet us consider what are the situations compliant with the above set of norms. Clearly,\nif C does not hold, then we have that the prohibition of A and prohibition of D are in force.\nTherefore, a situation where ¬A, ¬C, and ¬D hold is fully compliant (irrespective whether\nB holds or not). If C holds, then the permission of A derogates the prohibition of A, thus\nsituations with either A holds or ¬A holds are compliant with the Vrst two norms; in addition,\nthe permission of A allows us to derogate the prohibition of D. Accordingly, situations with\neither D or ¬D comply with the third norm. Let us go back to scenarios whereC does not hold,\nand let us suppose that we have A. This means that the prohibition of A has been violated;\n5Similarly, the violation of the obligation of A is the conjunction of obligation A and the negation of the\ncontent of the obligation, that is, ¬A.\n6 Thou Shalt is not You Will\nnevertheless the set of norms allows us to recover from such a violation by B. However, as we\njust remarked above to have a violation we have to have either an obligation or a prohibition\nthat has been violated: in this case the prohibition of A. Given that the prohibition of A and the\npermission of A are mutually incompatible, we must have, to maintain a consistent situation,\nthat A is not permitted. But if Awas not permittedD is not permitted either; actually, according\nto the third norm, D is forbidden. To sum up, a scenario where ¬C, A, B and ¬D hold is still\ncompliant (even if to a lesser degree given the compensated violation of the prohibition of A).\nIn any case, no situation where both ¬C and D hold is compliant.\nTable 1 summarises the compliant and not compliant situations. We only report the minimal\nsets required to identify whether a situation is compliant or not. For non-minimal sets the\noutcome is determined by the union of the status for the minimal subsets.\nMinimal Set Compliance Status\nC compliant\n¬C, A, B weakly compliant: compensated violation of the prohibition of A\n¬C, A, ¬B not compliant: uncompensated violation of the prohibition of A\n¬C, D not compliant: violation of prohibition of D\n¬C, ¬A, ¬D compliant\nTable 1: Compliance Status for the Privacy Act\n3 Logic Background\nLinear Temporal Logic [7] is equipped with three unary temporal operators:\n• Xφ : next φ (φ holds at the next time);\n• Fφ : eventually φ (φ holds sometimes in the future); and\n• Gφ : globally φ (φ always holds in the future).\nIn addition we have the following binary operators:\n• φ Uψ : φ until ψ (φ holds until ψ holds);\n• φWψ : φ weak until ψ (φ holds until ψ holds and ψ might not hold).\nThe operators above are related by the following equivalences establishing some interdeVnabil-\nity among them:\n• Fφ ≡>Uφ ,\n• Gφ ≡ ¬F¬φ ,\n• φWψ ≡ (φ Uψ)∨Gφ .\nThe semantics of LTL can be given in terms of transition systems. A transition system TS is a\nstructure\n(1) TS= 〈S,R,v〉\nwhere\n• S is a (non empty) set of states\n• R⊆ S×S such that ∀s ∈ S∃t ∈ S : (s, t) ∈ R\n• v is a valuation function v : S 7→ 2Prop\nNICTA Technical Report TR-8026 7\nwhere Prop is the set of atomic propositions.\nFormulas in LTL are evaluated against fullpaths (also called traces or runs). A fullpath is a\nsequence of states in S connected by the transition relation R. Accordingly, σ = s0,s1,s2 . . .\nis a fullpath if and only if (si,si+1) ∈ R. Given a fullpath σ , σi denotes the subsequence of σ\nstarting from the i-th element, and σ [i] denotes the i-th element of σ .\nEquipped with the deVnitions above, the valuation conditions for the various temporal\noperators are:\n• TS,σ \u000f p (p ∈ Prop) iU p ∈ v(σ [0]);\n• TS,σ \u000f ¬φ iU TS,σ 6\u000f φ ;\n• TS,σ \u000f φ ∧ψ iU TS,σ \u000f φ and TS,σ \u000f ψ ;\n• TS,σ \u000f Xφ iU TS,σ1 \u000f φ ;\n• TS,σ \u000f φ Uψ iU ∃k : k ≥ 0, TS,σk \u000f ψ and ∀ j : 0≤ j < k, TS,σ j \u000f φ ;\n• TS,σ \u000f Gφ iU ∀k ≥ 0, TS,σk \u000f φ ;\n• TS,σ \u000f Fφ iU ∃k ≥ 0, TS,σk \u000f φ .\nA formula φ is true in a fullpath σ iU it is true at the Vrst element of the fullpath. Next we\ndeVne what it means for a formula φ to be true in a state s ∈ S (TS,s \u000f φ ).\n(2) TS,s \u000f φ iU ∀σ : σ [0] = s, TS,σ \u000f φ .\n4 Scenario Formalised\nThe Vrst problem we have to address is how to model obligations and permissions in Linear\nTemporal Logic. When one considers the temporal lifecycle obligations, obligations can be\nclassiVed as achievement and maintenance obligations [4]. After an obligation enters into\nforce, the obligation remains in force for an interval of time. A maintenance obligation is an\nobligation whose content must hold for every instant in the interval in which the obligation\nis in force. On the other hand, for an achievement obligation, the content of the obligation\nhas to hold at least once in the interval of validity of the obligation. Accordingly, a possible\nsolution is to use G to model maintenance obligations6 and F for achievement obligations.\nA drawback of this proposal is that G and F are the dual of each other, i.e., Gα ≡ ¬F¬α . In\nDeontic Logic permission is typically deVned as the lack of the obligation to the contrary and\nthe deontic operators O and P to model obligations and permissions are deVned to be the dual\nof each other, namely Oα ≡ ¬P¬α . In addition, most deontic logics assume the following\naxiom (Axiom D)7\n(3) Oα → Pα\nto ensure consistency of sets of norms. The axiom is equivalent to Oα →¬O¬α meaning\nthat if α is obligatory, then its opposite (¬α) is not. Prohibitions can modelled as negative\nobligations, thus α is forbidden if its opposite is obligatory, that is O¬α . Furthermore, it has\nbeen argued that maintenance obligations are suitable to model prohibitions.\nBased on the discussion above, considering that the normative constraints in the scenario\nof Section 2 are actually prohibitions, we formalise the scenario using G for maintenance\n6We can use U instead of G to capture that an obligation is in force in an interval.\n7In terms of Kripke possible world semantics Axiom D is characterised by seriality, i.e., ∀x∃y(xRy), and this is\nthe property imposed on the transition relation R over the set of states S in a transition system for LTL.\n8 Thou Shalt is not You Will\nobligations (actually prohibitions) and F for permissions. We temporarily suspend judgement\nwhether using an operator suitable to model achievement obligations to model the dual\npermission for maintenance obligation is appropriate or not. All we remark here is that any\nformalism meant to model real life norms should account for both obligations and permissions\nas Vrst class citizens.\nA Vrst possible prima facie formalisation of the conditions set out in the Privacy Act is:\n1. G¬A, (G¬A∧A)→ GB;\n2. C→ FA;\n3. G¬D;\n4. FA→ FD.\nThe set of formulas above exhibits some problems. First of all, in a situation where we haveC\nwe get a contradiction from 1. and 2., i.e., G¬A and FA, and then a second from 3., and 2. and\n4., namely G¬D and FD. This is due to the fact that normative reasoning is defeasible. Shortly\nand roughly a conclusion can be asserted unless there are reasons against it. In addition, to get\nthe expected results, we have to consider that the scenario uses strong permissions, where the\npermissions derogates the obligations to the contrary, or, in other terms, that the permissions\nare exceptions to the obligations. To accomplish this we have to specify that 2. overrides 1.,\nand 4. overrides 3. Technically, the overrides relationship can be achieved using the following\nprocedure:8\n1. rewrite the formulas involved as conditionals. Thus G¬A can be rewritten as >→ G¬A.\n2. add the negation of the antecedent of the overriding formulas to the antecedent of the\nformulas overridden formula. Accordingly >→ G¬A is transformed into ¬C→ G¬A.9\nThe second aspect we concentrate on is the form of the formulas in 1., in particular on the\nexpression\n(4) (G¬A∧A)→ GB.\nTo start with they bear resemblance with the so called contrary-to-duty obligations. A contrary-\nto-duty obligation states that an obligation/prohibition is in force when the opposite of an\nobligation/prohibition holds. The template for contrary-to-duty obligations is given by the pair\n(a) Oα and (b) ¬α → Oβ . Contrary-to-duty obligations are typically problematic for deontic\nlogic and the source of inspiration for a wealth of research in the Veld (see [8, 3]). The formula\nunder scrutiny is indeed related, but there is a diUerence: it explicitly requires a violation,\nwhile the structure in (b) does not. In the context of the Privacy Act scenario (b) would mean\nthat an entity has the obligation to destroy collected personal information without accessing\nsimply because they collected it (even in the case the collection was legal, or even when they\nhad the mandate to collect it and eventually preserve it).\nAccordingly, we introduce the class of compensatory (contrary-to-duty) obligations. A\ncompensatory obligation states that an obligation/prohibition is in force as the result of the\nviolation of another obligation/prohibition. Thus the obligation triggered in response to the\nviolation (secondary obligation) compensates the violation of the violated obligation (primary\n8The focus of this paper is not how to implement defeasibility or non-monotonicity in LTL or in another\nmonotonic logic, thus we just exemplify a possible procedure.\n9A side-eUect of this procedure, which is harmless for the purpose of this paper, is that now the combination\nof 3. and 4. makes FA and FD equivalent, namely FA≡ FD.\nNICTA Technical Report TR-8026 9\nobligation). In other words a situation where the primary obligation is violated, but the\nsecondary obligation is fulVlled is still deemed legal, even if it is less ideal than the case\nwhere the primary obligation is fulVlled.10 The language employed in the Privacy Act suggests\nthat that the conditions stated in Section 1 and Section 2 of the Act correspond to a case of\ncompensatory obligation.\nWe turn now our attention to the issue of how to formalise compensatory obligations in\nLTL. The Vrst concern we have when we look at 4 we notice that its antecedent is always false,\ni.e., G¬A∧A≡⊥, since G¬A implies that A is false in all worlds following the world where the\nformula is evaluated including that world, but at the same time A is required to be true at that\nworld. The second issue is that the compensation is assumed to be a maintenance obligation\nwhile the textual provision suggests it is a achievement obligation. We shortly discuss that\nachievement obligation should be represented by F, but F is used to model permissions.\nTo avoid the issues just discussed we introduce a new binary (temporal) operator ⊗ for\ncompensatory obligations11. What we have to do for this end is to identify the conditions\nunder which a maintenance obligation is violated. The maintenance obligation Oα is violated\nif there is a instant in the interval of validity of the obligation where α does not hold, namely\n¬α holds. The second thing is to deVne what it means to compensate a violation. Suppose\nthat we are told that the violation of α is compensated by β . A natural intuition for this is that\nthere is an instant in the interval of validity of Oα where ¬α holds, and there is an instant\nsuccessive to the violation where the course of action described by β holds. Based on the\nintuition just described LTL seems well suited to this task. Here is the evaluation condition for\n⊗:12,13\n(5) TS,σ \u000f φ ⊗ψ iU ∀i≥ 0, TS,σi \u000f φ ; or ∃ j,k : 0≤ j ≤ k, TS,σ j \u000f ¬φ and TS,σk \u000f ψ.\nWe are now ready to provide the formalisation of the Privacy Act.\nN1. ¬C→ (¬A⊗B);\nN2. C→ FA;\nN3. G¬A→ G¬D;\nN4. FA→ FD.\nTransition systems can be use to model runs of systems, possible ways in which business\nprocesses can be executed, the actions of an agents or more in general the dynamic evolution\nof a system or the world. Norms are meant to regulate the behaviour of systems, how\norganisations run their business, the actions of agents and so on. So, how do we check if a\nparticular course of actions (modelled by a transition system) complies with a set of norms\n(where the norms are formalised in LTL)? Simply, if the transition system is a model for the set\nof formulas representing the norms.\n10We do not exclude the case that there are situations where norms have the form of what we call compensatory\nobligations, but where the obligation in response to the violation does not (legally) compensate the violation.\n11The idea of using a speciVc operator for compensatory (contrary-to-duty) obligations is presented in [5].\n12Again the focus of the paper is not on how to properly model compensatory (contrary-to-duty) obligations.\nThe operator presented here does its job in the context of the paper. For alternative deVnitions in the context of\ntemporal logic or inspired by temporal logic see [6, 1]. For a semantic approach not based on temporal logic see\n[2].\n13This condition implements compensatory obligations when the primary obligation is a maintenance obli-\ngation and the secondary obligation is an achievement obligation. Similar deVnitions can be given for other\ncombinations of primary and secondary obligations.\n10 Thou Shalt is not You Will\nConsider a transition system TS= 〈S,R,v〉 where\n1. S= {ti : i ∈ N},\n2. R= {(ti, ti+1) : i ∈ N},\n3. ¬C ∈ v(ti) for all i ∈ N, A ∈ v(t1), D ∈ v(t1) and B ∈ v(t2).\nThe transition system is such that\n(6) TS, ti \u000f ¬C, TS, t1 \u000f A, TS, t1 \u000f D, TS, t2 \u000f B.\nThis transition system implements the scenario where at no time there is a Court Order\nauthorising the collection of personal information (¬C for all ti), an entity collects personal\ninformation (A at time t1) and successively destroys it (B at time t2), and at the same time when\npersonal information was collected medical information was collected (D at time t1).\nIt is immediate to verify that the transition system TS is a model of N1–N4, namely:\n(7) ∀t ∈ S : TS, t \u000f N1∧N2∧N3∧N4.\nAccordingly, TS is compliant with N1–N4. However, there is state t1 where both ¬C and D\nhold. In Section 2 we argued that a situation where ¬C and D both hold is not compliant.\nTherefore, we have a paradox, the formalisation indicates that the scenario is compliant, the\ncourse of actions described by the transition system does not result in a contradiction, so\nno illegal action is performed (or better, the collection of personal information is illegal, but\nits compensation, destruction of the personal information, makes full amends to it), but our\nlegal intuition suggests that the collection of medical information in the circumstances of the\nscenario is illegal.14\n5 Conclusion\nThe contribution of this note is twofold. First we presented a novel paradox for Deontic Logic\ninspired by real life norms. In particular the logical structures used in the paradox appear\nfrequently in real life (legal) norms. The second contribution was a short analysis of how to\nrepresent norms in Linear Temporal Logic, and that the proposed formalisation results in a\nparadox, showing that LTL might not be suitable to model norms and legal reasoning.\nWe would like to point out that the discussion in the previous section just shows that a\nparticular formalisation based on LTL is not suitable to represent the scenario, not that LTL\nper se is not able to represent the scenario. Indeed one could create all possible full paths\nin a transition system not breaching the norms, and then using the paths to synthesise the\nnorms that regulate the transition system. However, we believe that such ex post analysis\nis useless. First humans have to perform the reasoning to determine which norms hold\nand when and then which paths violate the norms. In addition the strength of LTL is the\nability to verify speciVcations against transition systems. But in such a case, given that the\n14We run a pseudo empirical validation of the scenario by proposing the scenario and the Privacy Act to about\na dozen legal professionals ranging from corporate legal councillors, to high court judges to law professors. They\nall agree without any hesitation that the collection of medical information under the circumstances described\nby the scenario is illegal. However, a true validation can be only given either by a law court adjudication of a\ncase where the norms at hand are isomorphic to the Privacy Act, or by any body with the power to give a true\ninterpretation of an act isomorphic to the act we proposed for the scenario.\nNICTA Technical Report TR-8026 11\nspeciVcations are derived from the transition systems, the veriVcation is always positive and\ntotally uninformative. Furthermore, we believe that the formalisation we proposed, while naive,\nis extremely intuitive. The major objection, as we remarked in Section 2, is that permissions\nare modelled using F, and we hinted that F might be suitable to model achievement obligation,\nand using a particular type of obligation to model permissions is not appropriate and counter-\nintuitive outcomes are to be expected. We fully agree with this objection, but if we agree\nthat a permission is the lack of an obligation to the contrary, then F is the natural choice for\npermissions for prohibition (maintenance obligations). The other issue is that if we do not\nuse F, the issue is how to model permission, and the alternative is that LTL does not support\npermissions. The act we presented clearly shows that there are acts where permissions must\nbe represented and that permissions play an important role in determining which obligations\nare in force and when they are in force. Hence, any formalisation excluding permission is\ndoomed to be unable to represent the vast majority of real life legal norms.\nThe Vnal remark we want to make is that the paradox is not restricted to LTL. It can be\neasily replicated in Standard Deontic Logic (and it is well know that Standard Deontic Logic is\nplagued with many other contrary-to-duty paradoxes). A root-cause analysis of the paradox is\nthat a violation of a compensable obligation results in a sub-ideal state. Hence, there is a state\nwith a violation that is still deemed legal. This means, that there is a (somehow) legal state,\nand if permission is evaluated as being in at least one legal state, then the violation has to be\nevaluated as (somehow) permitted. Part of the problem is that in such somehow legal states\nthere might be other true legitimate permissions which are not the violation of compensable\nobligations. Accordingly, we conjecture, that logics using truth of a formula in at least one\n(somehow) legal state to determine whether something is permitted have counterparts of the\nparadox we presented. However, a careful analysis of existing deontic logics is needed to\nevaluate if they are actually aUected by the paradox.\nAcknowledgements\nI thank Antonino Rotolo and Giovanni Sartor for fruitful comments on previous drafts of\nthis paper. I also thank the participants to NorMAS 2014 for the discussions and valuable\nsuggestions.\nNICTA is funded by the Australian Government through the Department of Communica-\ntions and the Australian Research Council through the ICT Centre of Excellence Program.\nReferences\n[1] Johan van Benthem, Davide Grossi, and Fenrong Liu. “Priority Structures in Deontic\nLogic”. Theoria (2013). doi: 10.1111/theo.12028.\n[2] Erica Calardo, Guido Governatori, and Antonino Rotolo. “A Preference-based Semantics\nfor CTD Reasoning”. In: Deontic Logic in Computer Science (DEON 2014). Ed. by Fabrizio\nCariani, Davide Grossi, Joke Meheus, and Xavier Parent. Lecture Notes in Computer\nScience 8554. Springer, 2014, pp. 49–64.\n[3] José Carmo and Andrew J.I. Jones. “Deontic logic and contrary-to-duties”. In: Handbook\nof philosophical logic. Vol. 8.\n12 Thou Shalt is not You Will\n[4] Guido Governatori. “Business Process Compliance: An Abstract Normative Framework”.\nIT – Information Technology 55.6 (2013), pp. 231–238.\n[5] Guido Governatori and Antonino Rotolo. “Logic of Violations: A Gentzen System for\nReasoning with Contrary-To-Duty Obligations”. Australasian Journal of Logic 4 (2006),\npp. 193–215.\n[6] Guillaume Piolle. “A Dyadic Operator for the Gradation of Desirability”. In: ed. by Guido\nGovernatori and Giovanni Sartor. Lecture Notes in Computer Science 6181. Springer,\n2010, pp. 33–49.\n[7] Amir Pnueli. “The temporal logic of programs”. In: SFCS ’77: Proceedings of the 18th\nAnnual Symposium on Foundations of Computer Science. IEEE Computer Society, 1977,\npp. 46–57.\n[8] Henry Prakken and Marek J. Sergot. “Contrary-to-Duty Obligations”. Studia Logica 57.1\n(1996), pp. 91–115.\n[9] Richmond H. Thomans. “Deontic Logic Founded on Tense Logic”. In: New Studies on\nDeontic Logic. Ed. by Riisto Hilpinen. Kluwer, 1981, pp. 165–176.\n[10] Lennart Åqvist. “Deontic logic”. In: Handbook of philosophical logic, 2nd edition. Ed. by\nDov M. Gabbay and F. Guenthner. Kluwer Academic Publisher, 2001, 605–714.\n",
            "id": 17191128,
            "identifiers": [
                {
                    "identifier": "357332908",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:citeseerx.psu:10.1.1.1049.3485",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "1404.1685",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "25016821",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2169761899",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "260147259",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1145/2746090.2746105",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:arxiv.org:1404.1685",
                    "type": "OAI_ID"
                }
            ],
            "title": "Thou Shalt is not You Will",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2169761899",
            "oaiIds": [
                "oai:citeseerx.psu:10.1.1.1049.3485",
                "oai:arxiv.org:1404.1685"
            ],
            "publishedDate": "2014-01-01T00:00:00",
            "publisher": "'Association for Computing Machinery (ACM)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1404.1685"
            ],
            "updatedDate": "2021-08-03T13:49:19",
            "yearPublished": 2014,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1404.1685"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/17191128"
                }
            ]
        },
        {
            "acceptedDate": "2014-04-06T00:00:00",
            "arxivId": "1404.1987",
            "authors": [
                {
                    "name": "Bauereiss, Thomas"
                },
                {
                    "name": "Hutter, Dieter"
                }
            ],
            "contributors": [],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/87732987"
            ],
            "createdDate": "2014-10-24T19:25:39",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 645,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/645",
                    "logo": "https://api.core.ac.uk/data-providers/645/logo"
                }
            ],
            "depositedDate": "2014-04-06T00:00:00",
            "abstract": "In workflows and business processes, there are often security requirements on\nboth the data, i.e. confidentiality and integrity, and the process, e.g.\nseparation of duty. Graphical notations exist for specifying both workflows and\nassociated security requirements. We present an approach for formally verifying\nthat a workflow satisfies such security requirements. For this purpose, we\ndefine the semantics of a workflow as a state-event system and formalise\nsecurity properties in a trace-based way, i.e. on an abstract level without\ndepending on details of enforcement mechanisms such as Role-Based Access\nControl (RBAC). This formal model then allows us to build upon well-known\nverification techniques for information flow control. We describe how a\ncompositional verification methodology for possibilistic information flow can\nbe adapted to verify that a specification of a distributed workflow management\nsystem satisfies security requirements on both data and processes.Comment: In Proceedings GraMSec 2014, arXiv:1404.163",
            "documentType": "research",
            "doi": "10.4204/eptcs.148.4",
            "downloadUrl": "http://arxiv.org/abs/1404.1987",
            "fieldOfStudy": "computer science",
            "fullText": "Kordy, Mauw, and Pieters (Eds.):\nGraphical Models for Security 2014 (GraMSec’14)\nEPTCS 148, 2014, pp. 47–62, doi:10.4204/EPTCS.148.4\nc© T. Bauereiss, D. Hutter\nThis work is licensed under the\nCreative Commons Attribution License.\nPossibilistic Information Flow Control for Workflow\nManagement Systems∗\nThomas Bauereiss Dieter Hutter\nGerman Research Center for Artificial Intelligence (DFKI)\nBibliothekstr. 1\nD-28359 Bremen, Germany\nthomas.bauereiss@dfki.de hutter@dfki.de\nIn workflows and business processes, there are often security requirements on both the data, i.e. con-\nfidentiality and integrity, and the process, e.g. separation of duty. Graphical notations exist for spec-\nifying both workflows and associated security requirements. We present an approach for formally\nverifying that a workflow satisfies such security requirements. For this purpose, we define the seman-\ntics of a workflow as a state-event system and formalise security properties in a trace-based way, i.e.\non an abstract level without depending on details of enforcement mechanisms such as Role-Based\nAccess Control (RBAC). This formal model then allows us to build upon well-known verification\ntechniques for information flow control. We describe how a compositional verification methodol-\nogy for possibilistic information flow can be adapted to verify that a specification of a distributed\nworkflow management system satisfies security requirements on both data and processes.\n1 Introduction\nComputer-supported workflows and business process automation are widespread in enterprises and or-\nganisations. Workflow management systems support the enactment of such workflows by coordinating\nthe work of human participants in the workflow for human activities as well as by automatically exe-\ncuting activities that can be mechanised. Graphical notations such as BPMN allow for the specification\nof workflows in an intuitive way. In addition to the control and data flows, there are typically various\nsecurity requirements that need to be considered during the design, implementation and execution of\nworkflows. A well-known security requirement on workflows is separation of duty for fraud prevention\n[7]. Confidentiality of data is another important security requirement, e.g. the confidentiality of medical\ndata from non-medical personnel. These two can be seen as examples for different types of security re-\nquirements. On the one hand, there are security requirements on processes, i.e. constraints on the control\nflow and the authorisation of users, and on the other hand, there are security requirements on data, i.e.\nconstraints on the flow of information. Several proposals to extend BPMN with graphical notations for\nboth kinds of security requirements exist [6, 26, 33].\nIn this paper, we focus on the question of how the semantics of such a notation can be defined and\nhow to use them to formally verify both types of security requirements. We do this on an abstract level\nwithout having to refer to details of enforcement mechanisms such as role-based access control (RBAC).\nFor this purpose, we model the behaviour of a workflow as a set of traces of events, each representing a\npossible run of the workflow, and formalise our security requirements in a declarative way as properties\nof such trace sets. We map process requirements such as separation of duty to sets of allowed traces, cor-\nresponding to safety properties [3], whereas we map requirements on data to information flow properties,\n∗This research is supported by the Deutsche Forschungsgemeinschaft (DFG) under grant Hu737/5-1, which is part of the\nDFG priority programme 1496 “Reliably Secure Software Systems.”\n48 Possibilistic Information Flow Control for Workflow Management Systems\nwhich have been extensively studied [27, 15, 36, 21, 9]. This allows us to verify the absence not only\nof direct information leaks via unauthorised access, but also of indirect information leaks via observing\nthe behaviour of the system. For example, if the control flow depends on a confidential data item and an\nunauthorised user observes which path of the control flow has been taken, they might be able to deduce\nthe confidential value of the data item.\nThe relation between possibilistic information flow and safety properties is not trivial due to the re-\nfinement paradox, i.e. enforcing a safety property by removing disallowed traces might introduce new\ninformation leaks [18]. We discuss this relation for the case of separation of duty, give sufficient condi-\ntions for the compatibility with information flow properties, and show that these conditions are satisfied\nin our example setting.\nWe build upon the MAKS framework for possibilistic information flow control [15], which is suitable\nfor formulating and verifying information flow policies at the specification level, and in which many\ninformation flow properties from the literature can be expressed. We describe how a compositional\nverification methodology [13] can be applied to verify our system models, which has the advantage that\nwe can split up the verification task into separate verification tasks of the individual activities that make\nup the overall workflow.\nEssentially, our approach allows for the formal modelling of workflows and the verification of se-\ncurity requirements on data and processes at a high level of abstraction. We have verified our results\nusing the interactive theorem prover Isabelle/HOL [24]. To improve practicality, future work will focus\non refinement approaches of these specifications towards concrete implementations, while preserving as\nmuch as possible of the security properties established on the abstract level. The long-term goal of our\nwork is to facilitate the step-wise development of secure workflow management systems, starting from\nan abstract specification, derived from a workflow diagram, for example, then performing a series of\nrefinement steps and eventually arriving at a secure implementation.\nThe rest of this paper is structured as follows. In the following subsection, we present a running\nexample of a workflow that we will use for illustration throughout this paper. Section 2 introduces the\nsystem model. In Section 3, we elaborate on modelling confidentiality and separation-of-duty require-\nments, respectively. In Section 4, we describe how existing techniques for compositional verification of\ninformation flow properties can be applied and adapted for our workflow systems. Section 5 discusses\nrelated work and Section 6 concludes the paper.\n1.1 Example Scenario\nAs a running example, we use the workflow depicted in Figure 1, adapted from an industry use-case\ndescribed in an (unpublished) paper by A. Brucker and I. Hang. It models a hiring process including\ninterviews and medical examinations. The swimlanes represent two departments of the organisation\nrunning the hiring workflow, the Human Resources (HR) and the medical department. The placement of\nactivities in the swimlanes indicates the responsible department, and thus the authorised employees. The\ninput and output relations of activities are depicted as directed flows of documents between activities.\nWe use this workflow to illustrate security requirements with respect to both data and process. On\nthe process side, we demand separation of duty between the two medical examinations, i.e. they have to\nbe performed by different medical officers, such that no single medical officer can manipulate the hir-\ning process by rejecting unwanted candidates for fabricated medical reasons. Regarding confidentiality\nrequirements, we assume that the two medical reports are highly confidential due to their potentially sen-\nsitive contents. In particular, they are confidential for the employees in the HR department, who should\nonly be able to access information on a need-to-know basis, e.g. the CVs of applicants.\nT. Bauereiss, D. Hutter 49\nFigure 1: Example workflow (adapted from A. Brucker and I. Hang, unpublished)\nIn general, we assume there is a set of security domains, which are used to classify documents\nexchanged between activities, and a flow policy that specifies the allowed information flows between\ndomains. We assign a domain to every document (a security classification) and to every employee (a\nsecurity clearance). These classifications and clearances determine which users are allowed to participate\nin which activities of the workflow. For example, employees with an HR clearance are not allowed to\nparticipate in the activities creating the medical report; otherwise, they would have direct access to\nconfidential medical data. We formally define the constraints regarding classifications, clearances and\nflow policies in Section 3.1.\nBesides direct information flows via transfer of documents, we aim to control indirect flows, where\nconfidential information is deducible from observations of the system behaviour. For example, an HR\nemployee in the above workflow can deduce whether the two medical officers agreed about the fitness of\nthe candidate by observing whether the workflow proceeds to activity 12 after the medical examinations\nor reverts to activity 4. This is acceptable and actually necessary in our scenario, as long as this is the only\nbit of information about the medical condition of the candidate that can be deduced by HR personnel.\nOur goal is to verify that the workflow indeed does not leak any additional medical information to non-\nmedical personnel. In the next section, we begin by formally modelling the workflow, and then proceed\nto formalise the security requirements.\n1.2 Preliminaries\nWe briefly recall the definitions of (state-) event systems and security predicates from the MAKS frame-\nwork for possibilistic information flow [15] that we use in this paper. An event system ES = (E, I,O,Tr)\nis essentially a (prefix-closed) set of traces Tr ⊆ E∗ that are finite sequences of events in the event set E.\n50 Possibilistic Information Flow Control for Workflow Management Systems\nThe disjoint sets I ⊆ E and O⊆ E designate input and output events, respectively. We denote the empty\ntrace as 〈〉, the concatenation of traces α and β as α.β , and the projection of a trace α onto a set E as\nα|E . In the composition ES1‖ES2 of two event systems ES1 and ES2, input events of one system match-\ning output events of the other system are connected (and vice versa) and thus become internal events of\nthe composed system. The set of traces is the set of interleaved traces of the two systems, synchronised\non events in E1∩E2:\nTr(ES1‖ES2) = {α ∈ (E1∪E2)∗ | α|E1 ∈ Tr(ES1)∧α|E2 ∈ Tr(ES2)}\nA state-event system SES = (E, I,O,S,s0,T ) has a set of states S, a starting state s0 ∈ S, and a transition\nrelation T ⊆ (S×E×S). The event system induced by a state-event system has the same sets of events\nand the set of traces that is enabled from the starting state via the transition relation.\nThe MAKS framework defines a collection of basic security predicates (BSPs). Many existing in-\nformation flow properties from the literature can be expressed as a combination of these BSPs. Each\nBSP is a predicate on a set of traces with respect to a view V . A view V = (V,N,C) on an event system\nES = (E, I,O,Tr) is defined as a triple of event sets that form a disjoint partition of E. The set V defines\nthe set of events that are visible for an observer, C are the confidential events, and the events in N are\nneither visible nor confidential. Notable examples for BSPs, that we will use in this paper, are backwards-\nstrict deletion (BSD) and backwards-strict insertion of admissible confidential events (BSIA)1, defined in\n[19] as follows:\nBSDV(Tr)≡∀α,β ∈ E∗.∀c ∈C.(β .c.α ∈ Tr∧α|C = 〈〉)\n⇒∃α ′ ∈ E∗.(α ′|V = α|V ∧α ′|C = 〈〉∧β .α ′ ∈ Tr)\nBSIAV(Tr)≡∀α,β ∈ E∗.∀c ∈C.(β .α ∈ Tr∧α|C = 〈〉∧β .c ∈ Tr)\n⇒∃α ′ ∈ E∗.(α ′|V = α|V ∧α ′|C = 〈〉∧β .c.α ′ ∈ Tr)\nIntuitively, the former requires that the occurrence of confidential events must not be deducible, while\nthe latter requires that the non-occurrence of confidential events must not be deducible. Technically, they\nare closure properties of sets of traces. For example, if a trace in Tr contains a confidential event, then\nBSD requires that a corresponding trace without the confidential event exists in Tr that yields the same\nobservations. This means the two traces must be equal with respect to visible V -events, while N-events\nmight be adapted to correct the deletion of the confidential event.\n2 System Model\nIn order to verify that a workflow satisfies given security requirements, we need a formal model of\nworkflows and their behaviour. We first define our notion of workflows. For simplicity, we omit aspects\nsuch as exceptions or compensation handling, but our definition suffices for our purpose of discussing\nthe verification of security requirements for workflows.\nDefinition 1. A workflow W = (A,Docs,SF,MF,U) consists of\n• a set A of activities,\n1In [19], BSIA is defined with an additional parameter ρ that allows to strengthen the property by further specifying positions\nat which confidential events must be insertable. For simplicity, we choose to fix this parameter to ρE in the notation of [19],\ni.e we only require confidential events to be insertable into a trace without interfering with observations if they are in principle\nadmissible exactly at that point in the trace.\nT. Bauereiss, D. Hutter 51\n• a set Docs of data items,\n• a set SF ⊆ (A×A) of sequence flows, where (a1,a2)∈ SF represents the fact that upon completion\nof a1, it may send a trigger to a2 signalling it to start execution, and\n• a set MF ⊆ (A×Docs×A) of message flows, where (a1,d,a2)∈MF represents data item d being\nan output of activity a1 and an input to a2, and\n• a set U of users participating in the workflow.\nThe sets A and Docs correspond to the nodes of a workflow diagram such as Figure 1, while SF and\nMF correspond to the solid and dashed edges, respectively.\nWe define the behaviour of workflows, not in a monolithic way, but in terms of the behaviours of\ncomponents representing activities communicating with each other. As we will show in Section 4, this\nsimplifies the verification, because it allows us to use the decomposition methodology of [13] to verify\nthe security of the overall system by verifying security properties of the subcomponents. We believe\nthat such a decomposition approach can help in scaling up verification of information flow properties to\nlarger systems.\nEach activity a is therefore modelled as a state-event system SESa =\n(\nEa, Ia,Oa,Sa,s0a,Ta\n)\nanalo-\ngously to Definition 3 of [13]. The set of events Ea consists of events of the form\n• Starta(u), starting the activity a and assigning it to the user u ∈U ,\n• Enda(u), marking the end of the activity,\n• Senda(a′,msg) and Recva(a′,msg), representing the sending (or receiving, respectively) of a mes-\nsage msg from activity a to activity a′ (or vice versa),\n• Setvala(u, i,val) and Outvala(u, i,val), representing a user u ∈U reading (or setting, respectively)\nthe value val of data item i, and\n• a set of internal events τa.\nWe denote the set of events of a given activity a ∈ A as Ea, and the set of all events in a workflow\nas EW =\n⋃\na∈AEa. We denote the set of events of a given user u ∈ U as Eu = {Starta(u) | a ∈ A}∪\n{Setvala(u, i,val) | a ∈ A, i ∈ Docs,val ∈Val} ∪ {Outvala(u, i,val) | a ∈ A, i ∈ Docs,val ∈Val} ∪\n{Enda(u) | a ∈ A}, and the set of all user interaction events as EU = ⋃u∈U Eu. The messages between\nactivities can have the form\n• Trigger, used to trigger a sequence flow to a successor activity in the workflow,\n• Data(i,v), used to transfer the value v for data item i, and\n• AckData(i), used to acknowledge the receipt of a data item.\nUsing separate messages for data and sequence flows is inspired by the BPMN standard, which describes\nits (informal) execution semantics in terms of tokens that are passed from one activity to the next, repre-\nsenting control flow separately from data flows. In addition, this separation simplifies the modelling of\nconfidentiality, as it becomes straightforward to classify events transporting Data messages into confi-\ndential or non-confidential events based on the classification of the data items they transport.\nThe local states of the activities include program variables such as a program counter and a mapping\nMem : Docs→Val, storing the values of data items. After initialisation, the activity waits for messages\nfrom other activities, transferring input data or triggering a sequence flow. When one (or more) of the\nincoming transitions have been triggered, the activity internally computes output messages (possibly via\ninteraction with users), sends them via the outgoing data associations, and triggers outgoing sequence\nflows. In Appendix A, we formally specify two types of activities as examples, namely user activities\nthat allow users to read and write data items, and gateway activities that make a decision on the control\nflow based on the contents of their input data items.\nEach of these state-event systems SESa induces a corresponding event system ESa. The overall\nsystem then emerges from the composition of these event systems ESa for every activity a ∈A, together\n52 Possibilistic Information Flow Control for Workflow Management Systems\nwith a communication platform ESP:\nESW = (‖a∈AESa)‖ESP\nWe call ESW the workflow system for the workflow W . We reuse the communication platform of [13],\nwhich is formally specified in Section 2.3 of [13]. It asynchronously forwards messages between the\nactivities. As we do not assume that it provides guarantees regarding message delivery, its specification\nis very simple.2 Upon composition with the platform, the communication events between the activities\nbecome internal events of the composed system. Only the communication events with users remain input\nand output events. These events form the user interface of the workflow system.\nA simple version of our example workflow can be represented as a composition of instances of the\nactivity types specified in Appendix A. We can represent the activity T11a in Figure 1 as a gateway that\ndecides on the control flow based on the results of the medical examinations: If they are positive, the\nworkflow continues with dispatching the contract, otherwise it goes back to selecting another candidate\nfrom the shortlist. The other activities essentially consist of users reading and generating documents, so\nwe can represent them as user activities. Of course, these activities can be enriched with further details,\ne.g. the interviews can be expanded to subprocesses of their own, but we assume that this is handled in a\nsubsequent refinement step and consider only the abstract level in this paper.\n3 Security Policies\n3.1 Confidentiality\nHR\nMed\nFigure 2: Flow policy\nWe assign security domains from a set D of domains to the data\nitems exchanged between the activities of the workflow. We de-\nnote this domain assignment function by dom : Docs→D. A flow\npolicy is a reflexive and transitive relation on domains and speci-\nfies from which domains to which other domains information may\nflow.[23] Note that, even though we focus on confidentiality in\nthis paper, also integrity requirements can be seen as a dual to\nconfidentiality and handled using information flow control. For\nexample, in [23] a lattice of combined security levels is built as a product of a confidentiality lattice and\nan integrity lattice. For our example workflow, we only require two confidentiality domains HR and\nMed. The medical reports MedReport1 and MedReport2 created by activities T6–T8 and T9–T11 in\nFigure 1 are assigned to the Med confidentiality domain, and other data items to the HR domain. The ex-\nample flow policy states that information may flow from HR to Med, but not vice versa, i.e. HR Med\nand Med 6 HR (see Figure 2).\nUsers read and write the contents of data items via the inputs and outputs of activities they participate\nin. In order to exclude unwanted direct information flows, we have to make sure that the classifications\nof the data items that users work with are compatible with their clearances. A straightforward approach\nis to enforce a Bell-LaPadula style mandatory access control. This can be formulated in terms of classi-\nfications that are assigned to activities based on the classifications of their inputs and outputs:\nDefinition 2. An activity classification clA : A→D is an assignment of domains to activities such that\n2However, it is possible to adapt the decomposition methodology to other communication models, e.g. providing some\nnotion of reliability of message delivery or means for synchronous communication. For example, see Appendix A for some\nremarks on guaranteeing a notion of ordered message delivery.\nT. Bauereiss, D. Hutter 53\n1. for all input data items i of an activity a, dom(i) clA(a), and\n2. for all output data items i of an activity a that may be assigned to untrusted users, clA(a) dom(i).\nWe allow users to participate in an activity of a given classification only if they have a matching\nclearance. We denote the mapping of users to clearances as clU : U →D. The conditions in Definition 2\ncorrespond to the Simple Security and ∗-Property of the Bell-LaPadula model, respectively. Note that\nwe relax the ∗-Property by allowing trusted users to downgrade data items. Otherwise, we would not be\nable to assign a classification to the activities T8 and T11 in our example workflow, because they have\nhigh inputs (the medical reports) and low outputs (the statements about the final result of examinations).\nHowever, this specific flow of information in the example is acceptable and necessary, because the output\nshould contain only the non-confidential final decision of the medical officer3 required by the HR de-\npartment, while the detailed content of the medical reports remains classified as confidential. Essentially,\nwe admit inputs and outputs of trusted users to act as a channel for declassification that is not formally\ncontrolled by our information flow analysis. It would be possible to model declassification more explic-\nitly, e.g. using intransitive flow policies [17], but for simplicity we choose this solution for this paper.\nThe same approach is followed in [33], for example. See [32] for an early discussion of this approach to\ndowngrading and [28] for a general overview of principles and dimensions of declassification.\nRegardless of whether trusted users are present or not, we want to verify that the system itself does\nnot leak information about data items i with classification dom(i) 6 d to users with clearance d. The set\nof confidential events for a domain d thus consists of events setting or reading values of these data items,\nwhile events of activities whose classification is allowed to flow into d are considered to be potentially\nobservable for users in domain d:\nDefinition 3. Let d ∈ D be a domain. The security view on a workflow system ESW for d is defined as\nVd = (Vd ,Nd ,Cd), where\nVd =\n⋃\nclA(a) d\nEa\nCd ={Setvala(u, i,val) | ∃u ∈U, i ∈ Docs,v ∈Val. dom(i) 6 d∧ clA(a) 6 d}\n∪{Outvala(u, i,val) | ∃u ∈U, i ∈ Docs,v ∈Val. dom(i) 6 d∧ clA(a) 6 d}\nNd =E \\ (Vd ∪Cd)\nThe set Cd contains the confidential input and output events.4 Note that we assume that confidential\ninformation enters the system only via user input or output, and that the system does not generate con-\nfidential information by itself (e.g. by generating cryptographic key material). If that were the case, the\ncorresponding system events would have to be added to the set Cd . Moreover, it is worth pointing out\nthat we consider certain other types of information to be non-confidential. In particular, the information\nwhether an activity has been performed or not, or the information which user has performed which ac-\ntivity is considered to be non-confidential. Again, such requirements could be captured by formulating\nthe security view accordingly. For our setting, the above view reflects our security requirement that the\nvalues of confidential data items should be kept secret. Hence, we use this view for the rest of this paper.\nIn Section 4, we describe how to verify that a given workflow system satisfies the security predicate\nBSDVd ∧BSIAVd with respect to this view for every domain d. It expresses that confidential user inputs\nand outputs can be deleted or inserted without interfering with the observations of users in domain d.\n3Which can be further enforced by allowing only Boolean values as content of the low output.\n4The set Cd only contains events of activities a with clA(a) 6 d, because activities with clA(a) d are considered to be\nvisible, and the set of visible and confidential events must be disjoint.\n54 Possibilistic Information Flow Control for Workflow Management Systems\n3.2 Separation of Duties\nAs discussed in the introduction, separation of duties is another common security requirement in work-\nflow management systems. Separation of duties can be formally defined as a safety property [3]. The\n“bad thing” happens when the same user participates in two activities constrained by separation of duty,\nhence we only allow traces where this does not occur.\nDefinition 4. Let a,a′ ∈ A be two activities. We call the set of traces{\nα ∈ E∗W | ∀u,u′ ∈U. ∀e1,e2 ∈ α.(e1 ∈ (Ea∩Eu)∧ e2 ∈ (Ea′ ∩Eu′))→ u 6= u′\n}\na separation-of-duty property Pa,a\n′\nSoD.\nAs we have modelled user assignment explicitly as events, this property can also be characterised by\nrequiring that 1. constrained activities are assigned to different users, and 2. users may participate in an\nactivity only after they have been assigned to it:\nPa,a\n′\nSoD ⊇\n{\nα ∈ E∗W | ∀u,u′ ∈U. Starta1(u) ∈ α ∧Starta2(u′) ∈ α −→ u 6= u′\n}\n∩{α ∈ E∗W | ∀a ∈ A,u ∈U,e ∈ (Ea∩Eu). Starta(u) /∈ α −→ e /∈ α}\nA system with a set of traces Tr and events E satisfies such a property iff Tr ⊆ Pa,a′SoD. In our example\nworkflow, there are separation of duty constraints between the activities belonging to the two medical\nexaminations (T 6–8 in Figure 1 one the one hand, and T 9–11 on the other hand). Hence, we want to\nenforce Pa,a\n′\nSoD for the pairs (a,a\n′) ∈ {T 6,T 7,T 8}×{T 9,T 10,T 11}.\nSimilarly, other runtime-enforceable security policies [30] can be modelled as safety properties. In\nthis paper, we focus on the above notion of separation of duty as an example and investigate its relation\nto information flow in Section 4.2.\n4 Verification\n4.1 Information Flow Security\nTo ease the verification of the security of a workflow system, we decompose it into the individual activi-\nties of the workflow and make use of the methodology presented in [13] to verify the resulting distributed\nsystem. For each domain d ∈D, we verify that users in that domain can learn nothing about information\nthat is confidential for them. The first step of the methodology [13] is to partition the activities into a\nset of low activities AdL = {a ∈ A | clA(a) d} that are (potentially) visible in domain d and a set of\nhigh activitiesAdH = {a∈A | clA(a) 6 d} that are not visible and may handle confidential information.5\nIt follows that Vd from Definition 3 is a global security view as defined in [13, Definition 13], i.e. the\nvisible events are exactly the events of the low activities, the set of confidential events is a subset of the\nevents of the high activities, and the remaining events are non-visible and non-confidential.\nThe second step is finding suitable local views Vad for high activities a ∈ AdH in order to verify that\nthey do not leak confidential information to low activities. Hence, we cannot generally treat communi-\ncation events of these activities as N-events, as we did in the global view, but we have to consider some\nof them as V -events (e.g. a high activity sending a trigger or a declassified data item to a low activity)\n5In [13], the set AdL is called the set of observers, while AdH is called the set of friends. This might be a bit counterintuitive\nin our setting for some readers, as the friends would be the activities that are not visible. To avoid confusion, we simply speak\nof low and high activities, respectively.\nT. Bauereiss, D. Hutter 55\nand some of them as C-events (e.g. a high activity receiving a confidential data item). Intuitively, this\nmeans we split each of these activities into a part that visibly interacts with low activities and a part that\nhandles confidential data, and verify that the latter does not interfere with the former. Technically, these\nlocal views satisfy certain constraints that allow us to instantiate the compositionality result of [13], as\nwe discuss below.\nDefinition 5. Let d ∈ D be a domain, and a ∈ AdH be a high activity for d. Furthermore, let DocsCd =\n{i ∈ Docs | dom(i) 6 d} denote the set of data items that are confidential for d. The local view for a is\ndefined as Vad = (V ad ,Nad ,Cad) with\nV ad = (Ia∪Oa)\\\n⋃\ni∈DocsCd\nEi\nCad =\n⋃\ni∈DocsCd\n(Ei \\{Senda(b,m) | ∃v. m = Data(i,v)∨m = AckData(i)})\nNad = Ea \\ (V ad ∪Cad)\nwhere the set Ei of high communication events containing data item i is defined as\nEi =\n{\ne |∃b ∈ AdH ,m,u,v. (m = Data(i,v)∨m = AckData(i))\n∧ (e = Senda(b,m)∨ e = Recva(b,m)∨ e = Setvala(u, i,v)∨ e = Outvala(u, i,v))\n}\nCombining these local views, we define the composed view for d as Vd+ = (Vd+ ,Nd+ ,Cd+) where\nVd+ =\n⋃\na∈AdH V\na\nd ∪\n⋃\na∈AdL Ea Cd+ =\n⋃\na∈AdH C\na\nd Nd+ = EW \\ (Vd+ ∪Cd+)\nNote that the combined view Vd+ is stronger than our global view Vd in the sense that more events\nare considered confidential or visible for an observer in domain d. Theorem 1 of [19] tells us that\nBSDVd+ ∧BSIAVd+ for the stronger view implies BSDVd ∧BSIAVd .\nAlso note that all communication events with low activities are considered visible, and that the for-\nwarding of confidential data items from one high activity to another is considered non-confidential. The\njustification for this is that secrets enter and leave the subsystem of high activities through communi-\ncation with users and low activities, and the forwarding between high activities can be considered as\ninternal processing. Hence, we can use communication events between high activities for correcting per-\nturbations caused by inserting or removing confidential user inputs. We make use of this fact in the proof\nof the following theorem, which states the security of activities as we have specified them in Appendix A\nin terms of the transition relations T gena , T usera and T\ngw(Cond)\na .\nTheorem 1. Let W be a workflow, d ∈ D a domain and SESa for a ∈ A an activity. If the transition\nrelation of SESa is\n• T gena ∪T usera , or\n• T gena ∪T gw(Cond)a and Cond does not depend on confidential data for d,\nthen BSDVad (Tra)∧BSIAVad (Tra) holds.\nThe proof of this and the following theorems can be found in the extended version of this paper [5].\nWe use the unwinding technique [16] for the proof. Note that since the generic transition relation T gena\nand the activity-specific transition relations are disjoint, we can partition this proof into a generic part\nthat covers the events and states used in T gena , and an activity-specific part. Therefore, if we want to use\na different kind of activity than the ones specified in this paper, and we reuse the generic part T gena of the\ntransition relation, then we can also reuse most of this proof.\n56 Possibilistic Information Flow Control for Workflow Management Systems\nThe next step is to instantiate the compositionality result of [13], which states that the security of\nthe overall system with respect to the global security view is implied by the security of the subsystems\nwith respect to their local views. However, our local views do not quite satisfy the requirement of being\nC-preserving in the sense of Definition 18 of [13], because that definition disallows N-events in the\ncommunication interface between subsystems. Hence, we slightly adapt the notion C-preserving views,\nallowing Send events to be in N:\nDefinition 6. Let AdH ⊆ A and C ⊆ EAdH . A family (V\na)a∈AdH of views V\na = (Va,Na,Ca) for Ea is C-\npreserving for C iff\n1. a ∈ AdH and b /∈ AdH implies ∀m.Senda(b,m) ∈Va∧Recva(b,m) ∈Va.\n2. a,a′ ∈ AdH implies\n(a) Recva′(a,m) ∈Ca′ iff Senda(a′,m) 6∈Va and\n(b) Recva′(a,m) ∈Va′ iff Senda(a′,m) ∈Va\n3. C∩Ea ⊆Ca for all a ∈Φ.\nAs can be easily seen, our local views are C-preserving for the set of global confidential events Cd\nfrom Definition 3: communication with low activities is visible, corresponding Recv and Send events\nare either visible or non-visible (where non-visible Recv events need to be confidential, while the corre-\nsponding Send events are allowed to be treated as N-events), and events that are confidential in the global\nview are confidential for the local views.\nIt turns out that the compositionality result of [13] still holds for our weakened notion of C-preserving\nlocal views; a sufficient (but not necessary) condition is that the subsystems satisfy not only BSD (as in\n[13]), but BSD and BSIA, which our activities happen to do.\nTheorem 2. Let W be a workflow, ESW = (‖a∈AESa)‖ESP be a workflow system, Vd be a global security\nview for domain d, and\n(Vad)a∈AdH be a family of local views that is C-preserving for Cd . If for all a∈AdH ,\nESa satisfies BSDVad ∧BSIAVad , then ESW satisfies BSDVd+ ∧BSIAVd+ and, therefore, BSDVd ∧BSIAVd .\nNote that, if other kinds of activities than the ones from Appendix A should be part of the workflow,\nit is only required to prove that their specifications also satisfy the security predicates for the local views,\nin order to show that the overall workflow satisfies the information flow security predicates.\nWe have formalised and verified our model and proofs using the interactive theorem prover Isabelle\n[24]. Our development is based on a formalisation of the MAKS framework developed by the group of\nHeiko Mantel at TU Darmstadt (unpublished as of this writing). We intend to make our formalisation\npublicly available when the MAKS formalisation is released.\nConceptually, the main difference between our workflow management systems and the shopping\nmall system described in [13] lies in the relation between users and the system. In the shopping scenario,\nthere is a one-to-one correspondence between users and software agents running in the system. Com-\nmunication with the users happens only during initialisation, when users write their preferences into the\ninitial memory of their agents, which run autonomously thereafter. In our workflow systems, the interac-\ntion is much more dynamic, as multiple activities can be assigned to the same user at runtime and there\nis ongoing communication between users and the system. This has impact on the system model — we\nintroduced additional events for user interaction — and the construction of views. The partitioning into\nhigh and low activities is based on classifications of data items and activities, and access control has to\nensure that only users with a matching clearance can participate in an activity, so that our security views\nare actually in line with the possible runtime observations of users. Despite these differences, we have\nseen that the methodology of [13] can be applied with small technical adjustments.\nT. Bauereiss, D. Hutter 57\n4.2 Compatibility with Separation of Duties\nAs described in Section 3.2, we can formalise constraints such as separation of duty as safety properties.\nHaving established information flow security of our workflow system, we now ask whether these security\nproperties are preserved when enforcing separation of duty constraints. In general, this is not the case.\nAltering a system such that it satisfies a safety property can be seen as a refinement, and it is well-known\nthat possibilistic information flow security is not preserved under refinement in general [18]. Consider,\nfor example, the security predicate BSIA. Repeatedly inserting confidential events of different users into\na trace can exhaust the possible user assignments that would satisfy the separation of duty constraints,\nthus deadlocking the process and making further visible observations impossible. We can, however, try\nto find sufficient conditions under which information flow properties are preserved:\nTheorem 3. Let ES = (E, I,O,Tr) be an event system and V = (V,N,C) be a view for ES. Let Ea,E ′a⊆ E\nbe two disjoint sets of events corresponding to activities a and a′, and let Pa,a\n′\nSoD be an SoD property. Let\nEu ⊆V ∪C be the communication events with a user u and EU =⋃u∈U Eu the set of all user events. If\n1. user assignment is non-confidential, i.e. there is a set Eassign ⊆ E \\C of assignment events, and\na user u may only participate in an activity after having been assigned to it via an event from\nEassign∩Eu, or\n2. only confidential or only visible user I/O events of activities a and a′ are enabled in ES, i.e. there\nis a set Edisabled ⊆ E of events that never occur in a trace of ES, and V ∩(Ea∪E ′a)∩EU ⊆ Edisabled\nor C∩ (Ea∪E ′a)∩EU ⊆ Edisabled holds, or\n3. the SoD constraint between a and a′ is already enforced by ES, i.e. Tr ⊆ Pa,a′SoD,\nthen BSDV(Tr)∧BSIAV(Tr) implies BSDV(Tr∩Pa,a\n′\nSoD)∧BSIAV(Tr∩Pa,a\n′\nSoD).\nIn our running example, we can choose Eassign = {Starta(u) | u ∈U} and apply the first case of the\ntheorem for the workflow system ESW and a view Vd+ , because only the details of the results of the med-\nical examinations are confidential, not the information who carried out the examinations. Furthermore,\nin case clA(a) 6= clA(a′), the mandatory access control described in Section 3.1 already enforces SoD\nstatically, so the third condition also applies. In general, Theorem 3 gives us sufficient conditions for the\ncompatibility of SoD constraints and information flow properties, taking into account the classifications\nof events that are relevant for enforcing SoD. Similar results could be developed for other classes safety\nproperties that are of interest in workflows, but we leave this as future work. Note that Theorem 3 is\nnot specific to workflow systems as specified in this paper. It can be applied to any system where users\nperform different activities in the presence of separation of duty constraints.\n5 Related Work\nWe build upon the MAKS framework for possibilistic information flow control [15], which is suitable\nfor formulating and verifying information flow policies at the specification level. We have focused on\nconfidentiality of data from unauthorised employees within the organisation, but in principle information\nflow control can be adapted to different attacker models and security policies by choosing the security\nviews appropriately. Furthermore, approaches have been proposed to take into account factors such as\ncommunication over the Internet [12] or encrypted communication channels [14]. In [25], a connection\nbetween role-based access control (RBAC) and mandatory access control is drawn, which might be\nadapted to enforce the mandatory access control we described in Section 3.1 using RBAC mechanisms.\n58 Possibilistic Information Flow Control for Workflow Management Systems\nEarly examples for workflow management systems with distributed architectures include [2, 22, 31].\nLater, computing paradigms with a similar spirit have emerged, e.g. service-oriented architectures or\ncloud computing. We see these techniques and standards as complementary to our work, as they can be\nused for the implementation of our abstract specifications.\nBPMN extensions to annotate business process diagrams with security annotations can be found in\n[6, 26, 33]. Closest to the security requirements considered by us comes the notation proposed in [33]\nthat supports both the annotation of activities with separation of duty constraints and the annotation of\ndocuments and process lanes with confidentiality and integrity classifications or clearances, respectively.\nSeveral proposals for a formal semantics of workflow specifications can be found in the literature.\nFor example, [34] maps BPMN diagrams to CSP processes and describes how the formal semantics can\nbe leveraged to compare and analyse workflow diagrams, e.g. with respect to consistency. It focuses\non the control flow and does not model data flows. In [35], workflows are represented as statements in\na workflow description language, which is mapped to a representation as hierarchical state machines.\nAn information flow analysis algorithm is described, but the actual information flow property that it\nchecks is not stated in a declarative, mechanism-independent way. [1] represents workflows as Petri\nnets and describes an approach for information flow analysis. The focus is on keeping the occurrence\nof tasks confidential, whereas our work focuses on the confidentiality of the data that is processed in\nthe workflow. In [4] and [29], workflows are formalised as transition systems and model-checking is\nemployed to verify properties specified as LTL formulas. This is suitable to verify safety or liveness\nproperties, whereas the information flow predicates considered by us can be seen as hyperproperties [8].\n6 Conclusion\nGraphical notations such as BPMN are widely used for workflow specification. We have presented an\napproach to formally model both the behaviour of a workflow and the associated security requirements,\nand described how to apply the decomposition methodology of [13] and how to verify a distributed work-\nflow management system with ongoing user interaction. We have shown that, even though possibilistic\ninformation is in general not refinement-closed, the enforcement of separation of duty is compatible with\nthe information flow security of the system under certain assumptions.\nWe have sketched how a simple version of our example workflow can be represented as a composition\nof instantiations of the activity types specified in Appendix A. As we have shown the security of these\nactivities in Theorem 1, we can use Theorem 2 to derive the security of the composed system from the\nsecurity properties of the individual activities. This demonstrates how instantiations of a type of activities\nthat has been proven secure once can be plugged into larger workflows in a secure way. Hence, we believe\nthat this compositional approach can help in making verification techniques for information flow scale\nto larger workflow systems. However, more work is needed before this approach can actually be applied\nto realistic systems. For example, tool support for translating a more realistic subset of BPMN to our\nsystem model would be a major step in this direction, which would also help us evaluate our approach\nwith a sample of existing workflows.\nMoving from an abstract specification towards the implementation level is another important direc-\ntion of future work. This paper deals with workflows on a high level of abstraction. We intend to work\non notions of security-preserving refinement that allow us to expand abstract activities in a workflow into\nmore concrete subprocesses and refine the behaviour of atomic activities towards an executable imple-\nmentation. There is a large body of existing work that we can build upon for this purpose, such as action\nrefinement for replacing atomic events on the abstract level with sequences of more concrete events [11],\nT. Bauereiss, D. Hutter 59\nswitching between event-based and language-based notions of information flow [20], or directly gener-\nating executable code from specifications [10]. In the long term, we hope that these decomposition and\nrefinement techniques will contribute to making the step-wise development of secure workflow systems\nfrom workflow diagrams to executable code more scalable and efficient.\nAcknowledgements We thank Richard Gay, Sylvia Grewe, Steffen Lortz, Heiko Mantel and Henning\nSudbrock for providing a formalisation of the MAKS framework in Isabelle/HOL that allowed us to\nverify our main results in Isabelle, and the anonymous reviewers for helpful comments on the paper.\nReferences\n[1] Rafael Accorsi & Andreas Lehmann (2012): Automatic Information Flow Analysis of Business Process Mod-\nels. In: BPM, pp. 172–187, doi:10.1007/978-3-642-32885-5 13.\n[2] Gustavo Alonso, Roger Gu¨ntho¨r, Mohan Kamath, Divyakant Agrawal, Amr El Abbadi & C. Mohan (1996):\nExotica/FMDC: A Workflow Management System for Mobile and Disconnected Clients. Distributed and\nParallel Databases 4(3), pp. 229–247, doi:10.1007/BF00140951.\n[3] Bowen Alpern & Fred B. Schneider (1987): Recognizing safety and liveness. Distributed Computing 2(3),\npp. 117–126, doi:10.1007/BF01782772.\n[4] Wihem Arsac, Luca Compagna, Giancarlo Pellegrino & Serena Elisa Ponta (2011): Security Validation\nof Business Processes via Model-Checking. In: Engineering Secure Software and Systems, LNCS 6542,\nSpringer, pp. 29–42, doi:10.1007/978-3-642-19125-1 3.\n[5] Thomas Bauereiss & Dieter Hutter (2013): Possibilistic information flow security of workflow management\nsystems. Technical Report. Available at http://bauereiss.name/papers/WorkflowSecurity_TR.pdf.\n[6] Achim D. Brucker, Isabelle Hang, Gero Lu¨ckemeyer & Raj Ruparel (2012): SecureBPMN: Modeling and\nEnforcing Access Control Requirements in Business Processes. In: SACMAT 2012, ACM, pp. 123–126,\ndoi:10.1145/2295136.2295160.\n[7] David D. Clark & David R. Wilson (1987): A Comparison of Commercial and Military Computer Security\nPolicies. IEEE Symposium on Security and Privacy, pp. 184–194, doi:10.1109/SP.1987.10001.\n[8] Michael R. Clarkson & Fred B. Schneider (2010): Hyperproperties. Journal of Computer Security 18(6), pp.\n1157–1210, doi:10.3233/JCS-2009-0393.\n[9] Riccardo Focardi & Roberto Gorrieri (1995): A Classification of Security Properties for Process Algebras.\nJournal of Computer Security 3(1), pp. 5–33, doi:10.3233/JCS-1994/1995-3103.\n[10] Florian Haftmann & Tobias Nipkow (2007): A code generator framework for Isabelle/HOL. In: Theorem\nProving in Higher Order Logics: Emerging Trends. Available at http://es.cs.uni-kl.de/events/\nTPHOLs-2007/proceedings/B-128.pdf.\n[11] Dieter Hutter (2006): Possibilistic Information Flow Control in MAKS and Action Refinement. In: ETRICS,\nLNCS 3995, Springer, pp. 268–281, doi:10.1007/11766155 19.\n[12] Dieter Hutter (2007): Preserving Privacy in the Web by Using Information Flow Control. In Andreas U.\nSchmidt, Michael Kreutzer & Rafael Accorsi, editors: Long-Term and Dynamical Aspects of Information\nSecurity: Emerging Trends in Information and Communication Security, Nova Science.\n[13] Dieter Hutter, Heiko Mantel, Ina Schaefer & Axel Schairer (2007): Security of multi-agent systems: A case\nstudy on comparison shopping. Journal of Applied Logic 5(2), pp. 303–332, doi:10.1016/j.jal.2005.12.015.\n[14] Dieter Hutter & Axel Schairer (2004): Possibilistic Information Flow Control in the Presence of Encrypted\nCommunication. In: ESORICS, LNCS 3193, Springer, pp. 209–224, doi:10.1007/978-3-540-30108-0 13.\n[15] Heiko Mantel (2000): Possibilistic Definitions of Security - An Assembly Kit. In: CSFW, IEEE Computer\nSociety, pp. 185–199, doi:10.1109/CSFW.2000.856936.\n60 Possibilistic Information Flow Control for Workflow Management Systems\n[16] Heiko Mantel (2000): Unwinding Possibilistic Security Properties. In: ESORICS, LNCS 1895, Springer, pp.\n238–254, doi:10.1007/10722599 15.\n[17] Heiko Mantel (2001): Information Flow Control and Applications - Bridging a Gap. In: FME, LNCS 2021,\nSpringer, pp. 153–172, doi:10.1007/3-540-45251-6 9.\n[18] Heiko Mantel (2001): Preserving Information Flow Properties under Refinement. In: IEEE Symposium on\nSecurity and Privacy, IEEE Computer Society, pp. 78–91, doi:10.1109/SECPRI.2001.924289.\n[19] Heiko Mantel (2002): On the Composition of Secure Systems. In: IEEE Symposium on Security and Privacy,\nIEEE Computer Society, pp. 88–101, doi:10.1109/SECPRI.2002.1004364.\n[20] Heiko Mantel & Andrei Sabelfeld (2003): A Unifying Approach to the Security of Distributed and Multi-\nThreaded Programs. Journal of Computer Security 11(4), pp. 615–676. Available at http://iospress.\nmetapress.com/content/r0pr0ma4kv8wa542/.\n[21] J. McLean (1996): A general theory of composition for a class of “possibilistic” properties. IEEE Transac-\ntions on Software Engineering 22(1), pp. 53–67, doi:10.1109/32.481534.\n[22] Peter Muth, Dirk Wodtke, Jeanine Weissenfels, Angelika Kotz Dittrich & Gerhard Weikum (1998): From\nCentralized Workflow Specification to Distributed Workflow Execution. Journal of Intelligent Information\nSystems 10(2), pp. 159–184, doi:10.1023/A:1008608810770.\n[23] Andrew C. Myers, Andrei Sabelfeld & Steve Zdancewic (2006): Enforcing Robust Declassification and\nQualified Robustness. Journal of Computer Security 14(2), pp. 157–196. Available at http://iospress.\nmetapress.com/content/EYT2D3ERKY3A2H25.\n[24] Tobias Nipkow, Lawrence C Paulson & Markus Wenzel (2002): Isabelle/HOL: a proof assistant for higher-\norder logic. LNCS 2283, Springer, doi:10.1007/3-540-45949-9.\n[25] Sylvia Osborn, Ravi Sandhu & Qamar Munawer (2000): Configuring role-based access control to en-\nforce mandatory and discretionary access control policies. ACM Trans. Inf. Syst. Secur. 3(2), p. 85–106,\ndoi:10.1145/354876.354878.\n[26] Alfonso Rodrı´guez, Eduardo Ferna´ndez-Medina & Mario Piattini (2007): A BPMN Extension for the\nModeling of Security Requirements in Business Processes. IEICE Transactions 90-D(4), pp. 745–752,\ndoi:10.1093/ietisy/e90-d.4.745.\n[27] A. Sabelfeld & A.C. Myers (2003): Language-based information-flow security. IEEE Journal on Selected\nAreas in Communications 21(1), pp. 5–19, doi:10.1109/JSAC.2002.806121.\n[28] Andrei Sabelfeld & David Sands (2009): Declassification: Dimensions and principles. Journal of Computer\nSecurity 17(5), pp. 517–548, doi:10.3233/JCS-2009-0352.\n[29] Andreas Schaad, Volkmar Lotz & Karsten Sohr (2006): A model-checking approach to analysing organi-\nsational controls in a loan origination process. In David F. Ferraiolo & Indrakshi Ray, editors: SACMAT,\nACM, pp. 139–149, doi:10.1145/1133058.1133079.\n[30] Fred B. Schneider (2000): Enforceable security policies. ACM Trans. Inf. Syst. Secur. 3(1), p. 30–50,\ndoi:10.1145/353323.353382.\n[31] Hans Schuster, Stefan Jablonski, Thomas Kirsche & Christoph Bussler (1994): A Client/Server Architec-\nture for Distributed Workflow Management Systems. In: PDIS, IEEE Computer Society, pp. 253–256,\ndoi:10.1109/PDIS.1994.331708.\n[32] Daniel F. Stork (1975): Downgrading in a Secure Multilevel Computer System: The Formulary Con-\ncept. Technical Report, DTIC Document. Available at http://oai.dtic.mil/oai/oai?verb=\ngetRecord&metadataPrefix=html&identifier=ADA011696.\n[33] Christian Wolter & Christoph Meinel (2010): An approach to capture authorisation requirements in business\nprocesses. Requir. Eng. 15(4), pp. 359–373, doi:10.1007/s00766-010-0103-y.\n[34] Peter Y. H. Wong & Jeremy Gibbons (2008): A Process Semantics for BPMN. In: ICFEM, LNCS 5256,\nSpringer, pp. 355–374, doi:10.1007/978-3-540-88194-0 22.\nT. Bauereiss, D. Hutter 61\n[35] Ping Yang, Shiyong Lu, Mikhail I. Gofman & Zijiang Yang (2010): Information flow analysis of scientific\nworkflows. Journal of Computer and System Sciences 76(6), pp. 390–402, doi:10.1016/j.jcss.2009.11.002.\n[36] Aris Zakinthinos & E. Stewart Lee (1997): A General Theory of Security Properties. In: IEEE Symposium\non Security and Privacy, IEEE Computer Society, pp. 94–102, doi:10.1109/SECPRI.1997.601322.\nA Specification of Activities\nIn this appendix, we give a formal specification of the behaviour of our activities using PP-statements.\nIn this formalism, the transition relation of a state-event system is specified by listing pre- and post-\nconditions on the state for each event (see Section 2.1 of [13] for a formal semantics).\nRecva(b,Data(i,v)); affects: Mem, AQueue\nPre: pc = 0, (b, i,a) ∈MF , (b, i) /∈ AQueue\nPost: Mem′(i) = v, AQueue′ = AQueue ∪\n{(b, i)}\nSenda(b,AckData(i)); affects: AQueue\nPre: pc = 0, (b, i) ∈ AQueue\nPost: AQueue′ = AQueue\\ (b, i)\nRecva(b,Trigger); affects: TriggeredBy\nPre: pc = 0\nPost: TriggeredBy′ = b\nτActivea ; affects: pc\nPre: pc = 0, TriggeredBy 6=⊥, AQueue = /0\nPost: pc′ = 1\nτSendDataa ; affects: pc, MQueue\nPre: pc = 2\nPost: pc′ = 3, MQueue′ =\n{(b, i) | (a, i,b) ∈MF ∧Mem(i) 6=⊥}\nSenda(b,Data(i,v));\naffects: MQueue,AQueue\nPre: pc = 3, (b, i) ∈MQueue\nPost: MQueue′ = MQueue \\\n{(b, i)},AQueue′ = AQueue∪{(b, i)}\nRecva(b,AckData(i)); affects: AQueue\nPre: pc = 3, (b, i) ∈ AQueue\nPost: AQueue′ = AQueue\\{(b, i)}\nτAckTimeouta ; affects: AQueue\nPre: pc = 3\nPost: AQueue′ = /0\nτSendTriggersa ; affects: pc, SQueue\nPre: pc = 3, MQueue = /0, AQueue = /0\nPost: pc′ = 4, SQueue′ = {b | (a,b) ∈ SF}\nSenda(b,Trigger); affects: SQueue\nPre: pc = 4, b ∈ SQueue\nPost: SQueue′ = SQueue\\{b}\nFigure 3: PP-statements of generic transition relation T gena\nWe specify the behaviour of our activities in two parts. The PP-statements in Figure 3 specify the\ngeneric part of the behaviour of activities, i.e. the communication with other activities in order to ex-\nchange data items and trigger sequence flows. For this purpose, it maintains program variables MQueue\n(which data items still have to be sent), AQueue (which data items still have to be acknowledged), SQueue\n(which triggers still have to be sent), TriggeredBy (whether and from where a trigger has been received),\nand User (to which user this activity is assigned). The program counters 0, 3 and 4 correspond to the\nphases of waiting for inputs and triggers, sending outputs, and sending triggers, respectively.\nWhen the program counter reaches 1, an activity-specific transition relation takes over in order to\nperform the actual activity. In our simple example workflow, we only need two kinds of activities, namely\n62 Possibilistic Information Flow Control for Workflow Management Systems\nuser input/output and gateways (deciding on the control flow based on a condition Cond on input data).\nThe latter continues the workflow with that activity b for which Cond(b,Mem) evaluates to true. These\ntwo kinds of activities are specified in Figures 4 and 5, respectively. We denote the transition relations\ninduced by the PP-statements in Figures 3, 4, and 5 as T gena , T usera , and T\ngw(Cond)\na , respectively. The\noverall transition relation of an activity is the union of T gena and an activity-specific transition relation.\nStarta(u); affects: User\nPre: pc = 1, User =⊥, clU(u) = clA(a)\nPost: User′ = u\nSetvala(u, i,v); affects: Mem\nPre: pc = 1, User = u\nPost: Mem′(i) = v\nOutvala(u, i,v); affects:\nPre: pc = 1, User = u, Mem(i) = v\nEnda(u); affects: pc\nPre: pc = 1,User = u\nPost: pc′ = 2\nFigure 4: PP-statements of transition relation T usera for user activities\nSenda(b,Trigger); affects: pc\nPre: pc = 1, Cond(b,Mem) =>, (a,b) ∈ SF\nPost: pc′ = 5\nFigure 5: PP-statement of transition relation T gw(Cond)a for gateways\nAfter completion of the activity has been signalled by setting the program counter to 2, the generic\ntransition relation takes control again and starts sending output data items to the designated receivers.\nIt makes sure that they have been received by waiting for acknowledgements, and afterwards proceeds\nby sending triggers to the successor activities in the workflow. An exception to this rule is if a receiver\nfails to send an acknowledgement; in this case the τAckTimeouta event can be used to signal a timeout and\nproceed with the workflow. This is important for security, because otherwise a confidential activity could\nblock the progress of the workflow by refusing to acknowledge a data item.\nOf course, other modelling decisions are possible to solve this problem. As an alternative, we have\nalso modelled and verified a system specification where the communication platform guarantees causal\ndelivery of messages, i.e. messages from one activity to another are always received in the order that they\nare sent. This would make acknowledgements unnecessary, because an activity could always be sure that\na trigger message is received after all data items, if the messages are sent in this order. However, this\nshifts complexity from the individual activities to the communication platform and the interface, and it\nturns out that this makes the proof of compositionality more laborious. Essentially, we had to prove an\nadditional security predicate FCIA for the platform and the activities together with several additional side\nconditions on the local views in order to obtain compositionality. In this paper, we therefore present the\nabove model with explicit acknowledgements for simplicity. However, we intend to further investigate\nthe implications of different guarantees provided by the communication platform in future work.\n",
            "id": 17191170,
            "identifiers": [
                {
                    "identifier": "oai:arxiv.org:1404.1987",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "oai:doaj.org/article:63c889b2be9346718882bbd539be614c",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "1404.1987",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "10.4204/eptcs.148.4",
                    "type": "DOI"
                },
                {
                    "identifier": "87732987",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "25017123",
                    "type": "CORE_ID"
                }
            ],
            "title": "Possibilistic Information Flow Control for Workflow Management Systems",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:arxiv.org:1404.1987",
                "oai:doaj.org/article:63c889b2be9346718882bbd539be614c"
            ],
            "publishedDate": "2014-04-07T00:00:00",
            "publisher": "'Open Publishing Association'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1404.1987",
                "https://doaj.org/toc/2075-2180"
            ],
            "updatedDate": "2020-10-12T14:12:15",
            "yearPublished": 2014,
            "journals": [
                {
                    "title": "Electronic Proceedings in Theoretical Computer Science",
                    "identifiers": [
                        "2075-2180"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1404.1987"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/17191170"
                }
            ]
        },
        {
            "acceptedDate": "2014-04-23T00:00:00",
            "arxivId": "1404.6602",
            "authors": [
                {
                    "name": "Leino, K. Rustan M."
                },
                {
                    "name": "Wüstholz, Valentin"
                }
            ],
            "contributors": [],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/87735116"
            ],
            "createdDate": "2014-10-24T19:26:02",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 645,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/645",
                    "logo": "https://api.core.ac.uk/data-providers/645/logo"
                }
            ],
            "depositedDate": "2014-04-26T00:00:00",
            "abstract": "In recent years, program verifiers and interactive theorem provers have\nbecome more powerful and more suitable for verifying large programs or proofs.\nThis has demonstrated the need for improving the user experience of these tools\nto increase productivity and to make them more accessible to non-experts. This\npaper presents an integrated development environment for Dafny-a programming\nlanguage, verifier, and proof assistant-that addresses issues present in most\nstate-of-the-art verifiers: low responsiveness and lack of support for\nunderstanding non-obvious verification failures. The paper demonstrates several\nnew features that move the state-of-the-art closer towards a verification\nenvironment that can provide verification feedback as the user types and can\npresent more helpful information about the program or failed verifications in a\ndemand-driven and unobtrusive way.Comment: In Proceedings F-IDE 2014, arXiv:1404.578",
            "documentType": "research",
            "doi": "10.4204/eptcs.149.2",
            "downloadUrl": "http://arxiv.org/abs/1404.6602",
            "fieldOfStudy": "computer science",
            "fullText": "Dubois C., Giannakopoulou D., Méry D. (Eds): Formal Integrated\nDevelopment Environment 2014 (F-IDE 2014)\nEPTCS 149, 2014, pp. 3–15, doi:10.4204/EPTCS.149.2\nc© K.R.M. Leino, V. Wüstholz\nThe Dafny Integrated Development Environment\nK. Rustan M. Leino\nMicrosoft Research, Redmond, WA, USA\nleino@microsoft.com\nValentin Wüstholz\nETH Zurich, Department of Computer Science, Switzerland\nvalentin.wuestholz@inf.ethz.ch\nIn recent years, program verifiers and interactive theorem provers have become more powerful and\nmore suitable for verifying large programs or proofs. This has demonstrated the need for improving\nthe user experience of these tools to increase productivity and to make them more accessible to non-\nexperts. This paper presents an integrated development environment for Dafny—a programming\nlanguage, verifier, and proof assistant—that addresses issues present in most state-of-the-art verifiers:\nlow responsiveness and lack of support for understanding non-obvious verification failures. The\npaper demonstrates several new features that move the state-of-the-art closer towards a verification\nenvironment that can provide verification feedback as the user types and can present more helpful\ninformation about the program or failed verifications in a demand-driven and unobtrusive way.\n0 Introduction\nProgram verifiers and proof assistants integrate three major subsystems. At the foundation of the tool\nlies the logic it uses, for example a Hoare-style program logic or a logic centered around type theory. On\ntop of the logic sits some mechanism for automation, such as a set of cooperating decision procedures or\nsome proof search strategies (e.g., programmable tactics). The logic and automation subsystems affect\nhow a user interacts with the verification system, as is directly evident in the tool’s input language. The\nthird subsystem is the tool’s integrated development environment (IDE), which in a variety of ways tries\nto reduce the effort required by the user to understand and make use of the proof system.\nIn this paper, we present the IDE for the program verifier Dafny [15, 13]. The IDE is an extension of\nMicrosoft Visual Studio (VS). It goes beyond what has been done in previous IDEs (for Dafny and other\nverification systems) in several substantial ways.\ncontinuous processing The IDE runs the program verifier in the background, thus providing design-\ntime feedback. The user does not need to reach for a “Verify now” button.\nDesign-time feedback is common in many tools. For example, the spell checker in Microsoft\nWord is always on in this way. Anyone who remembers from the 1980s having to invoke the\nspell checker explicitly knows what a difference this can make in how we think about the in-\nteraction with the tool; the burden of having to go through separate spelling sessions was trans-\nformed into the interaction process that is hardly noticeable. Parsing and type checking in many\nprogramming-language IDEs is done this way, enabling completion and other kinds of IntelliSense\ncontext-sensitive editing and documentation assistance. The Spec# verifier was the first to integrate\ndesign-time feedback for a verifier [0]. The jEdit editor for Isabelle [23] also provides continuous\nprocessing in the background by running both a proof search and the Nitpick [2] checker which\nsearches for counterexamples to the proof goal.\nnon-linear editing The text buffer can be edited anywhere, just like in usual programming-language\neditors. Any change in the buffer will cause the verifier to reconsider proof obligations anywhere\n4 The Dafny IDE\nin the buffer. (Since the Dafny language is insensitive to the order of declarations, the proof\nobligations that have to be reconsidered can occur both earlier and later in the buffer.)\nAlthough such non-linear editing seems obvious, it is worth noting that it is in stark contrast to\ncommon theorem prover IDEs like ProofGeneral 0 and CoqIde 1, where the user manually moves\na high water mark in the buffer—anything preceding this mark in the buffer has already been\nprocessed by the system and is locked down to prevent editing, and anything following the mark\nhas not been processed and can be freely edited.\nmulti-threading The Dafny IDE makes more aggressive and informed use of available multi-threaded\nhardware. The number of concurrent threads used is adjusted dynamically, depending on what the\nverification tasks at hand are able to saturate.\nAlthough conceptually an obvious thing to do, the Dafny tool chain previously lacked the features\nto run separate verification tasks in parallel. The use of multiple threads is especially noticeable\nwhen a file is just opened in the editor, since caches are cold at that time and everything needs to\nbe verified.\nThe Isabelle/jEdit editor [23, 22] comes with support for multi-threading, which is motivated by\nthe fact that it also supports non-linear editing and therefore offers more opportunities to parallelize\nverification tasks. The SPARK 2014 toolset [7] also supports multi-threading, both in its transla-\ntion from SPARK into the intermediate verification language Why3 and in the Why3 processing\nitself.\ndependency analysis and caching The Dafny IDE caches verification results along with computed de-\npendencies of what is being verified. Before starting a new verification task, the system first\nconsults the cache. This feature makes the tool more responsive and reduces the user’s wait times.\nOur users have found this to be the most useful of our features for making the interaction between\nuser and system more effective. It is also what makes continuous processing desirable for large\nfiles. When a user gets stuck during a verification attempt, a typical response is to try many little\ninput variations that might explain or remove the obstacle at hand. It is during these times that the\nuser needs the tool the most, so supporting fluid interactions at this time is of utmost importance.\nThere has been a lot of work on caching, modifying, and replaying proofs for interactive proof\nassistants. For proofs performed by SMT solvers, Grigore and Moskal worked on these things in\nthe context of ESC/Java2 [10].\nshowing information Commonly, a verification system can supply various associated declarations au-\ntomatically. For example, common induction schemes may be constructed by default, some types\nand loop invariants may be inferred, and syntactic shorthands can reduce clutter in the program\ntext. Sometimes, a user may find it necessary to inspect this information. The Dafny IDE attempts\nto make this information available via hover text—when the user hovers the mouse cursor over a\npart of the program text, say, an identifier, any additional information about that identifier is dis-\nplayed. This makes the information easily accessible to users, but is at the same time not cluttering\nup the view of the program text.\nNote that in console-based interactive tools, for example like ACL2 [11], the unobtrusive nature of\ninformation in hover text is difficult to achieve. Such a tool has to either provide a set of commands\nthat can be used to query information gathered by the tool or optimistically spill out a stream of\n0http://proofgeneral.inf.ed.ac.uk\n1http://coq.inria.fr\nK.R.M. Leino, V. Wüstholz 5\ninformation to the console window in the off-chance that a user wants to see some part of that\ninformation.\nAn important consequence of making additional information easily accessible to the user is that\nit gives the verification system greater freedom in what can be computed automatically. Users\nno longer need to fully understand the creative and elaborate schemes employed to compute this\ninformation, because whatever is computed can be viewed by the user, if needed.\nThis feature is also common in programming-language IDEs, where inferred types or fully quali-\nfied identifier names are displayed as hover text. The Dafny IDE takes this a step further, showing\ninformation such as default termination measures, specifications of implicit methods (such as those\ngenerated for iterators), which calls are classified as co-recursive, and code inherited by Dafny’s\n“. . .” construct from a refined module.\nintegrated debugging Verification error messages can have a lot of associated information, some of\nwhich can be useful to users. Previously, the Dafny IDE would highlight, directly in the IDE\neditor, the error trace leading to a reported error. SPARK 2014 also does this, for example. To\nget information about the possible values of variables for the reported error, a Dafny user can use\nthe Boogie Verification Debugger (BVD) [9], which presents this information in a format akin to\nthat provided in modern source-level debuggers. We have done a deep integration of BVD into the\nDafny IDE.\nPreviously, BVD was accessible for Dafny only as a standalone tool, which meant the user manu-\nally had to correlate the source lines reported by BVD with the text buffer containing the program\nin the IDE. The program verifier VCC [4] integrates BVD into its Visual Studio IDE. The Dafny\nIDE now goes further, for example letting the user select which program state to inspect by click-\ning in the program text itself. It also uses hover text to present values of variables in the selected\nstate. OpenJML [5, 6] also presents error information in this way, letting users inspect values of\nany subexpression and letting the source code location of the expressions hovered over determine\nwhich execution state is used to look up the value to be displayed.\nAs an alternative to running Dafny in Visual Studio, Dafny can also be run from within a web browser\n(http://rise4fun.com/dafny) and from the command line. However, the bulk of the features we men-\ntion in this paper are available only in the Visual Studio IDE extension. Dafny, including its IDE, is\navailable as open source from http://dafny.codeplex.com/.\n1 Tool Architecture\nBefore presenting the new tool architecture, we will give an overview of the underlying components and\nthe tool architecture that was used in the past (see Fig. 0); it is similar to the architecture of other veri-\nfication tools that are built on top of the Boogie verification engine [0], such as Spec# [1] and VCC [4].\nAs the user is editing the program, the VS extension continuously sends snapshots of the program to\nthe underlying Dafny verifier, which encodes the correctness proof obligations as a translation into Boo-\ngie. Boogie is an intermediate language [19] for program verification (similar to Why3 [8]). Boogie\nprograms typically consist of several top-level declarations (e.g., axioms, variables, procedures) that are\nused to formalize programs in a higher-level language, such as Dafny. For instance, each Dafny method\nis translated to a Boogie procedure implementation that captures the well-definedness conditions of the\nmethod’s specification, a Boogie procedure specification that captures the method specification to be\nused by callers, and a Boogie procedure implementation that captures the method body and checks that it\n6 The Dafny IDE\nInitial tool architecture\nVS extension\nDafny\nBoogie\nZ3\nCurrent tool architecture\nVS extension\nDafny\nBoogie\nZ3. . .Z3 . . . Z3\nBVD\nFigure 0: Comparison of initial and current tool architecture. Arrows indicate data that is passed from one\ncomponent to another, where dashed arrows indicate that data is transferred asynchronously. Less thick,\nred arrows indicate error information (including counterexamples for BVD in the current architecture)\nthat is returned.\nsatisfies the method specification [14]. Similarly, each Dafny function is translated to a Boogie function\nand a Boogie procedure implementation that captures the corresponding well-definedness conditions.\nThe resulting Boogie program is sent to the Boogie verifier, which generates verification conditions for\neach Boogie implementation to discharge them using an automatic reasoning engine, typically the SMT-\nsolver Z3 [20]. Verification errors that are revealed during this process are propagated up to the VS\nextension, which displays them to the user.\nThis architecture gives rise to a pleasant and highly responsive user interaction for small programs,\nbut does not scale well to larger programs that consist of many methods and functions. Since the requests\nto the underlying solver can easily be parallelized, we have extended the Boogie verification engine to\nmake use of separate tasks for verifying Boogie implementations in parallel (using the .NET Task Parallel\nLibrary). Each task may discharge its verification conditions using one or more solver instances that are\nmanaged in a dynamically allocated pool of solvers. To take full advantage of this architectural change,\nwe made the propagation of verification errors to the user fully asynchronous (see dashed arrows in\nFig. 0). This lets error messages show up as soon as the corresponding verification condition has been\nprocessed by the solver. (Previously, Boogie only made use of multi-threading in one place, namely in\nits mode for verification-condition splitting [18]. We have preserved that functionality and integrated it\ninto the new task-based architecture.)\nThe Visual Studio extension for Dafny gets notified anytime there is a new snapshot, that is, anytime\nthe text buffer changes. Upon each such change, the extension recomputes syntax highlighting, which\nis done through a simple lexical scan (that is, the parser is not invoked and no abstract syntax tree\nis built). After 0.5 seconds of inactivity, the Dafny IDE invokes the Dafny parser, resolver, and type\nchecker on the current buffer snapshot. If the snapshot passes these phases without error, the additional\ninformation computed during these phases (e.g., which calls are co-recursive) is made available to the\nuser in hover text. Also, the snapshot is then asynchronously sent to the Dafny verifier, unless the verifier\nis already running on a previous snapshot. As verification errors are reported by the asynchronously\nrunning verifier, they are displayed in the IDE. Once a snapshot has been fully processed by the verifier,\na new verification task is started for the current snapshot, unless that is the snapshot that was just verified.\nA constant question that users would have about Dafny’s previous IDE was, “Is the verifier done\nyet?”. To give the user a sense of the processing that is taking place in the background, the new Dafny\nK.R.M. Leino, V. Wüstholz 7\nFigure 1: Progress indication via colors in the margins. The three program snapshots of the buffer\nare shown in chronological order (from left to right). The dark-orange margin in the middle snapshot\nindicates that changes have not yet been sent to the prover, while the purple margin in the right snapshot\nindicates that the verifier has started processing this snapshot.\nIDE uses colors in the margin (see Figure 1). A dark-orange color in the margin shows a line that has\nbeen edited in a snapshot that has not yet been sent to the verifier, and a violet color in the margin shows\na line that has been edited in a snapshot that is currently being processed by the verifier.\nWe also changed the tool architecture to integrate the Boogie Verification Debugger (BVD) [9] di-\nrectly. Under this change, which is independent of the parallelization, the solver is asked to include the\ncounterexample information needed by BVD with each verification error.\n2 On-demand Re-verification\nCaching is a popular technique for improving the responsiveness of systems that would need to repeatedly\nperform expensive computations whose output is a function of the given input. Since in a modular\nverification approach different entities of a program (e.g., modules, classes, or—as in Dafny—methods\nand functions) are verified in isolation, changes to one program entity usually invalidate only a small\nfraction of the verification results previously obtained for other program entities. More specifically, one\ncan safely avoid re-verification of an entity by caching previously computed verification results, except\nwhen the user has changed some other program entity on which it depends. This optimization is crucial\nin providing rapid feedback when the program is larger than just a handful of entities.\nOur technique for avoiding re-verification of methods and functions in Dafny deals with two core\nissues: 0) detecting changes to program entities and 1) tracking dependencies between different program\nentities to determine what needs to be re-verified. To solve the first issue, we extended Dafny to compute\nan entity checksum for each function, each method, and the specification (e.g., pre- and postconditions)\nof each method. This checksum is insensitive to various minor changes of the specific program text,\nbecause it is computed based on the Dafny abstract syntax tree. For instance, the checksum of a method\ndoes not change if a comment is edited by the user. To deal with the second issue, these entity checksums\nare used to track dependencies by computing dependency checksums for each program entity (function,\nmethod, or method specification) based on its own entity checksum and the dependency checksums of\nother entities on which it depends directly (e.g., methods it calls). This lets us compare the dependency\nchecksum of a given entity for the current program snapshot with the one stored in our verification result\ncache to determine if it needs to be re-verified.\nIn our implementation, we chose to compute the dependency checksums at the level of Boogie enti-\nties, thus making this feature available to other verifiers that target Boogie. To set an entity checksum, a\nBoogie client (here, Dafny) tags the entity with a particular custom attribute—a general mechanism sup-\nported by Boogie for attaching directives to declarations—and gives the checksum as an integer argument\nto this attribute.\n8 The Dafny IDE\nSnapshot 0\nmethod Foo()\nensures P();\n{ }\nmethod Bar() { }\nfunction P(): bool { true }\nSnapshot 1\nmethod Foo()\nensures P();\n{ }\nmethod Bar() { Foo(); }\nfunction P(): bool { true }\nSnapshot 2\nmethod Foo()\nensures P();\n{ }\nmethod Bar() { Foo(); }\nfunction P(): bool { false }\nFigure 2: Example of on-demand re-verification. The three program snapshots are ordered chronolog-\nically (i.e., snapshot 0 is the initial program and snapshot 2 is the final program) and changes between\nsnapshots are underlined. All entities in snapshot 0 need to be verified, while for snapshot 1 only method\nBar needs to be re-verified. Finally, for snapshot 2 all entities need to be re-verified since all of them\ndepend directly or indirectly on the modified function P.\nFigure 2 illustrates how our technique works on a concrete verification session that consists of three\nprogram snapshot, which are sent to the prover in chronological order (i.e., snapshot 0 is the initial\nprogram and snapshot 2 is the final program). All entities of the initial program snapshot need to be\nverified, since nothing has been cached yet. For snapshot 1, only method Bar needs to be re-verified:\nthe corresponding Boogie implementations (for checking the correctness and well-definedness of the\nmethod body) are tagged with an entity checksum that is different from the one in the cache, but the\nentity checksum of the corresponding Boogie procedure (for capturing the method specification) stays\nthe same. For snapshot 2, all entities need to be re-verified: the entity checksum of the Boogie function\nthat corresponds to the Dafny function P changes with respect to the previous snapshot, which affects the\ndependency checksums of all remaining Boogie implementations.\nOne interesting application of this technique has to do with prioritizing the program entities that are\nbeing verified. Ideally, we want to prioritize entities that are more directly affected by the latest change\nto the program text, because that is where the user is likely to want to see the effect of the re-verification\nfirst. To do that, we assign different levels of priority to an entity based on its current checksums and\nthe ones stored in the verification result cache: 0) low (current entity checksum is identical to the one in\nthe cache, but the dependency checksum is different; entity was unchanged, but some dependency was\nchanged), 1) medium (current entity checksum is different from the one in the cache; i.e., entity was\nchanged directly), 2) high (no cache entry found; i.e., entity was added recently), and 3) highest (current\ndependency checksum of the entity is identical to the one in the cache). This prioritization scheme is\nmotivated by the observation that users usually prefer to get rapid feedback regarding the entities that\nwere recently added or changed directly. Note that we assign the highest priority to entities that were\nnot affected by the change at all, since displaying the corresponding verification results only requires a\nsimple cache lookup and we want to minimize the time during which the corresponding errors are not\ndisplayed to the user. This prioritization scheme could be extended easily to support more fine-grained\npriority levels.\nOther verification systems have also used forms of checksums and dependencies in order to reduce\nthe need to construct new proofs. In the heterogeneous Why3 system, both the construction and ver-\nification of proof obligations can be parameterized by different transformations and different solvers.\nTo maintain proofs as much as possible when any subsystem changes, or if the program under scrutiny\nchanges, Why3 uses a scheme of proof sessions and goal shapes for tracking dependencies [3]. This has\nK.R.M. Leino, V. Wüstholz 9\nlet more than 100 program proofs be automatically maintained over a period of more than two years. For\nDafny, we have focused on reducing turnaround time for the user, rather than trying to be robust against\nchanges in components of Dafny itself. Still, perhaps Dafny could benefit from proof sessions and goal\nshapes as we, in the future, move to tracking finer-grain dependencies.\nChange management is also important in interactive proof assistants where large parts of proofs are\nauthored by users. Work on such change management has been done, for example, in the context of\nKIV [21] and KeY [12].\n3 User Interaction\n3.0 Computed Information as Hover Text\nA verification system typically computes various properties that determine how verification conditions\nare formulated. For example, Dafny uses heuristics to determine automatically generated induction\nhypotheses [16]. Sometimes, it can be unclear to the user which properties were computed. For instance,\nDafny uses some rules that determine if a function self-call is recursive or co-recursive; a user who does\nnot know the precise rules may want to find out which calls have been determined to be co-recursive.\nWe devised a simple mechanism by which the Dafny resolver and type checker can associate any\ninformation with any AST node. When Dafny is running in the IDE, this information then gets displayed\nas hover text for the region in the text buffer that corresponds to the respective AST node. We use this\nmechanism to display the type and kinds of variables (e.g., “(ghost local variable) x: List〈int〉” or\n“(destructor) List.head: T”), the default decreases clauses for methods and functions [15], the auto-\nmatically generated conclusions of forall statements, which methods are tail recursive, which function\ncalls are co-recursive, the expansion of the syntactic sugar for calls to prefix predicates and prefix meth-\nods [17], the class expansion of iterators, and code inherited from a refined module through Dafny’s “. . .”\nconstruct.\n3.1 Error Reporting\nWhen a verification attempt is not going through, a user has to debug the cause. For example, the\nexecutable program may be wrong, the specifications may be wrong, the given proof of a lemma may\nbe incorrect, more information may be needed to make the proof go through, or the problem could be\ncaused by some incompleteness of the SMT solver.\nOne way to debug such a situation is to ask the verifier questions like “does the following condition\nhold here?” (which is done by adding an assert statement in the program text) and “can the proof goal\nbe met under this additional assumption?” (which is done by temporarily adding an assume statement in\nthe program text). This kind of interactive dialog with the verifier is supported well in the Dafny IDE,\nbecause the caching (and sometimes parallelization) makes the interaction swift and fluid.\nIt is also possible to obtain more information about the failing situation. This is done by exploring\nthe counterexample produced by the solver. The Boogie Verification Debugger (BVD), via a Dafny plug-\nin, makes this counterexample intelligible at the source context [9]. BVD was previously available for\nDafny only as a standalone tool, but we have now integrated it directly in the IDE.\nLet us describe our interface to BVD. When an attempted verification fails, like the postcondition\nviolation shown in Fig. 3, a red dot (and a red squiggly line) indicate the return path along which the\nerror is reported. The error pane at the bottom of the screen shows the error message, which also appears\n10 The Dafny IDE\nas hover text for the squiggly line. The error pane also lists source locations related to the error, in this\ncase showing the particular postcondition that could not be verified.\nBy clicking on a red dot, the Dafny IDE will display more information related to that error, resulting\nin the screen shown in Fig. 4. The blue dots that now appear in the program text trace the control path\nfrom the start of the enclosing routine and leading to the error. There is state information associated with\neach blue dot and the user can click on a blue dot to select a particular state (by default, the last state is\nselected, which is the state in which the error was detected).\nIn addition to the blue dots, BVD is brought up in a pane to the right. BVD shows the variables\nin scope, in a familiar debugger-like fashion, but with two conspicuous differences: some of the values\nshown are underspecified (the names of these values begin with an apostrophe, like ’7 and ’8; distinct\nnames refer to distinct values), and some values are not shown at all, because they are not relevant to the\ncounterexample (like all of the array elements of a, except the one at index 2804).\nThe “Value” column in the BVD pane shows values in the currently selected state, whereas the\n“Previous” column shows the values in the previously selected state. This gives a simple way to compare\nthe values in two states. In the example in the figure, we had first had the error state selected and then\nselected the state one line earlier.\nFinally, the figure illustrates how values for variables of primitive types (in the currently selected\nstate) are also displayed as hover text.\nWhat all of this tells us for the example is that the postcondition cannot be verified when the bound\nvariable i in the postcondition is 2804. That index of the array is set by the assignment to a[end], but\nis then changed (from ’7 to ’8) in the next line where a recursive call to Fill is made. Some thinking\nthen reveals that the cause of the verification error is that the postcondition of Fill is too weak. We can\nfix the problem by adding a postcondition about the array indices between 0 and start, in particular by\nsaying that Fill leaves those array elements unchanged:\nensures ∀ i • 0 ≤ i < start =⇒ a[i] = old(a[i]);\nBy simply typing in this extra postcondition and then waiting a split second, the error goes away.\n4 Experience\nFigure 5 shows some preliminary performance numbers, comparing for some long-running verification\ntasks the effect of using one solver versus three solvers. Two of the four programs (ParallelBuilds.dfy\nand LnSystemF.dfy) were developed by users of our tool, whereas the two other programs contain solu-\ntions to several verification challenges from two different verification competitions.\nAs a proof that the cache is very important (actually, even more important than parallelization) for\nenabling a highly responsive user interaction, we measured the performance improvements gained by\nusing the cache. We considered 5 “versions” of two long-running verification tasks. The 5 versions are\n5 copies of the same program, but randomly changing one of the checksums, as if a user had edited the\nprogram. Figure 6 gives the results, which show that using the cache allows the 5 versions to be verified\nin a total time that is a small increment over verifying the program once. We did not perform these\nmeasurements for the two other programs (VSComp2010.dfy and VSTTE2012.dfy), since they contain\ncollections of independent programs, which might not be a representative use case.\nThe largest single project using Dafny is the Ironclad project in the systems and security research\ngroups at Microsoft Research, which currently comprises about 30,000 lines of Dafny specifications,\ncode, and proofs. The current Dafny IDE has benefited from feedback from the Ironclad team.\nK.R.M. Leino, V. Wüstholz 11\nFigure 3: A screenshot of the Dafny IDE. The verification error is displayed in the text buffer as a red\ndot, which can be selected to obtain more information.\nFigure 4: A screenshot showing additional information obtained by selecting an error (red dot). The blue\ndots show the program states along the control path leading to the error, and the BVD pane to the right\nshows values of variables in the selected state of the selected error.\n12 The Dafny IDE\nProgram LOC 1 solver instance 3 solver instances\nParallelBuilds.dfy 881 572 269\nLnSystemF.dfy 1736 354 109\nVSComp2010.dfy 536 34 26\nVSTTE2012.dfy 1063 110 71\nFigure 5: Preliminary performance numbers showing the effect of parallelization. Times are in seconds.\nProgram 3 solver instances (5 runs) 3 solver instances (5 snapshots)\nLnSystemF.dfy 510 (5∗102) 123\nParallelBuilds.dfy 1345 (5∗269) 311\nFigure 6: Comparison of verifying five versions of two programs with (third column) and without (second\ncolumn) using the cache. Times are in seconds.\n5 Conclusions and Future Work\nThe Dafny IDE represents a new generation of interaction between user and verification system. We\nhave built dependency analysis, caching, and concurrent verification into the design-time feedback loop\nto make re-verification responsive with minimal user effort. We have provided a deeper integration of\nthe Boogie Verification Debugger, whereby it both displays information in the program text and can be\ncontrolled directly from within the program text. And using hover text, we have given easy access to\ncomputed information without cluttering up the user display.\nThe new IDE provides many significant improvements. It has also let us discover a number of areas\nwhere the user interaction can be improved further. We will mention a number of them here.\nThe most pressing problem is what to do with verification tasks that require a long time. At the\nmoment, our IDE performs all verification on a per-method (or per-function) basis. When a method is\nlong and difficult, we often wish for breaking up the verification task into smaller pieces. Boogie has\nsome facilities for verification-condition (VC) splitting [18] and selective checking, but our Dafny IDE\nis currently not taking advantage of these. We would like to dynamically adjust the parameters of VC\nsplitting and selective checking based on previous verification attempts, and we would like to fit this into\na finer granularity of caching.\nAn important special case is where the verifier runs out of time. Subjectively, we find that time-outs\noccur in some part of any larger proof attempt, especially those that involve large recursive functions\nor non-linear arithmetic, while the user is working on getting the verification through. That is, time-\nouts are often a symptom of missing proof ingredients, and good performance tends to be restored once\nthe necessary ingredients have been supplied by the user. Time-outs during this time are bad, since\nthey are on the user’s time. We set the solver time-out to 10 seconds. We do allow this default to be\noverridden through Dafny custom attributes, but making it longer rarely seems to help in situations where\nthe verification attempt is really missing information. For a user to figure out what information is missing\n(let alone which proof obligations are taking a long time), the solver must end its proof search and return\na counterexample. Currently, the verifier does not produce as much information for verification attempts\nthat time out as it does for attempts that fail. A more ambitious goal would be to try to determine the\ncause of the time outs, perhaps by automatically trying to analyze the solver logs that the Z3 Axiom\nProfiler gives access to. The Why3 system guards against time-outs by being able to run several solvers\nat the same time [8].\nK.R.M. Leino, V. Wüstholz 13\nThere are also a number of places where we would like to improve the Dafny plug-in for BVD. For\nexample, the current version does not let users inspect values of functions in the counterexample. We\ncould also imagine a special BVD mode targeted to illustrate the proof state when Dafny is used to prove\ntheorems (not verify programs).\nCurrently, all additional information that we display is computed during Dafny’s resolution and type\nchecking phases. There is also some information that Dafny computes during the verification phase, but\nour current machinery has no hooks for displaying this information to the user.\nWhile we hope to work on these items to further improve the Dafny IDE, we hope that the current\nIDE will continue to be useful and that it will inspire the IDEs of other verification systems.\nAcknowledgments\nWe are grateful to Nada Amin and Maria Christakis for providing benchmark programs, and to Michał\nMoskal for helping with the BVD integration. We also thank Maria for helpful comments on a draft of\nthis paper and Nada, Maria, Arjun Narayan, and Bryan Parno for feedback on the tool.\nReferences\n[0] Mike Barnett, Bor-Yuh Evan Chang, Robert DeLine, Bart Jacobs & K. Rustan M. Leino (2006): Boogie:\nA Modular Reusable Verifier for Object-Oriented Programs. In Frank S. de Boer, Marcello M. Bonsangue,\nSusanne Graf & Willem-Paul de Roever, editors: Formal Methods for Components and Objects (FMCO),\nLNCS 4111, Springer, pp. 364–387. Available at http://dx.doi.org/10.1007/11804192_17.\n[1] Mike Barnett, Manuel Fähndrich, K. Rustan M. Leino, Peter Müller, Wolfram Schulte & Herman Venter\n(2011): Specification and Verification: The Spec# Experience. Communications of the ACM 54(6), pp.\n81–91. Available at http://dx.doi.org/10.1145/1953122.1953145.\n[2] Jasmin Christian Blanchette & Tobias Nipkow (2010): Nitpick: A Counterexample Generator for Higher-\nOrder Logic Based on a Relational Model Finder. In Matt Kaufmann & Lawrence C. Paulson, editors:\nInteractive Theorem Proving (ITP), LNCS 6172, Springer, pp. 131–146. Available at http://dx.doi.org/\n10.1007/978-3-642-14052-5_11.\n[3] François Bobot, Jean-Christophe Filliâtre, Claude Marché, Guillaume Melquiond & Andrei Paskevich\n(2013): Preserving User Proofs across Specification Changes. In Ernie Cohen & Andrey Rybalchenko,\neditors: VSTTE, LNCS 8164, Springer, pp. 191–201. Available at http://dx.doi.org/10.1007/\n978-3-642-54108-7_10.\n[4] Ernie Cohen, Markus Dahlweid, Mark A. Hillebrand, Dirk Leinenbach, Michał Moskal, Thomas Santen,\nWolfram Schulte & Stephan Tobies (2009): VCC: A Practical System for Verifying Concurrent C. In Ste-\nfan Berghofer, Tobias Nipkow, Christian Urban & Makarius Wenzel, editors: Theorem Proving in Higher\nOrder Logics (TPHOLs), LNCS 5674, Springer, pp. 23–42. Available at http://dx.doi.org/10.1007/\n978-3-642-03359-9_2.\n[5] David R. Cok (2010): Improved usability and performance of SMT solvers for debugging specifications.\nSoftware Tools for Technology Transfer (STTT) 12(6), pp. 467–481. Available at http://dx.doi.org/10.\n1007/s10009-010-0138-x.\n[6] David R. Cok (2014): OpenJML: Software verification for Java 7 using JML, OpenJDK, and Eclipse. In\nCatherine Dubois, Dimitra Giannakopoulou & Dominique Méry, editors: 1st Workshop on Formal-IDE.\n[7] Claire Dross, Pavlos Efstathopoulos, David Lesens, David Mentré & Yannick Moy (2014): Rail, Space,\nSecurity: Three Case Studies for SPARK 2014. In: 7th Europen Congress on Embedded Real Time Software\nand Systems (ERTS2 2014). Available at http://www.spark-2014.org/uploads/erts_2014.pdf.\n14 The Dafny IDE\n[8] Jean-Christophe Filliâtre & Andrei Paskevich (2013): Why3 — Where Programs Meet Provers. In Matthias\nFelleisen & Philippa Gardner, editors: European Symposium on Programming (ESOP), LNCS 7792,\nSpringer, pp. 125–128. Available at http://dx.doi.org/10.1007/978-3-642-37036-6_8.\n[9] Claire Le Goues, K. Rustan M. Leino & Michał Moskal (2011): The Boogie Verification Debugger (Tool\nPaper). In Gilles Barthe, Alberto Pardo & Gerardo Schneider, editors: Software Engineering and For-\nmal Methods (SEFM), LNCS 7041, Springer, pp. 407–414. Available at http://dx.doi.org/10.1007/\n978-3-642-24690-6_28.\n[10] Radu Grigore & Michał Moskal (2007): Edit and Verify. In: Workshop on First-Order Theorem Proving\n(FTP). Available at http://arxiv.org/abs/0708.0713.\n[11] Matt Kaufmann, Panagiotis Manolios & J Strother Moore (2000): Computer-Aided Reasoning: An Approach.\nKluwer Academic Publishers.\n[12] Vladimir Klebanov (2009): Extending the Reach and Power of Deductive Program Verification. Ph.D. thesis,\nDepartment of Computer Science, Universität Koblenz-Landau. Available at http://formal.iti.kit.edu/\n~klebanov/pubs/thesis-klebanov.pdf.\n[13] Jason Koenig & K. Rustan M. Leino (2012): Getting Started with Dafny: A Guide. In Tobias Nipkow,\nOrna Grumberg & Benedikt Hauptmann, editors: Software Safety and Security: Tools for Analysis and\nVerification, NATO Science for Peace and Security Series D: Information and Communication Security 33,\nIOS Press, pp. 152–181. Available at http://dx.doi.org/10.3233/978-1-61499-028-4-152. Summer\nSchool Marktoberdorf 2011 lecture notes. A version of this tutorial is available online at http://rise4fun.\ncom/dafny.\n[14] K. Rustan M. Leino (2009): Specification and verification of object-oriented software. In Manfred Broy,\nWassiou Sitou & Tony Hoare, editors: Engineering Methods and Tools for Software Safety and Security,\nNATO Science for Peace and Security Series D: Information and Communication Security 22, IOS Press, pp.\n231–266. Available at http://dx.doi.org/10.3233/978-1-58603-976-9-231. Summer School Markto-\nberdorf 2008 lecture notes.\n[15] K. Rustan M. Leino (2010): Dafny: An Automatic Program Verifier for Functional Correctness. In\nEdmund M. Clarke & Andrei Voronkov, editors: Logic for Programming Artificial Intelligence and\nReasoning (LPAR), LNCS 6355, Springer, pp. 348–370. Available at http://dx.doi.org/10.1007/\n978-3-642-17511-4_20.\n[16] K. Rustan M. Leino (2012): Automating Induction with an SMT Solver. In Viktor Kuncak & Andrey\nRybalchenko, editors: Verification, Model Checking, and Abstract Interpretation (VMCAI), LNCS 7148,\nSpringer, pp. 315–331. Available at http://dx.doi.org/10.1007/978-3-642-27940-9_21.\n[17] K. Rustan M. Leino & Michał Moskal (2013): Co-induction Simply: Automatic Co-inductive Proofs in a\nProgram Verifier. Technical Report MSR-TR-2013-49, Microsoft Research. Available at http://research.\nmicrosoft.com/pubs/192276/coinduction.pdf.\n[18] K. Rustan M. Leino, Michał Moskal & Wolfram Schulte (2008): Verification Condition Splitting.\nTechnical Report, Microsoft Research. Available at http://research.microsoft.com/pubs/77373/\nVerificationConditionSplitting(Draft2008).pdf. Manuscript KRML 192.\n[19] K. Rustan M. Leino & Philipp Rümmer (2010): A Polymorphic Intermediate Verification Language: Design\nand Logical Encoding. In Javier Esparza & Rupak Majumdar, editors: Tools and Algorithms for the Con-\nstruction and Analysis of Systems, 16th International Conference, TACAS 2010, LNCS 6015, Springer, pp.\n312–327. Available at http://dx.doi.org/10.1007/978-3-642-12002-2_26.\n[20] Leonardo de Moura & Nikolaj Bjørner (2008): Z3: An Efficient SMT Solver. In C. R. Ramakrishnan & Jakob\nRehof, editors: Tools and Algorithms for Construction and Analysis of Systems (TACAS), LNCS 4963,\nSpringer, pp. 337–340. Available at http://dx.doi.org/10.1007/978-3-540-78800-3_24.\n[21] Wolfgang Reif & Kurt Stenzel (1993): Reuse of Proofs in Software Verification. In R. K. Shyamasundar,\neditor: Foundations of Software Technology and Theoretical Computer Science, LNCS 761, Springer, pp.\n284–293. Available at http://dx.doi.org/10.1007/3-540-57529-4_61.\nK.R.M. Leino, V. Wüstholz 15\n[22] Christian Sternagel (2012): Getting Started with Isabelle/jEdit. In: Isabelle Users Workshop (IUW). Avail-\nable at http://arxiv.org/abs/1208.1368.\n[23] Makarius Wenzel (2010): Asynchronous Proof Processing with Isabelle/Scala and Isabelle/jEdit. In:\n9th International Workshop On User Interfaces for Theorem Provers (UITP 2010), Electronic Notes\nin Theoretical Computer Science, Elsevier. Available at http://www4.in.tum.de/~wenzelm/papers/\nasync-isabelle-scala.pdf.\n",
            "id": 17195109,
            "identifiers": [
                {
                    "identifier": "25021738",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2143987772",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "87735116",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:doaj.org/article:4fca39998e7643eeba9a798024237842",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "1404.6602",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "10.4204/eptcs.149.2",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:arxiv.org:1404.6602",
                    "type": "OAI_ID"
                }
            ],
            "title": "The Dafny Integrated Development Environment",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2143987772",
            "oaiIds": [
                "oai:doaj.org/article:4fca39998e7643eeba9a798024237842",
                "oai:arxiv.org:1404.6602"
            ],
            "publishedDate": "2014-04-01T00:00:00",
            "publisher": "'Open Publishing Association'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1404.6602",
                "https://doaj.org/toc/2075-2180"
            ],
            "updatedDate": "2020-10-12T14:12:49",
            "yearPublished": 2014,
            "journals": [
                {
                    "title": "Electronic Proceedings in Theoretical Computer Science",
                    "identifiers": [
                        "2075-2180"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1404.6602"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/17195109"
                }
            ]
        },
        {
            "acceptedDate": "2014-09-05T00:00:00",
            "arxivId": "1406.6102",
            "authors": [
                {
                    "name": "Mu, Kedian"
                },
                {
                    "name": "Wang, Kewen"
                },
                {
                    "name": "Wen, Lian"
                }
            ],
            "contributors": [
                "School of Information and Communication Technology, Griffith University, Australia",
                "Wang, Kewen",
                "School of Mathematical Sciences, Peking University, China"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/155601750"
            ],
            "createdDate": "2014-10-24T19:27:23",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 1611,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/1611",
                    "logo": "https://api.core.ac.uk/data-providers/1611/logo"
                }
            ],
            "depositedDate": "2014-09-05T00:00:00",
            "abstract": "This paper proposes a model, the linear model, for randomly generating logic\nprograms with low density of rules and investigates statistical properties of\nsuch random logic programs. It is mathematically shown that the average number\nof answer sets for a random program converges to a constant when the number of\natoms approaches infinity. Several experimental results are also reported,\nwhich justify the suitability of the linear model. It is also experimentally\nshown that, under this model, the size distribution of answer sets for random\nprograms tends to a normal distribution when the number of atoms is\nsufficiently large.Comment: 33 pages. To appear in: Theory and Practice of Logic Programmin",
            "documentType": "research",
            "doi": "10.1017/s1471068414000611",
            "downloadUrl": "http://arxiv.org/abs/1406.6102",
            "fieldOfStudy": "computer science",
            "fullText": "Under consideration for publication in Theory and Practice of Logic Programming 1\nRandom Logic Programs: Linear Model\nKewen Wang1, Lian Wen1 and Kedian Mu2\n1School of Information and Communication Technology\nGriffith University, Australia\n(e-mail: {k.wang,l.wen}@griffith.edu.au)\n2School of Mathematical Sciences\nPeking University, China\n(e-mail: mukedian@math.pku.edu.cn)\nsubmitted 17 June 2013; revised 21 March 2014, 10 June 2014; accepted 12 June 2014\nAbstract\nThis paper proposes a model, the linear model, for randomly generating logic programs with low density\nof rules and investigates statistical properties of such random logic programs. It is mathematically shown\nthat the average number of answer sets for a random program converges to a constant when the number of\natoms approaches infinity. Several experimental results are also reported, which justify the suitability of the\nlinear model. It is also experimentally shown that, under this model, the size distribution of answer sets for\nrandom programs tends to a normal distribution when the number of atoms is sufficiently large.\nKEYWORDS: answer set programming, random logic programs.\n1 Introduction\nAs in the case of combinatorial structures, the study of randomly generated instances of NP-\ncomplete problems in artificial intelligence has received significant attention in the last two\ndecades. These problems include the satisfiability of boolean formulas (SAT) and the constraint\nsatisfaction problems (CSP) (Achlioptas et al. 1997; Achlioptas et al. 2005; Cheeseman et al.\n1991; Gent and Walsh 1994; Huberman and Hogg 1987; Mitchell et al. 1992; Monasson et al.\n1999). In turn, these results on properties of random SAT and random CSP significantly help\nresearchers in better understanding SAT and CSP, and developing fast solvers for them.\nOn the other hand, it is well known that reasoning in propositional logic and in most constraint\nlanguages is monotonic in the sense that conclusions obtained before new information is added\ncannot be withdrawn. However, commonsense knowledge is nonmonotonic. In artificial intelli-\ngence, significant effort has been paid to develop fundamental problem solving paradigms that\nallow users to conveniently represent and reason about commonsense knowledge and solve prob-\nlems in a declarative way. Answer set programming (ASP) is currently one of the most widely\nused nonmonotonic reasoning systems due to its simple syntax, precise semantics and impor-\ntantly, the availability of ASP solvers, such as clasp (Gebser et al. 2009), dlv (Leone et al. 2006),\nand smodels (Syrja¨nen and Niemela¨ 2001). However, the theoretical study of random ASP has\nnot made much progress so far (Namasivayam and Truszczynski 2009; Namasivayam 2009;\nSchlipf et al. 2005; Zhao and Lin 2003).\n(Zhao and Lin 2003) first conducted an experimental study on the issue of phase transition\nfor randomly generated ASP programs whose rules can have three or more literals. (Schlipf\nar\nX\niv\n:1\n40\n6.\n61\n02\nv1\n  [\ncs\n.A\nI] \n 23\n Ju\nn 2\n01\n4\n2 K. Wang, L. Wen and K. Mu\net al. 2005) reported on their experimental work for determining the distribution of randomly\ngenerated normal logic programs at the Dagstuhl Seminar.\nTo study statistical properties for random programs, (Namasivayam and Truszczynski 2009;\nNamasivayam 2009) considered the class of randomly generated ASP programs in which each\nrule has exactly two literals, called simple random programs. Their method is to map some sta-\ntistical properties of random graphs into simple random programs by transforming a random\nprogram into that of a random graph through a close connection between simple random pro-\ngrams and random graphs. As the authors have commented, those classes of random programs\nthat correspond to some classes of random graphs are too restricted to be useful. Their effort fur-\nther confirms that it is challenging to recast statistical properties of SAT/CSP to nonmonotonic\nformalisms such as ASP.\nIn fact, the monotonicity plays an important role in proofs of major results for random\nSAT/CSP. Specifically, major statistical properties for SAT/CSP are based on a simple but im-\nportant property: An interpretation M is a model of a set of clauses/constraints if and only if M\nis a model of each clause/constraint. Due to the lack of monotonicity in ASP, this property fails\nto hold for ASP and other major nonmonotonic formalisms.\nFor this reason, it might make sense to first focus on some relatively simple but expressive\nclasses of ASP programs (i.e., still NP-complete). We argue that the class of negative two-literal\nprograms (i.e. normal logic programs in which a rule body has exactly one negative literal) is\na good start for studying random logic programs under answer set semantics for several rea-\nsons1: (1) The problem of deciding if a negative two-literal program has an answer set is still\nNP-complete. In fact, the class of negative two-literal programs is used to show the NP-hardness\nof answer set semantics for normal logic programs in (Marek and Truszczynski 1991) (Theo-\nrem 6.4 and its proof, where a negative two-literal program corresponds to a simple K1-theory).\n(2) Many important NP-complete problems can be easily encoded as (negative) two-literal pro-\ngrams (Huang et al. 2002). (3) Negative two-literal programs allow us to conduct large scale\nexperiments with existing ASP solvers, such as smodels, dlv and clasp.\nIn this paper we introduce a new model for generating and studying random negative two-\nliteral programs, called linear model. A random program generated under the linear model is of\nthe size about c×n where c is a constant and n is the total number of atoms. We choose such a\nmodel of randomly generating negative two-literal programs for two reasons. First, if we use a\nnatural way to randomly generate programs like what has been done in SAT and CSP, we would\ncome up with two possible models in terms of program sizes (i.e. linear in n and quadratic in\nn), since only n2 negative two-literal rules in total can be generated from a set of n atoms. We\nstudy statistical properties of such random programs and have obtained both theoretical and ex-\nperimental results for random programs generated under the linear model, especially, Theorem 1.\nThese properties include the average number of answer sets, the size distribution of answer sets,\nand the distribution of consistent programs under the linear model. Second, such results can be\nused in practical applications. For instance, it is important to compute all answer sets of a pro-\ngram in applications, such as diagnoses and query answering, in P-log (Baral et al. 2009). In\nsuch cases, the number of answer sets for a program is certainly relevant. If we know the number\nof answer sets and the average size of the answer sets for a logic program, such information\n1 Our definition of negative two-literal programs here is slightly different from that used by some other authors. But\nthese definitions are essentially equivalent if we notice that a fact rule a← can be expressed as a rule a← not a′ where\na′ is a new atom. Details can be found in Section 2.\nRandom Logic Programs: Linear Model 3\ncan be useful heuristics for finding all answer sets of a given program. Also, the linear model\nof random programs may be useful in application domains such as ontology engineering where\nmost of large practical ontologies are sparse in the sense that the ratio of terminological axioms\nto concepts/roles is relatively small (Staab and Studer 2004).\nThe contributions of this work can be summarised as follows:\n1. A model for generating random logic programs, called the linear model, is established.\nOur model generates random logic programs in a similar way as SAT and CSP, but we\ndistinguish the probabilities for picking up pure rules and contradiction rules. (Namasi-\nvayam and Truszczynski 2009) discusses some program classes of two-literal programs\nthat may not be negative. However, as their major results are inherited from the corre-\nsponding ones in random graph theory, such results hold only for very special classes of\ntwo-literal programs. For instance, in regard to the result on negative two-literal programs\nwithout contradiction rules (Theorem 2, page 228), the authors pointed out that the theo-\nrem “concerns only a narrow class of dense programs, its applicability being limited by the\nspecific number of rules programs are to have” (0 < c < 1, x is a fixed number, the number\nof rules m = bcN+ x√c(c−1)Nc and N = n(n−1))2.\n2. We mathematically show that the average number of answer sets for a random program\nconverges to a constant when the number of atoms approaches infinity. We note that the\nproofs of statistical properties, such as phase transitions, for random SAT and random\nCSP are usually obtained through the independence of certain probabilistic events, which\nin turn is based on a form of the monotonicity of classical logics (specifically, given a set\nof formulas S = {φ1, . . . ,φt} with t ≥ 0, it holds that Mod(S) =Mod(φ1)∩ ·· ·∩Mod(φt)\nwhen Mod(·) denotes the set of all models of a formula or a set of formulas). However, it is\nwell known that ASP is nonmonotonic. In our view, this is why many proof techniques for\nrandom SAT cannot be immediately adapted to random ASP. In order to provide a formal\nproof for Theorem 1, we resort to some techniques from mathematical analysis such as\nStirling’s Approximation and Taylor series. As a result, our proof is both mathematically\ninvolved and technically novel. We look into the application of our main result in predicting\nthe consistency of random programs (Proposition 5 and Section 4.3).\n3. We have conducted significant experiments on statistical properties of random programs\ngenerated under the linear model. These properties include the average number of answer\nsets, the size distribution of answer sets, and the distribution of consistent programs under\nthe linear model. For the average number of answer sets, our experimental results closely\nmatch the theoretical results obtained in Section 3. Also, the experimental results corrob-\norate the conjecture that under the linear model, the size distribution of answer sets for\nrandom programs obeys a normal distribution when n is large. The experimental results\nshow that our theories can be used to predict practical situations. As explained above, we\nneed to find all answer sets in some applications. For large logic programs, it may be in-\nfeasible to find all answer sets but we could develop algorithms for finding most of the\nanswer sets. If we know an average size of answer sets, we might need only to examine\nthose sets of atoms whose sizes are around the average size.\nThe rest of the paper is arranged as follows. In Section 2, we briefly review answer set se-\nmantics of logic programs and some properties of two-literal programs that will be used in the\n2 There may be an error here as c−1 < 0.\n4 K. Wang, L. Wen and K. Mu\nsubsequent sections. In Section 3, we first introduce the linear model for random logic programs\n(negative two-literal programs), study mathematical properties of random programs, and then\npresent the main result in a theorem. In Section 4 we describe some of our experimental results\nand compare them with related theoretical results obtained in the paper. We conclude the work\nin Section 5. For the convenience of readers, some mathematical basics required for the proofs\nare included in the Appendix at the end of the paper.\n2 Answer Set Semantics and Two-Literal Programs\nWe briefly review some basic definitions and notation of answer set programming (ASP). We\nrestrict our discussion to finite propositional logic programs on a finite set An of n atoms (n > 0).\nA normal logic program (simply, logic program) is a finite set of rules of the form\na← b1, . . . ,bs,not c1, . . . ,not ct , (1)\nwhere not is for the default negation, s, t ≥ 0, and a, bi and c j are atoms in An (i = 1, . . . ,s,\nj = 1, . . . , t).\nWe assume that all atoms appearing in the body of a rule are pairwise distinct.\nA literal is an atom a or its default negation not a. The latter is called a negative literal. An\natom a and its default negation not a are said to be complementary.\nGiven a rule R of form (1), its head is defined as head(R) = a and its body is\nbody(R) = body+(R)∪not body−(R) where body+(R) = {b1, . . . ,bs}, body−(R) = {c1, . . . ,ct},\nand not body−(R) = {not q | q ∈ body−(R)}.\nA rule R of form (1) is positive, if t = 0; negative, if s= 0. A logic program P is called positive\n(resp. negative), if every rule in P is positive (resp. negative).\nAn interpretation for a logic program P is a set of atoms S ⊆ An. A rule R is satisfied by S,\ndenoted S |= R, if S |= head(R) whenever body+(R) ⊆ S and body−(R)∩S = /0. Furthermore, S\nis a model of P, denoted S |= P, if S |= R for every rule R ∈ P. A model S of P is a minimal model\nof P if for any model S′ of P, S′ ⊆ S implies S′ = S.\nThe semantics of a logic program P is defined in terms of its answer sets (or equivalently, sta-\nble models) (Gelfond and Lifschitz 1988; Gelfond and Lifschitz 1990) as follows. Given an inter-\npretation S, the reduct of P on S is defined as PS = {head(R)← body+(R) |R∈P,body−(R)∩S=\n/0}. Note that PS is a positive logic program and every (normal) positive program has a unique\nleast model. Then we say S is an answer set of P, if S is the least model of PS. By AS(P) we\ndenote the collection of all answer sets of P. For an integer k ≥ 0, AS(P,k) denotes the set of\nanswer sets of size k for P.\nA logic program P may have zero, one or multiple answer sets. P is said to be consistent, if\nit has at least one answer set. It is well-known that the answer sets of a logic program P are\nincomparable: for any S and S′ in AS(P), S⊆ S′ implies S = S′.\nTwo logic programs P and P′ are equivalent under answer set semantics, denoted P ≡ P′,\nif AS(P) = AS(P′), i.e., P and P′ have the same answer sets. We can slightly generalise the\nequivalence of two programs as follows. Let P be a logic program on An and P′ a logic program\non An ∪E, where E is a set of new (auxiliary) atoms. We say P and P′ are equivalent if the\nfollowing two conditions are satisfied:\n1. if S ∈ AS(P), then there exists S′ ∈ AS(P′) such that S′ = S∪Se and Se ⊆ E.\n2. if S′ ∈ AS(P′), then S = S′ \\E is in AS(P).\nRandom Logic Programs: Linear Model 5\nFrom the next section and on, we will focus on a special class of logic programs, called nega-\ntive two-literal programs.\nA negative two-literal rule is a rule of the form a← not b where a and b are atoms. These two\natoms do not have to be distinct. If a 6= b, it is a pure rule; if a = b, it is a contradiction rule. A\nnegative two-literal program is a finite set of negative two-literal rules.\nWe note that our definition is slightly different from some other authors, such as (Janhunen\n2006; Lonc and Truszczynski 2002), in that fact rules are not allowed in our definition. This may\nnot be an issue since a fact rule of the form a← can be expressed as a negative two-literal rule\na← not c, where c is a new atom that does not appear in the program.\nIt is shown in (Marek and Truszczynski 1991) that the problem of deciding the existence of\nanswer sets for a negative two-literal program is NP-complete. This result confirms that the class\nof negative two-literal programs is computationally powerful and it makes sense to study the\nrandomness for such a class of logic programs.\nWe remark that, by allowing the contradiction rules, constraints of the form ←\nb1, . . . ,bs,not c1, . . . ,not ct (s, t ≥ 0) can be expressed in the class of negative two-literal pro-\ngrams. A contradiction rule a← not a is strongly equivalent to the constraint ← not a under\nanswer set semantics: for any logic program P, P∪{← not a} is equivalent to P∪{a← not a}\nunder answer set semantics. Notice also that a constraint of the form← not a,not b is strongly\nequivalent to the two constraints ← not a and ← not b, and a constraint of the form ← a is\nstrongly equivalent to two rules← not a′ and a′← not a where a′ is a fresh atom.\nIn the rest of this section we present three properties of negative two-literal programs. While\nProposition 1 is to demonstrate the expressive power of negative two-literal programs, Proposi-\ntions 2 and 3 will be used to prove our main theorem in the next section. These properties are\nalready known in the literature and we do not claim their originality here.\nFirst, each logic program can be equivalently transformed into a negative two-literal program\nunder answer set semantics. This result is mentioned in (Blair et al. 1999) but no proof is provided\nthere. For completeness, we provide a proof of this proposition in the appendix at the end of the\npaper.\nProposition 1\nEach normal logic program P is equivalent to a negative two-literal program under answer set\nsemantics.\nThe next result provides an alternative characterization for the answer sets of a negative two-\nliteral program, which is a special case of Theorem 6.81, Section 6.8 in (Marek and Truszczynski\n1993).\nProposition 2\nLet P be a negative two-literal program on An containing at least one rule. Then S is an answer\nset of P iff the following two conditions are satisfied:\n1. If b1,b2 ∈ An \\S, then b1← not b2 is not a rule in P.\n2. If a ∈ S, then there exists b ∈ An \\S such that a← not b is a rule in P.\nWe note that in condition 1 above, it can be the case that b1 = b2.\nWe note that if the empty set is an answer set of a negative two-literal program, the program\nmust be empty. Also, An is not an answer set for any negative two-literal program on An.\n6 K. Wang, L. Wen and K. Mu\nProposition 3\nLet P be a negative two-literal program on An containing at least one rule. If S is an answer set\nof P, then 0 < |S|< n. Here |S| is the number of elements in S.\n3 Random Programs and Their Properties\nIn this section we first introduce a model for randomly generating negative two-literal programs\nand then present some statistical properties of such random programs. The main result in this\nsection (Theorem 1) shows that the expected number of answer sets for a random program on An\ngenerated under our model converges to a constant when the number n of atoms approaches in-\nfinity. As the proof of Theorem 1 is lengthy and mathematically involved, some technical details,\nas well as necessary basics of mathematical analysis, are included in the appendix at the end of\nthe paper.\nIn this section, we assume that each negative two-literal program contains at least one rule.\nDefinition 1 (Linear Model L(N2,c1,c2))\nLet c1 and c2 be two non-negative real numbers with c1+c2 > 0. Given a set An of n atoms with\nn > max(c1,c2), a random program P on An is a negative two-literal program that is generated\nas follows:\n1. For any two different atoms a,b ∈ An, the probability of the pure rule a← not b being in\nP is p = c1/n.\n2. For any atom a ∈ An, the probability of the constraint a← not a being in P is d = c2/n.\n3. Each rule is selected randomly and independently based on the given probability.\nIn the above notation, ‘N2’ is for ‘negative two-literal programs’. For simplicity, we as-\nsume that a random program is non-empty. If c2 = 0, then a random program generated under\nL(N2,c1,c2) does not contain any contradiction rules.\nIn probability theory, the expected value (or mathematical expectation) of a random variable\nis the weighted average of all possible values that this random variable can take on. Suppose\nrandom variable X can take k possible values x1, . . . ,xm and each xk has the probability pk for\nk = 1, . . . ,m. Then the expected value of random variable X is defined as\nE[X ] =\nm\n∑\nk=1\npkxk.\nAlso, if a random variable X is the sum of a finite number of other variables X1, . . . ,Xs (s > 0),\ni.e.,\nX =\ns\n∑\nk=1\nXk,\nthen\nE[X ] =\ns\n∑\nk=1\nE[Xk].\nThe number |P| of rules in random program P (i.e., the size of P) is a random variable. As\nthere are n(n− 1) possible pure rules, each of which has probability p = c1/n, and n possible\nconstraints, each of which has the probability d = c2/n. Thus, the expected value of |P|, also\ncalled the expected number of rules for random program P, is the sum of expected number of\nRandom Logic Programs: Linear Model 7\npure rules and the expected number of constraints:\nE[|P|] = n(n−1)p+nd = c1(n−1)+ c2.\nThis means that the average size of random programs generated under the model L(N2,c1,c2) is\na linear function of n. This is the reason why we refer to our model for random programs as the\nlinear model of random programs under answer sets.\nFor S ⊆ An with |S| = k (0 < k < n), the probability of S being an answer set of random\nprogram P, denoted Pr(k), can be easily figured out as the next result shows. We remark that,\nby Proposition 3, for negative two-literal program P, neither the empty set /0 nor An can be an\nanswer set of P. So we do not need to consider the case of k = 0 or k = n.\nProposition 4\nLet P be a random program on a set An of n atoms, generated under L(N2,c1,c2), with n >\nmax(c1,c2). Then\nPr(k) =\n(\n1− c1\nn\n)(n−k)(n−k−1)(\n1−\n(\n1− c1\nn\n)n−k)k(\n1− c2\nn\n)n−k\n. (2)\nRecall that p = c1/n and d = c2/n. If we denote q = 1− p, then Eq.(2) can be simplified into\nPr(k) = q(n−k)(n−k−1)(1−qn−k)k(1−d)n−k. (3)\nProof\nLet S be a subset of An with |S|= k and T =An\\S. We can split the first condition in Proposition 2\ninto two sub-conditions. S is an answer set of negative two-literal program P iff the following\ntwo conditions are satisfied:\n(1) (1.1) for each pair b1,b2 ∈ T with b1 6= b2, the rule b1← not b2 is not in P.\n(1.2) for each a ∈ T , the rule a← not a is not in P.\n(2) for each a ∈ S, there exists an atom b ∈ T such that a← not b is in P.\nLet us figure out the probabilities that the above conditions (1.1), (1.2) and (2) hold, respec-\ntively.\nWe say that an atom a is supported w. r. t. S in P (or just, supported) if there exists a rule of the\nform a← not b in P such that b ∈ T . In this case, the rule a← not b is referred to as a supporting\nrule for a.\nFirst, since T contains n− k elements, there are (n− k)(n− k− 1) possible pure rules of the\nform b1← not b2 with b1,b2 ∈ T and b1 6= b2. By the definition of L(N2,c1,c2), the probability\nthat a pure rule does not belong to P is 1− p = q. Thus, the probability that none of the pure\nrules with b1,b2 ∈ T and b1 6= b2 belongs to P is q(n−k)(n−k−1). That is, the condition (1.1) will\nhold with the probability q(n−k)(n−k−1).\nNext, by the definition of L(N2,c1,c2), the probability that a constraint rule of the form\na← not a does not belong to P is 1−d. Since T contains n− k atoms, the probability that none\nof the constraint rules of the form a← not a with a ∈ T is (1−d)n−k. That is, the condition (1.2)\nwill hold with the probability (1−d)n−k.\nLast, we consider the condition (2). For each a ∈ S, if a pure rule supports a, then it must be\nof the form a← not b for some b ∈ T . There are n− k possible such pure rules. Also, a is not\nsupported by such pure rules only if P does not contain such rules at all. Thus, the probability that\na is not supported (by one of such pure rules) is qn−k. That is, the probability that a is supported\n8 K. Wang, L. Wen and K. Mu\nis 1−qn−k. As there are k atoms in S, the probability that every atom in S is supported by a pure\nrule in P is (1−qn−k)k.\nCombining the above three conditions, we know that the probability that S is an answer set of\nrandom program P is as follows.\nPr(k) = q(n−k)(n−k−1)(1−qn−k)k(1−d)n−k.\nNow we are ready to present the main result in this section, which shows that the average\nnumber of answer sets for random logic programs generated under the linear model converges to\na constant when the number of atoms approaches infinity. This constant is determined by c1 and\nc2, e. g., when c1 = 5 and c2 = 0, the constant is around 1.6.\nTheorem 1\nLet P denote a random program generated under the linear model L(N2,c1,c2) and E[|AS(P)|]\nbe the expected number of answer sets for random program P. Then\nlim\nn→∞E[|AS(P)|] =\nαe\nc1−c2\nα\nα+ c1\n, (4)\nwhere α > 1 is the unique solution of the equation lnα = c1/α .\nThis result gives an estimation for the average number of answer sets for a random program.\nBefore we prove Theorem 1, let us look at its application in predicting the consistency of a\nrandom program.\nFor a random program P and a set of atoms S, by eS we denote the (probabilistic) event that\na given set of atoms is an answer set for P. We introduce the following property for random\nprograms:\n(ASI) Given a random program P, Pr(eS|eS′) = Pr(eS) for any two sets S and S′ of atoms.\nThe ‘I’ in (ASI) is for ‘Independence’. Informally, the above property says that for any two\nsets of atoms S and S′, the events eS and eS′ are independent of each other. We remark that this\nproperty does not hold in general. For example, suppose S1 ⊂ S2 ⊂ An. If S1 is an answer set of P,\nthen S2 must not be an answer set of P. This implies that eS1 and eS2 are actually not independent.\nHowever, when the set of atoms An is sufficiently large, by Theorem 1, the average number of\nanswer sets will be relatively small compared to the number of all subsets of An. As a result,\nthere will be a relatively small number of pairs S ⊆ An and S′ ⊆ An with S 6= S′ such that eS and\neS′ are not independent. Thus, when n is sufficiently large, the impact of dependency for answer\nsets will be not radical. Under the (ASI) assumption, we are able to derive an estimation for the\nprobability that a random program has an answer set.\nProposition 5\nLet P be a random program on a set An of n atoms, generated under L(N2,c1,c2), with n >\nmax(c1,c2). If (ASI) holds and n is sufficiently large, then\nPr(E(|AS(P)|> 0))≈ 1− e−E(|AS(P)|). (5)\nAs explained, (ASI) does not hold in realistic situation. Our experiments indeed show that there\nis a shift between the estimated probability determined by Eq.(5) and the actual probability.\nHowever, The experimental results suggest that this shift can be remedied by applying a factor γ\nof around 0.5 to E(|AS(P)|) in Eq.(5), see Section 4 for details. So, combining Theorem 1 and\nProposition 5, we will be able to estimate the probability for the consistency of random programs.\nRandom Logic Programs: Linear Model 9\nProof\nLet eS,k be the event that S⊂ An is an answer set of size k for random program P. We first observe\nthat by Eq.(3), lim\nn→∞Pr(eS,k) = 0. Recall that AS(P,k) is the set of answer sets of size k for logic\nprogram P.\nIf n is sufficiently large, then\nPr(E(|AS(P)|)> 0) = 1−Pr(E(|AS(P)|) = 0)\n= 1− ∏\n0<k<n\nPr(E(|AS(P,k)|) = 0)\n= 1− ∏\n0<k<n\n[1−Pr(eS,k)](\nn\nk)\n= 1− ∏\n0<k<n\n[1−Pr(eS,k)]\n1\nPr(eS,k)\n·Pr(eS,k)·(nk)\n≈ 1− ∏\n0<k<n\ne−Pr(eS,k)×(\nn\nk), because lim\nx→0\n(1− x) 1x = e−1\n= 1− ∏\n0<k<n\ne−E(|AS(P,k)|)\n= 1− e−∑0<k<n E(|AS(P,k)|)\n= 1− e−E(|AS(P)|).\nIn the rest of this section, we will present a formal proof of Theorem 1. Let us first outline a\nsketch for the proof. In order to prove Eq.(4), our first goal will be to show that E[|AS(P)|] is the\nsum of E[Nk]’s for 0 < k < n.\nFor an integer k with 0 < k < n, we use AS(P,k) to denote the collection of answer sets of size\nk for program P, i.e., AS(P,k) = {S | S ∈ AS(P), |S|= k}. Then the number Nk = |AS(P,k)| is a\nrandom variable. It is easy to see that the expected number of answer sets of size k for random\nprogram P is\nE[Nk] =\n(\nn\nk\n)\nPr(k). (6)\nSo the expected (total) number of answer sets for P, denoted E[|AS(P)|], can be expressed as\nE[|AS(P)|] =\nn−1\n∑\nk=1\nE[Nk]. (7)\nNote that by Proposition 3, a random program generated under the linear model has neither\nanswer sets of size 0 nor n. So, we can ignore the cases of k = 0 and k = n.\nOur next goal is, based on Eq.(7), to show that\nlim\nn→∞E[|AS(P)|] = limn→∞\n∫ n\n1\nφ(x)dx. (8)\nwhere the function φ(x) is defined by\nφ(x) =\n√\nn\n2pix(n− x)\n(\nn(1−qn−x)\nx\n)x(nrqn−x\nn− x\n)n−x\n. (9)\n10 K. Wang, L. Wen and K. Mu\nAt the same time, we are going to show that\nlim\nn→∞\n∫ n\n1\nφ(x)dx = lim\nn→∞\n∫ ∞\n−∞\nχ(x)dx, (10)\nwhere the function χ(x) defined below is a normal distribution function multiplied by a constant.\nThus, it follows from Eq.(8) and Eq.(10) that\nlim\nn→∞E[|AS(P)|] = limn→∞\n∫ ∞\n−∞\nχ(x)dx.\nAs the above integral of χ(x) is αe\nc1−c2\nα /(α+c1), which can be figured out easily, the conclusion\nof Theorem 1 will be proven.\nHere α > 1 is the unique solution of the equation αα = ec1 and χ(x) is the normal distribution\nfunction\nNx0,σ (x) =\n1√\n2piσ\ne−\n(x−x0)2\n2σ2\nmultiplied by a constant\n√\n2piσφ(x0):\nχ(x) =\n(√\n2piσφ(x0)\n)\nNx0,σ (x) = φ(x0)e\n− (x−x0)\n2\n2σ2 . (11)\nwhile x0 and σ are defined, respectively, as follows.\nx0 =\n(α−1)n\nα\n. (12)\nσ =\n√\n(α−1)n\nα+ c1\n. (13)\nSome remarks are in order. As c1 > 0, if αα = ec1 for some α , it must be the case that α > 1.\nOn the other hand, if α > 1, the function αα is monotonically increasing and thus the equation\nαα = ec1 must have a unique solution.\nMoreover, we define\nc0 = max(\n√\n2(α+ c1)√\nα−1 ,\n1√\nc1\n). (14)\n∆= c0\n√\nn lnn. (15)\nBefore providing the proof of Theorem 1, we first prove some technical results.\nThe following result shows that φ(k), as defined in Eq.(9), is indeed a tight approximation to\nE[Nk].\nProposition 6\nLet P be a random program on a set An of n atoms, generated under L(N2,c1,c2), with n >\nmax(c1,c2). Let E[Nk] be the expected number of answer sets of size k for P (0 < k < n). Then ,\n4pi2\ne2\nφ(k)≤ E[Nk]≤ e2pi φ(k). (16)\nE[Nk] = φ(k)\n(\n1+O\n(\n1\nmin(k,n− k)\n))\n. (17)\nRandom Logic Programs: Linear Model 11\nProof\nNote that\nE[Nk] =\n(\nn\nk\n)\nPr(k) =\nn!\nk!(n− k)! Pr(k).\nBy Proposition 4,\nE[Nk] =\nn!\nk!(n− k)!q\n(n−k)(n−k−1)(1−qn−k)k(1−d)n−k.\nLet r = (1−d)/(1− p) = (1−d)/q. Then\nE[Nk] =\nn!\nk!(n− k)! (q\nn−kr)n−k(1−qn−k)k.\nApplying Stirling’s approximation to n!, k! and (n− k)!, and based on the two properties of\nStirling’s approximation presented in Section 5.2, Eq.(16) and Eq.(17) are obtained.\nBy Proposition 6, we can show the following result.\nProposition 7\nLet P be a random program on a set An of n atoms, generated under L(N2,c1,c2), with n >\nmax(c1,c2). If E[Nk] and φ(k) are defined as in Eq.(6) and Eq.(9, then\nlim\nn→∞\nn−1\n∑\nk=1\nE[Nk] = limn→∞\nn−1\n∑\nk=1\nφ(k). (18)\nProof\nLet ∆ be defined as in Eq.(15). Then\nn−1\n∑\nk=1\nE(Nk) =\nbx0−∆c\n∑\nk=1\nE(Nk)+\nbx0+∆c−1\n∑\nk=bx0−∆c+1\nE(Nk)+\nn−1\n∑\nk=bx0+∆c\nE(Nk).\nBy inequality (16),\nbx0−∆c\n∑\nk=1\nE(Nk)+\nn−1\n∑\nk=bx0+∆c\nE(Nk)≤ e2pi\n(bx0−∆c\n∑\nk=1\nφ(k)+\nn−1\n∑\nk=bx0+∆c\nφ(k)\n)\n.\nBy Lemma 6 (in Section 5.3),\nlim\nn→∞\n(bx0−∆c\n∑\nk=1\nφ(k)+\nn−1\n∑\nk=bx0+∆c\nφ(k)\n)\n= 0.\nBased on Eq.(17), and the fact that both φ(k) and E(Nk) are non-negative,\nlim\nn→∞\n(bx0−∆c\n∑\nk=1\nE(Nk)+\nn−1\n∑\nk=bx0+∆c\nE(Nk)\n)\n=\nlim\nn→∞\n(bx0−∆c\n∑\nk=1\nφ(k)\n(\n1+O\n(\n1\nmin(k,n− k)\n))\n+\nn−1\n∑\nk=bx0+∆c\nφ(k)\n(\n1+O\n(\n1\nmin(k,n− k)\n)))\n≤ lim\nn→∞\n(bx0−∆c\n∑\nk=1\nφ(k)+\nn−1\n∑\nk=bx0+∆c\nφ(k)\n)\n(1+O(1)) = 0.\n12 K. Wang, L. Wen and K. Mu\nAs E(Nk)≥ 0 for k ≥ 1, we have that\nlim\nn→∞\n(bx0−∆c\n∑\nk=1\nE(Nk)+\nn−1\n∑\nk=bx0+∆c\nE(Nk)\n)\n= 0.\nBy Eq.(17),\nbx0+∆c−1\n∑\nk=bx0−∆c+1\nE(Nk) =\nbx0+∆c−1\n∑\nk=bx0−∆c+1\n(\nφ(k)\n(\n1+O\n(\n1\nmin(k,n− k)\n)))\n=\n( bx0+∆c−1\n∑\nk=bx0−∆c+1\nφ(k)\n)(\n1+O\n(\n1\nmin(x0−∆,n− x0−∆)\n))\n.\nBy Eq.(12) and Eq.(15), we have that\nlim\nn→∞min(x0−∆,n− x0−∆) = ∞.\nSo\nlim\nn→∞\nbx0+∆c−1\n∑\nk=bx0−∆c+1\nE(Nk) = limn→∞\nbx0+∆c−1\n∑\nk=bx0−∆c+1\nφ(k).\nTherefore, the conclusion is proved.\nThe next result shows that the integral of φ(x) can be obtained through the integral of χ(x),\nwhich is useful as the integral of χ(x) can be easily figured out.\nProposition 8\nLet P be a random program on a set An of n atoms, generated under L(N2,c1,c2), with n >\nmax(c1,c2). If the continuous functions φ(x) and χ(x) are defined as in Eq.(9) and Eq.(11), then\nlim\nn→∞\n∫ n\n1\nφ(x)dx = lim\nn→∞\n∫ −∞\n∞\nχ(x)dx. (19)\nProof\nLet ∆= c0\n√\nn lnn be defined as in Eq.(15). By Lemma 6, it follows that\nlim\nn→∞\n(∫ x0−∆\n1\nφ(x)dx+\n∫ n\nx0+∆\nφ(x)dx\n)\n= 0.\nBy Lemma 8 and Lemma 9, Eq.(19) holds.\nNow we are ready to present the proof of Theorem 1, a main result in this paper,\nProof of Theorem 1\nGiven a random program P, the expected total number of answer sets for P is\nE[|AS(P)|] =\nn−1\n∑\nk=1\nE[Nk].\nBy Proposition 7 and Proposition 8,\nlim\nn→∞E[|AS(P)|] = limn→∞\nn−1\n∑\nk=1\nE[Nk] = limn→∞\nn−1\n∑\nk=1\nφ(k)\n= lim\nn→∞\n∫ n\n1\nφ(x)dx = lim\nn→∞\n∫ ∞\n−∞\nχ(x)dx.\nRandom Logic Programs: Linear Model 13\nThen\nlim\nn→∞\n∫ ∞\n−∞\nχ(x)dx = lim\nn→∞\n√\n2piσφ(x0) =\nαe\nc1−c2\nα\nα+ c1\n.\nTherefore,\nlim\nn→∞E[|AS(P)|] =\nαe\nc1−c2\nα\nα+ c1\n.\n4 Experimental Results\nIn this section, we describe some experimental results about the average number of answer sets,\nthe size distribution of answer sets, and the probability of consistency for random programs under\nthe linear model. For the average number of answer sets, our experimental results closely match\nthe theoretical results obtained in Section 3.\nTo conduct the experiments, we have developed a software tool to generate random logic\nprograms, which is able to randomly generate logic programs based on the user-input parameters,\nsuch as the type of programs, the number of atoms, the number of literals in a rule, the number of\nrules in a program and the number of programs in a test set etc. After a set of random programs\nare generated, the tool invokes an ASP solver to compute the answer sets of the random programs,\nrecords the test results in a file, and analyses them. The experimental results in this section were\nbased on the ASP solver clasp (Gebser et al. 2009), but same patterns were obtained for test cases\non which dlv (Leone et al. 2006) and smodels (Syrja¨nen and Niemela¨ 2001) were also used.\nWe have conducted a significant number of experiments to corroborate the theoretical results\nobtained in Section 3 including Theorem 1. In order to get a feel for how quickly the experimental\ndistribution converges to the theoretical one, we tested the difference rate of these two values for\nvaried numbers of atoms. The experimental results show that the theorem can be used to predict\npractical situations. Some other statistical properties of random programs generated under the\nlinear model were also experimented, such as the size distribution of answer sets. Positive results\nare received for nearly all of our experiments. In this section, we report the results from two of\nour experiments. In the first experiment, we set c2 = 0, which means there are no contradiction\nrules in the programs. In the second experiment, we set c2 from 0 to 20 to test the impact of\ncontradiction rules on the random programs.\n4.1 Experiment 1: Random Programs without contradiction rules\nIn this experiment, c1 = 5, c2 = 0, and n varies with values 50,100,150, ...,500, respectively.\nFor each of these values of n, 5,000 logic programs were randomly and independently generated\nunder the linear model.\nGiven that c1 = 5 and α > 1 is determined by αα = ec1 , we have that α ≈ 3.7687. Thus, by\nEq.(4), it follows that E(|AS(P)|)≈ 1.6274.\nWe use NAvg to denote the average number of answer sets for the 5,000 programs in each test\ngenerated under the linear model. The (experimental) values for NAvg and their corresponding\ntheoretical values (i.e., the expected number E[Nk] of answer sets for random programs deter-\nmined by Eq.(4)) are listed in Table 1. The experimental and theoretical results are visualized in\nFigure 1. We can see that these two values are very close even if n is relatively small.\n14 K. Wang, L. Wen and K. Mu\nTable 1. The average number and expected number of answer sets for random programs when\nc1 = 5 and c2 = 0.\nn NAvg n NAvg n NAvg n NAvg n NAvg\n50 1.6404 100 1.6674 150 1.6334 200 1.5794 250 1.6874\n300 1.6178 350 1.6738 400 1.5672 450 1.682 500 1.632\nFigure 1. The average number and expected number of answer sets for random programs when\nc1 = 5 and c2 = 0 (x-axis: the number of atoms; y-axis: the average number of the answer sets.\nAnother important result obtained from this experiment is about the size distribution of answer\nsets for random programs. Specifically, the experiment supports a conjecture that the distribution\nof the average size of answer sets for random programs obeys a normal distribution.\nThe experimental result can be easily seen by comparing the following three types of values\nfor 0 ≤ k ≤ n, which are visualized as three curves in Figure 2 and Figure 3 with n = 50 and\nn = 400 respectively.\nAverage number of answer sets for the 5,000 programs randomly generated in each test (referred\nto as ‘Experiment Result’ in Figure 2 and Figure 3): We took n = 50,100, ...,500, respectively,\nand for each of these values of n, we randomly generated 5,000 programs under the linear model.\nFor each k (0 ≤ k ≤ n), we calculated the average number of answer sets of size k for these\nprograms, i.e., the ratio of the total number of answer sets of size k for all these programs divided\nby 5,000.\nExpected number of answer sets for random programs under the linear model (referred to as\n‘The Model’ in Figure 2 and Figure 3): In order to compare the experimental values with their\ntheoretical counterparts, for each 0≤ k≤ n, we calculated the expected number E[Nk] of answer\nsets of size k for random programs under the linear model.\nNormal Distribution function: The above two types of values were also compared with the\nfunction χ(x) defined by Eq.(11), which is actually the normal distribution function N (x0,σ)\nmultiplied by a constant.\nRandom Logic Programs: Linear Model 15\nFigure 2. Three distributions of answer sets for c1 = 5, c2 = 0, and n = 50 (5,000 programs\nare generated; x-axis: the size of the answer sets; y-axis: the average number of answer sets of a\ngiven size).\nFigure 3. Three distributions of answer sets for c1 = 5, c2 = 0, and n = 400 (5,000 programs\nare generated; x-axis: the size of the answer sets; y-axis: the average number of answer sets of a\ngiven size).\nFigure 2 and Figure 3 show that even for relatively small values of n, the theoretical results\nare still very close to the experimental results. In order to see how quickly the experimental\ndistribution converges to the theoretical one, we consider the rate variance function D: For two\ndiscrete functions f and g on the interval [1,n−1] with f (k)> 0 (1≤ k ≤ n−1), we define\nD( f ,g) =\n∑n−1k=1( f (k)−g(k))2\n∑n−1k=1 f (k)2\n.\nClearly, the closer f and g, the smaller D( f ,g), and vice versa. The function D( f ,g) is often used\nin measuring the gap between two discrete functions f and g. If we take f as the normal distribu-\ntion function and g as the experimental distribution function (i.e., the average size of answer sets\n16 K. Wang, L. Wen and K. Mu\nbased on the 5,000 programs randomly generated in each test). The resulting rate variance func-\ntion is depicted in Figure 4. This diagram shows that, as n increases, the rate variance gradually\ndecreases. It also shows that the rate variance is very small even when n= 50. This experimental\nresult further suggests the conjecture that the size distribution of answer sets obeys a normal\ndistribution.\nFigure 4. The difference rate between the normal distribution and the experimental results of\nthe answer set distribution with c1 = 5, c2 = 0 (5,000 programs are generated with each testing\npoint; x-axis: the number of atoms; y-axis: difference rate).\n4.2 Experiment 2: Random Programs with contradiction rules\nIn this experiment, we tested random programs that may contain contradiction rules and ob-\ntained similar experimental results as in the first experiment. We set c1 = 10, n = 200, and\nc2 = 0,1,2, ...,20, respectively. For each value of c2, 5,000 programs were independently gener-\nated under the linear model.\nGiven c1 = 10, it follows by Eq.(12) and Eq.(13) that α ≈ 5.7289, x0 ≈ 165.0894, and σ ≈\n1.9552. The value φ(x0), which depends on c2, decreases roughly from 0.4257 (when c2 = 0) to\n0.01297 (when c2 = 20).\nOn the other hand, based on Eq.(4), we can figure out the expected number of answer sets for\neach c2.\nThese two types of values are visualized as two curves in Figure 5. It shows that these two\ncurves are very close to each other, which means our theoretical result on size distribution of\nanswer sets is corroborated by the experimental result.\nSimilar to the first experiment, the size distribution of answer sets was also investigated ex-\nperimentally. In this case, we took c2 = 4 and three types of values were obtained (shown in\nFigure 6). There is a slight shift between the linear model and the normal distribution. We expect\nthat when the number n is sufficiently large, this shift will become narrower. For example, when\nn increases from 200 to 400, the shift is significantly reduced.\nRandom Logic Programs: Linear Model 17\nFigure 5. The comparison of expected number and average number of answer sets for c1 = 10,\nn = 200, and c2 = 0,1, ...,20. The x-axis is for the number of contradiction rules (c2) and the\ny-axis is for the average number of answer sets.\nFigure 6. The comparison of the distribution of answer set. x-axis is the size of the answer sets.\ny-axis is the average number of answer sets for a program of that given size. The first curve shows\nthe experimental results. The second and the third curves are based on the theoretical estimation\nof χ(x) (normal distribution) and φ(x) (the model) respectively. (c1 = 10, c2 = 4, n = 200, 5000\nindependent programs are used to get the experimental results)\n4.3 Experiment 3: Approximating the probability for consistency of random programs\nIn this subsection we present our experimental results on verifying the formula for predicting\nconsistency of random programs (discussed in Section 3):\nPr(E(|AS(P)|> 0))≈ 1− e−γ·E(|AS(P)|). (20)\nHere γ is a constant around 0.5 (i.e. independent of n). We tested various pairs of c1 and c2. For\neach such pair, we took n = 100,150,200, ...,1000. Then for each value of n, we computed the\n18 K. Wang, L. Wen and K. Mu\nvalue determined by Eq.(20). For each value of n, we generated 5,000 programs randomly and\ncomputed the ratio of consistent programs to all 5,000 programs.\nOur experimental results corroborate the estimation in Eq.(20). So this formula can be used to\npredict the consistency of random programs generated under the linear model. The corresponding\nvalues for two cases we tested are depicted in Figures 7 and 8. In each figure, the upper curve is\nfor the value determined by Eq.(5), the middle curve is for the ratio of consistent programs to all\n5,000 programs randomly generated, and the lower curve is for the value determined by Eq.(20).\nFigure 7. Three values of estimating the probability for consistency of random programs when\nc1 = 3 and c2 = 0. γ = 0.5. The x-axis is for n, the number of atoms, the y-axis is the ratios of\nconsistent programs to all 5,000 programs.\n5 Conclusion\nWe have proposed a new model of randomly generating logic programs under answer set seman-\ntics, called linear model. The average size of a random program generated in this way is linear\nto the total number of atoms. We have proved some mathematical results and the main result\nshows that the expected number of answer sets of random programs under the linear model con-\nverges to a constant that is determined by the probabilities of both pure rules and constraints.\nThe formal proof of this result is mathematically involving as we have seen. The main result is\nfurther corroborated by our experiments. Another important experimental result reveals that the\n(size) distribution of answer sets for random programs generated under the linear model obeys a\nnormal distribution.\nThere are several issues for future work. First, it would be interesting to mathematically prove\nsome results presented in Section 4. Second, it would be both interesting and useful to study\nphase transition phenomena for hardness. In this case, a new model for random programs may\nneed to be designed based on an algorithm for ASP computation (for SAT and CSP, DPLL is\noften used for studying the hardness of random problems). Last, while the class of negative two-\nliteral programs is of importance, it would be interesting to study properties of random logic\nRandom Logic Programs: Linear Model 19\nFigure 8. Three values of estimating the probability for consistency of random programs when\nc1 = 4 and c2 = 4. γ = 0.5. The x-axis is for n, the number of atoms, the y-axis is the ratios of\nconsistent programs to all 5,000 programs.\nprograms that are more general than negative two-literal programs, such as the program classes\ndiscussed in (Janhunen 2006; Lonc and Truszczynski 2002). However, it is not straightforward\nto carry over our proofs to those program classes. For instance, Proposition 4 may not hold for\narbitrary two-literal programs.\nAcknowledgement\nThe authors would like to thank the editor Michael Gelfond and three anonymous referees for\ntheir constructive comments, which helped significantly improve the quality of the paper. Thanks\nto Fangzhen Lin and Yi-Dong Shen for discussions on this topic. This work was supported by\nthe Australian Research Council (ARC) under grants DP1093652 and DP130102302.\nAppendix\n5.1 Proofs for Section 2\nProposition 1 Each normal logic program P is equivalent to a negative two-literal program\nunder answer set semantics.\nProof\nFirst, it has been proven that each normal logic program is equivalent to a negative logic program\nunder answer set semantics (Brass and Dix 1999; Wang and Zhou 2005). So, without loss of\ngenerality, we assume that P is a negative normal program.\nNext, we show that each negative normal program P can be transformed into a logic program\nthat consists of only two-literal rules and fact rules. In fact, we can define the translation as\nfollows.\n20 K. Wang, L. Wen and K. Mu\nFor each rule R in P of the form a← not c1, . . . ,not cn (n≥ 0), R is replaced with the following\nn+1 rules:\na← not eR.\neR← c1.\n. . . . . .\neR← cn.\nHere eR is a new atom introduced for the rule R. The resulting logic program, denoted\nsimple(P), is exactly a logic program that consists of only two-literal rules and fact rules. We\nuse EP to denote the set of new atoms eR introduced above, that is, EP = {eR | R ∈ P}.\nNote that, by applying unfolding transformation, simple(P) can be easily transformed into\na logic program that consists of only negative two-literal rules and fact rules. As explained in\nSection 2, each rule can be expressed as a negative two-literal rule by introducing a new atom.\nThus, simple(P) is equivalent to a negative two-literal program under answer set semantics.\nSo, it is sufficient to show that simple(P) and P are indeed equivalent under answer set seman-\ntics.\n(1) Let S be an answer set of P. Take Se = {eR ∈ EP | R ∈ P,body−(R)∩S 6= /0}. Then we show\nthat S′ = S∪Se is an answer set of P′. It suffices to prove that S′ is a minimal model of (P′)S′ .\nBy the definition of Se, S′ is a model of (P′)S\n′\n.\nWe need only to show that S′ is minimal. Assume that there exists T ′ such that T ′ ⊆ S′ and T ′\nis also a model of (P′)S′ . Let T = T ′ \\EP. Then T is a model of PS. To see this, for each rule R of\nthe form a← not c1, . . . ,not ct such that ci 6∈ S for i= 1, . . . , t, if a 6∈ T , then a 6∈ T ′. Thus eR ∈ T ′\nby T ′ |= R, which implies that ci ∈ T ′ for some i (1≤ i≤ n). So we have ci ∈ T , that is, T |= R.\nBy the minimality of S, T = S.\nAlso, if eR ∈ S′, then ci ∈ S for some i (1 ≤ i ≤ n). This means ci ∈ T because S = T , which\nimplies that eR ∈ T ′. Therefore, T ′ = S′.\n(2) If S′ is an answer set of P′, we want to show that S = S′ \\EP is an answer set of P.\nS |= PS: for each rule R of the form a← not c1, . . . ,not cn, if R+ ∈ PS, then {c1, . . . ,cn}∩S= /0,\nwhich implies that eR 6∈ S′. Thus R+ = (a← not eR)+ ∈ (P′)S′ . By the assumption, a ∈ S′, that\nis, a ∈ S. Thus S |= R+.\nS is a minimal model of PS: Suppose that T ⊆ S and T |= PS. Take T ′ = T ∪ Se where Se =\n{eR ∈ EP | body−(R)∩S 6= /0}. We first show that T ′ |= (P′)S′ .\nLet R′ ∈ P′ with (R′)+ ∈ (P′)S′ . Consider two possible cases:\nCase 1. R′ is of the form a← not eR: Then eR 6∈ S′. By T ′ ⊆ S′, eR 6∈ T ′. Then {c1, . . . ,cn}∩S= /0.\nThis means R+ ∈ PS. Since T |= PS, we have a ∈ T . Thus T ′ |= (R′)+.\nCase 2. R′ is of the form eR← ci where 1≤ i≤ n: If ci ∈ T ′, then ci ∈ S. By this rule, eR ∈ S′ or\neR ∈ Se. Thus eR ∈ T ′. Again, we have T ′ |= (R′)+.\nTherefore, T ′ |= (P′)S′ . By the minimality of S′, T ′ = S′, which implies T = S. Thus S is a\nminimal model of PS.\nSo, we conclude the proof.\nProposition 2 Let P be a negative two-literal program on An containing at least one rule. Then\nS is an answer set of P iff the following two conditions are satisfied:\n1. If b1,b2 ∈ An \\S, then b1← not b2 is not a rule in P.\n2. If a ∈ S, then there exists b ∈ An \\S such that a← not b is a rule in P.\nRandom Logic Programs: Linear Model 21\nProof\n⇒: Let S be an answer set of P.\nTo prove condition 1, suppose that b1← not b2 is a rule in P and b2 ∈An\\S. Then the rule b1←\nis in PS. This implies that b1 ∈ S, which is in contradiction to b1 ∈ An \\S. Therefore, b1← not b2\ncannot be a rule in P if b1,b2 ∈ An \\S.\nFor condition 2, if a ∈ S, P contains at least one rule with the head a. On the contrary, suppose\nthat there does not exist any b ∈ An \\ S such that a← not b is in P. Then for every rule of the\nform a← not b in P, we would have b ∈ S, which implies the reduct PS would contain no rules\nwhose head is a. Therefore, a 6∈ S, a contradiction. Therefore, there must exist an atom b ∈ An \\S\nsuch that a← not b is in P.\n⇐: Assume that S ⊆ An satisfies the above two conditions 1 and 2. We want show that S is an\nanswer set.\nS |= PS: If R: a← not c is a rule of P such that R+ ∈ PS, then c 6∈ S. By condition 1, a ∈ S. This\nmeans that every rule of PS is satisfied by S. Thus, S |= PS.\nS is a minimal model of PS: By condition 2, for each a ∈ S, there exists a rule a← not b such\nthat b 6∈ S. Then the rule a← is in PS, which implies that every model of PS must contain a.\nThis implies that every model of PS is a superset of S. Therefore, S is minimal (actually the least\nmodel of PS).\nProposition 3 Let P be a negative two-literal program on An containing at least one rule. If S is\nan answer set of P, then 0 < |S|< n. Here |S| is the number of elements in S.\nProof\nIf |S| = 0, then S = /0. Since P contains at least one rule, we assume that a← not b is in P. By\nS = /0, a 6∈ S. Then it would be the case that b ∈ S, which is a contradiction to S = /0. Therefore,\n|S|> 0.\nIf |S| 6= 0, i.e. S 6= /0, then there exists an element a ∈ S. By the definition of answer sets, the\nrule a← not b must be in P for some b 6∈ S. This implies that S must be a proper subset of An.\n5.2 Basics of mathematical analysis\nIn this subsection, we briefly recall some basics of mathematical analysis and notation that are\nused in related proofs.\n1. Big O notation: let f (x), g(x), h(x) be three real functions. By f (x) = g(x)+O(h(x)), we\nmean that | f (x)−g(x)|= O(h(x)). That is, there exists a positive real number c and a real\nnumber x0 such that for all x > x0.\n| f (x)−g(x)| ≤ c|h(x)|.\nThe same notation is also applicable to discrete functions.\n2. Stirling’s approximation: for all integer n > 0\n1≤ n!\ne−nnn\n√\n2npi\n≤ e√\n2pi\n, (21)\nn! = e−nnn\n√\n2npi(1+O(\n1\nn\n)). (22)\n22 K. Wang, L. Wen and K. Mu\n3. Taylor series: Let f (x) be an infinitely differentiable real function on R, x0 ∈ R is a real\nnumber, then for all x ∈ R,\nf (x) =\n∞\n∑\ni=0\nf (i)(x0)\ni!\n(x− x0)i. (23)\nHere f (i)(x0) denotes the i-th derivative of f at x0 (i≥ 0). In particular, f (0)(x) = f (x).\n4. Properties of the natural exponential function:\nlim\nx→0\n(1+ x)\n1\nx = e. (24)\n(1+ x)≤ ex , and the equatility holds iff x = 0. (25)\nFor all n ∈ N, (\n1+\n1\nn\n)n\n= e+O\n(\n1\nn\n)\n. (26)\n5. Properties of the logarithmic function:\nlim\nx→0\nln(1+ x)\nx\n= 1 (27)\nif x > 0, ln(1+ x)< x.\n6. Concave functions: A real function f is said to be concave if, for any x,y ∈ R and for any\nt in [0,1],\nf (tx+(1− t)y)≥ t f (x)+(1− t) f (y).\nLet f (x) be a continuously differentiable function.\n(a) If f ′′(x) is negative for all x ∈ R, then f (x) is a concave function.\n(b) For x0 ∈ R, if f (x) is concave and f ′(x0) = 0, then f (x) reaches its apex at x0.\n(c) If f (x) is concave and reaches its apex at x0, then g(x) = e f (x) is strictly monotoni-\ncally increasing when x < x0 and strictly monotonically decreasing when x > x0.\n7. The complementary error function erfc(x) is defined by\nerfc(x) =\n2√\npi\n∫ ∞\nx\ne−t\n2\ndt, (28)\nwhich has the following property:\nlim\nx→∞erfc(x) = 0. (29)\n5.3 Lemmas\nRecall that φ(x), x0 and σ have been defined in Eq.(9), Eq.(12), and Eq.(13), respectively. We\nfirst define three real functions as follows.\nψ(x) = ln(φ(x))− ln(φ(x0)). (30)\nξ (x) = ln(χ(x))− ln(φ(x0)) =− (x− x0)\n2\n2σ2\n. (31)\nκ(x) = 1−qn−x. (32)\nRandom Logic Programs: Linear Model 23\nThen\nφ(x) = φ(x0)eψ(x),\nand\nχ(x) = φ(x0)eξ (x).\nAccording to Taylor series:\nln(1+ x) = 0+ x− x\n2\n2\n+\nx3\n3\n− ...\nWe have\nlnq = ln(1− c1\nn\n) =−c1\nn\n− c\n2\n1\n2n2\n+O\n(\n1\nn3\n)\n=−c1\nn\n+O\n(\n1\nn2\n)\n= O\n(\n1\nn\n)\n. (33)\nLemma 1\nqn−x0 =\n1\nα\n− c\n2\n1\n2α2n\n+O\n(\n1\nn2\n)\n. (34)\nκ(x0) =\nα−1\nα\n+\nc21\n2α2n\n+O\n(\n1\nn2\n)\n. (35)\nProof\nBy Eq.(12) and Eq.(33),\n(n− x0) lnq = nα\n[−c1\nn\n− c\n2\n1\n2n2\n+O\n(\n1\nn3\n)]\n=−c1\nα\n− c\n2\n1\n2nα\n+O\n(\n1\nn2\n)\n.\nThen\nqn−x0 = e−\nc1\nα e−\nc21\n2nα eO\n(\n1\nn2\n)\n.\nAs α > 1 satisfies the equation αα = ec1 , we can show that\ne−\nc1\nα =\n1\nα\n.\nNote that\nex = 1+ x+\n1\n2x2\n+ ...\nthen,\nqn−x0 =\n1\nα\n[\n1− c\n2\n1\n2nα\n+O\n(\n1\nn2\n)][\n1+O\n(\n1\nn2\n)]\n=\n1\nα\n− c\n2\n1\n2α2n\n+O\n(\n1\nn2\n) .\nBased on the definition of κ(x) in Eq.(32),\nκ(x0) = 1−qn−x0 = α−1α +\nc21\n2α2n\n+O\n(\n1\nn2\n)\n.\n24 K. Wang, L. Wen and K. Mu\nRemark: lnq and κ(x0) can be simplified into\nlnq =−c1\nn\n+O\n(\n1\nn2\n)\n= O\n(\n1\nn\n)\n,\nand\nκ(x0) =\nα−1\nα\n+O\n(\n1\nn\n)\n= O(1).\nLemma 2\nWhen n is sufficiently large,\nψ ′(1)> 0. (36)\nψ ′(n−1)< 0. (37)\nψ ′(x0) = O\n(\n1\nn\n)\n. (38)\nProof\nFrom the definitions of ψ(x) in Eq.(30) and φ(x) in Eq.(9), it follows that\nψ ′(x) =\n1\n2\n(\n1\nn− x −\n1\nx\n)+ ln\nn− x\nx\n− (2n−2x) lnq+ ln(1−qn−x)\n+\nx\n1−qn−x ×q\nn−x lnq− lnr.\n(39)\nTake x = 1 in Eq.(39), we can see that the second term is ln(n−1) and all the other terms are of\nthe order O(1). So, when n is sufficiently large,\nψ ′(1) = ln(n−1)+O(1)> 0.\nTake x= n−1 in Eq.(39), the most significant term is the fifth one. By Eq.(33), the fifth term can\nbe simplified as follows.\nn−1\n1−qn−(n−1) ×q\nn−(n−1) lnq =− (n−1)(n− c1)\nn\n+O(1)\nand the other terms are of the order O(ln(n)) or less. So, when n is sufficient large,\nψ ′(n−1) =− (n−1)(n− c1)\nn\n+O(lnn)< 0.\nFinally, take x = x0 =\n(α−1)n\nα in Eq.(39), the first term is of the order O\n( 1\nn\n)\n. The other five terms\ncan be simplified correspondingly into\nln\nn− x0\nx0\n=− ln(α−1),\n−(2n−2x0) lnq = 2c1α +O\n(\n1\nn\n)\n,\nln(1−qn−x0) = lnκ(x) = ln(α−1)− lnα+O\n(\n1\nn\n)\n,\nx0\n1−qn−x0 ×q\nn−x0 lnq =−c1\nα\n+O\n(\n1\nn\n)\n,\nRandom Logic Programs: Linear Model 25\n− ln(r) =− ln 1−d\n1− p =− ln\nn− c2\nn− c1 =− ln(1+\nc1− c2\nn− c1 ) = O\n(\n1\nn\n)\n.\nThen\nψ ′(x0) =\nc1\nα\n− lnα+O\n(\n1\nn\n)\n.\nSince α > 1 satisfies the equation αα = ec1 , we have that\nc1\nα\n− lnα = 0.\nThus,\nψ ′(x0) = O\n(\n1\nn\n)\n.\nLemma 3\nIf 1≤ x≤ n−1, then\nψ ′′(x)< 2lnq < 0, (40)\nψ ′′(x0) =− 1σ2 +O\n(\n1\nn2\n)\n(41)\nwhere σ is defined in Eq.(13).\nProof\nBy Eq.(39),\nψ ′′(x) =\n1\n2x2\n− 1\nx\n− 1\nn− x +\n1\n2(n− x)2\n+2lnq+\nqn−x lnq\n1−qn−x\n+\n(1−qn−x)(qn−x− xqn−x lnq)− xq2(n−x) lnq\n(1−qn−x)2 × lnq.\nThen it can be further simplified to\nψ ′′(x) =\n1\n2x2\n− 1\nx\n− 1\nn− x +\n1\n2(n− x)2\n+\n2lnq\nκ(x)\n+(\n1\nκ(x)\n− 1\nκ(x)2\n)x ln2 q.\n(42)\nFrom 1≤ x≤ n−1, we have that\n1\n2x2\n− 1\nx\n− 1\nn− x +\n1\n2(n− x)2 < 0.\nAs κ(x) = 1−qn−x, so 0 < κ(x)< 1, then(\n1\nκ(x)\n− 1\nκ(x)2\n)\nx ln2 q < 0.\nSo\nψ ′′(x)<\n2lnq\nκ(x)\n< 2lnq < 0.\n26 K. Wang, L. Wen and K. Mu\nTake x= x0 =\n(α−1)n\nα in Eq.(42) and split the formula into three partsas follows. Then by Eq.(33)\nand Lemma 1,\n1\n2x20\n− 1\nx0\n− 1\nn− x0 +\n1\n2(n− x0)2 =−\nα2\n(α−1)n +O\n(\n1\nn2\n)\n,\n2lnq\nκ(x0)\n=− 2αc1\n(α−1)n +O\n(\n1\nn2\n)\n,\n(\n1\nκ(x0)\n− 1\nκ(x0)2\n)x0 ln2 q =− c\n2\n1\n(α−1)n +O\n(\n1\nn2\n)\n.\nCombining the three parts above together and by the definition of σ in Eq.(13),\nψ ′′(x0) =−α\n2+2αc1+ c21\n(α−1)n +O\n(\n1\nn2\n)\n=− 1\nσ2\n+O\n(\n1\nn2\n)\n.\nLemma 4\nFor all i > 2, the i-th derivative of ψ(x) at x0 satisfies\nψ(i)(x0) = O\n(\n1\nni−1\n)\n. (43)\nProof\nBy the definition of κ(x) in Eq.(32), for i > 2, we have that\nκ(i)(x) = (−1)(i+1)qn−x(lnq)i.\nTake x = x0, by Eq.(33) and Lemma 1, the formula above can be simplified to\nκ(i)(x0) = O\n(\n1\nni\n)\n. (44)\nDefine\nψ1(x) =\n1\n2x2\n− 1\nx\n− 1\nn− x +\n1\n2(n− x)2 ,\nψ2(x) =\n2lnq\nκ(x)\n,\nψ3(x) =\n(\n1\nκ(x)\n− 1\nκ(x)2\n)\nx ln2 q.\nBy Eq.(42),\nψ ′′(x) = ψ1(x)+ψ2(x)+ψ3(x).\nThus\nψ(i)(x) = ψ(i−2)1 (x)+ψ\n(i−2)\n2 (x)+ψ\n(i−2)\n3 (x).\nAs x0 = O(n), we have that\nψ(i−2)1 (x0) = O\n(\n1\nni−1\n)\n. (45)\nThen\nψ2(x) = 2lnq(κ(x))−1 ,\nRandom Logic Programs: Linear Model 27\nψ ′2(x) = 2lnq\n(−κ(x)−2κ ′(x)) ,\nψ ′′2 (x) = 2lnq\n(\n2κ(x)−3κ ′(x)2−κ(x)−2κ ′′(x)) ,\nψ ′′′2 (x) = 2lnq\n(−6κ(x)−4κ ′(x)3+6κ(x)−3κ ′(x)κ ′′(x).−κ(x)−2κ ′′′(x)) .\nIn general, for i > 2, it holds that\nψ(i−2)2 (x) = 2lnq∑\nj\nΛi, j(x)\nwhere\nΛi, j(x) = ci, jκ(x)− j∏\ns\n(κ(is)(x))ts ,\nci, j is a constant determined by i and j, and\n∑\ns\nis× ts = i−2.\nThen by Eq.(44), we know that\nΛi, j(x0) = O\n(\n1\nni−2\n)\n.\nSince lnq = O\n( 1\nn\n)\n,\nψ(i−2)2 (x0) = O\n(\n1\nni−1\n)\n.\nSimilarly, we can show that\nψ(i−2)3 (x0) = O\n(\n1\nni−1\n)\n.\nTherefore,\nψ(i)(x0) = ψ\n(i−2)\n1 (x0)+ψ\n(i−2)\n2 (x0)+ψ\n(i−2)\n3 (x0) = O\n(\n1\nni−1\n)\n.\nLemma 5\nφ(x0) =\nαe\nc1−c2\nα√\n2pi(α−1)n +O(n\n− 32 ). (46)\nProof\nBy the definition of φ(x) and x0 in Eq.(9) and Eq.(12),\nφ(x0) =\n√\nn\n2pix0(n− x0)\n(\nn(1−qn−x0)\nx0\n)x0(nrqn−x0\nn− x0\n)n−x0\n=\n√\nα2\n2pi(α−1)n\n(\nκ(x0)\nα−1\nα\n) α−1\nα n\n(\nrqn−x0\n1\nα\n) n\nα\n.\nBy Lemma 1,\nφ(x0)=\n√\nα2\n2pi(α−1)n\n(\n1+\nc21\n2α(α−1)n +O\n(\n1\nn2\n)) α−1\nα n((\n1− c2− c1\nn− c1\n)(\n1− c\n2\n1\n2αn\n+O\n(\n1\nn2\n))) n\nα\n.\n28 K. Wang, L. Wen and K. Mu\nThen Eq.(26), the above equation can be further simplified as follows:\nφ(x0) =\n√\nα2\n2pi(α−1)n\n(\ne\nc21\n2α2 +O\n(\n1\nn\n))(\ne−\nc21\n2α2\n+\nc1−c2\nα +O\n(\n1\nn\n))\n=\nαe\nc1−c2\nα√\n2pi(α−1)n +O(n\n− 32 ).\nLemma 6\nLet ∆= c0\n√\nn lnn as defined in Eq.(15), where c0 is defined in Eq.(14). Then\nlim\nn→∞\n(∫ x0−∆\n1\nφ(x)dx+\n∫ n−1\nx0+∆\nφ(x)dx\n)\n= 0,\nand\nlim\nn→∞\n(bx0−∆c\n∑\nk=1\nφ(k)+\nn−1\n∑\nk=bx0+∆c\nφ(k)\n)\n= 0.\nProof\nBy the definition of ψ(x), we have\nφ(x) = φ(x0)eψ(x).\nBy Lemma 2 and Lemma 3, for all x ∈ [1,n−1],\nψ ′′(x)< 2lnq =−2c1\nn\n+O(\n1\nn2\n).\nThen the Taylor series for ψ at x ∈ [1,n−1] is\nψ(x)≤ ψ(x0)+(x− x0)ψ ′(x0)+ 12 (x− x0)\n2 max(ψ ′′(x)).\nAs x < n, so\nψ(x)≤−c1\nn\n(x− x0)2+O(1).\nWe note that the function − c1n (x− x0)2 is an upper bound for ψ(x), which is strictly increasing\nwhen x < x0 and strictly decreasing when x > x0. Thus,∫ x0−∆\n1\nφ(x)dx+\n∫ n−1\nx0+∆\nφ(x)dx≤\n∫ x0−∆\n1\nφ(x0−∆)dx+\n∫ n−1\nx0+∆\nφ(x0+∆)dx\n≤ φ(x0)ne− lnn+O(1) = O(φ(x0)).\nBy Lemma 5,\nφ(x0) =\nαe\nc1−c2\nα√\n2pi(α−1)n +O\n(\nn−\n3\n2\n)\n= O\n(\n1√\nn\n)\n.\nSo,\nlim\nn→∞\n(∫ x0−∆\n1\nφ(x)dx+\n∫ n−1\nx0+∆\nφ(x)dx\n)\n≤ 0.\nRandom Logic Programs: Linear Model 29\nFrom φ(x)≥ 0, it follows that\nlim\nn→∞\n(∫ x0−∆\n1\nφ(x)dx+\n∫ n−1\nx0+∆\nφ(x)dx\n)\n= 0.\nThus,\nlim\nn→∞\n(bx0−∆c\n∑\nk=1\nφ(k)+\nn−1\n∑\nk=bx0+∆c\nφ(k)\n)\n= 0.\nThe next lemma is a basic property of integral. We present it here for reader’s reference.\nLemma 7\nLet the function φ be defined as in Eq.(9). Then\nlim\nn→∞\nn−1\n∑\nk=1\nφ(k) = lim\nn→∞\n∫ n\n1\nφ(x)dx. (47)\nProof\nBy Lemma 3, ψ ′′(x) < 0 when x ∈ [1,n− 1]. We know that ψ(x) is a concave function in the\nrange. Also, by Lemma 2, ψ ′(1) > 0 and ψ ′(n− 1) < 0, which mean there exists a unique xˆ ∈\n(1,n−1) such that ψ(x) reaches its apex at xˆ. As φ(x) = φ(x0)eψ(x) and it is a concave function,\nφ(x) is strictly increasing for x ∈ (1, xˆ) and strictly decreasing for x ∈ (xˆ,n−1).\nFigure 9. The integral and its approximation\nTo compare the difference between the integral and the sum of the discrete values, we use\nFigure 9 as an example. The curve reaches its maximum at xˆ which is larger than 3 and smaller\nthan 4. Clearly, from Figure 9(a), it is difficult to compare the integral and the sum of the discrete\nvalues. However, if we remove the tallest bar, which is φ(3) and shift all the bars right of it\nleftward one step, then clearly (as shown in Figure 9(b)) the sum of the discrete values is smaller\nthan the integral of the curve. If we insert the bar of φ(xˆ) at the left of the bar of the smallest\nnumber which is larger than xˆ, (in this example it is 4), and shift all the bars left of it leftward\none step (as shown in Figure 9(c)), then the total of the discrete values is larger than the integral\nof the curve. Therefore we have:\nn−1\n∑\nk=1\nφ(k)+φ(xˆ)>\n∫ n\n1\nφ(x)dx >\nn−1\n∑\nk=1\nφ(k)−φ(xˆ).\n30 K. Wang, L. Wen and K. Mu\nBy Lemma 5, we know that\nφ(x0) =\nαe\nc1−c2\nα√\n2pi(α−1)n +O\n(\nn−\n3\n2\n)\n= O\n(\n1√\nn\n)\n.\nAlso, from the proof of Lemma 6, we can see that, for x ∈ [1,n− 1], φ(x) = φ(x0)eψ(x) and\nψ(x)≤−c1(x−x0)2/n+O(1). So, φ(x) =O(φ(x0)), which implies φ(xˆ) =O(n− 12 ). That means\nlim\nn→∞φ(xˆ) = 0. Therefore,\nlim\nn→∞\nn−1\n∑\nk=1\nφ(k) = lim\nn→∞\n∫ n\n1\nφ(x)dx.\nLemma 8\nlim\nn→∞\n(∫ x0−∆\n−∞\nχ(x)dx+\n∫ ∞\nx0+∆\nχ(x)dx\n)\n= 0. (48)\nProof\nNote that ∫ x0−∆\n−∞\nχ(x)dx+\n∫ ∞\nx0+∆\nχ(x)dx = 2\n∫ ∞\nx0+∆\nχ(x)dx\n= 2φ(x0)\n∫ ∞\n∆\ne−\nx2\n2σ2 dx.\nLet x =\n√\n2σt, then ∫ ∞\n∆\ne−\nx2\n2σ2 dx =\n√\n2σ\n∫ ∞\n∆√\n2σ\ne−t\n2\ndt =\n√\n2piσ\n2\nerfc(\n∆√\n2σ\n),\nwhere erfc is the complementary error function. Then\nlim\nn→∞\n(∫ x0−∆\n−∞\nχ(x)dx+\n∫ ∞\nx0+∆\nχ(x)dx\n)\n= lim\nn→∞\n√\n2piσφ(x0)erfc(\n∆√\n2σ\n).\nBy Eq.(13) and Lemma 5, we know that\nσφ(x0) = O(1).\nAnd then by Eq.(15), we have\n∆√\n2σ\n= O\n(√\nlnn\n)\n→ ∞.\nFrom Eq.29 (the property of complementary error function), it follows that\nlim\nz→∞erfc(z) = 0.\nThus,\nlim\nn→∞\n(∫ x0−∆\n−∞\nχ(x)dx+\n∫ ∞\nx0+∆\nχ(x)dx\n)\n= 0.\nRandom Logic Programs: Linear Model 31\nLemma 9\nlim\nn→∞\n∫ x0+∆\nx0−∆\n|φ(x)−χ(x)|dx = 0. (49)\nProof\nFrom the definitions of φ(x) and χ(x) in Eq.(9) and Eq.(11),∫ x0+∆\nx0−∆\n|φ(x)−χ(x)|dx = φ(x0)\n∫ x0+∆\nx0−∆\n|eψ(x)− eξ (x)|dx\n= φ(x0)\n∫ x0+∆\nx0−∆\neξ (x)|eψ(x)−ξ (x)−1|dx,\nwhere\nξ (x) =− (x− x0)\n2\n2σ2\n.\nNote that eξ (x) ≤ 1 and when |δ | is small enough,\n|eδ −1| ≤ 2|δ |.\nIf we can show that ψ(x)−ξ (x)→ 0 when x ∈ [x0−∆,x0+∆], then∫ x0+∆\nx0−∆\n|φ(x)−χ(x)|dx≤ 2φ(x0)\n∫ x0+∆\nx0−∆\n|ψ(x)−ξ (x)|dx. (50)\nFrom the definition of ξ (x), it follows that\nξ (x0) = ξ ′(x0) = 0,\nξ ′′(x0) =− 1σ2 ,\nand\nξ (i)(x0) = 0, for i > 2.\nFrom the definition of ψ(x) in Eq.(30),\nψ(x0) = 0.\nBy Lemma 2, Lemma 3 and Lemma 4,\nψ ′(x0) = O\n(\n1\nn\n)\n,\nψ ′′(x0) =− 1σ2 +O\n(\n1\nn2\n)\n,\nψ(i)(x0) = O\n(\nn−(i−1)\n)\n, for i > 2.\nBased on the Taylor series for the function ψ(x)−ξ (x),\n|ψ(x)−ξ (x)| ≤\n∞\n∑\ni=0\n∣∣∣∣∣ψ(i)(x0)−ξ (i)(x0)i! (x− x0)i\n∣∣∣∣∣ .\nAs x ∈ [x0−∆,x0+∆],\n|x− x0| ≤ O\n(√\nn lnn\n)\n.\n32 K. Wang, L. Wen and K. Mu\nThus,\n|ψ(x)−ξ (x)| ≤ O\n(√\nlnn\nn\n)\n→ 0,\nwhich indicates Eq(50) holds. By Eq(50), we have∫ x0+∆\nx0−∆\n|φ(x)−χ(x)|dx≤ O(2φ(x0))\n∫ x0+∆\nx0−∆\n|ψ(x)−ξ (x)|dx\n≤ O\n(\n1√\nn\n)\n×O\n(√\nn lnn\n)\n×O\n(√\nlnn\nn\n)\n= O\n(\nlnn√\nn\n)\n→ 0.\nReferences\nACHLIOPTAS, D., KIROUSIS, L., KRANAKIS, E., KRIZANC, D., MOLLOY, M., AND STAMATIOU, Y.\n1997. Random constraint satisfaction: A more accurate picture. In Proceedings of the 3rd International\nConference on Principles and Practice of Constraint Programming (CP-97). 107–120.\nACHLIOPTAS, D., NAOR, A., AND PERES, Y. 2005. Rigorous location of phase transitions in hard opti-\nmization problems. Nature 435, 7043, 759764.\nBARAL, C., GELFOND, M. AND RUSHTON, N. 2009. Probabilistic reasoning with answer sets. Theory\nand Practice of Logic Programming 9, 1, 57–144.\nBLAIR, H., DUSHIN, F., JAKEL, D., RIVERA, D., AND SEZGIN, M. 1999. Continuous models of com-\nputation for logic programs: importing continuous mathematics into logic programming’s algorithmic\nfoundations. In The Logic Programming Paradigm. 231–255.\nBRASS, S. AND DIX, J. 1999. Semantics of disjunctive logic programs based on partial evaluation. Journal\nof Logic Programming 38, 3, 167–312.\nCHEESEMAN, P., KANEFSKY, B., AND TAYLOR, W. M. 1991. Where the really hard problems are. In\nProceedings of the 12th International Joint Conference on Artificial Intelligence (IJCAI-91). 331–340.\nGEBSER, M., KAUFMANN, B., AND SCHAUB, T. 2009. The conflict-driven answer set solver clasp:\nProgress report. In Proceedings of the 10th International Conference on Logic Programming and Non-\nmonotonic Reasoning (LPNMR-09). 509–514.\nGELFOND, M. AND LIFSCHITZ, V. 1990. The stable model semantics for logic programming. In Proceed-\nings of the 5th International Conference on Logic Programming (ICLP-88). 1070–1080.\nGELFOND, M. AND LIFSCHITZ, V. 1988. Logic programs with classical negation. In Proceedings of the\n7th International Conference on Logic Programming (ICLP-90). 579–597.\nGENT, I. AND WALSH, T. 1994. The sat phase transition. In Proceedings of the Eleventh European\nConference on Artificial Intelligence (ECAI-94). 105–109.\nHUANG, G., JIA, X., C., AND YOU, J. 2002. Two-literal logic programs and satisfiability representation\nof stable models: A comparison. In Proceedings 15th Canadian Conference on Artificial Intelligence.\n119–131.\nHUBERMAN, B. AND HOGG, T. 1987. Phase transitions in artificial intelligence systems. Artificial Intel-\nligence 33, 2, 155–171.\nJANHUNEN, T. 2006. Some (in)translatability results for normal logic programs and propositional theories.\nJournal of Applied Non-Classical Logics , 1-2, 35–86.\nLEONE, N., PFEIFER, G., FABER, W., EITER, T., GOTTLOB, G., PERRI, S., AND SCARCELLO, F. 2006.\nThe dlv system for knowledge representation and reasoning. ACM Transactions on Computational\nLogic 7, 3, 499–562.\nRandom Logic Programs: Linear Model 33\nLONC, Z. AND TRUSZCZYNSKI, M. 2004. Computing stable models: worst-Case performance estimates.\nTheory and Practice of Logic Programming 4, 1-2, 193–231.\nMAREK, V. AND TRUSZCZYNSKI, M. 1991. Autoepistemic logic. Journal of the Association for Comput-\ning Machinery 38, 3, 588–619.\nMAREK, V. AND TRUSZCZYNSKI, M. 1993. Nonmonotonic Logic: Context-Dependent Reasonong.\nSpringer, 1993.\nMITCHELL, D., SELMAN, B., AND LEVESQUE, H. 1992. Hard and easy distributions of sat problems. In\nProceedings of the 10th National Conference on Artificial Intelligence (AAAI-92). 459–465.\nMONASSON, R., ZECCHINA, R., KIRKPATRICK, S., SELMAN, B., AND TROYANSKY, L. 1999. 2+p-\nsat: Relation of typical-case complexity to the nature of the phase transition. Random Structures and\nAlgorithms 15, 3-4, 414–435.\nNAMASIVAYAM, G. 2009. Study of random logic programs. In Proceedings of the 25th International\nConference on Logic Programming (ICLP-09). 555–556.\nNAMASIVAYAM, G. AND TRUSZCZYNSKI, M. 2009. Simple random logic programs. In Proceedings of\nthe 10th International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR-09).\n223–235.\nSCHLIPF, J., TRUSZCZYNSKI, M., AND WONG, D. 2005. On the distribution of programs with stable\nmodels. In 05171 Abstracts Collection - Nonmonotonic Reasoning, Answer Set Prorgamming and Con-\nstraints.\nSTAAB, S. AND STUDER, R. 2004. Handbook on Ontologies. Springer-Verlag, 2004.\nSYRJA¨NEN, T. AND NIEMELA¨, I. 2001. The smodels system. In Proceedings of the 6th International\nConferenceLogic Logic Programming and Nonmonotonic Reasoning (LPNMR-01). 434–438.\nWANG, K. AND ZHOU, L. 2005. Comparisons and computation of well-founded semantics for disjunctive\nlogic programs. ACM Transactions on Computational Logic 6, 2, 295–327.\nZHAO, Y. AND LIN, F. 2003. Answer set programming phase transition: A study on randomly generated\nprograms. In Proceedings of the 19th International Conference on Logic Programming (ICLP-03). 239–\n253.\n",
            "id": 17207740,
            "identifiers": [
                {
                    "identifier": "155601750",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:localhost:20.500.11897/328476",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "2127206515",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "10.1017/s1471068414000611",
                    "type": "DOI"
                },
                {
                    "identifier": "25037068",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "1406.6102",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "oai:arxiv.org:1406.6102",
                    "type": "OAI_ID"
                }
            ],
            "title": "Random Logic Programs: Linear Model",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2127206515",
            "oaiIds": [
                "oai:arxiv.org:1406.6102",
                "oai:localhost:20.500.11897/328476"
            ],
            "publishedDate": "2014-06-23T00:00:00",
            "publisher": "'Cambridge University Press (CUP)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1406.6102"
            ],
            "updatedDate": "2020-12-24T13:32:46",
            "yearPublished": 2014,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "1471-0684"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1406.6102"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/17207740"
                }
            ]
        },
        {
            "acceptedDate": "2005-11-15T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Ding, Yi"
                },
                {
                    "name": "Redmill, DW"
                }
            ],
            "contributors": [],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/29026459",
                "https://api.core.ac.uk/v3/outputs/223310062"
            ],
            "createdDate": "2015-02-17T16:06:10",
            "dataProviders": [
                {
                    "id": 286,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/286",
                    "logo": "https://api.core.ac.uk/data-providers/286/logo"
                }
            ],
            "depositedDate": "2005-01-01T00:00:00",
            "abstract": null,
            "documentType": "research",
            "doi": "10.1109/icip.2005.1529933",
            "downloadUrl": "https://core.ac.uk/download/29026459.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "                          Ding, Y., & Redmill, D. W. (2005). Direct virtual viewpoint synthesis from\nmultiple viewpoints. 1045 - 1048. 10.1109/ICIP.2005.1529933\nLink to published version (if available):\n10.1109/ICIP.2005.1529933\nLink to publication record in Explore Bristol Research\nPDF-document\nUniversity of Bristol - Explore Bristol Research\nGeneral rights\nThis document is made available in accordance with publisher policies. Please cite only the published\nversion using the reference above. Full terms of use are available:\nhttp://www.bristol.ac.uk/pure/about/ebr-terms.html\nTake down policy\nExplore Bristol Research is a digital archive and the intention is that deposited content should not be\nremoved. However, if you believe that this version of the work breaches copyright law please contact\nopen-access@bristol.ac.uk and include the following information in your message:\n• Your contact details\n• Bibliographic details for the item, including a URL\n• An outline of the nature of the complaint\nOn receipt of your message the Open Access Team will immediately investigate your claim, make an\ninitial judgement of the validity of the claim and, where appropriate, withdraw the item in question\nfrom public view.\nDirect Virtual Viewpoint Synthesis from Multiple \nViewpoints \nYi Ding \nMachine Vision Laboratory, CIMMS, \nUniversity of the West of England \nBristol, BS16 1QY, UK \nE-mail: yi3.ding@uwe.ac.uk \nDavid Redmill \nCenter for Communications Research \nUniversity of Bristol \nBristol, BS8 1UB, UK \nE-mail: david.redmill@bristol.ac.uk\n \n \nAbstract— This paper presents a novel approach for synthesizing \nintermediate or Virtual Viewpoints (VVs) of a 3D scene based on \ninformation from a number of known Reference Viewpoints \n(RVs). The proposed approach directly estimates the pixel value \n(and corresponding depth) for each pixel in the VV. This is \ncontrast to the more traditional 2 stage approach of firstly \nbuilding a full 3D or 2.5D model for the scene and then \nsynthesising the desired VV. The potential advantage of this \napproach is that it works directly with the target virtual view and \nis hopefully less susceptible to the propagation of errors from the \ndepth estimation stage to the interpolation stage. \nKeywords-direct viewpoint synthesis; depth estimation \nI.  INTRODUCTION  \nFor many modern applications including special effects for \nfilm, TV as well as surveillance and other applications, it is \noften desirable to generate a high quality Virtual View (VV) of \na 3D scene from a viewpoint for which no direct information is \navailable. The VV is typically synthesised from information \nfrom a number of known Reference Views (RVs). We shall \nassume that both intrinsic and extrinsic camera parameters are \nknown for both the VV and RVs. Although these parameters \nmay not be accurately known, they can be derived using \nvarious camera calibration algorithms such as those in Hartley \nand Zisserman [3]. Given these camera parameters, and the \nposition (u,v) and depth z of any pixel in the ith view, it is \npossible to both project this pixel to a corresponding point in \nthe jth view. \n⎟⎟\n⎟⎟\n⎟\n⎠\n⎞\n⎜⎜\n⎜⎜\n⎜\n⎝\n⎛\n=\n⎟⎟\n⎟⎟\n⎟\n⎠\n⎞\n⎜⎜\n⎜⎜\n⎜\n⎝\n⎛\n1\n.\n.\n1\n.\n.\ni\nii\nii\nij\nj\njj\njj\nz\nzv\nzu\nz\nzv\nzu\nA\n \nProviding we can assume that the 3D scene comprises \nopaque objects, and that any visible point will have similar \ncolour in all viewpoints, it is in principal possible to estimate \nthe 3D scene geometry by matching points in different RVs. \nWhile complete 3D scene geometry is useful for many \ncomputer vision purposes, it is common for view synthesis \npurposes to estimate a 2.5D description comprising a depth or \ndisparity estimate for all pixels in each of the RVs. Once the \ngeometry is known, it is then possible to synthesise a virtual \nview [5]. The first disadvantage of this 2-stage approach is that \nany errors in the depth estimation stage can propagate via the \nsynthesis stage to cause more significant errors in the desired \ninterpolated view. The second disadvantage is the disparity of \nevery pixel in the RVs estimated irrespective of whether it is \nvisible in the VV. There are two main challenges involved in \ndepth estimation. The first is that of occlusions where portions \nof the scene may not be visible in some of the RVs because \nthey are occluded by (behind) other objects. For these regions it \nis unreasonable to expect pixel values to match. The 2nd main \nproblem is that of un-textured regions, which can result in \nmultiple matches at incorrect depth values. In recent years, \nsome authors including Grammalidis et al [1] and Kang et al \n[3] have observed that using more than two images can \ndramatically improve the quality of the construction at the \nexpense of increased semi-occluded regions (pixels visible in \nsome but not all images) as the basic pair of stereo views has \nweakness of dealing with occlusions. In general, this is true for \nany viewpoint from which additional cameras could be used to \ndisambiguate the possible depth interpretation except some \nspecial regions e.g. un-textured regions. \nII. DIRECT VIEWPOINT SYNTHESIS \nIn this paper, we present a novel approach based on directly \nestimating the depth and corresponding pixel values for the \ndesired VV. Our novel direct viewpoint synthesis bears \nresemblance with Ng, et al [5] which consists of “Search”, \n“Match” and “Render”. Rather than matching in range space, \nwe propose an approach to search in space but match in image \nspace (epipolar scanlines).Tackling the problem in a single \nstage, has the potential to avoid the problems of inaccurate \ndepth estimates propagating during synthesis. The proposed \nalgorithm involves taking each pixel in the VV and searching \nalong the depth z direction. For each candidate depth, we find \nthe corresponding projected pixel values in each of the RVs. \nThe estimated value of depth is then determined as the one \nwhich gives the ‘best’ match between projected pixel values. \nHaving determined the depth, the pixel value can be estimated \nfrom the projected pixel values in the RVs. Another way to \nview this is to consider the pixel in the VV as corresponding to \nan epipolar line in each of the RVs. We then search along these \nepipolar lines for a ‘best match’. \n0-7803-9134-9/05/$20.00 ©2005 IEEE\nAuthorized licensed use limited to: UNIVERSITY OF BRISTOL. Downloaded on October 1, 2009 at 04:29 from IEEE Xplore.  Restrictions apply. \n \nFigure 1:   Illustration of our Direct Virtual Viewpoint Synthesis. \nFigure 1 shows an illustration of the algorithm for pixel vp in \nVirtual Viewpoint v. Searching along the depth direction \n''' PP →  maps to the epipolar lines ''' 11 pp →  in RV1 and \n''' 22 pp →  in RV2. \n0\n20\n40\n60\n80\n100\n120\n140\n160\n13 13.1 13.2 13.3\nDepth/m\nIn\nte\nns\nity\nabove\nleft\nright\nbelow\n \nFigure 2:   Search process for an example pixel. \nFigure 2 illustrates the process for an example pixel in the \nVV with 4 RVs. The ‘best’ match is at approximately 13.15m \nwith approximate synthesised pixel intensity of 80. This figure \nalso illustrates that in order to get a good match, we will \ntypically have to search at sub-pixel accuracy [1] and use bi-\nlinear interpolation [6]. \nIn order to utilise this algorithm, we need to formalise both \nwhat we mean by ‘best’ match, and the predicted pixel value. \nThe simplest measure of we can use is to minimise the standard \ndeviation or variance of the projected pixel values, and set the \npredicted pixel value to the mean of these projected pixel \nvalues. We shall refer to this combination of dissimilarity \nmeasure and prediction as Mean and Standard Deviation \n(MSD). \nIII. EXPERIMENTAL RESULTS \nThe proposed algorithm has been tested using a set of 4 \nparallel RVs which are taken from 0.1m to left, right, above \nand below, the target VV. For comparison purposes, we will \ncompare the synthesised VV to a ‘ground truth’ value taken \nfrom a 5th camera. The cameras all have a focal length of 593.5 \npixels, and the minimum and maximum depths for the search \nprocess are set to 4.5m and 14m respectively. These depths are \nchosen from prior knowledge of the scene content to limit the \nsearch process. Figures 4a and 5a show the synthesised view \nand depth map achieved using the proposed MSD algorithm. \nFigure 6a shows the error compared to the ground truth image. \nThe total Mean Squared Error (MSE) is found to be 44.72. \nFigure 7a shows the minimum standard deviation (or cost) \nmap. \nLooking at the error image (Figure 6a), we can see that the \nmajority of the error occurs around the edges of objects (e.g. \nlamp and sculpture). Comparing with the depth map (Figure \n5a) we see that the majority of the error is associated with large \nchanges in depth. These correspond to regions where the \ndesired data for the VV is occluded in some of the RVs. \nComparing with the cost map (Figure 7a) we can also see that \nthe majority of error occurs in regions where the minimum cost \nwas relatively high, i.e. places where we know there is no good \nmatch. \nLooking at the depth map (Figure 5a), we see three main \nclasses of error. Firstly we have errors at object edges resulting \nfrom occlusions (discussed above). Secondly there is \nsignificant impulsive noise spread across the whole image. \nThirdly there are significant errors in un-textured regions e.g. \nthe shadow under the table and the wall above the whiteboard. \n0\n10\n20\n30\n40\n50\n60\n4 5 6 7 8 9 10 11 12 13 14 15\nDepth/m\nIn\nte\nns\nity\nabove\nleft\nright\nbelow\n0\n5\n10\n15\n20\n25\n4 5 6 7 8 9 10 11 12 13 14 15\nDepth/m\nV\nar\nia\nnc\ne\n \nPoint A \n0\n20\n40\n60\n80\n100\n120\n140\n160\n4 5 6 7 8 9 10 11 12 13 14 15\nDepth/m\nIn\nte\nns\nity\nabove\nleft\nright\nbelow\n0\n5\n10\n15\n20\n25\n30\n35\n40\n4 5 6 7 8 9 10 11 12 13 14 15\nDepth/m\nV\nar\nia\nnc\ne\n \nPoint B \n22.5\n23\n23.5\n24\n24.5\n25\n25.5\n26\n26.5\n27\n27.5\n4 5 6 7 8 9 10 11 12 13 14 15\nDepth/m\nIn\nte\nns\nity\nabove\nleft\nright\nbelow\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n4 5 6 7 8 9 10 11 12 13 14 15\nDepth/m\nVa\nria\nnc\ne\n \nPoint C \nFigure 3: Snapshots of 3 example points showing the projected pixel \nintensities (left) and standard deviation (right). \nTo illustrate the algorithms performance more clearly, \nFigure 3 shows the projected pixel values and standard \ndeviation for 3 different points in the image. At point A we \nhave a single clear global minimum standard deviation at about \n6.75m. The clear global minimum implies a high confidence \nand likelihood of a good synthesis result. At point B, the \nminimum standard deviation is at about 11.8m. However, the \nvalue of this minimum is relatively large, and there are other \ncomparable local minima at 6.8m and 4.5m. The high standard \ndeviation and presence of other comparable local minima at \ndifferent depths implies that there is no good match and thus \nwe can have low confidence in the synthesis result. This is \nprobably a result of occlusions. At first sight it looks like point \nC suffers from a similar problem of local minima. However, \nlooking at the values of the standard deviation, we see that they \nare low for the whole depth range. This is because point C is in \nAuthorized licensed use limited to: UNIVERSITY OF BRISTOL. Downloaded on October 1, 2009 at 04:29 from IEEE Xplore.  Restrictions apply. \nan un-textured region. As a result we have low confidence in \nthe depth estimate, although the synthesis result is reasonably \ngood since the synthesised intensity is largely independent of \nestimated depth. \nA.   Depth refinement \nOne reason for the noisy depth map is that the matching is \nbased on individual pixels rather than local regions. While this \nhas advantages in regions with non-uniform depth, it can lead \nto localised false matches. A simple way to improve the depth \nmap, is to post-filter it with a noise removal filter. In order to \npreserve sharp discontinuities at object boundaries, it is \npreferable to use a non-linear filter. A variety of morphological \nand median filters [7] were tried, with a 3x3 median filter \ngiving the best results. Figures 4b, 5b, and 6b show the \nsynthesised result, depth map and error image after applying a \n3x3 median filter to the depth map. The MSE has been reduced \nfrom 44.72 to 32.63. The reduction in noise is particularly \napparent in the depth map which is visibly better. Note that \nsince this is post-filtering operation, the cost map is no-longer \nrelevant and is thus not shown. \nB. New cost function \nAn alternative to post-filtering the depth map is to use a \nmore sophisticated cost function encompassing both matching \nlocal regions and incorporating a smoothness factor. Similar to \n[4], our cost function for pixel at u,v with depth z is defined as: \n( ) ( ) smoothvuSD EzvuEzvuE\nvu\n+++= ∑\nδδ\nδδ\n,\n,,,,\n⎪⎩\n⎪⎨\n⎧\n>\n≤=\nδηγ\nδηδ\nηπγη\n \n))\n2\n||cos(-(1)(smoothE\n \nwhere the constant γ controls the strength of smoothness \nand η  is the depth gradient between the current processing \npixel and neighbouring processed pixels in both horizontal and \nvertical directions. The smoothness term is chosen to \nencourage smoothness where discontinuities are small δ<  \n(within an object) and avoid over-smoothing where \ndiscontinuities are large δ>  (across object boundaries).  \nFigures 4c, 5c, 6c and 7c show the synthesised result, depth \nmap, error image and cost map for the proposed direct \nsynthesis algorithm using the proposed cost function with a \n3x3 window, 10=γ  and 2=δ . The MSE is 34.45.  While this is \nslightly more than the result achieved by using MSD with a \nmedian post-filter of the depth map, the synthesis does exhibit \nsome advantages, particularly in regions around narrow \nforeground objects such as the lamp support. The execution \ntimes are 33s, 36s and 82s for a) MSD, b) MSD with median \nfilter and c) improved cost function respectively on a PIII \n933MHz computer.  \nIV. CONCLUSIONS \nThis paper has presented a novel direct approach for \ngenerating arbitrary Virtual Views (VVs) from a set of known \nReference Views (RVs). Although our algorithms are tested \nusing parallel camera setup and just for one intermediate VV, \nthey can be used with an arbitrary camera setup and VV \nposition(s) since no specific geometrical assumptions have \nbeen used. \nResults are presented which demonstrate the effectiveness \nof the proposed techniques. It should be noted that for the \nproblem tackled using just 4 reference views, traditional depth \nestimation techniques would have significant difficulties, since \nall of the RVs contain some data which is occluded in all the \nother RVs. This would lead to significant problems in depth \nestimation which would manifest themselves as significant \nproblems in the synthesis. \nThe proposed method is relatively efficient for synthesising \na single VV since depth estimation is only performed once for \nthe VV rather than many times for each of the RVs. However, \nif we wish to synthesise many difference VVs, then complexity \nmay be a serious issue. \nThe proposed approach is a novel approach which could \nbenefit from further research to improve the cost function. It \ncould also be easily extended to problems with more RVs. A \nfurther possible refinement would be to utilise colour \ninformation rather than just luminance. Further work is also \nneeded to compare the proposed approach with a traditional 2-\nstage approach. \nREFERENCES \n \n[1] S. Birchfield, et al, “Depth Discontinuities by Pixel-to-Pixel Stereo”, \nSixth International Conference on Computer Vision, Bombay, India, pp. \n1073-1080, January 1998. \n[2] N. Grammalidis, et al, “Disparity and Occlusion Estimation in \nMultiocular Systems and Their Coding for the Communication of \nMultiview Image Sequences”, IEEE Transactions on Circuits and \nSystems for Video Technology, Vol. 8, No. 3, pp. 328-344, June 1998. \n[3] R. Hartely, and A. Zisserman, Multiple View Geometry in Computer \nvision, Cambridge University Press, 2000. \n[4] S. –B. Kang, et al, “Handling Occlusions in Dense Multi-view Stereo”, \nIEEE Computer Society Conference on Computer Vision and Pattern \nRecognition, I-103-I-110, December 2001. \n[5] K. C. Ng, et al, “Generalized Multiple Baseline Stereo and Direct Virtual \nView Synthesis Using Range-Space Search, Match, and Render”, \nInternational Journal of Computer Vision, Special Issue on Multicamera \nStereo, Vol. 47, Numbers 1/2/3, pp. 131-147, April-June 2002. \n[6] D. Scharstein, “View synthesis using stereo vision”, Lecture notes in \ncomputer science, Springer, October 1999. \n[7] M. Sonka, and V. Hlavac and R. Boyle, Image Processing, Analysis, and \nMachine Vision, Brooks/Cole Publishing Company, 1999. \n \n \n \n \n \nAuthorized licensed use limited to: UNIVERSITY OF BRISTOL. Downloaded on October 1, 2009 at 04:29 from IEEE Xplore.  Restrictions apply. \n       \nFigure 4: Synthesised view using: a) MSD, b) MSD + median filter of depth map, c) Improved cost function \n \n       \nFigure 5: Synthesised depth map using: a) MSD, b) MSD + median filter of depth map, c) Improved cost function \n \n       \nFigure 6: Error image using: a) MSD, b) MSD + median filter of depth map, c) Improved cost function. MSE = 44.72, 32.63, and 34.45 \nrespectively. Note that in order to exclude boundary effects, these MSE figures exclude pixels within 5 pixels of the image edges. \n \n                                                                    \nFigure 7: Cost map using: a) MSD, c) Improved cost function. Note that the median filter of the depth map doesn’t alter the cost map which is \nthe same as MSD. \nAuthorized licensed use limited to: UNIVERSITY OF BRISTOL. Downloaded on October 1, 2009 at 04:29 from IEEE Xplore.  Restrictions apply. \n",
            "id": 17384151,
            "identifiers": [
                {
                    "identifier": "oai:research-information.bris.ac.uk:publications/2989d57a-e9a7-4124-9ae5-9034cf60e62d",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "223310062",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "29026459",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:research-information.bris.ac.uk:openaire_cris_publications/f0da09d0-5092-4dc3-9c5f-dc73d7bb17cb",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "oai:research-information.bris.ac.uk:openaire_cris_publications/2989d57a-e9a7-4124-9ae5-9034cf60e62d",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "385707401",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1109/icip.2005.1529933",
                    "type": "DOI"
                },
                {
                    "identifier": "385688010",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:research-information.bris.ac.uk:publications/f0da09d0-5092-4dc3-9c5f-dc73d7bb17cb",
                    "type": "OAI_ID"
                }
            ],
            "title": "Direct virtual viewpoint synthesis from multiple viewpoints",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:research-information.bris.ac.uk:publications/2989d57a-e9a7-4124-9ae5-9034cf60e62d",
                "oai:research-information.bris.ac.uk:openaire_cris_publications/2989d57a-e9a7-4124-9ae5-9034cf60e62d",
                "oai:research-information.bris.ac.uk:openaire_cris_publications/f0da09d0-5092-4dc3-9c5f-dc73d7bb17cb",
                "oai:research-information.bris.ac.uk:publications/f0da09d0-5092-4dc3-9c5f-dc73d7bb17cb"
            ],
            "publishedDate": "2005-09-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 36636686,
                    "title": "Depth Discontinuities by Pixel-to-Pixel Stereo”,",
                    "authors": [],
                    "date": "1998",
                    "doi": "10.1109/iccv.1998.710850",
                    "raw": "S. Birchfield, et al, “Depth Discontinuities by Pixel-to-Pixel Stereo”, Sixth International Conference on Computer Vision, Bombay, India, pp. 1073-1080, January 1998.",
                    "cites": null
                },
                {
                    "id": 36636687,
                    "title": "Disparity and Occlusion Estimation in Multiocular Systems and Their Coding for the Communication of Multiview Image Sequences”,",
                    "authors": [],
                    "date": "1998",
                    "doi": "10.1109/76.678630",
                    "raw": "N. Grammalidis, et al, “Disparity and Occlusion Estimation in Multiocular Systems and Their Coding for the Communication of Multiview Image Sequences”, IEEE Transactions on Circuits and Systems for Video Technology, Vol. 8, No. 3, pp. 328-344, June 1998.",
                    "cites": null
                },
                {
                    "id": 36636690,
                    "title": "Generalized Multiple Baseline Stereo and Direct Virtual View Synthesis Using Range-Space Search, Match, and Render”,",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1109/smbv.2001.988764",
                    "raw": "K. C. Ng, et al, “Generalized Multiple Baseline Stereo and Direct Virtual View Synthesis Using Range-Space Search, Match, and Render”, International Journal of Computer Vision, Special Issue on Multicamera Stereo, Vol. 47, Numbers 1/2/3, pp. 131-147, April-June 2002.",
                    "cites": null
                },
                {
                    "id": 36636689,
                    "title": "Handling Occlusions in Dense Multi-view Stereo”,",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1109/cvpr.2001.990462",
                    "raw": "S. –B. Kang, et al, “Handling Occlusions in Dense Multi-view Stereo”, IEEE Computer Society Conference on Computer Vision and Pattern Recognition, I-103-I-110, December 2001.",
                    "cites": null
                },
                {
                    "id": 36636693,
                    "title": "Image Processing, Analysis, and Machine Vision, Brooks/Cole Publishing Company,",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1007/978-1-4899-3216-7",
                    "raw": "M. Sonka, and V. Hlavac and R. Boyle, Image Processing, Analysis, and Machine Vision, Brooks/Cole Publishing Company, 1999. Authorized licensed use limited to: UNIVERSITY OF BRISTOL. Downloaded on October 1, 2009 at 04:29 from IEEE Xplore.  Restrictions apply.",
                    "cites": null
                },
                {
                    "id": 36636688,
                    "title": "Multiple View Geometry in Computer vision,",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1017/cbo9780511811685.013",
                    "raw": "R. Hartely, and A. Zisserman, Multiple View Geometry in Computer vision, Cambridge University Press, 2000.",
                    "cites": null
                },
                {
                    "id": 36636691,
                    "title": "View synthesis using stereo vision”, Lecture notes in computer science,",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1007/3-540-48725-5",
                    "raw": "D. Scharstein, “View synthesis using stereo vision”, Lecture notes in computer science, Springer, October 1999.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "https://research-information.bris.ac.uk/files/3016948/Ding_IEEE_ICIP_2005.pdf"
            ],
            "updatedDate": "2021-10-13T19:26:38",
            "yearPublished": 2005,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/29026459.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/29026459"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/29026459/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/29026459/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/17384151"
                }
            ]
        },
        {
            "acceptedDate": "2002-08-24T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Beach, MA"
                },
                {
                    "name": "Cheung, JCS"
                },
                {
                    "name": "McGeehan, JP"
                }
            ],
            "contributors": [],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/29025331"
            ],
            "createdDate": "2015-02-17T16:04:29",
            "dataProviders": [
                {
                    "id": 286,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/286",
                    "logo": "https://api.core.ac.uk/data-providers/286/logo"
                }
            ],
            "depositedDate": "1994-11-01T00:00:00",
            "abstract": null,
            "documentType": "research",
            "doi": "10.1109/35.330228",
            "downloadUrl": "https://core.ac.uk/download/29025331.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "                          Cheung, J. C. S., Beach, M. A., & McGeehan, J. P. (1994). Network planning\nfor third-generation mobile radio systems. IEEE Communications J. Nov,\n32(11), 54 - 59. 10.1109/35.330228\nLink to published version (if available):\n10.1109/35.330228\nLink to publication record in Explore Bristol Research\nPDF-document\nUniversity of Bristol - Explore Bristol Research\nGeneral rights\nThis document is made available in accordance with publisher policies. Please cite only the published\nversion using the reference above. Full terms of use are available:\nhttp://www.bristol.ac.uk/pure/about/ebr-terms.html\nTake down policy\nExplore Bristol Research is a digital archive and the intention is that deposited content should not be\nremoved. However, if you believe that this version of the work breaches copyright law please contact\nopen-access@bristol.ac.uk and include the following information in your message:\n• Your contact details\n• Bibliographic details for the item, including a URL\n• An outline of the nature of the complaint\nOn receipt of your message the Open Access Team will immediately investigate your claim, make an\ninitial judgement of the validity of the claim and, where appropriate, withdraw the item in question\nfrom public view.\nNetwork Planning for Third-Generation \nMobile Radio Systems \nThe success of UMTS relies not only on the development of a flexible air \ninterface, efficient coding techniques, and handset technology; it is equally \nimportant to design a system that can support the underlying technology and \nto interface with other networks. \nJoseph C. 5. Cheung, Mark A. Beach, and Joseph P. McGeehan \nJOSEPH C. S. CHEUNG is \nwith the Centre for Commu- \nnicotions Research at the \nUnivmiry of BristoL \nMARKA. BEACHba \n\" b e r  of lecturing staff at \nthe Vniversby of Btistol and \nhe& the C D M ,  aahptive \nantenna, and widebandprop- \nagation research activities \nwithin the Centre for Com- \nmunications Research. \nJOSEPH P. MCGEEHAN is \nhead of the department of \nElectrical and Elecbvnic \nEngineering at the Universiry \nof B M  and the &tor of the \nCeme for Communications \nResearch. \nn recent years there has been a huge \nincrease in the demand for wireless \ncommunications, and the number of \nwireless subscribers will continue to \ngrow. Market research shows that up to 50 \npercent of communication terminals \nwill become mobile by the year 2005 [l]. Current- \nly, wireless communication consists of two main \ncategories, cellular and cordless systems, which \ndiffer in terminal mobility management. Cellular \nsystems such as the Global System for Mobile \nCommunications (GSM) and Advanced Mobile \nPhone System (AMPS) provide radio coverage \nby high-power base stations that support both \nportable and vehicular-based units. Terminal \nmobility is supported by call handover between \nbase stationswithin the network. The pan-European \nsystem GSM also supports inter-country roaming \nwithin the European community. On the other \nhand, cordless systems such as the second-gen- \neration cordless telephones (CT2) only offer lim- \nited coverage by personal base stations or low-power \npublic base stationswith no handover capability. Sys- \ntems such as Telepoint in the United Kingdom only \nhandle outgoing calls, while the Pointel system \nin France will support two-way calling capability. \nFuture-generation mobile systems will see a con- \nvergence of the cellular and cordless systems into \na single universal personal communications sys- \ntem where a single terminal can be used in a vari- \nety of environments. Moreover, simple speech \nterminals will evolve into sophisticated personal \ncommunicators that will combine telephone, pager, \nfax, answering machine, digital diary, and even \nfull-motion video communications within a single \nunit. \nIn Europe, extensive research activities are \nunderway to evaluate the market and technical \nrequirements, as well as the physical implementa- \ntion aspects, of the Universal Mobile Telecommu- \nnication System (UMTS) [l]. The aim of UMTS \nis to provide a multitude of communications ser- \nvices, ranging from speech communication and \nvideo telephony to high-data-rate file transfer up to \n2 Mb/swith quality comparable to contemporary \nfixed networks. Extensive collaborative research \nactivities sponsored by the Commission of the \nEuropean Community (CEC) under the auspices of \nResearch into Advanced Communications in Europe \n(RACE) have devoted many resources to the study \nof various aspects of UMTS. The Code Division \nTestbed (CODIT) and Advanced TDMA Mobile \nAccess (ATDMA) projects investigate the rela- \ntive merits of spread spectrum Code Division \nMultiple Access (CDMA) and Time Division \nMultiple Access (TDMA), respectively. The RACE \nproject's technology in smart antennas for univer- \nsal advanced mobile intrastructure (TSUNAMI) \nis considering the benefits that adaptive antenna \ntechnololgy can bring to both air interface techniques, \nas well as developing appropriate conponent tech- \nnologies. The mobile network (MONET) project \nstudies the network management and security \nrequirements of UMTS. The Mobile Audio-visual \nTerminal (MAVT) project evaluates different \nsource and channel coding techniques for low- \nbit-rate image and speech transmission for future \nmobile terminals. Apart from UMTS, the Mobile \nBroadband System (MBS) project investigates the \nrequirement of high-bit-rate systems using millimeter- \nwave technology. \nThe success of UMTS relies not only on the \ndevelopment of a flexible air interface, efficient \ncoding techniques, and handset technology; it is \nequally important to design a system that can \nsupport the underlying technology and to inter- \nface with other networks. Therefore, the proper \ndesign of the network, in order to provide sufficient \ncapacity of high-quality coverage and efficient \nterminal mobility management, is of utmost impor- \ntance. The Advanced Planning Methods and Tools \nfor Third Generation Mobile Radio Network \n(PLATON) project studies the network planning \naspects for UMTS. In this article, we discuss the \nrequirements of the network planning tool for \nfuture mobile radio systems and how these require- \nments are met by the proposed PLATON plan- \nning methodology. \nService Requirements \nirst-generation analog and second-generation F digital mobile radio systems are designed to \nsupport voice communication with limited data \n0163-6804/94/$04.00 1994 0 IEEE \n-~ ~ -~ \nIEEE Communications Magazine November 1994 \n~ ~~ ~ ~~ \nTelephony \nTeleconference \nVoice mail \nProgram sound \nVideo telephony \nVideo conference \nRemote terminal \n8 - 32 10E-3 1 \n32 1 OE-3 \n32 1 OE-3 \n128 1 OE-6 \n64 10E-7* ' \n384 - 768 1 OE-7* ' \n1.2 - 9.6 1 OE-6 \nI I \nUser profile editing \nI Telefax (group 4) \n1.2 - 9.6 1 OE-6 \n64 1 OE-6 \nW Table 1. A subset ofproposed teleservices for \nthird-generation Universal Mobile Telecommuni- \ncations Service (UMTS) (21. \ncommunication capabilities. Third-generation \nsystems, such as UMTS, aim to offer a wide variety \nof communication services as illustrated in Table 1. \nMost of the listed services are wireless extensions of \nISDN, while services such as navigation and loca- \ntion information are mobile-specific. Wireless net- \nwork users will expect quality of service (QOS) \nsimilar to that provided by contemporary fixed \nnetworks such as ISDN. However, it is unrealis- \ntic for the wireless network to emulate the per- \nformance of ISDN in the physical link layer due \nto the unpredictable variation of the radio propa- \ngation environment and the inherent terminal \nmobility in a wireless network. Service providers \nwill use higher-layer protocols such as powerful \nforward error correction and digital speech inter- \npolation techniques, so that the perceived QOS \nmatches that of the fixed network. As shown in \nTable 1, objective quality measures such as bit \nerror rate (BER) and data throughput vary con- \nsiderably with the type of teleservice. For exam- \nple, a BER of 10\" is adequate for speech telephony \nbut is insufficient for video transmission due to \nthe high compression video coding algorithm, \nwhich is more susceptible to transmission errors. An \nintelligent network planning tool for UMTS \nshould hide the low-level design from the system \nplanners and allow them to specify services sup- \nported and the associated quality requirements, \nrather than low-level parameters such as the sig- \nnal-to-interference ratio requirement. The planning \ntool will then map the physical characteristics of \nthe propagation environment to subjective quali- \nty assessments through a set of sophisticated \nalgorithms. \nCapacity Requirements \ns aresult of the multitude of teleservicesoffered A in different operating scenarios, the teletraf- \nfic density generated will depend on the environ- \nment, the mix of terminal types, and the terminal \ndensity. Teletraffic density will vary substantially \nin the case of high-bit-rate services provided in \nbusiness areas, whereas basic services such as \nspeech and video telephony will be offered in all \nUMTS environments. Currently, system capacity \nis described as Erlang-per-square-kilometer, which \nindicates the number of simultaneous telephone \ncalls that can be supported. Since different services \nrequire different transmission rates that can vary \nup to two orders of magnitude (as seen in Table l), \nthe traffic capacity required can no longer be spec- \nified by a single unit based on speech telephony. \nThe equivalent telephone Erlangs (ETE) per square \nkilometer has been proposed to characterize the \ntraffic density where the capacity is specified as the \nequivalent number of telephone calls [ 11. There- \nfore, the traffic capacityrepresented by this unitwill \ndepend on the transmission rate of the basic tele- \nphony service. Assuming the basic telephone call \ntransmits at 8 kb/s, a video telephone call requires \n8 ETE while a 2-Mb/s service requires 256 ETE. \nHowever, the basic rate of telephone calls and \nother services may change during the lifetime of \nUMTS when more advanced data encoding tech- \nniques are developed, thus it is best to minimize \nthe dependency of the traffic capacity representa- \ntion on the transmission rate of a telephone call. \nAn alternative is to use the transmission rate as \nthe unit of the traffic capacity while maintaining \nthe teleservice information. Therefore, the traffic \ncan be represented as megabits-per second-per- \nsquare-kilometer (Mb/s/km2), which is calculated \nfrom the transmission rate of different teleser- \nvices and the density of different terminal types in an \nenvironment. For indoor environments, the unit can \nbe modified to Mb/s-per-floor. Regardless of the \nunit used to specify the traffic capacity, it is essen- \ntial to  retain information on the terminal type \nand density to aid planning. \nMobility and Call Handover \nuture-generation wireless systems will support F true mobility in which a single terminal can be \nused regardless of the environment, provided that \nthe infrastructure necessary to support the partic- \nular service exists. Terminal mobility is a distinct \nfeature of wireless communications that offers \nadvantages over a fixed network. Careful system \nplanning is essential in order to ensure the con- \ntinuation of the call and to maintain quality when \nthe mobile terminal moves. The degree of mobility \ndepends on the operational environment as shown \nin Fig. 1. For example, domestic dwellings will be \nserved by a personal base station to which access \nis restricted to authorized users. This can be con- \nsidered as an enhancement of the current cord- \nless telephone, but with a common radio interface \nthat is compatible with UMTS. The personal base \nstation connects either directly to the public land \ntelephone network or to the public UMTS via an \nexternal radio port that relays the data to a public \nbase station. Radio coverage is confinedwithin acer- \ntain radius (50 m, for example) around the base \n- \nWireless \nnetwork \nusers will \nexpect \nquality of \nservice \nsimilar \nto that \nprovided by \ncontemporary \nnetworks \nsuch as \nISDN \nf ied  \n, \nIEEE Communications Magazine *November 1994 55 \n- \nQ \nPedestrian \nswitchinq I \n\\ i s  \nOff ice \n& \nMobile \nstation, and handover capabilitywill not be supported \nbetween personal base stations. \nNext in the system hierarchy is the customer \npremises network (CPN) that consists of a network \nof low-power base stations that provide coverage \nin a business environment. This can be considered \nas awireless extension to the private branch exchange \n(PBX). Due to thelarge amount oftrafficgenerated \nin a business environment, the wireless CPN con- \nnects to the mobile switching center (MSC) via an \noptical fiber link or a dedicated radio link so as to \nminimize the signaling burden on the public UMTS \nair interface. The provision of a wireless CPN \nallows the telephone to be associated with a person \nrather than attached to a desk. As a result, terminal \nmobility should be supported by allowing han- \ndover between different base stations within the \nsame CPN. \nThe public UMTS network will support full \nmobility by efficient call handover between pub- \nlic base stations. Moreover, handover between \npersonal base stations and the business CPN to a \npublic network will be provided by intelligent \nmobility management by transferring the call that \noriginates from an indoor network to an outdoor \npublic network and vice versa without the user's \nnotice. \nA UMTS network must provide transparent \nhandover between base stations by minimizing \nthe probability of call disconnection due to handover \nfailure. Current analog and second-generation \ndigital systems, such as GSM, require handover \nfailure probability of less than one percent. To \nimprove the handover success rate, and to maintain \nhigh service quality when the mobile terminal \nenters the transition region between the coverage \narea of two cell sites, macroscopic diversity (which \nmaintains a communication link between the mobile \nterminal and multiple base stations) has been pro- \nposed. This is known as soft handover in a direct \nsequence code division multiple access (DS-CDMA) \nsystem [3]. In TDMA, such as the Digital European \nCordless Telephone (DECT), base station diver- \nsity is implemented as seamless handover where \nthe network continuously switches between base \nstations that give better communication quality \nwhen the mobile enters the transition region. The \nprovision of soft handover not only affects termi- \nnal mobility and service quality, it has a direct impact \non the system capacity. However, there is a fun- \ndamental difference in the impact of soft han- \ndover on the overall system capacity for a DS-CDMA \nand a TDMA system that must be considered \nwhen designing the corresponding network. \nFor a spectrum spreading technique such as \nthe DS-CDMA, every additional user attached to \nthe system will look like additional noise as far as \nother users are concerned. Therefore, there is a \ngraceful degradation in the system's performance \nwhen more users are added to the system. Intuitively, \nthe uplink (mobile-to-base) capacity can be improved \nby reducing the transmission power of mobile ter- \nminals while the reduction of base station trans- \nmission power enhances the downlink (base- \ntoymobile) capacity. Soft handover achieves this \n'capacity enhancement by exploiting the diversity \ngain obtained from multiple communication \nlinks that in turn reduces the transmission power \nrequired to maintain an acceptable service quality. \nPreliminary study showed that the system capacity \n. * for the DS-CDMA uplink can be improved by 16 \npercent when 50 percent of the mobile users \noperated in soft handover mode that resulted in a \nreduction of the average transmit power by 1 dB \ndue to diversity gain [4]. Furthermore, the make- \nbefore-break characteristics of soft handovergreat- \nly enhance the user's perceived quality by reducing \nthe handover failure probability. \nFuture TDMA systems will use some form of \ndynamic channel allocation as opposed to a fixed \nfrequency plan to improve spectrum utilization. \nTo implement soft handover or seamless handover \ncapability, multiple time slots (possibly on different \nfrequencies) must be available. Therefore, the \noverall system capacity will be reduced when soft \nhandover is implemented for a TDMA system. \nRegardless of the multiple access techniques \nused by UMTS, the implementation of soft handover \nincreases the signaling load over the fixed net- \nwork and the radio interface since data are being \nsent and processed by multiple base stations. Net- \nwork planning that supports soft handover must \noptimize the tradeoff between service quality and \nnetwork signaling load. \nPlanning Methodology \ne have discussed the requirements and factors W affecting the design of a planning tool and \nidentified that the planning of a UMTS network \nwill concentrate on the service quality, system \ncapacity, and mobility issues. The question remains \nhow to implement the UMTS concept to an oper- \national system by efficient network planning. It is \na well-accepted fact that system capacity can be \nimproved by using smaller cells and the reuse of \nfrequency channels in ageographically ordered fash- \nion. The high quality and capacity required by a \nUMTS network can only be provided by utilizing \ndifferent cell structures according to the operational \nenvironment [5]. Cell structures to support UMTS \nrange from conventional macrocells to indoor \npicocells. Typical characteristics for UMTS cell \nstructures are shown in Table 2, and an opera- \ntional scenario of the mixed-cell architecture is \nshown in Fig. 2. In particular, microcell with low \ntransmission power will be widely deployed in urban \nareas while other cell structures are used according \nto the environment to provide ubiquitous coverage. \nThe need to provide an underlying network to \nsupport a microcellular-based system presents a \n56 IEEE Communications Magazine November 1994 \n, \n-- \nchallenging task to the network planner [6]. We \nanticipate that the cost of base station equipment \nfor microcells will be significantly reduced due to \nthe elimination of costly high power amplifiers \nand the economies of scale in microcell base sta- \ntion equipment manufacturing. Nevertheless, the \nsystem’s cost will still play a determining role in \nthe design of the network infrastructure since \nmore microcellular base stations are needed to \nprovide adequate radio coverage. \nMixed-cell architectures tailor the various cell \nsizes and shapes to the environment, expected \nterminal characteristics, density, and mobility. \nMicrocells with a radius less than 1,000 meters \nwill be used extensively to provide coverage in urban \ndistricts. Microcell base stations will be mounted \non lamp posts or on buildings where electric sup- \nply is readily available. Due to the low elevation \nof microcell antennas comparedwith the surrounding \nbuildings, the so-called waveguide or canyon \nstreet effectwill provide better radio coverage when \na line-of-sight component exists between the base \nstation and the mobile terminal. Furthermore, \nsurrounding buildings act as barriers that limit \nsignal spillage and interference to other streets, thus \nimproving frequency reuse efficiency. For high \nuser-density areas such as airport terminals and \nshopping malls, picocells with coverage of tens of \nmeters will be used. To facilitate efficient han- \ndover when the vehicle-based user crosses micro- \ncells at high speed, these calls will be handled by \numbrella cells (or overlay macrocells) whose cov- \nerage areas contain several to tens of microcells. \nThe aforementioned discussion suggests that \nthe planning of third-generation UMTS is more \ncomplicated than the design of current speech- \noriented, macrocell-based mobile radio systems, \nand thus requires a more advanced and intelligent \nnetwork planning tool. Existing network planning \ntools are basically a “prediction tool,” where the \nsystem planner specifies the cell site (base sta- \ntion) locations and the planning tool predicts the \nMacrocell > 1,000 1 - 10 \nUmbrella cell z 1,000 1 - 10 > 30 \ni \nI Table 2 .  Cell structures to support UMTS. \nI Macrocetl \nI \n.._. %. Umbrella \nPicocell \nr‘“ \n___ , \nW Figure 2 .  Mixed-cell architecture for UMTS. \nsignal strength, coverage area, and interference \nlevel according to some established propagation \nmodels such as the Hata-Okumura model for \nmacrocells and Walfisch-Ikegami model for \nmicrocells [7]. Based on the information obtained, \nthe planning tool may also produce other objec- \ntive measures, such as bit error rate, outage prob- \nability, call blocking probability, etc. This approach \nworks well for systems where the primary service \nis speech transmission. As more sophisticated \nservices (each with different quality and traffic \nrequirements) are proposed for next-generation \nwireless communications systems, the simple sig- \nnal level and bit error rate prediction may not be ade- \nquate. \nFor the network planning tool to keep up with \nthe advancement of UMTS, the design of an effi- \ncient planning tool should be flexible and intelligent \nin order to  aid the human network planner by \nautomating the lower levels of the design process. \nThese include estimating the traffic density require- \nments from the terrain and building databases, \nMacrocells are used in conventional CellLlar radio \nsystems The coverage IS maximized to reduce the \nsystem’s infrastructure cost Macrxells will be used \nto provide coverage for areas with low terminal \ndensity in UMTS. \nMicrocells will be widely deployed in UMTS to provide \nubiquitous coverage, mainly in urban areas. Compact \nbase station units will be mounted on lamp posts \nand surrounding buildings. \nPicocelts are used to provide services for areas with \nhigh terminal density and are usually deployed for \nindoor areas. \nUmbrella cells are used to maintain continuous i \ncoverage and to assist handover for mobile terminals I \nthat traverse throuqh microcells in high-speed. \n1 \nHighway cells are used to provide coverage for \nsections of road using compact base station units \nwith directional antennas that tailor the radiation \npattern for the environment. \nIEEE Communications Magazine November 1994 57 \n- __ __ \n, \n.- \nData bases \nTeleservices \nHandoff parameters \nPropagation models \nExpected traffic Cell topography \nFigure 3. Proposed PLATONplanning tool. \nassigning appropriate cell structures to fit the \nenvironment, and mapping of subjective quality such \nas mean opinion score of speech transmission to \nphysical parameters (e.g., bit error rate, carrier- \nto-interference requirements, etc.). Consequently, \nthe network planner will specify higher-level \ndesign parameters, such as the user’s perceived ser- \nvice quality, and fine tune the estimates given by \nthe planning tool. Also, the proposed PLATON \nplanning tool attempts to take a radical approach to \nplanning by employing “synthesis” techniques. The \ndeployment of the synthesis technique enables \nthe system planner to perform forward planning \nthat produces a radio network layout based on \nthe high-level parameters specified by the system \nplanner and other stored information such as \ndigital maps and propagation models. Associated \nwith the network layout are parameters such as \ncell site characteristics, i.e., base station transmis- \nsion power, frequency allocation, achieved system \ncapacity, service quality, and estimate of the sys- \ntem installation cost. A simplified block diagram \nof the proposed PLATON planning tool is shown \nin Fig. 3. \nThe PLATON tool operates on a set of input \nparameters that describe the quality objectives of \nthe desired system and some pre-defined rules. It \nis unlikely that the system can be designed with- \nI Figure 4. The PLATONplanning methodology. \nout any constraints. Therefore, the support of \nnetwork evolution is essential to the planning \ntool. One of the most important constraints is the \nphysical location of the base station site. For \nexample, UMTS may operate on base station \nsites occupied by existing systems in order to min- \nimize the construction cost and to avoid the cost \nof acquiring real estate, especially in downtown busi- \nness areas. To  support the network evolution \npath, the planning tool will reconfigure the exist- \ning network by adding and moving base stations \nin order to optimize the performance objectives. \nAlthough we anticipate that the planning tool \nwill primarily be used for forward planning, it \nwill also incorporate features to allow more con- \nventional reverse planning where the perfor- \nmance of an existing network is evaluated given \nthe location of cell sites. This feature is essential \nnot only to provide performance evaluation of an \nexisting system, it also provides a means to vali- \ndate the forward planning proposed by the plan- \nning tool. By providing both forward and reverse \nplanning capability within a single tool as depict- \ned in Fig. 4, network planning can be optimized \nby going through the iterative cycle of modifica- \ntion of cell site characteristics and evaluation of \nsystem performance in order to  achieve the \nrequired performance objective. \nThe forward and reverse planning operates by \nmapping the physical parameters to  the high- \nlevel user’s perceived quality by a hierarchical \napproach in direction shown in Fig. 5. At each \nlayer of the hierarchy, the system performance is \nmeasured by different parameters. The lowest \nlevel evaluates the channel propagation charac- \nteristics such as path loss, channel delay spread, fad- \ning statistics through propagationmodels, and terrain \ndatabase. The deployment of microcells with base \nstation antennas mounted below the average rooftop \ncauses the propagation condition to be more site- \nspecific. These areas are also more likely to expe- \nrience high traffic capacity and require highquality \nservices where careful planning is most critical. There- \nfore, site-specific propagation models that adapt \nto changes in the environment are required. \nDue to the well-defined boundaries of micro- \ncell propagation, ray tracing has emerged as an accu- \nra te  technique for propagation prediction. \nRecently developed ray tracing tools [8 ,9 ]  enable \nchannel characteristics such as path loss, power delay \nprofiles, fast fading, and root mean square delay \nspread to be determined by analyzing the signal arriv- \ning at the mobile terminal from the base station \nvia wall reflections, corner diffraction, diffuse \nwall scattering, and over rooftop diffraction. System \nperformance parameters such as bit error rate \nand outage probability are then evaluated based \non the radio interface techniques employed and \nthe ray tracing results, thus giving the perfor- \nmance of the data linklayer. Progressively, the PLA- \nTON tool translates the objective parameters \ninto a subjective performance indication of the com- \nmunication link layer according to higher-level \nprotocol specifications such as error control algo- \nrithm, multiple access techniques, etc. Therefore, \nthe built-in mapping between intrinsic propaga- \ntion parameters and higher-level quality parameters \nhides the system planner from the low-level sys- \ntem design. \nOwing to the hierarchical mapping from phys- \n58 IEEE Communications Magazine November 1994 \n-- ~ __ ._ \n- \nI \n-__ \nlink \nquality \nData \nObjective link \nPhysical \nlink \ntracing models \n1.  \nTerrain U database \n7 \nW Figure 5. Mapping of low-levelphysical linkper- \nfomance to high-level user's perceived service \nqualig. \nical quantities to the user's perceived quality, we must \nmaintain the accuracy of our prediction from the \nlowest level. A digital map that provides detailed ter- \nrain data can be integrated into the planning tool \nto enhance the precision of propagation predic- \ntion. The accuracy of ray tracing can be improved \nby having the knowledge of the physical proper- \nties of building materials such as reflection coeffi- \ncient and penetration loss [lo]. Further up in the \nmapping path, performance of the data link and \nthe communication link layers can be evaluated \nby extensive computer simulation, hardware testbed, \nand field trials. Therefore, we must have a thor- \nough understanding of the performance of vari- \nous entities of UMTS and provide sufficient accuracy \nin different layers of the planning process in \norder to devise an integrated planning tool for future \nmobile radio systems. \nDiscussion \nhe success of third-generation UMTS relies T on intelligent network planning to achieve the \nsuperior service quality, high capacity, and effi- \ncient management of terminal mobility. Intelli- \ngent radio network planning tools assist system \nplanners by automating low-level design, such as esti- \nmating capacity requirements, and matching \nchannel propagation characteristics to the envi- \nronment, while the operator specifies high-level \ndesign objectives such as user's perceived system \nperformance and overall design strategies. The \nPLATON planning tool meets these requirements \nby providing both forward and reverse planning \nwithin a single tool where system design is opti- \nmized by iteratively going through the design and \nanalysis cyde. \nAcknowledgments \nElements of this work were supported by the CEC \nunder RACEproject PLATON (R2007). The authors \nwould like to express their gratitude to the mem- \nbers of the PLATON consortium: AT&T Network \nand Systems UK, Ltd. (United Kingdom), Dassault \nAutomatismes et TClCcommunications (France), \nNational Technical University of Athens (Greece), \nTClCdiffussion de France C2R (France), and Cen- \ntro de Estudos de TCICcommunica@s (Portugal). \nReferences \n. .  \n~ . .  \n[ 11 CEURACE Industrial Consortium, \"IBC Common Functional Specifi- \ncations: MobileNetworkSubsystems,\"RACED730. issueE. Dec. 1991. \n[21 CEVRACE deliverable R2007/ATT/Bl/DS/P/060, \"Quality of Service \nRequirements,\" Nov. 1992. \n[3] Qualcomm. Inc..\"AnOverviewoftheApplicationofCodeDivision MuC \ntiple Access (CDMA) to  Digital C'?llular Systems and Personal Cel- \nlular Networks,\" document number: EX60-10010, May 1992. \n[41 J. C. S.  Cheung, S .  C. Swales, and M. A. Beach, \"On the Impact of \nSoft Handoff on a Direct Sequence Code Division Multiple Access \nCapacity,\" Centre for Communications Research Internal Report. PLA- \nTON/UOB/037MIPD2/07/93. issue 1, July 1993. \n[SI R. Steele, \"Toward a High-Capacity Digital Cellular Mobile Radio \nSystem,\" IEE Proc., vol. 132, pt. F, no. 5, Aug. 1985. pp. 405-415. \n[61 L. J. Greenstein, et al., \"Microcell in Personal Communications Sys- \ntems,\" IEEE Commun. Mag., vol. 30, no. 12, Dec. 1992, pp. 76-88. \n[71 CEVRACE Deliverable R202O/EBN/PS/DS/003/bl, \"Draft Propaga- \ntion Model,\" June 1992. \n[SI W.T. Webb, ''Sizing up the Microcell for Radio Communications,\" \nIEE Electronics and Commun. Eng. lour., June 1993, pp. 133-140. \n[9] A. R. Nix, H. R. Anderson, and 1. P. McGeehan. \"Estimating Wide- \nband Bit Error Rates Using Pilot Tone Envelope Fading Statistics,\" \n4th International Symposium on Personal and Mobile Radio Com- \nmunications, Yokohama, Japan, Sept. 8-1 1. 1993. \n[lO]S.G.Chard, M.A. Beach.and P. Syverson.\"MaterialCharacterisation,\" \nCentre for  Communications Internal Report, PLATON/ \nUOWO35MIPD2/07/93, issue 1, July 1993. \nBiographies \nJOSEPHC. S .C~~u~~rece iveda  6.Sc.inelectricalandelectronicengineering \nfrom the University of Manchester Institute of Science and Technology \nin 1988 and a Ph.D. from the University of Southhampton in 1992. He \nis now a t  the Centre for Communications Research at the University of \nBristol, United Kingdom, where he is involved in a European collabora- \ntion on the study of radio network planning aspects for future mobile \nradio systems. His research interests are digital signal processing, \nmodeling and performance evaluation of communication systems. \nMARKA. B~c~graduatedfromtheUniversityofYorkin 1983witha degree \ninelectronicengineering.AfteraninitialperiodwiththeGECHint Research \nLaboratories, he joined the department of electrical and electronic \nengineering atthe Universityof Bristol asa postgraduatestudent. Hethen \nreceived his Ph.D. in 1969 for his work on adaptive antennas for mul- \ntiple spread spectrum signal sources. Post-doctoral research at Bristol \nincluded work regarding the application of adaptive antenna tech- \nniques in mobile cellular networks for which the research team \nreceived the IEEE Neal Shepherd award in 1990. Since August 1990, \nhe has been engaged as a member of the lecturing staff at Bristol and \nheads the CDMAand wideband propagation research activities within the \nCentre for Communications Research, Bristol, United Kingdom. He cur- \nrently holds the UK DTVSERC LINK grant concerning the evaluation of \nCDMA techniques for UMTS, and is the project manager of both the \nEC RACEll PLATON (R2007) and ESPRlTlll LAURA (7359) projects, as \nwell as other closely related radio-based research activities. \nJOSEPH P. MCGEEHAN obtained the degrees of B.Eng. and Ph.D. in elec- \ntrical and electronic engineering from the University of Liverpool in \n1967 and 1971, respectively. In 1970, he was appointed senior scien- \nt i s t  a t  the Allan Clark Research Centre, Plessey Company, Ltd., where \nhewasresponsibleforR & Doftwo-and three-terminal GunnEffect Dwices \nand theirapplication to high-speed logic, teclecommunicationsand radar \nsystems. In 1972 he became a member of the academic staff of the \nschool of electrical and engineering at the University of Bath and initi- \nated research in the area of SS6 (subsequently linear modulation) for \nmobile radio. In 1984 he was appointed chair of communications \nengineering at the University of Bristol. He is a recipient of the IEE Pro- \nceedings Mountbatten Premium, the IEEE Transactions Neal Shepard \naward, and other awards in recognition of his research contribution \nto  radiocommunications. He is  a Fellow of the IEE, a Fellow of the \nRoyal Society of Arts and Commerce, and serves on numerous nation- \nal and international committees concerned with mobile communications. \nInJuly 1994, hewaselectedaFellowoftheRoyalAcademyofEngineering. \n- \nThe success \nof third- \ngeneration \nUMTS relies \non intelligent \nnetwork \nplanning to \nachieve the \nsuperior \nservice \nquality, high \ncapacity, and \neflcient \nmanagement \nof terminal \nmobility. \nIEEE Communications Magazine November 1994 59 \n_-__ - _  ~ \n",
            "id": 17386156,
            "identifiers": [
                {
                    "identifier": "29025331",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1109/35.330228",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:research-information.bris.ac.uk:publications/8112b502-3c86-436a-8825-633fb1a11f82",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "1975135758",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "oai:research-information.bris.ac.uk:openaire_cris_publications/8112b502-3c86-436a-8825-633fb1a11f82",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "385637944",
                    "type": "CORE_ID"
                }
            ],
            "title": "Network planning for third-generation mobile radio systems",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "1975135758",
            "oaiIds": [
                "oai:research-information.bris.ac.uk:openaire_cris_publications/8112b502-3c86-436a-8825-633fb1a11f82",
                "oai:research-information.bris.ac.uk:publications/8112b502-3c86-436a-8825-633fb1a11f82"
            ],
            "publishedDate": "1994-11-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 36631069,
                    "title": "21 CEVRACE deliverable R2007/ATT/Bl/DS/P/060, &quot;Quality of Service Requirements,&quot;",
                    "authors": [],
                    "date": "1992",
                    "doi": null,
                    "raw": "[21 CEVRACE deliverable R2007/ATT/Bl/DS/P/060, &quot;Quality of Service Requirements,&quot; Nov. 1992.",
                    "cites": null
                },
                {
                    "id": 36631074,
                    "title": "71 CEVRACE Deliverable R202O/EBN/PS/DS/003/bl, &quot;Draft Propagation Model,&quot;",
                    "authors": [],
                    "date": "1992",
                    "doi": null,
                    "raw": "[71 CEVRACE Deliverable  R202O/EBN/PS/DS/003/bl, &quot;Draft Propagation Model,&quot; June 1992.",
                    "cites": null
                },
                {
                    "id": 36631076,
                    "title": "Estimating Wideband Bit Error Rates Using Pilot Tone Envelope Fading Statistics,&quot;",
                    "authors": [],
                    "date": "1993",
                    "doi": null,
                    "raw": "[9] A.  R. Nix, H.  R. Anderson, and 1.  P.  McGeehan. &quot;Estimating Wideband Bit Error Rates Using Pilot Tone Envelope Fading Statistics,&quot; 4th International Symposium on Personal and Mobile Radio Communications, Yokohama, Japan, Sept. 8-1  1.  1993.",
                    "cites": null
                },
                {
                    "id": 36631070,
                    "title": "Inc..&quot;AnOverviewoftheApplicationofCodeDivision MuC tiple Access (CDMA) to Digital C'?llular Systems and Personal Cellular Networks,&quot; document number: EX60-10010,",
                    "authors": [],
                    "date": "1992",
                    "doi": null,
                    "raw": "[3]  Qualcomm.  Inc..&quot;AnOverviewoftheApplicationofCodeDivision  MuC tiple Access (CDMA) to  Digital C'?llular Systems and Personal Cellular Networks,&quot; document number: EX60-10010, May 1992.",
                    "cites": null
                },
                {
                    "id": 36631073,
                    "title": "Microcell in Personal Communications Systems,&quot;",
                    "authors": [],
                    "date": "1992",
                    "doi": "10.1109/icupc.1992.240784",
                    "raw": "[61 L. J.  Greenstein, et al.,  &quot;Microcell in Personal Communications  Systems,&quot;  IEEE Commun. Mag., vol. 30,  no. 12, Dec. 1992, pp. 76-88.",
                    "cites": null
                },
                {
                    "id": 36631071,
                    "title": "On the Impact of Soft Handoff on a Direct Sequence Code Division Multiple Access Capacity,&quot; Centre for Communications Research Internal Report.",
                    "authors": [],
                    "date": "1993",
                    "doi": null,
                    "raw": "[41 J.  C.  S. Cheung, S. C.  Swales, and M. A.  Beach, &quot;On the Impact of Soft Handoff on a Direct Sequence Code Division Multiple Access Capacity,&quot; Centre  for Communications  Research Internal  Report. PLATON/UOB/037MIPD2/07/93. issue 1, July 1993.",
                    "cites": null
                },
                {
                    "id": 36631075,
                    "title": "Sizing up the Microcell for Radio Communications,&quot;",
                    "authors": [],
                    "date": "1993",
                    "doi": "10.1049/ecej:19930026",
                    "raw": "[SI W.T. Webb, ''Sizing up the Microcell for Radio Communications,&quot; IEE Electronics  and  Commun. Eng. lour., June 1993, pp. 133-140.",
                    "cites": null
                },
                {
                    "id": 36631077,
                    "title": "Syverson.&quot;MaterialCharacterisation,&quot; Centre for Communications Internal Report, PLATON/ UOWO35MIPD2/07/93, issue 1,",
                    "authors": [],
                    "date": "1993",
                    "doi": null,
                    "raw": "[lO]S.G.Chard, M.A. Beach.and P.  Syverson.&quot;MaterialCharacterisation,&quot; Centre  for  Communications  Internal  Report,  PLATON/ UOWO35MIPD2/07/93,  issue 1, July 1993. Biographies JOSEPHC.  S.C~~u~~receiveda  6.Sc.inelectricalandelectronicengineering from the University of Manchester Institute of Science and Technology in 1988 and a Ph.D. from the University of Southhampton  in 1992. He is now at  the Centre for Communications Research at the University of Bristol, United Kingdom, where he is involved in a European collaboration on the study of radio network planning aspects for future mobile radio systems. His research interests are digital signal processing, modeling and performance  evaluation  of communication  systems.",
                    "cites": null
                },
                {
                    "id": 36631072,
                    "title": "Toward a High-Capacity Digital Cellular Mobile Radio System,&quot;",
                    "authors": [],
                    "date": "1985",
                    "doi": null,
                    "raw": "[SI R. Steele, &quot;Toward a High-Capacity Digital Cellular Mobile Radio System,&quot;  IEE Proc.,  vol. 132, pt. F, no. 5,  Aug. 1985. pp. 405-415.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "https://research-information.bris.ac.uk/files/2993902/cheung_IEEE_ComsMag_Nov1994.pdf"
            ],
            "updatedDate": "2021-10-13T19:17:45",
            "yearPublished": 1994,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "0163-6804"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/29025331.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/29025331"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/29025331/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/29025331/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/17386156"
                }
            ]
        },
        {
            "acceptedDate": "2007-12-03T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Armour, SMD"
                },
                {
                    "name": "Huang, G"
                },
                {
                    "name": "Nix, AR"
                }
            ],
            "contributors": [
                "Gillian"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/208007092",
                "https://api.core.ac.uk/v3/outputs/29025651"
            ],
            "createdDate": "2015-02-17T16:04:59",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 286,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/286",
                    "logo": "https://api.core.ac.uk/data-providers/286/logo"
                }
            ],
            "depositedDate": "2007-01-01T00:00:00",
            "abstract": null,
            "documentType": "research",
            "doi": "10.1109/pimrc.2007.4394297",
            "downloadUrl": "https://core.ac.uk/download/29025651.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "                          Huang, G., Nix, A. R., & Armour, S. M. D. (2007). Impact of radio resource\nallocation and pulse shaping on PAPR of SC-FDMA signals. In IEEE 18th\nInternational Symposium on Personal, Indoor and Mobile Radio\nCommunications, 2007 (PIMRC 2007), Athens. (pp. 1 - 5). Institute of\nElectrical and Electronics Engineers (IEEE). 10.1109/PIMRC.2007.4394297\nLink to published version (if available):\n10.1109/PIMRC.2007.4394297\nLink to publication record in Explore Bristol Research\nPDF-document\nUniversity of Bristol - Explore Bristol Research\nGeneral rights\nThis document is made available in accordance with publisher policies. Please cite only the published\nversion using the reference above. Full terms of use are available:\nhttp://www.bristol.ac.uk/pure/about/ebr-terms.html\nTake down policy\nExplore Bristol Research is a digital archive and the intention is that deposited content should not be\nremoved. However, if you believe that this version of the work breaches copyright law please contact\nopen-access@bristol.ac.uk and include the following information in your message:\n• Your contact details\n• Bibliographic details for the item, including a URL\n• An outline of the nature of the complaint\nOn receipt of your message the Open Access Team will immediately investigate your claim, make an\ninitial judgement of the validity of the claim and, where appropriate, withdraw the item in question\nfrom public view.\nThe 18th Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC'07) \n1-4244-1144-0/07/$25.00 ©2007 IEEE. \nIMPACT OF RADIO RESOURCE ALLOCATION AND PULSE SHAPING ON \nPAPR OF SC-FDMA SIGNALS  \nGillian Huang, Andrew Nix and Simon Armour \nCentre for Communications Research, University of Bristol \nMerchant Venturers Building, Woodland Road, Bristol BS8 1UB, UK \nEmail: G.Huang@bristol.ac.uk \n \n \nABSTRACT \n“Single Carrier Frequency Division Multiple Access” (SC-\nFDMA) is a strong candidate for uplink transmissions in the \n3G Long Term Evolution (LTE) standard. SC-FDMA is a \nmultiple access scheme that uses DFT spreading prior to \nOFDM modulation to map the signal from each user to a \nsubset of the available subcarriers. SC-FDMA has a \nsignificantly lower peak-to-average power ratio (PAPR) \ncompared to OFDM/OFDMA. This greatly improves its RF \npower amplifier efficiency and also the mean power output \nfrom a battery driven mobile terminal. In this paper, the \nimpact of radio resource allocation and pulse shaping on the \nPAPR of SC-FDMA signals are investigated, especially for \nlocalized FDMA (LFDMA) signals. It is shown that \ndistributed FDMA (DFDMA) signals with raised cosine \nfiltering experience reduced PAPR as α  increases. However, \nfor LFDMA signals the PAPR increases with increasing α  \nand further varies according to the allocated resource unit.  \nI. INTRODUCTION \nBroadband wireless communication systems must achieve \nhigh data rates in a spectrally efficiency manner. For this \nreason, Orthogonal Frequency Division Multiplexing \n(OFDM) [1,2] has been widely employed in systems such as \nthe IEEE 802.11a/g standards. Orthogonal Frequency \nDivision Multiple Access (OFDMA) is a multiple access \nscheme for OFDM that works by assigning each user a unique \nset of subcarriers. OFDMA is currently employed in the IEEE \n802.16 standard. One major drawback of OFDM and \nOFDMA is the high peak-to-average power ratio (PAPR) that \nresults from a multicarrier signal [2]. High-PAPR transmit \nsignals require significant back-off in the power amplifier and \nthis reduces their power efficiency and mean power output. \nThis can be problematic – particularly on the uplink where \nbattery powered terminals struggle to match the data rate and \nrange of the downlink. \n3G Long-Term Evolution (LTE) is standardized by the \nThird Generation Partnership Project (3GPP) and is an \nevolution to existing 3G technology in order to meet \nprojected customer needs over the next 10-15 years. 3GPP2 is \ndeveloping CDMA2000 1x Evolution Data-Optimized \n(1xEV-DO) Revision C as its 3G long term evolution. These \nstandards, along with IEEE 802.16e (Mobile-WiMAX) and \nIEEE 802.20 (Mobile Broadband Wireless Access), provide a \nroute to 4G technology that supports high mobile data rates.  \nIn order to solve the high PAPR problem seen in the uplink \nof OFDM/OFDMA, research is now addressing techniques \nsuch as a Single-Carrier with Frequency Domain Equalization \n(SC-FDE). It was proposed in [3] to use OFDM on the \ndownlink and SC-FDE on the uplink. Similarly, the 3G LTE \nstandard uses OFDMA on the downlink and SC-FDMA on \nthe uplink [4]. SC-FDMA has significantly lower PAPR than \nOFDMA and therefore can greatly improve the power \nefficiency, operating range or data rate of the mobile \nterminals. The base station can tolerate high power \nconsumption and greater signal processing complexity. \nInterleaved Frequency Division Multiple Access (IFDMA) \nwas proposed in [5]. This approach can be viewed as a single \ncarrier system with the ability to achieve multiple-access in \nthe frequency domain, where each user shares the spectrum \nusing a different set of equi-distant subcarriers spaced across \nthe spectrum. Each user has a unique phase vector to multiply \nwith the transmit signal so the equi-distant subcarrier \nmapping is performed in the time domain. IFDMA is very \nsimilar to distributed FDMA (DFDMA). The difference is \nthat IFDMA signals are generated in the time domain whilst \nDFDMA signals are generated in the frequency domain. \nSC-FDMA (or DFT-spread OFDM) is used in the context \nof the 3G LTE standard [4]. Here the term “spread” refers to \nsubcarrier allocation for the multiple users. This technique is \nreferred to as OFDM-FDMA with DFT spreading in [6,7]. \nOnly distributed subcarrier mapping was considered in [6,7] \nsince it results in a PAPR that is as low as conventional \nsingle-carrier systems. In the 3G LTE standard, both \ndistributed FDMA (DFDMA) and localized FDMA \n(LFDMA) are under consideration [4]. Each subcarrier \nmapping scheme has its own strengths and weaknesses. For \nexample, a FDMA system with localized subcarrier mapping \nis more robust to multiple access interference [8], but \nDFDMA has a lower PAPR. More importantly, DFDMA is \nmore sensitive to timing and frequency error. Overall, \nLFDMA still offers significant PAPR improvements \ncompared to OFDMA [9]. An overview of SC-FDMA for \nuplink transmission is given in [10]. \nPulse shaping is required for a single-carrier system to \nbandlimit the transmit signal. We extend the PAPR analysis \nfirst reported in [9] for pulse shaped SC-FDMA and further \ninvestigate the impact of resource allocation. When raised \ncosine pulse shaping is applied to DFDMA signals, the PAPR \nreduces with increasing α  and the allocation of radio \nresource does not affect the PAPR. This paper demonstrates \ntwo interesting pulse shaping effects for LFDMA signals: (1) \nan increase in PAPR with increasing α  and (2) a variation in \nPAPR as a function of resource allocation.  \nThis paper is organized as follows: Section II gives an \noverview of SC-FDMA and describes the time-domain \nsignals for DFDMA and LFDMA with varying resource unit \n(RU) allocation. Section III analyzes the simulation results of \na PAPR comparison for DFDMA and LFDMA with varying \nresource unit allocation and raised cosine pulse shaping.  \nSection IV concludes the paper. \nThe 18th Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC'07) \nEncoder\n+ MOD\nDecoder\n+ DEMOD\nN-point\nDFT\nSubcarrier\nMapping\nM-point\nIDFT\nPulse\nShaping\nDAC +\n*RF UC Channel\nADC +\n*RF DC\nM-point\nDFT\nSubcarrier\nDemapping\n*FDE\nN-point\nIDFT\n*FDE: Frequency Domain Equalization\n*RF UC: RF Up-Converter\n*RF DC: RF Down-Converter\n)( p\nnx\n)(~ p\nlX\n)(~ p\nmx\n)( p\nkX\n \n \nFig. 1: Baseband system model of SC-FDMA \nII. SC-FDMA SYSTEM \nA. Overview of SC-FDMA \nA baseband system model of SC-FDMA is shown in Fig. 1. \nThe mathematical notation used in this paper follows that \nreported in [9]. M represents the total number of available \nsubcarriers, where NQM .= . Q denotes the spreading factor \nand N represents the number of subcarriers assigned to each \nuser. We further assume that each user occupies the same \nnumber of subcarriers, so in this case Q also represents the \nnumber of users. Each user p, 1,...,0 −= Qp  , generates a \nblock of N complex-valued symbols, )( pnx , 1,...,0 −= Nn . \nBy applying an N-point DFT to )( pnx , the frequency domain \nsymbols )( pkX can be described as   \n ∑−\n=\n−\n=\n1\n0\n2\n)()(\nN\nn\nkn\nN\njp\nn\np\nk exX\nπ\n (1) \nThe frequency domain symbols are then mapped onto a set \nof user-dependent subcarriers. Fig. 2 shows an example of a \nfrequency division multiple-access scheme with distributed \nand localized subcarrier mapping. There are 16 available \nsubcarriers shared by 4 users and each user occupies 4 \nsubcarriers. Each user must map its frequency domain \nsymbols onto the assigned resource unit (RU) and zeros are \ninserted for the remaining subcarriers. In DFDMA, the RU \nfor each user is a set of interleaved subcarriers across the \navailable transmission band. DFDMA is robust against \nfrequency selective fading since it better exploits the available \nfrequency diversity [10]. In LFDMA, the RU for each user is \na set of adjacent subcarriers. LFDMA can potentially achieve \nmultiuser diversity if for each user the localized section of \nallocated bandwidth experiences high channel gain. This form \nof multiuser diversity requires independent frequency \nselective fading per user combined with intelligent radio \nresource unit allocation [10].  \nSubcarrier\nIndex(b) Localized Mode\n(a) Distributed Mode\nRU#0\nRU#1\nRU#2\nRU#3\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Subcarrier\nIndex\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n*RU#: Resource Unit Number\n \nFig. 2: Multiple access scheme of SC-FDMA:  \n(a) distributed mode, (b) localized mode.  \nAn example of M = 16, Q = 4 and N = 4.  \nThe frequency domain samples )(~ plX  after the subcarrier \nmapping process (distributed and localized) are represented \nby equations (2) and (3) respectively \n \n \n\n +==\n=\n−\notherwise ,                         0\n ,  ~ )( /)(\n)(\n)( pQklXXX\np\nQpl\np\nkp\nl  (2) \n \n\n +==\n=\n−\notherwise ,                       0\n ,   ~ )()()( NpklXXX\np\nNpl\np\nkp\nl  (3) \nwhere l represents the subcarrier index going into the M-point \nIDFT, 1,...,0 −= Ml  and p is the resource unit number \n(RU#). It should be noted that p can also denote the user \nindex, i.e. 1,...,0 −= Qp . After subcarrier mapping, the \nsampling period T~ is reduced, i.e. TMNT )./(~ = , where T is \nuser input symbol period. \nThe frequency domain samples )(~ plX  are transformed back \ninto the time domain by an M-point IDFT. The output time \ndomain samples of SC-FDMA before the application of pulse \nshaping can generally be described as \n ∑−\n=\n=\n1\n0\n2\n)()( ~1~ M\nl\nml\nM\njp\nl\np\nm eXM\nx\nπ\n (4) \nB. Time-Domain Signal of DFDMA \nThe derivation of time domain signals for SC-FDMA in [9] is \nvalid when RU#0 is used (see Fig. 2). However, the phase \nrotations within the SC-FDMA signal are related to the RU \nbeing used. Here the RU allocation is taken into account and \nthe method described in [9] is used to derive the SC-FDMA \ntime domain signals. Therefore the DFDMA time domain \nsamples can be obtained by substituting (2) into (1). Let \niNqm += , where 1,...,0 −= Qq  and 1,...,0 −= Ni . The \ntime domain samples for DFDMA can be described as \nThe 18th Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC'07) \n ( ) impMjiNqm xeQxx ..\n1~~\n2π\n== +  (5) \nwhere i = n, which allows ix  to be written as nx .  \nThe resulting DFDMA samples mx\n~  are a repetition of the \nuser input symbols nx , with a phase rotation based on the \nparticular allocation of subcarriers (i.e. RU#). The repetition \nis determined by the spreading factor, Q. Equation (5) shows \nthat time domain samples are “localized” when the frequency \ndomain samples are interleaved. One simple way to \nunderstand the resulting time domain samples in a DFDMA \nsystem is to exploit the duality between time and frequency. \nUpsampling a time domain signal by inserting zeros between \nthe input samples results in frequency spectrum repetition. \nHence, by exploiting duality, an interleaved subcarrier \nmapping scheme should result in sequence repetition of the \ntime domain signal. \nUsing RU#0 as a reference, the other RU# can be seen as \nfrequency shifted versions of RU#0, as shown in Fig. 2(a). \nHence different RU# give different phase rotations in the \nDFDMA time domain samples, as shown in (5). \nC. Time-Domain Signal of LFDMA \nThe time domain samples for LFDMA can be derived by \nsubstituting (3) into (1). Let qQim += , where 1,...,0 −= Ni  \nand 1,...,0 −= Qq . If 0=q , the LFDMA time domain \nsamples can be described as \n ( ) iqQim xQxx .\n1~~\n== +  (6) \nwhere i = n, which again allows ix  to be written as nx . If \n0≠q , the LFDMA time domain samples are \n \n( )\n∑−\n= \n\n\n+−\n+\n\n\n\n\n\n\n−\n\n\n\n\n−=\n=\n1\n0 )(\n2\n22\n1\n1.1..1     \n~~\nN\nn\nQ\nqni\nN\nj\nn\nq\nQ\njqp\nQ\nj\nqQim\ne\nx\nN\nee\nQ\nxx\nπ\nππ  (7) \nFrom (6), the LFDMA time domain samples mx\n~  have exact \ncopies of the input symbols nx  at Q-multiple sample \npositions, which are independent of the RU# being used. It \nalso shows that time domain samples are “interleaved” when \nthe frequency domain samples are localized. The in-between \nsamples of the LFDMA signal are shown in (7). \nLocalized subcarrier mapping can be viewed as frequency \ndomain over-sampling. Frequency domain over-sampling can \nbe performed by padding zeros onto unused subcarriers. This  \nresults in the interpolation of the input time-domain samples. \nSimilarly, the LFDMA time domain samples mx\n~  are \neffectively an interpolated version of the input symbols nx . \nGiven this situation it is possible for the resulting waveform \nto produce a high output peak. As a result, LFDMA has a \nhigher PAPR than DFDMA without pulse shaping [9].   \nThe summation term in (7) indicates that the in-between \nsamples are strongly correlated with their adjacent input \nsymbols nx . The first exponential term in (7) represents the \nphase rotation in the LFDMA time domain samples according \nto the chosen RU, p. This occurs since the other RU# are \nfrequency shifted versions of RU#0, as shown in Fig. 2(b). It \nshould be noted that phase rotation could make the correlated \nLFDMA signals more in-phase or out-of-phase depending on \nthe RU#.  \nD. Pulse Shaping and PAPR \nIn a single-carrier system, pulse shaping is required to \nbandlimit the signal and ensures it meets the spectrum mask. \nGenerally speaking, there is a trade-off between spectrum \nefficiency and PAPR reduction in conventional single-carrier \nsystems [4], but this is not necessarily the case in LFDMA. In \nthis paper, a raised cosine filter is used to pulse shape the SC-\nFDMA signals.  \nPAPR is a measure of the peak-to-average power ratio \nquoted in dB. A transmit signal with high PAPR requires \nlarge back-off to ensure the power amplifier operates within \nits linear region. This reduces the power efficiency of the \namplifier and results in a lower mean output power for a \ngiven peak power rated device. Hence, a transmit signal with \nlow PAPR is highly desirable for battery powered terminals, \nwhere power efficiency is paramount. After pulse shaping, the \nPAPR of the transmit signal, x(t), can be calculated using the \nfollowing equation, where }{⋅E  denotes the expected value.  \n \n}|)({|\n}|)(max{|\n2\n2\ntxE\ntxPAPR = . (8) \nIII. RESULTS \nThe effects of pulse shaping on the PAPR of SC-FDMA \nsignals with different RU# are investigated in this section. In \nthe simulation, the total number of subcarriers M is set to be \n512 and each user has access to 64 subcarriers (N = 64) with a \nspreading factor Q of 8. The baseband modulation is QPSK \nand 100,000 input data blocks are used to calculate the CCDF \n(Complementary Cumulative Distributed Function) of the \nPAPR. Time-domain pulse shaping is performed using a \nraised cosine filter with its impulse response truncated from \nT~6−  to T~6 , where T~  represents the time period of the SC-\nFDMA time-domain samples, mx\n~ . The power of the raised \ncosine filter is normalized to unity for all α . SC-FDMA \nsignals are over-sampled by 8 times for pulse shaping.  \nFig. 3 shows the effects of pulse shaping on the CCDF of \nthe PAPR for DFDMA and LFDMA with RU#0, RU#1, \nRU#2 and RU#3 respectively. Fig. 3(a) with RU#0 is \nconsistent with the PAPR simulations in Fig. 7(a) of [9]. For \nDFDMA, the convolution of random samples with the raised \ncosine filter gives a smaller output peak power with \nincreasing α . The output mean power is always the same \nwhen filtering random samples with different α , and hence \nthe PAPR decreases as α  increases. The DFDMA time \ndomain samples remain random with a phase rotation that\nThe 18th Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC'07) \n \n0 1 2 3 4 5 6 7 8 9 10\n10-4\n10-3\n10-2\n10-1\n100\nPAPR0 / dB\nPr\n(P\nAP\nR>\nPA\nPR\n0)\nCCDF of PAPR for DFDMA and LFDMA with RC pulse shaping filter, RU#0\n \n \nro=0\nro=0.2\nro=0.4\nro=0.6\nro=0.8\nro=1\n \n(a) RU#0 \n \n0 2 4 6 8 10 12\n10-4\n10-3\n10-2\n10-1\n100\nPAPR0 / dB\nPr\n(P\nAP\nR>\nPA\nPR\n0)\nCCDF of PAPR for DFDMA and LFDMA with RC pulse shaping filter, RU#2\n \n \nro=0\nro=0.2\nro=0.4\nro=0.6\nro=0.8\nro=1\n \n(c) RU#2 \n0 1 2 3 4 5 6 7 8 9 10\n10-4\n10-3\n10-2\n10-1\n100\nPAPR0 / dB\nPr\n(P\nAP\nR>\nPA\nPR\n0)\nCCDF of PAPR for DFDMA and LFDMA with RC pulse shaping filter, RU#1\n \n \nro=0\nro=0.2\nro=0.4\nro=0.6\nro=0.8\nro=1\n \n(b) RU#1 \n \n0 2 4 6 8 10 12\n10-4\n10-3\n10-2\n10-1\n100\nPAPR0 / dB\nPr\n(P\nAP\nR>\nPA\nPR\n0)\nCCDF of PAPR for DFDMA and LFDMA with RC pulse shaping filter, RU#3\n \n \nro=0\nro=0.2\nro=0.4\nro=0.6\nro=0.8\nro=1\n \n(d) RU#3 \n \nFig. 3: CCDF of PAPR for DFDMA (dashed line) and LFDMA (solid line) signals using  \nraised cosine (RC) pulse shaping filter with α = 0, 0.2, 0.4, 0.6, 0.8 and 1. *ro = rolloff factor,α . \n \nTable 1: Phase rotation of LFDMA in-between samples vs. RU#, *SF: Spreading Factor \n \nPhase rotation RU#0 RU#1 RU#2 RU#3 RU#4 RU#5 RU#6 RU#7 \nSF (deg) 22.5 22.5 22.5 22.5 22.5 22.5 22.5 22.5 \nSF and RU# (deg) 0 45 90 135 180 -135 -90 -45 \nTotal phase rotation (deg) 22.5 67.5 112.5 157.5 -157.5 -112.5 -67.5 -22.5 \nrelates to the choice of RU. As a results, the PAPR of a \nDFDMA signal does not change with the choice of RU, as \nshown in Fig. 3. \nFor pulse shaped LFDMA signals, the PAPR of RU#0, \nRU#1, RU#2 and RU#3 are the same as the PAPR of RU#7, \nRU#6, RU#5 and RU#4 respectively. Given this relationship, \nFig. 3 is limited to showing the CCDF of the PAPR from \nRU#0 to RU#3. The link between the PAPR and the RU# \narises after pulse shaping because of the phase rotation \nintroduced into the LFDMA in-between samples. Table 1 \nillustrates that the magnitude of the phase rotation of the \nLFDMA in-between samples is identical for the RU pairs \nwith matching PAPR. It should be noted that the phase \nchange is inverted between each of these matching pairs. The \nsecond row of Table 1 shows the phase rotation due to the \nspreading factor (SF) Q, which is generated from \nq\nQ\nj\ne\nπ2\n1−  in \n(7). The third row shows the phase rotation due to the SF, Q \nand the RU#, p, i.e. the \nqp\nQ\nj\ne\nπ2\n term in (7). Since the total \nphase rotation of the LFDMA signals are effectively the \nsame, it follows that the PAPR will be identical. \nThe phase rotation makes the LFDMA samples combine \nthrough the pulse shaping filter to be in-phase or out-of-\nphase, which results in the observed variation of output mean \npower. Fig. 4 shows the ratio of the output mean power to the \ninput mean power (defined relative to the pulse shaping \nblock) for LFDMA signals with different RU. For RU#0 the \nLFDMA signal results in a strong in-phase combination \n(given the phase rotation of °5.22 ) after pulse shaping, so the\nThe 18th Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC'07) \n \n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\nRolloff factor\nRa\ntio\n \no\nf o\nut\npu\nt m\nea\nn \npo\nwe\nr \nto\n \nin\npu\nt m\nea\nn \npo\nwe\nr \n/ d\nB\n \n \nRU#0\nRU#1\nRU#2\nRU#3\n \nFig. 4: Ratio of output mean power to input mean power of \nLFDMA signal with raised cosine pulse shaping \n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nRolloff factor\nPA\nPR\n \nat\n \nCC\nDF\n \nof\n \n0.\n01\n-p\ner\nce\nnt\nile\n \n/ d\nB\n \n \nRU#0\nRU#1\nRU#2\nRU#3\n \nFig. 5: Comparison of CCDF of PAPR at 0.01-percentile for \nDFDMA (dashed line) and LFDMA (solid line) signals with \nraised cosine pulse shaping \n \noutput mean power increases. For RU#3, the phase of the \nLFDMA samples are strongly out-of-phase (phase rotation \nof °5.157 ) and hence the output mean power decreases. For \nthe same α , the output mean power shows the following \ntrend RU#0 > RU#1 > RU#2 > RU#3, as shown in Fig. 4. \nDue to the phase rotation described earlier, it is unlikely \nthat the LFDMA samples will all add up constructively to \nproduce a high output peak after pulse shaping. From the \nsimulations, the output peak often occurs when the input peak \nsample convolves with the peak RC filter tap. Since the raised \ncosine filter power is normalized to unity, the peak power of \nthe raised cosine filter tap becomes larger with larger α . This \nimplies that after pulse shaping the output peak power of the \nLFDMA signal increases with increasing α .  \nFig. 5 shows a comparison of the CCDF of the PAPR at the \n0.01-percentile for DFDMA and LFDMA signals with raised \ncosine pulse shaping. This was produced using the simulation \nresults shown in Fig. 3. It is shown that DFDMA has a lower \nPAPR than LFDMA when pulse shaping is applied. This is \nalso true without pulse shaping, as mentioned in Section II. \nFor DFDMA, the PAPR is the same for all RU# and the \nPAPR decreases with increasing α . For LFDMA with RU#0, \nboth the output peak power and the output mean power \nincrease after pulse shaping. Overall, for RU#0 the PAPR \nbarely changes with α . However, when other RU# are \nselected the resulting PAPR is dominated by the variation in \nthe output mean power after pulse shaping. The PAPR \nincreases when the output mean power decreases (see Fig. 4 \nand Fig. 5) and as such the PAPR of an LFDMA signal tends \nto increase with increasing α . \nIV. CONCLUSIONS \nIn this paper, the effects of pulse shaping and the choice of \nRU on the PAPR of SC-FDMA were investigated. SC-FDMA \nis attractive for uplink transmissions since it reduces the high \nPAPR seen with OFDM/OFDMA. Pulse shaped DFDMA \nexperiences a lower PAPR with increasing α . Furthermore, \nthe PAPR does not vary with the choice of RU. Pulse shaped \nLFDMA tends to result in an increased PAPR with increasing \nα . The PAPR was also shown to vary with the choice of RU. \nFor LFDMA, the time-domain interpolation effect and the \nphase rotations that result from the choice of RU result in \nvariations in the output mean power after the application of \npulse shaping. Hence the resource allocation scheme must \ntake this into account to avoid saturating the RF devices. \nOverall, in terms of PAPR, DFDMA is shown to be better \nthan LFDMA (with and without pulse shaping). However, in \npractice it is likely that LFDMA will be easier to implement.  \n \nREFERENCES \n[1] L. Cimini Jr., “Analysis and Simulation of a Digital Mobile Channel \nUsing Orthogonal Frequency Division Multiplexing,” IEEE Trans. on \nCommun., vol. 33, no. 7, pp. 400-411, July 1985. \n[2] R. van Nee and R. Prasad, “OFDM for Wireless Multimedia \nCommunications,” Artech House, 2000. \n[3] D. Falconer, S. L. Ariyavisitakul, A. Benyamin-Seeyar and B. Eidson, \n“Frequency Domain Equalization for Single-Carrier Broadband \nWireless Systems,” IEEE Commun. Mag., vol. 40, no. 4, pp. 58-66, \nApril 2002. \n[4] 3rd Generation Partnership Project (3GPP), “Technical Specification \nGroup Radio Access Network; Physical Layer Aspects for Evolved \nUTRA,” http://www.3gpp.org/ftp/Specs/html-info/25814.htm, Sep 2006. \n[5] U. Sorger, I. De Broeck and M. Schnell, “Interleaved FDMA – A New \nSpread-Spectrum Multiple-Access Scheme,” in Proc. of the IEEE ICC, \nvol. 2, pp. 1013-1017, June 1998. \n[6] D. Galda and H. Rohling, “A Low-Complexity Transmitter Structure \nfor OFDM-FDMA Uplink Systems,” in Proc. of the IEEE VTC, vol. 4, \npp. 1737-1741, May 2002. \n[7] R. Dinis, D. Falconer, C. T. Lam and M. Sabbaghian, “A Multiple \nAccess Scheme for the Uplink of Broadband Wireless Systems,” in \nProc. of the IEEE Globecom, vol. 6, pp. 3803-3812, Nov. 2004. \n[8] A. M. Tonello, N. Laurenti and S. Pupolin, “Analysis of the Uplink of \nan Asynchronous Multi-user DMT OFDMA System Impaired by Time \nOffsets, Frequency Offsets and Multi-path Fading,” in Proc. of the \nIEEE VTC, vol. 3, pp. 1094-1099, Oct. 2000. \n[9] H. G.  Myung, J. Lim and D. J. Goodman, “Peak-to-Average Power \nRatio of Single Carrier FDMA Signals with Pulse Shaping,” in Proc. of \nthe IEEE PIMRC, pp. 1-5, Sep. 2006. \n[10] H. G.  Myung, J. Lim and D. J. Goodman, “Single Carrier FDMA for \nUplink Wireless Transmission,” IEEE Vehicular Technology Mag., vol. \n1, pp.30-38, Sep. 2006. \n",
            "id": 17386304,
            "identifiers": [
                {
                    "identifier": "208007092",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "29025651",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2146107018",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "oai:research-information.bris.ac.uk:publications/19b188ee-e9ba-4301-bf02-ddfe7f49e83d",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "oai:research-information.bris.ac.uk:openaire_cris_publications/19b188ee-e9ba-4301-bf02-ddfe7f49e83d",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "10.1109/pimrc.2007.4394297",
                    "type": "DOI"
                },
                {
                    "identifier": "385681715",
                    "type": "CORE_ID"
                }
            ],
            "title": "Impact of radio resource allocation and pulse shaping on PAPR of SC-FDMA signals",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2146107018",
            "oaiIds": [
                "oai:research-information.bris.ac.uk:publications/19b188ee-e9ba-4301-bf02-ddfe7f49e83d",
                "oai:research-information.bris.ac.uk:openaire_cris_publications/19b188ee-e9ba-4301-bf02-ddfe7f49e83d"
            ],
            "publishedDate": "2007-01-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 36629630,
                    "title": "3rd Generation Partnership Project (3GPP), “Technical Specification Group Radio Access Network; Physical Layer Aspects for Evolved UTRA,” http://www.3gpp.org/ftp/Specs/html-info/25814.htm,",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "3rd Generation Partnership Project (3GPP), “Technical Specification Group Radio Access Network; Physical Layer Aspects for Evolved UTRA,” http://www.3gpp.org/ftp/Specs/html-info/25814.htm, Sep 2006.",
                    "cites": null
                },
                {
                    "id": 36629632,
                    "title": "A Low-Complexity Transmitter Structure for OFDM-FDMA Uplink Systems,”",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1109/vtc.2002.1002918",
                    "raw": "D. Galda and H. Rohling, “A Low-Complexity Transmitter Structure for OFDM-FDMA Uplink Systems,” in Proc. of the IEEE VTC, vol. 4, pp. 1737-1741, May 2002.",
                    "cites": null
                },
                {
                    "id": 36629633,
                    "title": "A Multiple Access Scheme for the Uplink of Broadband Wireless Systems,”",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1109/glocom.2004.1379081",
                    "raw": "R. Dinis, D. Falconer, C. T. Lam and M. Sabbaghian, “A Multiple Access Scheme for the Uplink of Broadband Wireless Systems,” in Proc. of the IEEE Globecom, vol. 6, pp. 3803-3812, Nov. 2004.",
                    "cites": null
                },
                {
                    "id": 36629627,
                    "title": "Analysis and Simulation of a Digital Mobile Channel Using Orthogonal Frequency Division Multiplexing,”",
                    "authors": [],
                    "date": "1985",
                    "doi": "10.1109/tcom.1985.1096357",
                    "raw": "L. Cimini Jr., “Analysis and Simulation of a Digital Mobile Channel Using Orthogonal Frequency Division Multiplexing,” IEEE Trans. on Commun., vol. 33, no. 7, pp. 400-411, July 1985.",
                    "cites": null
                },
                {
                    "id": 36629634,
                    "title": "Analysis of the Uplink of an Asynchronous Multi-user DMT OFDMA System Impaired by Time Offsets, Frequency Offsets and Multi-path Fading,”",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1109/vetecf.2000.886275",
                    "raw": "A. M. Tonello, N. Laurenti and S. Pupolin, “Analysis of the Uplink of an Asynchronous Multi-user DMT OFDMA System Impaired by Time Offsets, Frequency Offsets and Multi-path Fading,” in Proc. of the IEEE VTC, vol. 3, pp. 1094-1099, Oct. 2000.",
                    "cites": null
                },
                {
                    "id": 36629629,
                    "title": "Frequency Domain Equalization for Single-Carrier Broadband Wireless Systems,”",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1109/35.995852",
                    "raw": "D. Falconer, S. L. Ariyavisitakul, A. Benyamin-Seeyar and B. Eidson, “Frequency Domain Equalization for Single-Carrier Broadband Wireless Systems,” IEEE Commun. Mag., vol. 40, no. 4, pp. 58-66, April 2002.",
                    "cites": null
                },
                {
                    "id": 36629631,
                    "title": "Interleaved FDMA – A New Spread-Spectrum Multiple-Access Scheme,”",
                    "authors": [],
                    "date": "1998",
                    "doi": "10.1007/978-1-4615-6231-3_13",
                    "raw": "U. Sorger, I. De Broeck and M. Schnell, “Interleaved FDMA – A New Spread-Spectrum Multiple-Access Scheme,” in Proc. of the IEEE ICC, vol. 2, pp. 1013-1017, June 1998.",
                    "cites": null
                },
                {
                    "id": 36629628,
                    "title": "OFDM for Wireless Multimedia Communications,” Artech House,",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "R. van Nee and R. Prasad, “OFDM for Wireless Multimedia Communications,” Artech House, 2000.",
                    "cites": null
                },
                {
                    "id": 36629635,
                    "title": "Peak-to-Average Power Ratio of Single Carrier FDMA Signals with Pulse Shaping,”",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/pimrc.2006.254407",
                    "raw": "H. G.  Myung, J. Lim and D. J. Goodman, “Peak-to-Average Power Ratio of Single Carrier FDMA Signals with Pulse Shaping,” in Proc. of the IEEE PIMRC, pp. 1-5, Sep. 2006.",
                    "cites": null
                },
                {
                    "id": 36629636,
                    "title": "Single Carrier FDMA for Uplink Wireless Transmission,”",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/mvt.2006.307304",
                    "raw": "H. G.  Myung, J. Lim and D. J. Goodman, “Single Carrier FDMA for Uplink Wireless Transmission,” IEEE Vehicular Technology Mag., vol. 1, pp.30-38, Sep. 2006.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "https://research-information.bris.ac.uk/files/3008029/huang_IEEE_PIMRC_2007.pdf"
            ],
            "updatedDate": "2021-10-13T19:23:11",
            "yearPublished": 2007,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/29025651.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/29025651"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/29025651/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/29025651/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/17386304"
                }
            ]
        },
        {
            "acceptedDate": "2010-04-20T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Armour, SMD"
                },
                {
                    "name": "Beh, KC"
                },
                {
                    "name": "Doufexi, A"
                }
            ],
            "contributors": [
                "Kian Chung"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/29025883",
                "https://api.core.ac.uk/v3/outputs/282825543"
            ],
            "createdDate": "2015-02-17T16:05:16",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 286,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/286",
                    "logo": "https://api.core.ac.uk/data-providers/286/logo"
                }
            ],
            "depositedDate": "2009-09-01T00:00:00",
            "abstract": null,
            "documentType": "research",
            "doi": "10.1109/pimrc.2009.5450347",
            "downloadUrl": "https://core.ac.uk/download/29025883.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "                          Beh, K. C., Doufexi, A., & Armour, S. M. D. (2009). On the performance of\nSU-MIMO and MU-MIMO in 3GPP LTE downlink. In IEEE 20th\nInternational Symposium on Personal, Indoor and Mobile Radio\nCommunications, 2009 (PIMRC 2009), Tokyo, Japan. (pp. 1482 - 1486).\nInstitute of Electrical and Electronics Engineers (IEEE).\n10.1109/PIMRC.2009.5450347\nLink to published version (if available):\n10.1109/PIMRC.2009.5450347\nLink to publication record in Explore Bristol Research\nPDF-document\nUniversity of Bristol - Explore Bristol Research\nGeneral rights\nThis document is made available in accordance with publisher policies. Please cite only the published\nversion using the reference above. Full terms of use are available:\nhttp://www.bristol.ac.uk/pure/about/ebr-terms.html\nTake down policy\nExplore Bristol Research is a digital archive and the intention is that deposited content should not be\nremoved. However, if you believe that this version of the work breaches copyright law please contact\nopen-access@bristol.ac.uk and include the following information in your message:\n• Your contact details\n• Bibliographic details for the item, including a URL\n• An outline of the nature of the complaint\nOn receipt of your message the Open Access Team will immediately investigate your claim, make an\ninitial judgement of the validity of the claim and, where appropriate, withdraw the item in question\nfrom public view.\nOn the Performance of SU-MIMO and  \nMU-MIMO in 3GPP LTE Downlink \n \nKian Chung Beh, Angela Doufexi, Simon Armour \nCentre for Communication Research, University of Bristol, Woodland Road, Bristol, BS8 1UB, U.K \n \n \nAbstract - LTE (Long Term Evolution) is a next major step in \nmobile radio communications, and will be introduced as Release \n8 in the 3rd Generation Partnership Project (3GPP). The new \nevolution aims to reduce delays, improve spectrum flexibility \nand reduce cost for operators and end users [1]. To fulfil these \ntargets, new enabling technologies need to be integrated into the \ncurrent 3G radio network architectures. Multiple Input and \nMultiple Output (MIMO) is one of the crucial enabling \ntechnologies in the LTE system particularly in the downlink to \nachieve the required peak data rate. The unitary codebook \nbased precoding technique is proposed in the standard to \nincrease the capacity of the system. This paper presents a link \nlevel analysis of the LTE downlink and an investigation of the \nperformance of both Single User (SU) MIMO and Multi User \n(MU) MIMO with codebook based unitary precoding. \nI. INTRODUCTION \nMultiple-Input Multiple-Output (MIMO) communication \ntechniques have been studied extensively in the recent past. \nThere are two popular techniques which have drawn much \nattention due to their promising capability to increase the \nspectral efficiency and reliability. One of the techniques is \nspace-time block coding (STBC) [2], which is able to \nachieve full transmit diversity and enable reliable \ncommunication. Thus in LTE, an Alamouti based Space-\nFrequency Block Coding (SFBC) technique is proposed in \nthe standard. However the transmit diversity based method \ndoes not provide a linearly increasing channel capacity as the \nnumber of transmit and receive element grows \nsimultaneously. Another technique that is also proposed in \nthe LTE standard is spatial multiplexing (SM) or Vertical \nBell Labs Layered Space-Time (V-BLAST) [3] which aims \nto increase the ultimate spectral efficiency. However, this \ntechnique is limited by the transmission environment and \nhighly dependent on the channel characteristics, which are \ndetermined by antenna configuration and richness of \nscattering. The performance degrades severely when the \nspatial channel correlation is high, e.g. in a line of sight \n(LOS) scenario. \nSM employs multiple antennas at the transmitter and the \nreceiver to provide simultaneous transmission of multiple \nparallel data streams over a single radio link. According to \nthe latest LTE specification [4], multi-antenna transmission \nwith 2 and 4 transmit antennas are supported. The most \ncommon configuration is expected to be 2x2 (particularly in \nearlier systems) and thus only this configuration is \nconsidered in this paper. Spatial multiplexing of multiple \nmodulation symbol streams to a single user equipment (UE) \nusing a same time-frequency resource is referred to as \nSingle-User MIMO (SU-MIMO). However, additional \ndiversity can be exploited in the spatial domain besides the \ndiversity in the time and frequency domains. Scheduling \ndifferent UEs on different spatial streams over the same time \nfrequency resource is referred to as MU-MIMO and can give \nmore flexibility to the scheduler. MU-MIMO can also be \nreferred to as Spatial Division Multiple Access (SDMA) and \nis expected to achieve the most overall system performance \ngain. \nOne of the key improvements of the LTE spectral efficiency \nis through the use of a codebook based unitary precoding, \nalso known as per user unitary and rate control (PU2RC). \nPrecoding is essentially a generalized beamforming scheme \nwhere the multiple streams of the signals are emitted from \nthe transmit antennas with independent and appropriate \nweighting so as to increase the link throughput at the receiver \noutput. Unitary precoding is able to suppress the co-channel \ninterference (CCI) efficiently through orthogonal precoding \nvectors and increase the MU-MIMO capacity even with \nlimited feedback. Details of the precoding scheme in LTE \nwill be given in the next section. Though MU-MIMO offers \ngreater flexibility in the spatial domain, it requires additional \nsignalling overhead for different spatial layers and for the \npreferred precoding matrix. The complexity of resource \nallocation at the base station is also inevitably increased.  \nThe rest of this paper is organized as follows. The codebook \nbased precoding technique is described in Section II. In \nSection III, the system and channel model will be presented. \nSimulation results are presented and discussed in Section IV. \nSection V concludes the paper. \nII. UNITARY CODEBOOK BASED PRECODING \nThe purpose of the precoding is to optimize the transmissions \nto the characteristics of the radio channel so that when the \nsignals are received, they can be more easily separated back \ninto the original data streams. When used appropriately, \nprecoding can achieve significant spectral efficiency \nimprovement and many precoding methods been proposed in \nthe literature. Non-linear methods such as Dirty paper coding \n(DPC) [5] can achieve the optimal performance but \ndeployment of DPC in real-time is infeasible due to high \ncomplexity. Some linear precoding methods such as channel \ninversion method [6], Block Diagonalization [7] (BD) and \ncodebook based precoding [8] have also been proposed. In \nparticular, the codebook based precoding method has \nreceived considerable attention recently as this linear \nprecoding method has been adopted in the LTE specification \ndue to its practicality and simplicity.  \n \nFigure 1: Configuration of MU-MIMO System 978-1-4244-5213-4/09/ $26.00 ©2009 IEEE 1482\nIn this codebook based scheme, a UE only needs to find out a \nmost suitable matrix (e.g. capacity maximising) from the \ncodebook and feedback the corresponding index to the base \nstation (BS). Thus, this scheme keeps the overhead and \nsystem complexity at a reasonable level but with \nconsiderable improvements in error performance. The \nconfiguration of the simulated MU-MIMO with precoding is \nshown in Figure 1.  \nOne of the requirements for the pre-coder is that it must be \nunitary and orthogonal. The proposed unitary pre-coder for \nLTE is the Fourier basis pre-coder given in [8]. According to \n[9], only the codebook size of 2 is currently supported. As a \nreference for comparison purposes, a linear precoding matrix \nobtained by using the singular value decomposition method \n(SVD) is also considered. SVD is optimal in terms of error \nperformance but has higher complexity and requires higher \noverhead. \nA MIMO system with Nt transmit antennas and Nr receiver \nantennas is considered. Precoding is performed at every \nOFDMA sub-band or physical resource block (PRB). \nAssuming perfect timing and synchronization, the received \nsignal at the UE for the kth PRB can be represented by:  \n)()()()()( kNkXkEkHkY +=    (1) \nwhere H(k) is the complex channel between the transmitter \nand receiver antennas, E(k) is the precoding matrix, X(k) is \nthe transmit vector and N(k) is the additive white Gaussian \nnoise which can be modelled as independent and distributed \naccording to CN(0,N0). In MIMO detection, a linear receiver \nis designed to detect the transmitted data. Zero Forcing (ZF) \nor Minimum Mean Squared Error (MMSE) detection \ncriterions are often used. In order to obtain a good \nperformance with reasonable complexity, a linear MMSE \nreceiver is adopted at the UE in this paper. The linear MMSE \nreceivers can be obtained from [10]: \n)'()'()])/()()()'()'([ 10)( kHkEINMkEkHkHkEG Ms\nMMSE\nk\n−\n+= ε    (2)  \nwhere M is the number of data streams, εs is the total transmit \nenergy. The received signal Y(k) is then multiplied by G(k) to \nobtain the detected data stream, )(ˆ kS  for the kth PRB. \n)(ˆ)(ˆ\n)(*)()(ˆ\nkNkX\nkYkGkS\n+=\n=     (3) \nFor a 2x2 SM system, the MIMO channels have two \nsubspaces that can be considered as 2 data streams \ntransmitting through 2 parallel sub-channels.  For data stream \nm at every PRB, the UE j computes the effective SINR for \nevery data stream. The SINR for each data stream can be \ncalculated from [10]: \n1\n])/()()()'()'([ 1\n)(\n−\n+\n=\n−\nMsOo\nsMMSE\nm\nIMNkEkHkHkEMN\nSINR\nε\nε  (4) \nIn the case of SU-MIMO SM, both the spatial streams will be \nallocated to the same UE. Thus in this work, the allocation is \nproposed to be based on the sum of achievable capacity of \nboth spatial streams. The achievable data rate for PRB k is \ngiven by: \n∑ +=\nrN\nm\nmk SINRr )1(log2\n    (5) \nThe scheduler then uses this feedback information to allocate \nthe PRB to the UE with the highest achievable data rate rk. \nSince each of the spatial streams can be allocated and \nscheduled independently in MU-MIMO 2x2 SM, the UE j \ncalculates the capacity data rate of each spatial layer and \nfeeds that back to the BS. The data rate is calculated on a \nPRB basis by using: \n )1(log2 m\nm\nk SINRr +=     (6) \nAgain, for every PRB, the scheduler allocates each spatial \nlayer to different UE(s), where channel conditions of the \ncorresponding layer are the best. In the case of SISO and \nSFBC, the resource allocation is based on the channel gain, \nwhere the detailed description is well known and hence \nomitted here due to limited space. \nIn order to maximize the capacity of a precoded system, the \nmost suitable precoding matrix needs to be based on the \nfeedback from all users to transmit on each PRB. Two \nfeedback strategies, namely full feedback scheme and partial \nfeedback scheme are considered and compared. In the full \nfeedback scheme, a UE feeds back a channel quality \nindicator (CQI) value for every matrix in the codebook, \nwhich gives more flexibility and accurate CQI information \nfor scheduling. In the partial feedback scheme, the UE only \nfeeds back a CQI value for the preferred matrix. In the UE, \nthe preferred precoding matrix for a PRB is chosen by \nselecting the highest average SINR that is perceived by the \nuser. Based on the feedback, the scheduler at BS chooses the \nprecoding matrix with the highest sum capacity and apply to \nthe PRB. In the full feedback scheme, when a precoding \nmatrix for the PRB is chosen, the corresponding SINR can be \nfed into the scheduler which provides a more accurate CQI \ninformation than the partial feedback scheme. Users with the \nhighest SINR for each stream will be selected and the \nselected users will then be precoded to share the same time \nand frequency resources to maximize the system capacity. In \nthe case of MU-MIMO, the amount of feedback increases by \nM times compared to SU-MIMO, depending on the number \nof spatial layers. In the full feedback scheme, the amount of \nfeedback is further increased by G folds, where G depends \non the size of the codebook. In practice, a partial feedback \nstrategy will be used where only the CQI value of the best \nprecoding matrix is fed back. \nIII SYSTEM AND CHANNELMODEL \nThe performance analysis is performed in the downlink of a \n3GPP LTE OFDMA system. In the LTE, the total bandwidth \nin a system is divided into sub-channels, denoted as physical \nresource blocks (PRBs). Resources are allocated per PRB \nrather than individual subcarrier. In this paper, a 10MHz \nsystem bandwidth is assumed. The key parameters of the \nLTE OFDMA downlink system assumed are given Table 11. \nThere are 50 PRBs in the 10MHz system, each consisting of \n12 neighbouring sub-carriers. The sub-carrier bandwidth is \n15 kHz and the PRB bandwidth is 180kHz. To feedback all \nthe CQI for all the subcarriers is impractical in system design \nas this will create an enormous amount of overhead. \nTherefore, a single channel quality indicator (calculated from \nthe average quality of the 12 sub-carriers) can be fed back for \neach PRB and is assumed to be perfectly known at the BS. \nPerfect channel estimation is also assumed. A 24 bits Cyclic \nRedundancy Check (CRC) enables error detection at the \nreceiver. \n_____________________________________________________ \nNotation: T is used to denote transposition, ' to denote conjugate \ntransposition, -1 to denote matrix inversion, + to denote matrix pseudo-\ninverse and IM to denote the MxM identity matrix 1483\nIn the case where frequency domain (PRB) dynamic \nallocation is employed, 10 users are simulated in the system \nunless otherwise stated. Due to the increased computational \ncomplexity and the insignificant gain of power control in the \nfrequency domain dynamic allocation, equal power \nallocation is assumed throughout the simulation. \nIn the simulation, a channel remains the same during a \npacket transmission. The channel model used in the \nsimulation is the Spatial Channel Model Extension [11] \n(SCME) Urban Macro scenario which is specified in 3GPP \n[12]. SCME provides a reduced variability tapped delay-line \nmodel which is well suited for link level as well as system \nlevel simulation. To evaluate the performance of MIMO \nschemes in LTE, different scenarios have been considered. \nAntenna spacing at the BS with 0.5λ-spacing, 4λ-spacing and \n10λ-spacing are considered. Users with 0.5λ, 4λ and 10λ \nspacing have an average correlations of 0.9 (very high) 0.5 \n(low) and 0.1 (very low) respectively. 2000 independently \nand identically distributed (i.i.d.) channel realisations are \nconsidered in each simulation. \nTable 1: Parameters for LTE OFDMA downlink \nTransmission BW 10 MHz \nTime Slot/Sub-frame duration 0.5ms/1ms \nSub-carrier spacing 15kHz \nSampling frequency 15.36MHz (4x3.84MHz) \nFFT size 1024 \nNumber of occupied \nsub-carriers \n601 \nNumber of OFDM symbols  \nper time slot (Short/Long CP) \n7/6 \nCP length \n(µs/samples) \nShort (4.69/72)x6 \n(5.21/80)x1 \n Long (16.67/256) \nAs specified in [4], three data modulation schemes are \nsupported. These are QPSK, 16QAM and 64QAM. Six \nModulation and Coding Schemes (MCS) levels are \nconsidered in this paper, as shown in table 2. The spectral \nefficiency of MIMO schemes is slightly reduced due to \nadditional pilot overheads. \nTable 2: Modulation and Coding Schemes \nMode Modulation Coding \nRate \nData bits per  time \nslot (1x1), (2x2) \nBit Rate \n(Mbps) \n1 QPSK 1/2 4000/7600 8/15.2 \n2 QPSK 3/4 6000/11400 12/22.8 \n3 16 QAM 1/2 8000/15200 16/30.4 \n4 16 QAM 3/4 12000/22800 24/45.6 \n5 64 QAM 1/2 12000/22800 24/45.6 \n6 64 QAM 3/4 18000/34200 36/68.4 \n \nIV. SIMULATION RESULTS \n   A. LTE Downlink Link Level Simulation \nFigure 2 shows the PER performance of a SISO scenario in \nthe LTE OFDMA system for various MCS in the urban \nmacro scenario. From the figure, it can be seen that an UE \nwill be out of service when the SNR is below 0dB while the \nUE will be at the highest MCS at approximately 24dB given \nthat PER transmission target of 10% is often expected by the \noperators. It is also worth mentioning that Mode 4, i.e. \n16QAM ¾ coding rate becomes obsolete for these channel \nconditions since it is outperformed by 64QAM ½ coding rate \nover the whole SNR range and gives the same nominal data \nrate. Since MIMO is supposed to be the better choice for \nhigher spectral efficiency, SISO performance is mainly \nsimulated for comparison purposes. \nFigure 3 shows the PER performances of the MIMO 2x2 \nSFBC for the LTE OFDMA system for different MCS in the \nurban macro scenario.  From the figure, it can be seen that \nSFBC generally achieved a clear diversity gain of up to 7dB \ncompared to the SISO scenario for all transmission modes. In \nparticular, higher gain can be obtained for transmission \nmodes with higher coding rate, e.g. ¾ coding rate as more \ndiversity gain can be obtained to improve the performance of \nthe channel coding. However in the case of SFBC, the peak \ndata rate remains the same but higher throughput can be \nexpected for the same SNR compared to the SISO. To \ninvestigate the performance of SFBC in ill conditioned \nchannels, scenarios with various correlation factors are \nsimulated and presented. From Figure 4 it can be seen that \nthe performance of SFBC is not affected much by the \ncorrelation of the channels. The performance drops by \napproximately 1-2dB in highly correlated channels even for \nhigh MCS.    \n-5 0 5 10 15 20 25 30\n10\n-4\n10\n-3\n10\n-2\n10\n-1\n10\n0\nSNR(dB)\nP\nE\nR\n \n \nQPSK 1/2\nQPSK 3/4\n16QAM 1/2\n16QAM 3/4\n64QAM 1/2\n64QAM 3/4\n \nFigure 2: SISO PER Performance for Urban Macro channel \n-10 -5 0 5 10 15 20 25\n10\n-3\n10\n-2\n10\n-1\n10\n0\nSNR(dB)\nP\nE\nR\n \n \nQPSK 1/2\nQPSK 3/4\n16QAM 1/2\n16QAM 3/4\n64QAM 1/2\n64QAM 3/4\n \nFigure 3:  MIMO 2x2 SFBC PER Performance for Urban Macro channel \nwith very low correlation (0.1) \n-5 0 5 10 15 20 25\n10\n-4\n10\n-3\n10\n-2\n10\n-1\n10\n0\nSNR(dB)\nP\nE\nR\n \n \nQPSK 1/2 -Very Low Corr.\nQPSK 1/2 -Low Corr.\nQPSK 1/2 -High Corr.\n64QAM 3/4 -Low Corr.\n64QAM 3/4 -Very Low Corr.\n64QAM 3/4 -High Corr.\n \nFigure 4: MIMO 2x2 SFBC PER Performance for QPSK ½ rate and \n64QAM ¾ rate with different correlation modes \nFigure 5 shows the PER performances of the MIMO 2x2 SM \nLTE OFDMA system for various MCS in the urban macro \nscenario. Figure 5 shows that to achieve the same level of 1484\nPER performance as in the SISO case, the SM generally \nrequires slightly higher SNR for all the MCS. Nevertheless, \nin the case of 2x2 SM, data rate can be almost doubled due to \nthe simultaneous transmission of multiple parallel data \nstreams. Performance of spatial multiplexing is highly \ndependent on the channel characteristics, which is \ndetermined by antenna configuration and richness of \nscattering. Therefore, from Figure 6 it can be seen that the \nSM performance is reduced by approximately 3 dB when the \ncorrelation of the channel increases from 0.1 to 0.5. When \nthe correlation of the channel becomes very high, e.g. 0.9, \n2x2 SM becomes almost unusable, especially at high MCS. \n0 5 10 15 20 25 30 35\n10\n-4\n10\n-3\n10\n-2\n10\n-1\n10\n0\nSNR(dB)\nP\nE\nR\n \n \nQPSK 1/2\nQPSK 3/4\n16QAM 1/2\n16QAM 3/4\n64QAM 1/2\n64QAM 3/4\n \nFigure 5: MIMO 2x2 SM PER Performance for Urban Macro channel with \nvery low correlation (0.1)  \n0 5 10 15 20 25 30 35 40 45 50\n10\n-4\n10\n-3\n10\n-2\n10\n-1\n10\n0\nSNR(dB)\nP\nE\nR\n \n \nQPSK 1/2 -Very Low Corr.\nQPSK 1/2 -Low Corr.\nQPSK 1/2 -High Corr.\n64QAM 3/4 -Very Low Corr.\n64QAM 3/4 -Low Corr.\n64QAM 3/4 -High Corr.\n \nFigure 6: MIMO 2x2 SM PER Performance for QPSK ½ rate with different \ncorrelation factors \n0 2 4 6 8 10 12 14\n10\n-3\n10\n-2\n10\n-1\n10\n0\nSNR\nP\nE\nR\n \n \nSVD\nNo Precoding\nUnitary Precoding\n \nFigure 7: PER Performance of 2x2 SM with unitary precoding \nFigure 7 shows the PER performance of MIMO 2x2 SM with \nunitary precoding in comparison to the SVD and non-\nprecoded system for the QPSK ½ rate transmission mode. \nFrom the figure it can be seen that unitary precoding of size 2 \noutperforms the non-precoded system by approximately 1dB. \nSVD, on the other hand offers the best performance but full \nchannel state information is required at the base station. \nHowever in this case, no multi-user diversity is exploited. \nThat will be investigated in the following section.  \n \nB. SU-MIMO and MU-MIMO with Unitary Precoding \nIn this section, the performance with dynamic sub-channel \n(PRB) allocation in frequency domain is presented for both \nSISO and MIMO schemes. A greedy algorithm is employed \nto exploit the inherent multi-user diversity. \nFigure 8 shows that the PER performance of SU-MIMO SM \nis 4 dB better than the MIMO SM in QPSK ½ rate mode. \nMU-MIMO SM with full feedback is another 2-3 dB better \nthan the SU-MIMO SM. The significant additional gain of \nMU-MIMO SM is attributed to the ability to exploit both the \nspatial and spectral multi-user diversity gain. The full \nfeedback scheme is superior to the partial feedback scheme \nbecause of its greater flexibility and accurate CQI \ninformation when selecting PRBs.  \n-6 -4 -2 0 2 4 6 8 10 12 14\n10\n-3\n10\n-2\n10\n-1\n10\n0\nSNR (dB)\nP\nE\nR\n \n \nMIMO 2x2 SM\nSU-MIMO 2x2 SM\nMU-MIMO 2x2 SM Partial Feedback\nMU-MIMO 2x2 SM Full Feedback\n \nFigure 8: PER performance of SU-MIMO and MU-MIMO SM \n-6 -4 -2 0 2 4 6 8 10 12 14\n10\n-4\n10\n-3\n10\n-2\n10\n-1\n10\n0\nSNR(dB)\nP\nE\nR\n \n \n1 user\n2 users\n5 users\n10 users\n25 users\n \nFigure 9: PER Performance of SU-MIMO SM with different numbers of \nusers \n-6 -4 -2 0 2 4 6 8 10 12 14\n10\n-4\n10\n-3\n10\n-2\n10\n-1\n10\n0\nSNR (dB)\nP\nE\nR\n \n \n1 user\n2 users\n5 users\n10 users\n25 users\n \nFigure 10: PER Performance of MU-MIMO SM with different numbers of \nusers \nFigure 9 and Figure 10 show the PER performance of the \nSU-MIMO and MU-MIMO SM with different number of \nusers in the system respectively. As the number of users is \nincreased from 1 to 25, the PER performances of both the \nSU-MIMO and MU-MIMO SM with unitary precoding \ngradually increases as a result of richer spectral multi-user \ndiversity gains. However, in the case of MU-MIMO SM, \nmore gain can be achieved through the additional dimension 1485\nof diversity in the spatial domain. MU-MIMO SM can \ntherefore achieve similar level of diversity gain even with \nfewer users in the system.  \nC. Throughput Performance Analysis \nThe average achievable throughput for SISO and MIMO \nschemes in the urban macro scenario is presented and \ncompared in Figure 11 and Figure 12. The achievable \nthroughput is given by: Throughput = R(1-PER), where R \nand PER are the bit rate and the packet error rate for a \nspecific mode respectively. The throughput envelope is \nobtained by using ideal adaptive modulation and coding \n(AMC) based on the (throughput) optimum switching point. \nFrom Figure 11, it can be seen that a maximum spectral \nefficiency of 3.6bits/Hz/s can be achieved in a SISO scenario \nat an average SNR of 27dB. In the case of MIMO 2x2 SFBC, \nthis maximum spectral efficiency can be achieved at an \naverage SNR of approximately 19dB, an 8dB gain in \ncomparison to the SISO scenario. SISO is completely \noutperformed by SFBC across the whole SNR range. For the \nMIMO 2x2 SM, a spectral efficiency up to 6.84bits/Hz/s can \nbe achieved at an average SNR of 30dB. It can be seen that \nthe switching point between the SFBC and SM is \napproximately 20dB. However, this is only applicable to low \ncorrelated channels. For highly correlated channels, the \nperformance of the SM will degrade significantly while high \ncorrelation only has minimal effect on SFBC. Thus for \nhighly correlated channels, the switching point is shifted to \napproximately 28dB.  \nFigure 12 shows the average throughput performance of the \nLTE system where dynamic allocation in frequency domain \nis employed. In this case, low correlated channels are \nassumed. SU-SISO is again outperformed by SU-SFBC \nacross all the SNR range but with reduced margin. SM also \nachieves more diversity gain than the SFBC in terms of the \nfrequency domain dynamic allocation. The switching point \nhas been shifted to a smaller SNR value, at approximately \n8dB.   The SU-MIMO SM allows for almost doubling the \nthroughput of a SU-SISO system but only at high SNR. \nHowever when the additional spatial diversity can be \nexploited, the MU-MIMO SM can provide almost double the \nthroughput across the whole SNR range. MU-MIMO SM \nwith full feedback outperforms all other schemes except SU-\nSFBC at very low SNRs. The partial feedback scheme is \nmarginally inferior to the full feedback scheme. However, \nthese results are only applicable to the scenario where all the \nusers have very low correlated channels. MU-MIMO SM \nschemes also require additional signalling overheads in the \nuplink that will reduce the overall spectral efficiency.  \nV. CONCLUSION \nIn this paper, a thorough analysis of LTE downlink including \ncodebook based unitary precoding is presented. Simulation \nresults have shown that the performance gain of unitary \nprecoding in a conventional single user MIMO scenario is \nlimited. When spectral and multi-user diversity are exploited, \nsignificant gains can be achieved.  Additional diversity in the \nspatial domain can be achieved when the same time-\nfrequency resources are shared among different users. MU-\nMIMO SM with full feedback achieves superior performance \nthan all other schemes but at the cost of higher signalling \noverhead and scheduling complexity.  \n-5 0 5 10 15 20 25 30 35 40 45\n0\n10\n20\n30\n40\n50\n60\n70\nSNR (dB)\nA\nv\ne\nra\ng\ne\n T\nh\nro\nu\ng\nh\np\nu\nt \n(M\nb\np\ns\n)\n \n \nSISO\n2x2 SFBC Very Low Corr.\n2x2 SFBC Low Corr.\n2x2 SFBC High Corr.\n2x2 SM Very Low Corr.\n2x2 SM Low Corr.\n2x2 SM High Corr.\n \nFigure 11: Average Throughput of SISO, MIMO 2x2 SM and MIMO 2x2 \nSFBC with different correlation factors \n-5 0 5 10 15 20\n0\n10\n20\n30\n40\n50\n60\n70\nSNR (dB)\nA\nv\ne\nra\ng\ne\n T\nh\nro\nu\ng\nh\np\nu\nt \n(M\nb\np\ns\n)\n \n \nSU-SISO\nSU-MIMO 2x2 SFBC\nSU-MIMO 2x2 SM\nMU-MIMO 2x2 Sm -Full Feedback\nMU-MIMO 2x2 SM -Partial Feedback\n \nFigure 12: Average Throughput of SISO and MIMO schemes with \nfrequency domain allocation at very low correlation channels \nACKNOWLEDGEMENTS \nThis project is funded as part of the Core 4 Research Programme of the \nVirtual Centre of Excellence in Mobile & Personal Communications, Mobile \nVCE, (www.mobilevce.com), whose funding support, including that of \nEPSRC is gratefully acknowledged. \nREFERENCES \n[1]  “3GPP; Technical Specification Group Radio Access Network; \nRequirements for E-UTRA and E-UTRAN (R7)”, 3GPP TR 25.913 \nV7.3.0, March. 2006. [Online] \nAvailable:http://www.3gpp.org/ftp/Specs/html-info/25913.htm \n[2] S. Alamouti, “A simple transmit diversity technique for wireless \ncommunications”, IEEE JSAC, Vol. 16, No. 8, pp. 1451-1458, 1998 \n[3] Gerard J. Foschini, Glen D. Golden, Reinaldo A. Valenzuela, and \nPeter W. Wolniansky, “Simplified Processing for High Spectral \nEfficiency Wireless Communication Employing Multi-Element \nArrays”, IEEE Transactions on Selected Areas in Communications, \nVol. 17, No. 11, Nov 1999 \n[4] ‘Technical Specification Group Radio Access Network; (E-UTRA) \nand (E-UTRAN): Overall Description’, 3GPP TS 36.300 V8.4.0, \nMar.08 [Online]: http://www.3gpp.org/ftp/Specs/html-info/36300.htm \n[5] M.Costa, ‘Writing on Dirty Paper’, IEEE Trans. Information Theory, \nvol. 29, no. 3, May 1983, pp. 439 – 441 \n[6] T. Haustein, C.von Helmolt, E. Jorswieck, et al, ; Performance of \nMIMO systems with channel inversion’, IEEE 55th VTC, vol. 1, May \n2002, pp. 35-39 \n[7] Q. H. Spencer, A. L. Swindlehurst, and M. Haardt, ‘ Zero-forcing \nmethods for downlink spatial multiplexing in multiuser MIMO \nchannels’ IEEE Trans. Signal Processing, Vol. 52, no. 2, Feb. 2004, \npp. 461-471 \n[8] R1-051353, Samsung, “Downlink MIMO for EUTRA,” 3GPP RAN1 \n#43, Seoul, Korea, November 2005. \n[9] ‘Technical Specification Group Radio Access Network; (E-UTRA) \nand (E-UTRAN): Physical Channels and Modulation’, 3GPP TS \n36.211 V8.4.0, Sept 08. [Online]: \nhttp://www.3gpp.org/ftp/Specs/html-info/36211.htm \n[10] David J.Love, Robert W. Health, ‘ Limited Feedback Unitary \nPrecoding for Spatial Multiplexing Systems’, IEEE Transactions on \nInformation Theory, Vol. 51, No.8, August 2005 \n[11] D.S.Baum, J.Hansen, J.Salo, “An interim channel model for beyond-\n3G systems: extending the 3GPP spatial channel model (SCM)”, VTC \n2005 Spring, Volume 5, Page 3132 – 3136 \n[12] “Spatial channel model for MIMO simulations”, 3GPP TR 25.996 \nV6.1.0, Sep’03. [Online]: http://www.3gpp.org/ftp/Specs/html-\ninfo/25996.htm 1486\n",
            "id": 17386330,
            "identifiers": [
                {
                    "identifier": "2131952508",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "10.1109/pimrc.2009.5450347",
                    "type": "DOI"
                },
                {
                    "identifier": "282825543",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:research-information.bris.ac.uk:publications/1b0ccec4-04f3-409f-a76c-233f5948f758",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "29025883",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "385689075",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:research-information.bris.ac.uk:openaire_cris_publications/1b0ccec4-04f3-409f-a76c-233f5948f758",
                    "type": "OAI_ID"
                }
            ],
            "title": "On the performance of SU-MIMO and MU-MIMO in 3GPP LTE downlink",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2131952508",
            "oaiIds": [
                "oai:research-information.bris.ac.uk:publications/1b0ccec4-04f3-409f-a76c-233f5948f758",
                "oai:research-information.bris.ac.uk:openaire_cris_publications/1b0ccec4-04f3-409f-a76c-233f5948f758"
            ],
            "publishedDate": "2009-09-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 36634147,
                    "title": "3GPP; Technical Specification Group Radio Access Network; Requirements",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "“3GPP;  Technical  Specification  Group  Radio  Access  Network; Requirements for E UTRA and E UTRAN (R7)”, 3GPP TR 25.913 V7.3.0,  March.  2006.  [Online] Available:http://www.3gpp.org/ftp/Specs/html info/25913.htm",
                    "cites": null
                },
                {
                    "id": 36634148,
                    "title": "A simple transmit diversity technique for wireless communications”,",
                    "authors": [],
                    "date": "1998",
                    "doi": "10.1109/49.730453",
                    "raw": "S.  Alamouti,  “A  simple  transmit  diversity  technique  for  wireless communications”, IEEE JSAC, Vol. 16, No. 8, pp. 1451 1458, 1998",
                    "cites": null
                },
                {
                    "id": 36634156,
                    "title": "An interim channel model for beyond 3G systems: extending the 3GPP spatial channel model (SCM)”, VTC",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1109/vetecs.2005.1543924",
                    "raw": "D.S.Baum, J.Hansen, J.Salo, “An interim channel model for beyond 3G systems: extending the 3GPP spatial channel model (SCM)”, VTC 2005 Spring, Volume 5, Page 3132 – 3136",
                    "cites": null
                },
                {
                    "id": 36634155,
                    "title": "Limited Feedback Unitary Precoding for Spatial Multiplexing Systems’,",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1109/tit.2005.850152",
                    "raw": "David  J.Love,  Robert  W.  Health,  ‘  Limited  Feedback  Unitary Precoding for Spatial Multiplexing Systems’, IEEE Transactions on Information Theory, Vol. 51, No.8, August 2005",
                    "cites": null
                },
                {
                    "id": 36634153,
                    "title": "Performance of MIMO systems with channel inversion’,",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1109/vtc.2002.1002659",
                    "raw": "T. Haustein, C.von Helmolt, E. Jorswieck, et al, ; Performance of MIMO systems with channel inversion’, IEEE 55 th VTC, vol. 1, May 2002, pp. 35 39",
                    "cites": null
                },
                {
                    "id": 36634149,
                    "title": "Simplified Processing for High Spectral Efficiency Wireless Communication Employing Multi Element Arrays”,",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1109/49.806815",
                    "raw": "Gerard  J.  Foschini,  Glen  D.  Golden,  Reinaldo  A.  Valenzuela,  and Peter  W.  Wolniansky,  “Simplified  Processing  for  High  Spectral Efficiency  Wireless  Communication  Employing  Multi Element Arrays”, IEEE Transactions on Selected Areas in Communications, Vol. 17, No. 11, Nov 1999",
                    "cites": null
                },
                {
                    "id": 36634157,
                    "title": "Spatial channel model for MIMO simulations”,",
                    "authors": [],
                    "date": null,
                    "doi": null,
                    "raw": "“Spatial  channel  model  for  MIMO  simulations”,  3GPP  TR 25.996 V6.1.0,  Sep’03.  [Online]:  http://www.3gpp.org/ftp/Specs/html info/25996.htm",
                    "cites": null
                },
                {
                    "id": 36634150,
                    "title": "Technical Specification Group Radio Access Network;",
                    "authors": [],
                    "date": null,
                    "doi": null,
                    "raw": "‘Technical  Specification  Group Radio  Access  Network;  (E UTRA) and  (E UTRAN):  Overall  Description’,  3GPP  TS  36.300  V8.4.0, Mar.08 [Online]: http://www.3gpp.org/ftp/Specs/html info/36300.htm",
                    "cites": null
                },
                {
                    "id": 36634151,
                    "title": "Writing on Dirty Paper’,",
                    "authors": [],
                    "date": "1983",
                    "doi": "10.1109/tit.1983.1056659",
                    "raw": "M.Costa, ‘Writing on Dirty Paper’, IEEE Trans. Information Theory, vol. 29, no. 3, May 1983, pp. 439 – 441",
                    "cites": null
                },
                {
                    "id": 36634154,
                    "title": "Zero forcing methods for downlink spatial multiplexing in multiuser",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1109/tsp.2003.821107",
                    "raw": "Q. H. Spencer, A. L. Swindlehurst, and M. Haardt, ‘ Zero forcing methods  for  downlink  spatial  multiplexing  in  multiuser  MIMO channels’ IEEE Trans. Signal Processing, Vol. 52, no. 2, Feb. 2004, pp. 461 471",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "https://research-information.bris.ac.uk/files/3019612/Beh_IEEE_PIMRCl_2009.pdf"
            ],
            "updatedDate": "2021-10-13T19:24:11",
            "yearPublished": 2009,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/29025883.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/29025883"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/29025883/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/29025883/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/17386330"
                }
            ]
        },
        {
            "acceptedDate": "2011-07-21T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Beach, MA"
                },
                {
                    "name": "Halls, DE"
                },
                {
                    "name": "Nix, AR"
                }
            ],
            "contributors": [
                "David"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/29026000",
                "https://api.core.ac.uk/v3/outputs/190662587"
            ],
            "createdDate": "2015-02-17T16:05:28",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 286,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/286",
                    "logo": "https://api.core.ac.uk/data-providers/286/logo"
                }
            ],
            "depositedDate": "2011-05-01T00:00:00",
            "abstract": null,
            "documentType": "research",
            "doi": "10.1109/vetecs.2011.5956681",
            "downloadUrl": "https://core.ac.uk/download/29026000.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "                          Halls, D. E., Nix, A. R., & Beach, M. A. (2011). System level evaluation of\ninterference in vehicular mobile broadband networks. In IEEE 73rd\nVehicular Technology Conference (VTC Spring), 2011. (pp. 1 - 5). Institute\nof Electrical and Electronics Engineers (IEEE).\n10.1109/VETECS.2011.5956681\nLink to published version (if available):\n10.1109/VETECS.2011.5956681\nLink to publication record in Explore Bristol Research\nPDF-document\nUniversity of Bristol - Explore Bristol Research\nGeneral rights\nThis document is made available in accordance with publisher policies. Please cite only the published\nversion using the reference above. Full terms of use are available:\nhttp://www.bristol.ac.uk/pure/about/ebr-terms.html\nTake down policy\nExplore Bristol Research is a digital archive and the intention is that deposited content should not be\nremoved. However, if you believe that this version of the work breaches copyright law please contact\nopen-access@bristol.ac.uk and include the following information in your message:\n• Your contact details\n• Bibliographic details for the item, including a URL\n• An outline of the nature of the complaint\nOn receipt of your message the Open Access Team will immediately investigate your claim, make an\ninitial judgement of the validity of the claim and, where appropriate, withdraw the item in question\nfrom public view.\nSystem Level Evaluation of Interference in Vehicular \nMobile Broadband Networks \nDavid Halls, Andrew Nix and Mark Beach \nCentre for Communications Research, \nUniversity of Bristol, Bristol, United Kingdom. \nEmail: {David.Halls.03, Andy.Nix, M.A.Beach}@bristol.ac.uk \n \n \nAbstract — This paper presents results from a novel OFDMA \nmulti-cell mobile broadband system-level simulator. The tool is \nused to statistically characterize uplink and downlink inter-cell \ninterference. Fully loaded interference studies cannot be \nperformed on real-world networks until they have been fully \ndeployed. As such, interference analysis and management must \nbe accurately performed pre-deployment using detailed \nnetwork simulators. This paper discusses our simulator \narchitecture and the steps necessary to reduce computational \ncomplexity inherent in such simulations. The paper then \ndemonstrates the impact of inter-cell interference in OFDMA \nnetworks. The results demonstrate that it is the frame-to-frame \nfluctuations in interference, and not the received signal level, \nthat dominate inaccuracy in the Channel Quality Index (CQI). \nCQI errors lead to the wrong MCS mode being chosen by the \nAMC algorithm and this leads to throughput reduction. \nResults show that this problem is exacerbated by high \nvehicular speeds and the absence of interference \nrandomization. Compared to ideal MCS selection, throughput \nreductions of up to 45% are reported. \nKeywords- mobile broadband; interference characterization; \ncellular; WiMAX; system level evaluation. \nI. INTRODUCTION \nThis paper evaluates the inter-cell interference \nperformance of a Wave-2 mobile WiMAX system. The \nsimulator is based on the IEEE 802.16m amendment, which \nprovides the basis for next generation mobile WiMAX \nsystems. WiMAX uses scalable OFDMA in the radio access \nnetwork [1]. In order to achieve the spectral efficiency and \nper-sector throughput required by voice, data and media \napplications, WiMAX utilizes Adaptive Modulation and \nCoding (AMC) and Multiple Input Multiple Output (MIMO) \nantenna processing. To achieve high user capacities it is \nnecessary for the network to perform well at high traffic \nload. Billions of dollars have been invested in such networks \n[2] and unless deployed correctly, these can collapse under \nheavy load due to self-interference. The commercial success \nof mobile broadband networks is dependent on accurate \ninterference characterization and management. It is therefore \ncritical, pre-deployment, to accurately evaluate system level \nperformance in the presence of uplink (UL) and downlink \n(DL) interference. It is also important to understand how \ninterference is affected by the radio environment and a wide \nrange of propagation scenarios. Due to real-world difficulties \nin loading a network and varying the propagation scenarios \nduring drive tests, the comprehensive interference \ncharacterization of multi-cell systems cannot easily be \nachieved in practice. Heavy UL traffic loads are particularly \ndifficult to engineer in controlled tests. This makes accurate \ninterference characterization via simulation a vital step prior \nto network deployment. \nInterference arises in the form of inter-cell and intra-cell \ninterference. Whereas intra-cell interference is relatively \neasy to manage by means of orthogonal frequencies and \nscheduling strategies, inter-cell interference is problematic \nand remains a key issue in OFDMA based mobile cellular \nnetworks. This arises since in broadband wireless networks, \nsector frequencies are reused in adjacent cells to improve \nspectral efficiency. The problem is particularly acute at the \ncell boundaries, where wanted signal powers are weak and \ninterfering signal powers are strong; resulting in reduced user \nthroughput. Characterization of inter-cell interference for \nOFDMA networks is not well addressed in the literature – \nparticularly on the UL. This is mainly due to the complexity \nof simulating a vast quantity of links over a statistically \nsignificant time period. \nII. BACKGROUND \nIn order to characterize interference accurately in a \nmobile broadband network it is necessary to model a multi-\ncell environment that implements a realistic cellular reuse \nfactor, uses full size frame structures over many frames, and \nimplements subcarrier allocation and packet scheduling. \nAs discussed in [3], the link budget constraint on the UL \nof any mobile network necessitates the need for smaller cell \nsizes. The use of small site-to-site distances makes for highly \ninterference limited systems. In order to analyze the UL \ninterference, knowledge is required of the desired MS \nlocation, as well as the relative locations of all other \ninterfering MSs. All the necessary fading channels must then \nbe computed for the scheduled users in each frame. The \nauthors in [4] considered interference randomization on both \nthe UL and DL of a WiMAX system. However, they failed \nto implement a full system level simulator. The authors in [5] \nexamined UL performance using a proprietary simulator \n(SHINE). This paper also explores a range of complexity \nreduction techniques. These are vital in order to create a \nsimulator that can accurately characterize interference in a \ntimely manner. \nComplexity can be reduced by simplifying the \ncomputation of interference. In [6] the subcarriers from \nadjacent cells are simply scaled by a ‘loading factor’ to \nmimic the loading process, rather than accurately scheduling \nusers and computing the resulting UL and DL interference. \nAlthough other simulators make use of correlated fast fading \nmodels, they commonly use random [7] or uniform [8] MS \nlocations between drops, and uncorrelated (spatially) log \nnormal shadowing [9]. \nTo aid complexity reduction, many simulators use a \nmuch reduced frame size. In [8] the authors use just 48 sub-\ncarriers and 6 sub-bands. Very few papers use the full \nnumber of subchannels and slots per frame. In [9] the full \nframe structure is implemented as per [10], however they do \nnot consider power control, scheduling, or UL traffic. \nAlthough many commercial network simulators exist, \nsuch as OpNet and QualNet, these tend to provide weak \nphysical layer support. QualNet uses bit error rate (BER) \nlook up tables that already include the fast-fading effects. \nAccurate simulators need to work with instantaneous \nperformance in a fading channel, and not the expected \nperformance averaged over the fading processes. \nIn this paper, a system-level simulation framework is \npresented that allows the performance of a Wave-2 mobile \nWiMAX system to be derived. The simulator closely follows \nthe Evaluation Methodology Document (EMD) [10] in terms \nof its system parameters (such as transmit powers and \nantenna patterns) and simulation parameters (cell \nconfiguration, frame structure and traffic model). \nEnhancements above and beyond the EMD baseline are \nhighlighted as they are introduced in the paper. \nOur system implements AMC alongside Dynamic \nTransmit Power Control (DTPC) on the UL [11]. It \nschedules and models all users in both the reference and \ninterfering sectors. Unlike other reported works, a fully \ntemporally and spatially correlated model is used for fast \nfading and shadowing [12]. The simulator comprehensively \nmodels the mobility of users (indoor, outdoor pedestrian and \nvehicular) to allow the analysis of fast link adaptation. The \nbespoke spatial channel model includes a realistic range of \nK-factors, RMS delay spreads and angular spreads. \nA PHY layer abstraction technique known as Received \nBit Mutual Information (RBIR) is used to predict the \ninstantaneous BLER. The abstraction model has been \nvalidated against link-level results from our own link-layer \nsimulator. This in turn has been validated against results \nobtained from carrier-class mobile WiMAX equipment. Our \nnovel AMC algorithm uses the ‘effective’ SINR (ESINR), as \nits Channel Quality Index, to calculate the optimal burst \nprofile (i.e the MCS mode) for each user in order to \nmaximize throughput for a given BER requirement. This \nESINR is calculated from all the subcarriers allocated to that \nuser using the RBIR technique. We then simulate the full \nframe structure including UL and DL. We implement the \nmandatory PUSC subcarrier permutation scheme together \nwith a detailed packet scheduler. The scheduler runs using a \nproprietary proportional fair (PF) algorithm. It is applied to \nall the WiMAX traffic classes in strict priority to provide \nclass prioritization. As per the EMD baseline our simulator: \n• Models a tri-sector multi-cell environment with micro and \nmacro scenarios, \n• Models full size frame structures for UL/DL with PUSC, \n• Uses correlated fast fading, \n• Implements a range of MIMO modes, \n• Uses an accurate and validated PHY abstraction model. \nIn addition our simulator: \n• Exhaustively models all interferers, \n• Uses a sophisticated correlated shadowing model with a \nvalidated autocorrelation function (ACF) and continuity of \nuser mobility between frames, \n• Implements AMC and AMS with channel feedback delay, \n• Implements a proprietary PF scheduler with service class \nprioritization, \n• Implements DTPC. \nIII. SIMULATOR DESCRIPTION \nA. System Overview \nIn this paper our results are confined to a SISO system. \nWe also assume perfect channel estimation and \nsynchronization. Equal power per subcarrier on the DL is \napplied. A full-buffer traffic model is assumed with best \neffort (BE) flows, since these most clearly highlight the \nimpact of interference on AMC performance. \nTABLE I.  LINK AND SYSTEM SIMULATOR PARAMETERS \nParameters Value \nCarrier frequency 2.5GHz \nTransmission bandwidth 5MHz \nFFT size 512 \nCellular configuration Hexagonal cell/2 tiers/7 cells \nAntenna patterns 3 sectors \nFrame length 5ms (48 OFDMA symbols) \nData symbols 28 DL/9 UL \nSubchannels 15 DL/17 UL \nMCS modes QPSK 1/2, 3/4, 16QAM 1/2 2/3, 64QAM \n2/3, 3/4 \nAMC BER threhold 10-6 \nMCS feedback delay 0 and 3 frames \nFrequency reuse factor 1/3 \nAntenna scheme SISO \nScheduling algorithm Round robin, (1 subchannel/partition) \nMobility Pedestrian (3kmph) and Vehicular \n(120kmph) \nPenetration loss 10dB \nTraffic class Best Effort \nTraffic model Full buffer \nShadowing SD 8dB \nDe-correlation distance 50m \nPower control, Pnom 21.4dB \nSpatial channel model CDL using spatial correlation \nPathloss model COST 231 Hata \nTables I and II summarize the key system and simulation \nparameters. In mobile WiMAX the minimum resource unit is \na ‘slot’. This is 1 subchannel x 2 OFDM symbols on the DL \nand 1 subchannel x 3 OFDM symbols on the UL. Each slot is \ndynamically assigned to an MS using the packet scheduler. \nTABLE II.  SYSTEM PARAMETERS \nParameters Value \nBS \nTransmit power 43dBm \nAntenna height 32m \nAntenna gain (boresight) 17dBi \n3dB beamwidth 70° \nFront-to-back power ratio 20dB \nNumber of antennas (Rx and Tx) 2 \nAntenna spacing 4λ \nNoise figure 4dB \nCable loss 2dB \nMS \nTransmit power 23dBm \nAntenna height 1.5m \nAntenna gain (boresight) 0dBi \nAntenna pattern Omni \nNumber of antennas (Rx and Tx) 2 \nAntenna spacing λ/2 \nNoise figure 7dB \nCable loss 0dB \nB. Link-Layer Simulator Description \nThe link-level simulator is fully described in [13] and the \nresults have been fully validated against carrier class \nequipment. \nC. Spatial Channel Model (SCM) \nA correlation-based Cluster Delay Line model is used \nwith parameters taken from the ‘Urban Macrocell’ scenario \n(Table III). The full power delay profile is provided in [10]. \nTABLE III.  CHANNEL MODEL PARAMETERS \nScenario Cell Radius (N)LOS AS (BS,MS) \nUrban Macrocell 500m NLOS 2°, 15° \nD. Pathloss, Antenna Gain and Shadowing \nBS and MS heights of 32m and 1.5m respectively are \nused in the modified COST 231 Hata path loss model \n(‘Urban Macro cell’) shown in (1). The carrier frequency f \n[GHz] lies in the range 2 < f < 6, as defined in [10]. \n             [ ] ( ) ( )2/log26log352.35 1010 fddBPL ++=  (1) \nIn (1) d is defined in meters and represents the BS-MS \nseparation distance. At the BS a Uniform Linear Array \n(ULA) is assumed with an antenna pattern as defined in [10]. \nThe MS are assumed to have omni-directional antenna \npatterns. We implement a spatially correlated shadowing \nmodel as proposed in [12]. This is used to generate the \nshadowing values for the BS-MS links. The parameters are \nsummarized in Table IV. \nTABLE IV.  SHADOWING PARAMETERS \nParameter Value \nShadowing SD 8dB \nDecorrelation distance 50m \nNumber of sinusoids [12] 500 \nFrequency resolution [12] 0.002 \nSpatial resolution [12] 0.5m \nWaveform table size [12] 1000 \nE. Link-to-System Mapping (PHY Abstraction) \nTo simplify the interface between the link and system \nlevel simulations, whilst still modelling dynamic system \nbehavior, a technique known as Effective SINR Mapping \n(ESM) can be used. This compresses the SINR (per \nsubcarrier) vector into a single ESINR. In this work the \nMutual Information ESM (MIESM) approach is applied. The \ntechnique is described fully in [14]. This approach has been \nexhaustively validated against our link-level simulator. \nF. Simulator Complexity \nChannel information for 107,520 links is required to \ncompute a single time slot in the simulator. This is in \naddition to performing the permutation, link adaptation and \nscheduling algorithms. In order to reduce complexity the \nchannel is assumed to remain stationary over a ‘slot’. Only \n512 subcarriers are used and the analysis is limited to the 7 \ncentral cells. It is shown in [15] that the second tier of \ninterferers can be ignored. Statistics are only collected for the \ncentral cell. In order to further reduce computation time the \nchannel responses are calculated offline. \nIV. RESULTS \nA. Signal and Interference Variability \nIn this paper we report results assuming 1) each active \nuser is allocated a random subchannel in each time slot with \nPUSC, 2) each active user is scheduled the same subchannel \nacross all time (horizontal strip) with PUSC, and 3) each \nactive user is allocated a random subchannel in each time \nslot without any subcarrier permutation. The results were \ncollected with all users at a) pedestrian speed (3kmph), and \nb) vehicular speed (120kmph). \nOur novel fast link-adaptation operates by collecting the \nCQI per MS in the form of the ESINR over all subcarriers \nallocated to that MS in a reference frame. The algorithm then \nuses this information to predict the highest order MCS mode \nthat can be supported by each MS whilst maintaining a BER \nthat can be specified for each service flow as part of the QoS \nparameters. Here, for simplicity, all users have a BER target \nof 10\n-6\n. The simulator was run with: a) a zero frame delay \n(i.e. perfect per-frame CQI knowledge), and b) a 3 frame \nfeedback delay (as per the EMD). Due to the fact that ESINR \nvalues have different bounds in different MCS modes, which \ndistorts representation of the data, the SIR, SNR and SINR \nvalues are shown over all of a user’s allocated subcarriers in \na frame as a mean value. These are now denoted by ASIR, \nASNR and ASINR respectively. \nTABLE V.  DL SUB-FRAME  CQI ERROR RESULTS \nSpeed Allocation \nASIR Err \nVar (dB) \nASNR Err \nVar (dB) \nASINR Err Var \n(dB) \nPed \nRandom (PUSC) 26.8 3.3 15.5 \nH. Strip (PUSC) 26.6 3.4 15.6 \nRandom (No Perm) 33.0 8.9 20.7 \nVeh \nRandom (PUSC) 57.1 5.8 37.8 \nH Strip (PUSC) 56.6 5.9 39.2 \nRandom (No Perm) 64.4 11.8 44.9 \nTables V and VI show the variance of errors between the \nCQI in the frame that the AMC uses as reference (with 3 \nframe delay), and the current frame, for all active users over \n1000 frames. Table V shows the DL case, and table VI \nshows the UL case. \nTABLE VI.  UL SUB-FRAME CQI ERROR RESULTS \nSpeed Allocation \nASIR Err \nVar (dB) \nASNR Err \nVar (dB) \nASINR Err Var \n(dB) \nPed \nRandom (PUSC) 17.8 3.3 6.1 \nH. Strip (PUSC) 10.5 3.4 4.9 \nRandom (No Perm) 27.8 9.7 13.5 \nVeh \nRandom (PUSC) 27.1 6.3 9.5 \nH. Strip (PUSC) 25.3 6.5 11.0 \nRandom (No Perm) 47.4 14.1 20.3 \nTable V shows that in the pedestrian case, the ASNR \nerror variance is not affected by the allocation strategy as \nlong as PUSC is enabled. If there is no subcarrier \npermutation, however, the value more than doubles due to \nthe removal of the frequency diversity provided by PUSC. \nAt vehicular speeds exactly the same trend is displayed but \nwith higher values. This is because the channel will have \nchanged more significantly over the 3 frames in the vehicular \ncase. The ASIR values follow the same trend as the ASNR \nvalues in all cases, but are significantly higher; nearly ten \ntimes higher in the vehicular case with random subchannel \nallocation and PUSC. \n-30 -20 -10 0 10 20 30\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\nPDF of ASINR Error - DL Random Allocation (PUSC Enabled)\nP\nr(\nx\n=\nX\n)\ndB\n \n \nVehicular ASINR Error\nPedestrian ASINR Error\n \nFigure 1.  ASINR error in vehicular and pedestrian scenario \nMode selection is based on the ESINR from the delayed \nframe so as these ASINR errors equate to ESINR errors, they \nwill produce MCS mode choice errors. If the ESINR in the \nscheduled frame is better than that predicted by the AMC \nalgorithm, using delayed CQI information, then the \nscheduler MCS mode will be too low and network capacity \nwill be reduced. If the opposite is true, then the scheduled \nMCS mode will be too high and this leads to high BLER. It \nis clear that in this scenario the ASINR error is completely \ndominated by the ASIR term. This means that it is the \nfluctuation in interference power and not that in the signal \npower that controls the accuracy of the AMC algorithm. This \nhighlights the necessity for interference characterization and \nmitigation. The fact that the ASINR error variance is \nsignificantly higher in the vehicular case means that worse \nsystem throughput will be experienced and it highlights the \ndifficulties related to accurate channel feedback in vehicular \ncommunications systems. The difference between the \nASINR error in the vehicular and pedestrian cases is shown \nin Figure 1 for random allocation with PUSC enabled. \nIn the UL case the same trends and very similar values \ncan be seen for the ASNR error variance values. It can be \nseen, however, that the ASIR error variance values are \nsignificantly lower than the DL case and this leads to lower \nASINR error variance. The reduced error variance observed \nin the UL case is due to the use of lower UL transmit powers, \nwhich results in a smaller dynamic range in the received \ninterference power. The fact that the ASINR is significantly \nworse without permutation in both the UL and DL case \nhighlights the need for frequency randomization or \nfrequency coordination techniques [3]. \nResults from Table VI show that the use of a horizontal \nstrip schedule reduces the ASINR error variance in the UL \npedestrian case. This is due to the fact that unlike the random \nallocation scheme, the interference now comes from a \nconstant source. This reduction is not seen in the DL case \nbecause the BS transmits with equal power on all subcarriers \nirrespective of the allocation strategy. A much smaller \nreduction in the ASIR variance is observed in the vehicular \ncase as the relative motion of the MSs is much greater, \ncausing larger fluctuation in the SIR over the 3 frame period. \n-6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6\n0\n0.5\n1\n1.5\n2\n2.5\n3\nlo\ng\n1\n0\n(N\n)\n1.3 UL 3 Frame Delay, Veh, Random, Full PUSC - PDF of MCS Error, Wrong Selection =19.4%\nMode Error\n \nFigure 2.  UL MCS mode choice error for MS 1.3 at vehicular speed \nB. Effect of Signal Variability on Throughput \nAs discussed in the previous section, the ASINR errors \nlead to MCS mode choice errors, which in turn lead to \ncapacity reduction. Figure 2 shows the MCS mode choice \nerrors for the worst-case UL user (MS 1.3, i.e. user 3 in \nsector 1) in the vehicular case, with random subchannel \nallocation and PUSC. In this case the wrong MCS mode is \nchosen 19.4% of the time and this leads to a throughput \nreduction over the 1000 frames (compared with perfect per-\nframe CQI knowledge) of 35% from 114kbps to 74kbps. If \nno subcarrier permutation is applied, the wrong MCS mode \nis chosen 39.3% of the time, and mode selection errors all \nthe way up to ±6 are seen. In this case the throughput falls \neven further to 58kbps. \nOn average, over all of the users, it was found that on the \nDL the MCS mode choice error for the pedestrian case was \n20% with PUSC and 23% without PUSC. These values rise \nto 40% and 45% in the vehicular case. MS 1.3 is the worst \ncase DL user suffering a 31% reduction in throughput due to \ninaccurate CQI in the vehicular case (535kbps reduced to \n368kbps). Without subcarrier permutation this falls to \n307kbps (a 43% reduction). \nC. Autocorrelation of Channel Quality Information \nThe use of autoregressive filters to implement AMC \nalgorithms that use channel prediction has received much \ninterest in the literature (see [16] and references therein). \nFigure 3 shows the autocorrelation function of the ASINR \nerror between the delayed frame and the current frame when \nusing a horizontal strip schedule for MS 0.3. It can be seen \nthat, in the pedestrian case, the autocorrelation follows a \nsmooth function and does not fall to zero until there is a 4 \nframe shift. In the vehicular case the correlation falls quickly \nto values below 0.1 with a 2 frame shift, and to zero after a 3 \nframe shift. This shows that unlike in the pedestrian case, at \nvehicular speeds the channel changes too rapidly over a 3 \nframe period to allow the effective implementation of an \nautoregressive filter. \n10 20 30 40 50 60 70 80 90 100\n-0.5\n0\n0.5\n1\n \n \n0.3 DL 3 Frame Delay, H. Strip, Full PUSC - Autocorrelation Of Frame ASINR Errors\nC\no\nrr\ne\nla\nti\no\nn\nShift (Frames)\nVehicular\nPedestrian\n \nFigure 3.  Autocorrelation of ESINR errors \nV. CONCLUSIONS \nThis paper has introduced a detailed and efficient \nOFDMA based system level simulator. This simulator \nenables the investigation of complex issues such as inter-cell \ninterference and MCS mode selection on both the UL and \nDL, with a variety of traffic speeds and subcarrier allocation \nstrategies. \nIt was shown that it is the fluctuation in interference \npower, and not in signal power, that dominates the \ninaccuracies seen in the predicted CQI. This prediction is \nused by the fast AMC algorithm to select the MCS mode in \nfuture frames. Selection of incorrect MCS modes causes a \nreduction in throughput. In the worst case scenario, at \nvehicular speeds, the inaccurate CQI information caused a \n31% reduction in DL throughput and a 24% reduction in UL \nthroughput. Without the use of subcarrier permutation the \nthroughputs fall even further. It was found that the CQI error \nwas smaller on the UL due to a reduced ASIR error variance, \nand could be reduced further (i.e. improving system \nperformance) on the UL using a horizontal strip schedule. \nOur results highlight the necessity for detailed \ninterference characterization and management in OFDMA \nbased networks. Although the simulator is based on mobile \nWiMAX, its algorithms, methods and results can easily be \nextended to other mobile broadband wireless networks, such \nas LTE. \nACKNOWLEDGMENTS \nThe authors would like to thank Dr Mai Tran. David \nHalls would also like to that the UK EPSRC and Motorola \nfor their kind financial assistance. \nVI. REFERENCES \n[1] \"IEEE Std 802.16e-2005 and IEEE Std 802.16-2004/Cor 1-2005,\" \n2006. \n[2] “Sprint Nextel Announces 4G Wireless Broadband Initiative with \nIntel, Motorola and Samsung,” http://www2.sprint.com/mr/news \ndtl.do?id=12960. \n[3] G. Boudreau, J. Panicker, N. Guo, R. Chang, N. Wang, and S. Vrzic, \n\"Interference coordination and cancellation for 4G networks - [LTE \npart II: 3GPP release 8],\" IEEE Communications Magazine, vol. 47, \npp. 74-81, 2009. \n[4] A. V. Sarad and S. Srikanth, \"Improved interference diversity in \nmulticellular OFDMA systems,\" COMSNETS, pp. 1-8, Jan. 2009. \n[5] M. Hunukumbure, B. Upase, and S. Vadgama, \"Modelling \ninterference margins in FFR enabled WiMAX systems for cell \ndimensioning,\" IEEE PIMRC, pp. 1-5, Sept, 2008. \n[6] B. J. Lee, J. W. Kim, J. C. Kim, and C. G. Kang, \"System-level \nPerformance of MIMO-based Mobile WiMAX System,\" IEEE MWS, \npp. 189-194, July 2009. \n[7] M. Shariat, A. Ul Quddus, and R. Tafazolli, \"On the efficiency of \ninterference coordination schemes in emerging cellular wireless \nnetworks,\" IEEE PIMRC, pp. 1-5, Sept 2008. \n[8] A. L. Stolyar and H. Viswanathan, \"Self-Organizing Dynamic \nFractional Frequency Reuse for Best-Effort Traffic through \nDistributed Inter-Cell Coordination,\" IEEE INFOCOM, pp. 1287-\n1295, April 2009. \n[9] S. Hamouda, C. Yeh, J. Kim, S. Wooram, and D. S. Kwon, \"Dynamic \nhard Fractional Frequency Reuse for mobile WiMAX,\". IEEE \nPerCom, pp. 1-6, March 2009. \n[10] J. Zhuang, L. Jalloul, R. Novak, and J. Park, \"IEEE 802.16m \nEvaluation Methodology Document,\" 2009. \n[11] Y. Q. Bian, A. R. Nix, E. K. Tameh, and J. P. McGeehan, \"MIMO-\nOFDM WLAN Architectures, Area Coverage, and Link Adaptation \nfor Urban Hotspots,\" IEEE Transactions on Vehicular Technology, \nvol. 57, No. 4, pp. 2364-2374, 2008. \n[12] Z. Wang, E. K. Tameh, and A. R. Nix, \"Joint Shadowing Process in \nUrban Peer-to-Peer Radio Channels,\" IEEE Transactions on \nVehicular Technology, vol. 57, No. 1, pp. 52-64, 2008. \n[13] M. Tran, D. Halls, A. Nix, A. Doufexi, and M. Beach, \"Mobile \nWiMAX: MIMO Performance Analysis from a Quality of Service \n(QoS) Viewpoint,\" IEEE WCNC, pp. 1-6, April 2009. \n[14] L. Wan, S. Tsai, and M. Almgren, \"A fading-insensitive performance \nmetric for a unified link quality model,\" IEEE WCNC, pp. 2210-2114, \nApril 2006. \n[15] M. K. Karray, \"Electromagnetic Exposure and Quality of Service in \nthe Downlink of Wireless Cellular Networks,\" ICWMC, Sept. 2010. \n[16] D. Rhee, J.H. Kwon, H. K. Hwang, and K. S. Kim, \"Adaptive \nModulation and Coding on Multipath Rayleigh Fading Channels \nBased on Channel Prediction,\" ICACT, pp. 5-10, Feb 2006. \n",
            "id": 17386370,
            "identifiers": [
                {
                    "identifier": "190662587",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "385698733",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "29026000",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:research-information.bris.ac.uk:openaire_cris_publications/cadc81a5-19e5-4e7b-a01c-a2c80a88ff91",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "10.1109/vetecs.2011.5956681",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:research-information.bris.ac.uk:publications/cadc81a5-19e5-4e7b-a01c-a2c80a88ff91",
                    "type": "OAI_ID"
                }
            ],
            "title": "System level evaluation of interference in vehicular mobile broadband networks",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:research-information.bris.ac.uk:publications/cadc81a5-19e5-4e7b-a01c-a2c80a88ff91",
                "oai:research-information.bris.ac.uk:openaire_cris_publications/cadc81a5-19e5-4e7b-a01c-a2c80a88ff91"
            ],
            "publishedDate": "2011-05-01T01:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 36633218,
                    "title": "A fading insensitive performance metric for a unified link quality model,&quot;",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/wcnc.2006.1696622",
                    "raw": "L. Wan, S. Tsai, and M. Almgren, &quot;A fading insensitive performance metric for a unified link quality model,&quot; IEEE WCNC, pp. 2210 2114, April 2006.",
                    "cites": null
                },
                {
                    "id": 36633220,
                    "title": "Adaptive Modulation and Coding on Multipath Rayleigh Fading Channels Based on Channel Prediction,&quot;",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/icact.2006.205950",
                    "raw": "D.  Rhee,  J.H.  Kwon,  H.  K.  Hwang,  and  K.  S.  Kim,  &quot;Adaptive Modulation  and  Coding  on  Multipath  Rayleigh  Fading  Channels Based on Channel Prediction,&quot; ICACT, pp. 5 10, Feb 2006.",
                    "cites": null
                },
                {
                    "id": 36633213,
                    "title": "Dynamic hard Fractional Frequency Reuse for mobile WiMAX,&quot;.",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1109/percom.2009.4912869",
                    "raw": "S. Hamouda, C. Yeh, J. Kim, S. Wooram, and D. S. Kwon, &quot;Dynamic hard  Fractional  Frequency  Reuse  for  mobile  WiMAX,&quot;.  IEEE PerCom, pp. 1 6, March 2009.",
                    "cites": null
                },
                {
                    "id": 36633219,
                    "title": "Electromagnetic Exposure and Quality of Service in the Downlink of Wireless Cellular Networks,&quot; ICWMC,",
                    "authors": [],
                    "date": "2010",
                    "doi": "10.1109/icwmc.2010.38",
                    "raw": "M. K. Karray, &quot;Electromagnetic Exposure and Quality of Service in the Downlink of Wireless Cellular Networks,&quot; ICWMC, Sept. 2010.",
                    "cites": null
                },
                {
                    "id": 36633214,
                    "title": "IEEE 802.16m Evaluation Methodology Document,&quot;",
                    "authors": [],
                    "date": "2009",
                    "doi": null,
                    "raw": "J.  Zhuang,  L.  Jalloul,  R.  Novak,  and  J.  Park,  &quot;IEEE  802.16m Evaluation Methodology Document,&quot; 2009.",
                    "cites": null
                },
                {
                    "id": 36633208,
                    "title": "Improved interference diversity in multicellular OFDMA systems,&quot;",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1109/comsnets.2009.4808897",
                    "raw": "A.  V.  Sarad  and  S.  Srikanth,  &quot;Improved  interference  diversity  in multicellular OFDMA systems,&quot; COMSNETS, pp. 1 8, Jan. 2009.",
                    "cites": null
                },
                {
                    "id": 36633207,
                    "title": "Interference coordination and cancellation for 4G networks [LTE part II: 3GPP release 8],&quot;",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1109/mcom.2009.4907410",
                    "raw": "G. Boudreau, J. Panicker, N. Guo, R. Chang, N. Wang, and S. Vrzic, &quot;Interference coordination and cancellation for 4G networks   [LTE part II: 3GPP release 8],&quot; IEEE Communications Magazine, vol. 47, pp. 74 81, 2009.",
                    "cites": null
                },
                {
                    "id": 36633216,
                    "title": "Joint Shadowing Process in Urban Peer to Peer Radio Channels,&quot;",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1109/tvt.2007.904513",
                    "raw": "Z. Wang, E. K. Tameh, and A. R. Nix, &quot;Joint Shadowing Process in Urban  Peer to Peer  Radio  Channels,&quot;  IEEE  Transactions  on Vehicular Technology, vol. 57, No. 1, pp. 52 64, 2008.",
                    "cites": null
                },
                {
                    "id": 36633215,
                    "title": "MIMO OFDM WLAN Architectures, Area Coverage, and Link Adaptation for Urban Hotspots,&quot;",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1109/tvt.2007.909289",
                    "raw": "Y. Q. Bian, A. R. Nix, E. K. Tameh, and J. P. McGeehan, &quot;MIMO OFDM WLAN Architectures, Area Coverage, and Link Adaptation for Urban Hotspots,&quot; IEEE Transactions on Vehicular Technology, vol. 57, No. 4, pp. 2364 2374, 2008.",
                    "cites": null
                },
                {
                    "id": 36633217,
                    "title": "Mobile WiMAX: MIMO Performance Analysis from a Quality of Service (QoS) Viewpoint,&quot;",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1109/wcnc.2009.4917808",
                    "raw": "M.  Tran,  D.  Halls,  A.  Nix,  A.  Doufexi,  and  M.  Beach,  &quot;Mobile WiMAX:  MIMO  Performance  Analysis  from  a  Quality  of  Service (QoS) Viewpoint,&quot; IEEE WCNC, pp. 1 6, April 2009.",
                    "cites": null
                },
                {
                    "id": 36633209,
                    "title": "Modelling interference margins in FFR enabled WiMAX systems for cell dimensioning,&quot;",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1109/pimrc.2008.4699682",
                    "raw": "M.  Hunukumbure,  B.  Upase,  and  S.  Vadgama,  &quot;Modelling interference  margins  in  FFR  enabled  WiMAX  systems  for  cell dimensioning,&quot; IEEE PIMRC, pp. 1 5, Sept, 2008.",
                    "cites": null
                },
                {
                    "id": 36633211,
                    "title": "On the efficiency of interference coordination schemes in emerging cellular wireless networks,&quot;",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1109/pimrc.2008.4699874",
                    "raw": "M. Shariat, A. Ul Quddus, and R. Tafazolli, &quot;On the efficiency of interference  coordination  schemes  in  emerging  cellular  wireless networks,&quot; IEEE PIMRC, pp. 1 5, Sept 2008.",
                    "cites": null
                },
                {
                    "id": 36633212,
                    "title": "Self Organizing Dynamic Fractional Frequency Reuse for Best Effort Traffic through Distributed Inter Cell Coordination,&quot;",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1109/infcom.2009.5062043",
                    "raw": "A.  L.  Stolyar  and  H.  Viswanathan,  &quot;Self Organizing  Dynamic Fractional  Frequency  Reuse  for  Best Effort  Traffic  through Distributed  Inter Cell  Coordination,&quot;  IEEE  INFOCOM,  pp.  1287 1295, April 2009.",
                    "cites": null
                },
                {
                    "id": 36633210,
                    "title": "System level Performance of MIMO based Mobile WiMAX System,&quot;",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1109/mws.2009.43",
                    "raw": "B.  J.  Lee,  J.  W.  Kim,  J.  C.  Kim, and  C.  G.  Kang,  &quot;System level Performance of MIMO based Mobile WiMAX System,&quot; IEEE MWS, pp. 189 194, July 2009.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "https://research-information.bris.ac.uk/files/3021880/Hall_IEEE_VTCs_2011.pdf"
            ],
            "updatedDate": "2021-10-13T19:25:25",
            "yearPublished": 2011,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/29026000.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/29026000"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/29026000/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/29026000/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/17386370"
                }
            ]
        },
        {
            "acceptedDate": "2007-12-03T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Bull, David"
                },
                {
                    "name": "Doufexi, Angela"
                },
                {
                    "name": "Ferre, Pierre L"
                },
                {
                    "name": "Nix, Andrew R"
                },
                {
                    "name": "Sgardoni, Victoria"
                }
            ],
            "contributors": [],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/29025701"
            ],
            "createdDate": "2015-02-17T16:05:03",
            "dataProviders": [
                {
                    "id": 286,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/286",
                    "logo": "https://api.core.ac.uk/data-providers/286/logo"
                }
            ],
            "depositedDate": "2007-01-01T00:00:00",
            "abstract": null,
            "documentType": "research",
            "doi": "10.1109/pimrc.2007.4394641",
            "downloadUrl": "https://core.ac.uk/download/29025701.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "                          Sgardoni, V., Ferre, P. L., Doufexi, A., Nix, A. R., & Bull, D. (2007). Frame\ndelay and loss analysis for video transmission over time-correlated\n802.11A/G channels. In IEEE18th Annual International Symposium on\nPersonal, Indoor and Mobile Radio Communications (PIMRC'07) Athens,\nGreece. (pp. 1 - 5). Athens: Institute of Electrical and Electronics Engineers\n(IEEE). 10.1109/PIMRC.2007.4394641\nLink to published version (if available):\n10.1109/PIMRC.2007.4394641\nLink to publication record in Explore Bristol Research\nPDF-document\nUniversity of Bristol - Explore Bristol Research\nGeneral rights\nThis document is made available in accordance with publisher policies. Please cite only the published\nversion using the reference above. Full terms of use are available:\nhttp://www.bristol.ac.uk/pure/about/ebr-terms.html\nTake down policy\nExplore Bristol Research is a digital archive and the intention is that deposited content should not be\nremoved. However, if you believe that this version of the work breaches copyright law please contact\nopen-access@bristol.ac.uk and include the following information in your message:\n• Your contact details\n• Bibliographic details for the item, including a URL\n• An outline of the nature of the complaint\nOn receipt of your message the Open Access Team will immediately investigate your claim, make an\ninitial judgement of the validity of the claim and, where appropriate, withdraw the item in question\nfrom public view.\nThe 18th Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC'07) \n1-4244-1144-0/07/$25.00 ©2007 IEEE. \nFrame Delay and Loss Analysis for Video Transmission over time-correlated \n802.11A/G channels \nVictoria Sgardoni Pierre Ferré Angela Doufexi Andrew Nix David Bull \nCentre for Communication Research - University of Bristol \nWoodland Road, Bristol, BS8 1UB, U.K. \n \nABSTRACT \nThis paper presents simulation results for the transmission of \nunicast MAC frames over 802.11a/g. Fading channel models \nat various Doppler frequencies are developed to generate time-\ncorrelated SNR waveforms. These are then used together with \na bit accurate MAC/PHY simulator to estimate the frame loss \nrate, the transmission delay, and the jitter for a steady flow of \ntransmit frames. Time correlated channels are required to \ncorrectly simulate the bursty nature of packet loss in a wireless \nchannel. The Doppler spread is shown to have a strong effect \non the performance of the ARQ mechanism in the MAC layer. \nDelay is computed as the sum of the transmission delay and \nthe accumulated queuing delay in the MAC buffer. Delay and \nframe loss are compared for time correlated and time \nuncorrelated fading channels. Compared to the slow fading \ncase, in a fast fading channel fewer retransmissions are \nrequired and the end-to-end delay is significantly reduced. \nWhen channel conditions are poor the simulated delay and \nframe loss rate are seriously underestimated when time \nuncorrelated fading is assumed. To analyze the performance of \nvideo codecs, we show that a time correlated channel model \nmust be combined with a dedicated 802.11a/g MAC/PHY \nsimulation.  \nI. INTRODUCTION \nIEEE 802.11 based WLANs are increasingly being used in \nvideo surveillance and multimedia distribution. The more \nrecent 802.11a/g standard combines a COFDM physical layer \n(PHY) with the legacy 802.11 medium access control (MAC). \nWhen unicast transmissions are used, the MAC layer supports \nthe automatic retransmission of errored data frames using a \nstop-and-wait ARQ mechanism. This technique improves the \npacket error rate (PER) observed at the higher layers. In the \nreceiver, erroneous MAC frames are dropped, and hence only \nerror-free frames are observed at the application layer. MAC \nframes that fail to be acknowledged are resent up to a \nmaximum retry count. When the radio channel is characterized \nby a low signal to noise ratio (SNR), high frame loss rate \n(FLR), delay and jitter is encountered at the MAC layer. Delay \nand jitter occur as a consequence of variable frame \nretransmission and this degrades applications that rely on \ntimely packet reception [1].  \nIn the past few years many publications have investigated the \nperformance of the 802.11 protocol, particularly in terms of \nframe loss and throughput. The importance of MAC layer \nretransmission and the ARQ retry limit has been stressed and \nits effect on frame loss and delay has been investigated. For \nexample, [2] proposes a retransmission strategy for delay \nsensitive wireless transmission. A throughput analysis of \nWLANs was also reported in [3]. However, most studies are \nbased on static channel models, where the PHY layer packet \nerror rate is independent of time. It is well known that packet \nerrors over a wireless medium are bursty in nature [4]. The \npacket error rate for consecutive packets is not independent, \ndue to the time-correlated characteristics of the mobile \nchannel. This paper shows how the delay and FLR are affected \nby the time-varying fading channel. Frame delay is severely \nunderestimated when the correlation of errors is not taken into \naccount. To the best of the authors’ knowledge, very few \npublications address this issue. Some analytic performance \nstudies have been presented, but these focus on throughput and \npacket loss. [5] presents a packet loss model and a throughput \nanalysis using stochastic models; while [6] evaluates packet \nloss using the Gilbert-Elliot model. In [7] throughput \nestimation of an adaptive ARQ protocol is presented over a \ntime-varying channel based on multiple-state Markov chains. \nThis type of model is most suited to the simulation of slow \nfading channels [7]. Error burst lengths and the bursty nature \nof the wireless channel was studied in [4], with packet delay \nand loss rates explored in [8] via field trials. \nThe impact of Doppler spread on the packet loss and delay \nstatistics for an 802.11 WLAN was investigated in [9] via \nexperimental measurement. In [10], the packet loss rate and \ndelay was analysed using a Jakes time-correlated Rayleigh \nfading simulator [11] for various Doppler frequencies. A \ndetailed analysis of delay and frame loss based on MAC/PHY \ninteraction and the effect of ARQ in a time-correlated fading \nchannel is not reported in the literature. \nIn this paper we study the cross-layer performance of the IEEE \n802.11a/g standard [12] by simulating MAC-to-MAC FLR (or \nframe error rate, FER) and delay. The simulator models the \ntransmission of a time series of queued MAC frames over the \nwireless channel. To replicate the bursty nature of packet \nerrors, an accurate time-correlated channel model is \nimplemented based on the Power Spectral Density (PSD) of \nthe radio channel. Frame delay takes into account the channel \naccess control mechanism, MAC retransmissions and queuing \ndelay in the transmit MAC buffer. This study focuses on FLR \ndue to poor channel conditions and the impact of channel \ncollisions are ignored.  \nSection II provides a brief description of the MAC and PHY \nlayers relating to the IEEE 802.11a/g standard. Section III \ndescribes the fading channel model. Section IV describes the \nMAC frame simulator and section V analyzes the results \nobtained. Finally, section VI provides a set of conclusions. \nII. OVERVIEW OF IEEE 802.11a/g \nMedium Access Control (MAC): The IEEE 802.11 MAC \noffers shared access to the wireless channel using the \nCSMA/CA protocol. Use of the MAC Distributed \nCoordination Function (DCF) with the Basic Access scheme is \nassumed in this work. Before a station starts transmitting it \nmust sense the medium to determine if another transmission is \nalready active. Inter Frame Space (IFS) timing is used to \ncontrol access to the channel. A station starts the transmit \nThe 18th Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC'07) \n \n \nprocess after having sensed the medium for at least the \nduration of a DIFS (Distributed IFS). If the medium is found \nto be idle then the station must wait for a further random back-\noff period. If the channel remains idle during the back-off \nperiod then the station is clear to transmit a packet of data. \nOnce the PHY layer packet has been sent, for unicast \noperation the station expects to receive an acknowledgement \n(ACK) within a SIFS (short IFS) time period. The next \ntransmission cycle begins when the ACK is received (or the \nSIFS time period expires). If no ACK is received within the \nACKtimeout = SIFS period, the MAC frame is scheduled for \nretransmission. This process continues until an ACK is \nsuccessfully received, or the maximum retry count is reached, \nas specified by the maxARQ setting. MaxARQ is user defined, \nwith typical values in the range 0-32. The back-off period after \nthe DIFS interval is divided into slots, each of duration \nSlot_Time. It is possible for a station to transmit at the \nbeginning of each slot, depending on the value of its back-off \ncounter. The slot size, as well as the IFS timing, depends on \nthe chosen PHY technology. Table I quotes the key timing \nparameters for the 802.11b/g MAC. The back-off time is \ndefined as  \nTBo = BackoffCounter * Slot_Time                     (1) \nThe back-off counter is a random variable chosen from a \nuniform distribution in the range [0, W-1], where W represents \nthe Contention Window (CW), based on an exponential back-\noff mechanism. The operation of the back-off counter is \ndescribed in publications such as [3]. The contention window \nparameters are dependent on the PHY layer technology, as \nshown in Table I. A successful transmission cycle, when no \nretransmission is required, has a duration Tsucc defined as \n                  Tsucc = DIFS + TBo  + TData + SIFS + TACK           (2) \nTData represents the duration of the PHY burst and depends on \nthe packet length and the chosen link-speed. TACK represents \nthe duration required to acknowledge successful reception of \nthe MAC frame. If an ACK is not received within ACKtimeout \n=SIFS, the station makes a limited number of retransmission \nattempts, as specified by the maxARQ setting. Assuming that \nN retransmissions take place before a successful \nacknowledgement is received, the total duration of a \nsuccessful transmission cycle is given by (3), where N<= \nmaxARQ: \n         Ttx = N * (DIFS + TData + SIFS) + ∑N\ni\nBo,iT + TACK         (3) \nTABLE I - MAC PARAMETRS \nParameter 802.11 b/g \nSlot Time (µs) 20 \nSIFS (µs) 10 \nDIFS (µs) 50 \nCWmin 31 \nCWmax 1023 \nACK (µs) Mode Dependent \nPHY Header length (µs) 96 - 192 \n \nPhysical Layer (PHY): This paper assumes the use of the \nIEEE 802.11g PHY layer, which operates in the 2.4 GHz \nband. The detailed MAC/PHY model is also capable of \nsupporting 802.11a in the 5 GHz band. 802.11a/g makes use \nof COFDM and provides 8 unique link-speeds via different \ncombinations of modulation and coding, as specified in [12]. \nMAC data frames are mapped to PDU packets for \ntransmission over the PHY layer. The PHY layer simulator \npreviously described in [13] is used to perform bit accurate \nscrambling, convolutional encoding, interleaving and \nmodulation. This model has been extended to support \ncorrelated time varying channel gains for each tap in the \nchannel impulse response. Hence, the instantaneous SNR \nvaries with time (depending on the Doppler Spread) and \nresults are presented in terms of average FLR versus average \nSNR. All averaging is performed over the entire data \ntransmission sequence, which may last for several hundred \nseconds. This PHY layer model is used to evaluate the \noutcome of each PDU packet transmission. A packet error is \nsaid to occur at the PHY layer if an error is encountered during \nthe MAC layer Frame Check Sum (FCS) process. \nIII. CHANNEL MODEL \nAnalytical models of packet loss and throughput for IEEE \n802.11 WLANs were previously reported using stochastic \nmodels, Markov chains or the Gilbert-Elliot model [5][6][7]. \nIn this paper we use a time-varying channel model to replicate \nthe time correlated nature of the SNR observed at the target \nstation.  \nSince the instantaneous channel power varies slowly \n(compared to the packet duration) with time, the resulting \npacket errors at the PHY layer tend to be bursty. This implies \nthat the probability of receiving a packet in error at the PHY \nlayer is strongly correlated in time. Since the probability of \nerror for neighbouring packets is correlated, it is inappropriate \nto model this mechanism independently on a per packet basis.  \nThe channel model replicates multipath fading as a function of \nterminal velocity, carrier frequency and Doppler spectrum. \nThe fading model is based on a Tapped Delay Line (TDL) \nwith each tap experiencing Rayleigh or Ricean fading. The \nseverity of the Rician fading on each tap is controlled via a set \nof K-factors. The spaced-time autocorrelation of the fading \nenvelope is controlled via the definition of a PSD for each \ndelay line. The autocorrelation is imposed onto a set of i.i.d. \nRayleigh samples using a Doppler filter [14][15]. For the \nresults given here, a single Rayleigh fading tap is used with a \nclassical Jakes PSD [11]. Maximum Doppler frequencies of \n4Hz, 24 Hz and 80Hz are considered. \nThe instantaneous signal power is simulated at the receiver for \na given time period. To cover retransmissions, this needs to be \nlonger than the time required to fill the transmit buffer. Given \nknowledge of the noise floor and the average received power \nover the entire time period, the level of signal attenuation \nrequired to model any given average SNR level is computed. \nIV. DESCRIPTION OF SIMULATOR \nThe simulator is capable of predicting the MAC layer FLR \n(and most importantly the time pattern of these losses) as a \nfunction of average SNR, K-factor, PSD, link-speed and \nmaxARQ. An end-to-end block diagram of the simulator is \nshown in fig. 1. The simulator generates evenly time spaced \ndata packets of equal (and user definable) length assuming a \nConstant Bit Rate (CBR) source. These data packets arrive at \nthe transmit 802.11 MAC and are encapsulated into MAC \nframes (one data packet mapped to one MAC frame). As a \nresult, the terms packet and frame are used interchangeably \nhereon. The MAC frames are then passed through a buffer and \nThe 18th Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC'07) \n \n \nultimately encapsulated into PDUs for transmission over the \nwireless medium. Each transmit packet is either received \nsuccessfully, in which case an ACK is sent back, or \nunsuccessfully. The information signal r(k), for received \npacket k, is computed using equation (4), where hc(tk,r) \nrepresents the channel impulse response at the time of \ntransmission, and r denotes the retransmission number. \n                             r(k) = s(k) * hc(tk,r)                                (4) \nIf a packet is received in error, it is sent again in the next \navailable cycle, having contended for the medium. This occurs \nuntil the packet is received, or r reaches the maxARQ limit \n \nFig. 1 Time series MAC/PHY simulator block diagram \nFor the kth packet and rth retransmission, the channel sample \ntime tk,r is computed. If the channel coherence time is low (i.e. \na fast changing channel), then the probability of error may \nimprove significantly after several retransmissions. For a \nslowly changing channel the probability of error is unlikely to \nimprove significantly over the short retransmission period. \nThe FLR is computed at the transmit MAC as the ratio of lost \nframes (i.e. unacknowledged after retransmission up to \nmaxARQ) to the total number of unique transmit frames (i.e. \nretransmit frames do not increment this counter). The total \nframe delay at the MAC, Ttot, is the sum of the transmission \ndelay, Ttran, and the queuing delay, TQ. For the i-th frame, the \ntransmission delay of the resulting PDU (Ttran,i) is computed \nfrom equation (3). The transmission delay occurs as a result of \nthe CSMA/CA protocol, the IFS timings, and any \nretransmissions that are required. The queuing delay \nrepresents the time spent in the MAC frame buffer prior to \ntransmission. The queuing delay for the i-th frame, TQ,i, is \ncomputed as the time difference between the start time of the \nfirst transmission cycle for the i-th frame transmission, Ttx,i, \nand the time the frame was submitted to the MAC queue, TG,i  \nas shown in fig. 2. \n  TQ,i,= Ttx,i  -  TG,I                               (5) \n \nFig 2. Example Timings for a series of MAC frames \nTo compute the queuing delay, it is necessary to simulate a \ntime series of MAC frames passing over the wireless medium. \nIn this analysis, we assume an infinite transmit buffer. In order \nto quantitatively assess the impact of time-correlated channel \nmodelling on the 802.11a/g performance, we simulate two \ncases: a) time-correlated packet transmissions/retransmissions, \nas described above and b) uncorrelated packet transmissions/ \nretransmissions. In the second case the packet errors are \nindependent of time. \nV. RESULTS AND ANALYSIS \nFor the following results, MAC frames are generated at a \nconstant rate of 1 Mbps. The frame length is fixed, taking \nvalues of 800, 1200 and 1400 bytes. The mean SNR was \nvaried over a range from 2-25dB. The maxARQ limit was \nvaried from 0, 4 and 16. Each of the 8 PHY layer link-speeds \nwas modelled. A time-varying channel response was generated \nfor a period of 250 seconds. The maximum Doppler frequency \nwas varied from 4Hz, 24Hz and 80Hz (corresponding to \nmobile speeds of 0.5m/s, 3m/s and 10m/s in a 2.4 GHz \nchannel). Simulations were carried out for 1500 frames with a \nframe length of 800 bytes (resulting in around 10 seconds of \ntransmit data). Results presented here are for link-speed 3 and \nframe length 800 bytes. \n \n(a) \n \n(b) \nFig 3 a-b.  FLR at the MAC vs. SNR, for all Doppler frequencies and \nmaxARQ=0, 4, 16 for a) correlated  and b) uncorrelated transmissions \nFig. 3a shows the FLR at the MAC layer versus average SNR, \nwith and without ARQ, for time-correlated transmissions \nbased on the three Doppler frequencies mentioned above. It \ncan be seen that the FLR improves with increasing ARQ. For a \ngiven maxARQ, the improvement in FLR is better for higher \nDoppler frequencies. When no ARQ is applied the channel \nperformance is similar across all Doppler values. However, for \nmaxARQ =4 and 16, the 80Hz channel clearly generates the \nlowest FLR. This occurs since ARQs are more effective at \nreducing the FLR when the channel decorrelates more rapidly \nwith time. This result agrees with [10]. By comparison, when \npacket transmissions are uncorrelated in time, fig. 3b shows \nthere is no difference in FLR with Doppler.  \nIn fig. 4a the log of the Probability Distribution Function \n(PDF) for the total frame delay is shown for time-correlated \ntransmissions. The mean SNR computed over the entire \ntransmission period was 15dB, with maxARQ =16, and results \nwere generated for all three Doppler frequencies. It can be \nseen that the total MAC-to-MAC delay increases significantly \nas the maximum Doppler frequency decreases, and this agrees \nThe 18th Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC'07) \n \n \nwell with results reported in [9], [10]. A peak delay of around \n12ms was observed for a maximum 80Hz Doppler shift, while \ndelays reached 85ms for a maximum 4Hz Doppler shift. In all \ncases the classic Jakes PSD was assumed. This increase in the \nperceived delay occurs since the number of required ARQs is \nlower in channels with higher Doppler spreads. This arises \nsince the probability of significant channel improvement is \nmuch higher for a given number of ARQ in a fast changing \nchannel. By comparison, fig. 4b shows the delay statistics for \nuncorrelated fading in each of the ARQ cycles. The total delay \nis now much lower for the same set of parameters, and clearly \nthere is no difference between the Doppler frequencies (since \nthe PSD no longer controls the correlation of the channel over \ntime). The range of total delay in fig. 4b is very low, and is \nequal to the case with infinitely high Doppler spread. 802.11 \nsimulations that assume uncorrelated fading in the \nretransmission cycles not only under predict the FER, but also \nseriously under predict the transmission delay.  \n \n(a) \n \n(b) \nFig. 4 a-b. Log(PDF) of total frame delay when maxARQ=16, for \nDoppler frequencies 4, 24, 80Hz, a) for correlated - b) for \nuncorrelated transmissions \nUsing the total MAC-to-MAC delay as a function of average \nSNR, we now compute the percentage of frames that are \ndelayed by more than a given threshold. A threshold of 100ms \nis used since this is a typical upper limit for real-time video \ntransmission [1]. Excessively delayed frames beyond this \nvalue are ignored at the decoder and are thus treated as lost \nframes. We also compute the percentage of lost frames at the \nMAC layer, in addition to the sum of lost frames and \nexcessively delayed frames (i.e. the effective FLR for the real-\ntime video decoder). Fig. 5 shows these various percentages as \na function of SNR. Correlated channel fading for a maximum \nDoppler frequency of 4Hz is assumed together with \nmaxARQ=4.  \nFigs 6a and b show the percentage of effectively dropped \nframes (the sum of those dropped and/or excessively delayed) \nversus mean SNR in a time correlated channel, computed for \neach of the three Doppler frequencies for maxARQ values of 4 \nand 16. We observe that for the same maxARQ, the effective \ndropped frame rate decreases with increasing Doppler. The \nlowest percentages were obtained for the 80Hz channel, with \nnear error free performance for average SNR values in excess \nof 7dB. This result occurs since the total delay encountered by \na frame is much lower at higher Doppler frequencies. For the \nhighest Doppler frequency and at high SNR the percentage of \neffective dropped frames is significantly lower for higher \nmaxARQ values. At high SNR the improvement with \nincreasing maxARQ is reduced for lower Doppler values. At \nlow average SNR values we also observed that the percentage \nof effective dropped packets can increase with increasing \nmaxARQ. This leads to the conclusion that there is a trade-off \nbetween excessively delayed frames and dropped frames as \nthe number of ARQs increases and the mean SNR is low.  \n \nFig. 5. Percentage of frames with total delay>100ms, of frames \ndropped, and of effective frames dropped vs. SNR for Doppler \nfrequency 4Hz, MaxARQ=4  \n(a) \n(b) \nFig. 6 a,b. Percentage of effective dropped frames vs. SNR for time \ncorrelated transmissions as a function of maximum Doppler \nfrequency– top: maxARQ=4  –bottom: maxARQ=16  \n \nFig. 7. Percentage of effective dropped frames vs. SNR for \nuncorrelated transmissions. MaxARQ=4. \nFig. 7 shows the percentage of effective dropped frames \nversus the mean SNR for uncorrelated transmissions. The \npercentage of effective dropped frames is lower than that \nshown previously for correlated transmissions (for all Doppler \nfrequencies and for the same maxARQ). This occurs since the \ntotal delay is evaluated in the uncorrelated case with much \nfewer ARQs and with much lower FLR (as discussed in fig. \n4b). Furthermore, the performance shown for the 4Hz Doppler \nThe 18th Annual IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC'07) \n \n \nchannel with ARQ is much better than the correlated \ntransmission case. Overall, if uncorrelated fading is used in the \nARQ and packet transmission process then the resulting FLR \nand delay statistics seriously under predict the realistic case \nwhere spaced-time correlation is accurately modelled using \nknowledge of the PSD. \nFig. 8 shows, on the left, the PDF of delay between correctly \nreceived frames for a mean SNR=15dB, maxARQ=4, and on \nthe right, the log(PDF) of the error burst length for maxARQ=4 \nand a mean SNR of 5dB. Time correlated transmissions are \nassumed for each of the three Doppler frequencies discussed \nearlier. We observe that delay decreases as Doppler shift \nincreases, since the maxARQ limit is often not required when \nthe fading causes the channel to change rapidly during the \nretransmission cycle. We define the error burst length as the \nnumber of consecutive frames dropped at the receiver. It can \nbe seen that the error burst length decreases significantly with \nincreasing channel Doppler shift. A similar trend is seen for \ndifferent maxARQ values. These results agree well with the \nresults reported in [10]. \n \nFig. 8. (left) Log(PDF) of delay between correct frames MaxARQ=4, \nmean SNR=15dB - (right) Log(PDF) of error burst length \nMaxARQ=4, mean SNR=5dB \n \nFig. 9 Total delay per frame in the simulation flow for correlated \ntransmissions, maxARQ=4, mean SNR=10 dB, Doppler=4Hz \nIn fig. 9 we observe the total delay for each transmit frame \nover a time correlated transmission for fixed values of \nDoppler, mean SNR, link-speed and maxARQ. Under poor \nchannel conditions (low SNR) the transmit frames experience \na high PER and hence a high likelihood of retransmission, \nwhich results in a rapid build up of queuing delay for \nsubsequent frames. Fig. 9 shows an example where the total \ndelay can build up to 160 ms, even with a low maxARQ value. \nDropped frames are shown as marks on the delay=-10 ms line. \n VI. CONCLUSIONS  \nWhile for video applications the reduction in FLR that \nnormally accompanies an increase in maxARQ is desirable, for \nsome scenarios and channel conditions the resulting increase \nin delay and jitter becomes unacceptable. Results show that in \norder to accurately model frame loss, error bursts and delay in \nan 802.11a/g system it is necessary to accurately model the \nradio channel, the PHY layer, and the stop-and-wait ARQ \nmechanism at the MAC layer. \nResults demonstrate that it is vital to include a spaced-time \ncorrelated channel model. Time-correlated modelling includes \nthe impact of Doppler on the ARQ mechanism. If the channel \nis correlated in time, ARQ packets are unlikely to be received \ncorrectly, and hence the FLR and delay will increase. \nFurthermore, for low Doppler spreads the length of the error \nbursts increase and these may have significant impact on error \nresilient video schemes.  \nFor a slow fading time correlated channel the total delay is \nsignificantly higher than that estimated using a simple \nuncorrelated (packet to packet) fading channel. When ARQ is \nused, the percentage of packets delayed beyond 100ms was \nshown to increase together with the dropped packet rate for \nlow Doppler spreads. A trade-off between delay and FLR can \nbe achieved by adjusting the maxARQ parameter. For a given \nvideo application, it is possible to determine the best maxARQ \nvalue for any given Doppler frequency and mean channel \nSNR. Future work will explore the impact of error bursts, \nframe loss and delay on the 802.11a/g performance of error \nresilient video codecs for a range of Doppler spreads.   \nACKNOWLEDGEMENTS \nThis work was partly funded by the Department of Trade and Industry (DTI, \nUK), Technology Programme project, VISUALISE. \nREFERENCES \n[1] Mario Baldi and Yoram Ofek, ‘End-to-End Delay Analysis of \nVideoconferencing over Packet-Switched Networks’, IEEE/ACM \nTransactions on Networking, VOL. 8, NO. 4, AUGUST 2000 \n[2] M. van der Schaar and D. Turaga, ‘Cross-Layer Packetisation and \nRetransmission strategies for delay sensitive wireless multimedia \ntransmission’, IEEE transactions on Multimedia, VOL.9, No.1, Jan. 2007 \n[3] Pierre Ferre, Angela Doufexi, Andrew Nix and David Bull, ‘Throughput \nAnalysis of IEEE 802.11 and IEEE 802.11e MAC’, IEEE WCNC 2004 \n[4] Andreas Willig, Martin Kubisch, Christian Hoene, and Adam Wolisz, \n‘Measurements of a Wireless Link in an Industrial Environment using an \nIEEE 802.11-Compliant Physical Layer’, IEEE Transactions on \nIndustrial Electronics, volume 49-6, Dec. 2002  \n[5] Giuseppe Bianchi, ‘Performance Analysis of the IEEE802.11 Distributed \nCoordination Function,’ IEEE JSAC, March 2000. \n[6] Pierre Ferre, Dimitris Agrafiotis, Tuan Kiang Chiew, Angela Doufexi, \nAndrew Nix, David Bull, ‘Packet Loss Modelling for H.264 Video \nTransmission over IEEE 802.11g Wireless LANs’, IEEE WIAMIS 2005 \n[7] H. Minn et al, ‘An efficient ARQ Protocol for adaptive error control over \ntime-varying channels’, Wireless Personal Communications, Kluwer \nAcademic, 17: 3-20, 2001 \n[8] T.K. Chiew, P. Ferre, D. Agrafiotis, A. Molina, A. Nix, and D. Bull, \n‘Cross-Layer WLAN Measurement and Link Analysis for Low Latency \nError Resilient Wireless Video Transmission’, ICCE, Las Vegas, Jan. \n2005. \n[9] C. Hoene, A. Gunther, A. Wolisz, ‘Measuring the impact of slow user \nmotion on packet loss and delay over IEEE 802.11b wireless links’, \nIEEE International Conference on Local Computer Networks(LCN’03), \n2003 \n[10] K-W. Lee, M. Cheng, L.F. Chang, ‘Wireless QoS Analysis for a \nRayleigh Fading Channel’, IEEE International Conference on \nCommunications (ICC’98), June 1998 \n[11]  W.C. Jakes, ‘Microwave Mobile Communications’, IEEE Press, New \nYork, 1974 \n[12] IEEE Std 802.11g; Part 11: Wireless LAN Medium Access Control \n(MAC) and Physical Layer (PHY) Specifications: Further High-Speed \nPhysical Layer in the 2.4 GHz Band, d1.1, 2001. \n[13] A. Doufexi, S. Armour, M. Butler, A. Nix, D. Bull, and J. McGeehan, ‘A \nComparison of the HIPERLAN/2 and IEEE 802.11a Wireless LAN \nStandards, IEEE Communications Magazine, May 2002, pp172-180. \n[14]  C. Komninakis, ‘A Fast and Accurate Rayleigh Fading Simulator’, IEEE \nGLOBECOM 2003. \n[15] IEEE 802.16 standards, IEEE 802.16.3c-01/2 9r4, ‘Channel Models for \nFixed Wireless Applications’.  \n",
            "id": 17386414,
            "identifiers": [
                {
                    "identifier": "29025701",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "385682453",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1109/pimrc.2007.4394641",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:research-information.bris.ac.uk:openaire_cris_publications/d9c47fc1-db53-4d88-b52b-3d9825c686c0",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "oai:research-information.bris.ac.uk:publications/d9c47fc1-db53-4d88-b52b-3d9825c686c0",
                    "type": "OAI_ID"
                }
            ],
            "title": "Frame delay and loss analysis for video transmission over time-correlated 802.11A/G channels",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:research-information.bris.ac.uk:publications/d9c47fc1-db53-4d88-b52b-3d9825c686c0",
                "oai:research-information.bris.ac.uk:openaire_cris_publications/d9c47fc1-db53-4d88-b52b-3d9825c686c0"
            ],
            "publishedDate": "2007-09-03T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 36628315,
                    "title": "A Fast and Accurate Rayleigh Fading Simulator’,",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1109/glocom.2003.1258847",
                    "raw": "C. Komninakis, ‘A Fast and Accurate Rayleigh Fading Simulator’, IEEE GLOBECOM 2003.",
                    "cites": null
                },
                {
                    "id": 36628309,
                    "title": "An efficient ARQ Protocol for adaptive error control over time-varying channels’,",
                    "authors": [],
                    "date": "2001",
                    "doi": null,
                    "raw": "H. Minn et al, ‘An efficient ARQ Protocol for adaptive error control over time-varying channels’, Wireless Personal Communications, Kluwer Academic, 17: 3-20, 2001",
                    "cites": null
                },
                {
                    "id": 36628305,
                    "title": "Cross-Layer Packetisation and Retransmission strategies for delay sensitive wireless multimedia transmission’,",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1109/tmm.2006.886384",
                    "raw": "M. van der Schaar and D. Turaga, ‘Cross-Layer Packetisation and Retransmission strategies for delay sensitive wireless multimedia transmission’, IEEE transactions on Multimedia, VOL.9, No.1, Jan. 2007",
                    "cites": null
                },
                {
                    "id": 36628310,
                    "title": "Cross-Layer WLAN Measurement and Link Analysis for Low Latency Error Resilient Wireless Video Transmission’, ICCE,",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1109/icce.2005.1429775",
                    "raw": "T.K. Chiew, P. Ferre, D. Agrafiotis, A. Molina, A. Nix, and D. Bull, ‘Cross-Layer WLAN Measurement and Link Analysis for Low Latency Error Resilient Wireless Video Transmission’, ICCE, Las Vegas, Jan. 2005.",
                    "cites": null
                },
                {
                    "id": 36628308,
                    "title": "Dimitris Agrafiotis, Tuan Kiang Chiew, Angela Doufexi, Andrew Nix, David Bull, ‘Packet Loss Modelling for H.264 Video Transmission over",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1117/12.633376",
                    "raw": "Pierre Ferre, Dimitris Agrafiotis, Tuan Kiang Chiew, Angela Doufexi, Andrew Nix, David Bull, ‘Packet Loss Modelling for H.264 Video Transmission over IEEE 802.11g Wireless LANs’, IEEE WIAMIS 2005",
                    "cites": null
                },
                {
                    "id": 36628304,
                    "title": "End-to-End Delay Analysis of Videoconferencing over Packet-Switched Networks’,",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1109/90.865076",
                    "raw": "Mario Baldi  and Yoram Ofek, ‘End-to-End Delay Analysis of Videoconferencing over Packet-Switched Networks’, IEEE/ACM Transactions on Networking, VOL. 8, NO. 4, AUGUST 2000",
                    "cites": null
                },
                {
                    "id": 36628306,
                    "title": "Measurements of a Wireless Link in an Industrial Environment using an IEEE 802.11-Compliant Physical Layer’,",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1109/tie.2002.804974",
                    "raw": "Andreas Willig, Martin Kubisch, Christian Hoene, and Adam Wolisz, ‘Measurements of a Wireless Link in an Industrial Environment using an IEEE 802.11-Compliant Physical Layer’, IEEE Transactions on Industrial Electronics, volume 49-6, Dec. 2002",
                    "cites": null
                },
                {
                    "id": 36628311,
                    "title": "Measuring the impact of slow user motion on packet loss and delay over",
                    "authors": [],
                    "date": null,
                    "doi": "10.1109/lcn.2003.1243198",
                    "raw": "C. Hoene, A. Gunther, A. Wolisz, ‘Measuring the impact of slow user motion on packet loss and delay over IEEE 802.11b wireless links’, IEEE International Conference on Local Computer Networks(LCN’03),",
                    "cites": null
                },
                {
                    "id": 36628314,
                    "title": "Microwave Mobile Communications’,",
                    "authors": [],
                    "date": "1974",
                    "doi": "10.1109/9780470545287",
                    "raw": "W.C. Jakes, ‘Microwave Mobile Communications’, IEEE Press, New York, 1974",
                    "cites": null
                },
                {
                    "id": 36628307,
                    "title": "Performance Analysis of the",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "Giuseppe Bianchi, ‘Performance Analysis of the IEEE802.11 Distributed Coordination Function,’ IEEE JSAC, March 2000.",
                    "cites": null
                },
                {
                    "id": 36628312,
                    "title": "Wireless QoS Analysis for a Rayleigh Fading Channel’,",
                    "authors": [],
                    "date": "1998",
                    "doi": "10.1109/icc.1998.685178",
                    "raw": "K-W. Lee, M. Cheng, L.F. Chang, ‘Wireless QoS Analysis for a Rayleigh Fading Channel’, IEEE International Conference on Communications (ICC’98), June 1998",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "https://research-information.bris.ac.uk/ws/files/3007645/Sgardoni_PIMRC07.pdf"
            ],
            "updatedDate": "2021-10-13T19:23:17",
            "yearPublished": 2007,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/29025701.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/29025701"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/29025701/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/29025701/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/17386414"
                }
            ]
        },
        {
            "acceptedDate": "2008-12-09T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Beach, MA"
                },
                {
                    "name": "Nix, AR"
                },
                {
                    "name": "Pan, Y"
                }
            ],
            "contributors": [
                "Yuwen"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/194023677",
                "https://api.core.ac.uk/v3/outputs/29025777"
            ],
            "createdDate": "2015-02-17T16:05:08",
            "dataProviders": [
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                },
                {
                    "id": 286,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/286",
                    "logo": "https://api.core.ac.uk/data-providers/286/logo"
                }
            ],
            "depositedDate": "2008-09-01T00:00:00",
            "abstract": null,
            "documentType": "research",
            "doi": "10.1109/pimrc.2008.4699870",
            "downloadUrl": "https://core.ac.uk/download/29025777.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "                          Pan, Y., Nix, A. R., & Beach, M. A. (2008). A game theoretic approach to\ndistributed resource allocation for OFDMA-based relaying networks. In\nIEEE Personal and Indoor Mobile Radio Conference 2008 (PIMRC), Cannes.\n(Vol. 1, pp. 1 - 5). Institute of Electrical and Electronics Engineers (IEEE).\n10.1109/PIMRC.2008.4699870\nLink to published version (if available):\n10.1109/PIMRC.2008.4699870\nLink to publication record in Explore Bristol Research\nPDF-document\nUniversity of Bristol - Explore Bristol Research\nGeneral rights\nThis document is made available in accordance with publisher policies. Please cite only the published\nversion using the reference above. Full terms of use are available:\nhttp://www.bristol.ac.uk/pure/about/ebr-terms.html\nTake down policy\nExplore Bristol Research is a digital archive and the intention is that deposited content should not be\nremoved. However, if you believe that this version of the work breaches copyright law please contact\nopen-access@bristol.ac.uk and include the following information in your message:\n• Your contact details\n• Bibliographic details for the item, including a URL\n• An outline of the nature of the complaint\nOn receipt of your message the Open Access Team will immediately investigate your claim, make an\ninitial judgement of the validity of the claim and, where appropriate, withdraw the item in question\nfrom public view.\nA Game Theoretic Approach to Distributed\nResource Allocation for OFDMA-Based Relaying\nNetworks\nYuwen Pan, Andrew Nix and Mark Beach\nDepartment of Electrical and Electronic Engineering, University of Bristol, UK\nEmail: {Yuwen.Pan.Andy.Nix.M.A.Beach}@bristol.ac.uk\nAbstract-In this paper, algorithms on distributed resource\n(spectrum and power) sharing for relay stations are investigated\nfor downlink transmissions in an OFDMA-based relay-aided cell.\nBoth system capacity and user fairness are considered. By group-\ning the relay stations into coalitions according to the set of users\nthey are relaying, the optimal resource allocation can be solved by\nconsidering resource allocation within and among the coalitions.\nThe algorithm for intra-coalition resource allocation is proposed\nby utilizing the key observation: for each data symbol transmitted\nfrom the base station to a user (in a subcarrier), only one among\nall the available relay stations is required to relay the symbol.\nThe inter-coalition resource allocation is modeled by both a non-\ncooperative and a cooperative game, where the cooperative game\nis solved by a nonsymmetric Nash bargaining solution. Simulation\nresults show that the non-cooperative algorithm outperforms\nrandom allocation by approximately 50% in system capacity\nwith 3 relay stations in each coalition. The cooperative algorithm\nhas approximately 5% loss in system capacity comparing with\nthe non-cooperative algorithm, but achieves a significant gain in\nterms of fairness performance.\nI. INTRODUCTION\nBoth orthogonal frequency division multiple access\n(OFDMA) and relaying are regarded as leading candidates for\nfuture generation cellular networks [1], [2]. OFDMA serves\nas a promising multiple-access technique for high-data-rate\ntransmissions while relaying helps to increase system capacity,\ntransmission reliability, as well as coverage. In an OFDMA\nrelaying network, the base station (BS) first sends out data\nsymbols which are carried by subcarriers, and relay stations\n(RS) retransmit these data symbols carried by their own\nsubcarriers; most likely in different frequency bands and in\na different order to explore multiuser diversity and utilize\nfrequency selective fading channels. In this paperl , based on\nthe estimated channel conditions of the multiuser downlinks,\nwe investigate a distributed optimal spectrum sharing and\npower allocation strategy for the RSs in a decode-and-forward\nOFDMA-based relaying system in terms of both total system\ncapacity and user fairness.\nOFDMA resource allocation without relaying is well re-\nported in the literature [3], [4], [5], [6], while relaying cases\n1The work reported in this paper has formed part of the Delivery Efficiency\nCore Research Programme of the Virtual Centre of Excellence in Mobile and\nPersonal Communications, Mobile VCE, www.mobilevce.com.This research\nhas been funded by EPSRC and by the Industrial Companies who are\nMembers of Mobile VCE. Fully detailed technical reports on this research\nare available to Industrial Members of Mobile VCE.\n978-1-4244-2644-7/08/$25.00 ©2008 IEEE\nhave only been considered in a number of limited scenarios\ndue to their high complexity [7], [8], [9]. All the previous\nworks in OFDMA relaying either do not consider the resource\nallocation problem or make unrealistic assumptions to simplify\nthe problem. In [10], [11], [12], the authors take a different\napproach and make use of the concept of bargaining in game\ntheory to model subcarrier sharing among the user equipment\n(UE) for the OFDMA downlink/uplink transmissions (without\nrelaying). The concept of a Nash bargaining solution is used\nin these studies, which provides a fair operation point in a\ndistributed implementation.\nOur approach differs from previous work since we consider\npractical assumptions on relaying scenarios. RSs are assumed\nto have individual power constraints and they use the same\nor different frequency spectrum for relaying (Le., sensed by\ncognitive radio). Some RSs may be designated to the same\nset of UEs and they have shared objectives, while some RSs\nmay be designated to a completely different set of UEs and\nthey may compete among each other for the shared resource.\nSuch a system can be naturally modeled as a game. Greedy\nsubcarrier and power allocation schemes for each RS-UE link\nare smartly chosen to explore the global resource utilization\nefficiency in improving capacity and fairness.\nWe group the set of RSs designated to the same set of\nUEs into a coalition. The optimal resource allocation of the\nwhole system is then divided into two subproblems: (1) how\nto share the spectrum and allocate power among the RSs in\nthe same coalition, and (2) how to share the spectrum among\ndifferent coalitions. For the first subproblem, we prove that the\noptimal resource allocation theorem for the relaying network\nproposed in our previous work [9] still holds under the new\nassumptions. A greedy algorithm is proposed. For the second\nsubproblem, we first model it as a non-cooperative game and\nshow that under good channel conditions, it converges fast.\nWe then model the game as a cooperative bargaining problem\nand nonsymmetric Nash bargaining solutions are utilized for\nthe weighted fairness among the coalitions. The proposed\nalgorithms have low complexity and simulation results show\nthat they effectively enhance the total capacity and maintain\nthe user fairness.\nThe paper is organized as follows. Section II describes our\nrelaying network model. In Section III, the preliminaries in\ngame theory are introduced and the problem is formulated\nAuthorized licensed use limited to: UNIVERSITY OF BRISTOL. Downloaded on January 13, 2009 at 10:53 from IEEE Xplore.  Restrictions apply.\nusing game theory. In Sections IV, V and VI, we investigate\nthe resource allocation problems both intra-coalition and inter-\ncoalition. Section VII evaluates the performance of these\nalgorithms. Finally, conclusions are drawn in Section VITI.\nII. SYSTEM MODEL\nThis paper investigates the problem of resource allocation\nin an OFDMA-based relaying wireless network. OFDMA\ndownlink transmissions (with N data subcarriers) in a single\ncell are studied. The system consists of one base station (BS),\nK mobile users or user equipment (UE) and L relay stations\n(RS), and each is equipped with a single antenna. Inter-\ncell interference is not considered in the additive Gaussian\nnoise. Let U = {u1, ... , UK} denote the set of UEs, and\nR = {rl' ... , r L} denote the set of RSs. Unlike most previous\nwork, we consider a practical and flexible relaying scenario\nbased on fixed relays. As shown in Figure l(a), each RS\nr i relays for some UEs Ui ~ U. The set of UEs Ui can be\ndetermined in many ways, for example location proximity or\nthe UEs who pay for the relaying service. In the case that\nsome RSs use the same spectrum to relay for different UEs,\nthe interference to each other is considered as noise. The set\nof RSs who relay for UE Ui is denoted as R i ~ R, Le., a UE\nmay have more than one RS that is dedicated to relay for it.\nThe relaying scenario works as follows. The BS firstly trans-\nmits information symbols to UEs, and RSs also decode the\nsymbols and retransmit them (decode-and-forward relaying) in\ndifferent time slots or frequency bands (free spectrum chunks\ndetected by cognitive radio [13]). We assume that channel\ngains between the BS and RSs are high enough (e.g., selective\ndecode-and-forwarding is performed) so that the probability of\na decoding error at each RS is small regardless of the power\nand modulation schemes adopted at the BS. RSs select and\nallocate different spectrum (subcarriers) and power for each\nUE to utilize the multiuser diversity, as shown in Figure 1(b).\nPerfect channel state information at the receivers is as-\nsumed, and it is fed back to the senders (BS or RSs).The RSs\nnegotiate with the UEs after the subcarrier allocation so that\nthe UEs are able to correctly combine the relayed symbols with\nthe symbol sent by the BS for better decoding. We also assume\nthat UEs send back to the BS the information of combine\nchannels (e.g. combined signal-to-noise ratio). This allows the\nBS to exploit adaptive modulation and coding schemes to\nimprove system capacity.\nB. Nonsymmetric Nash Bargaining Solution\nSimilar to the NBS, the nonsymmetric Nash bargaining\nsolution [14], [15] satisfies,\nIII. GAME THEORY CONCEPTS AND PROBLEM\nFORMULATION\nGame theory aims to study the interactions among a set\nof decision makers, called players. The resource allocation\nproblem is similar to a bargaining problem in game theory,\nwhere a set of decision makers bargain among themselves for\nsome shared resource. In this section, we will first introduce\nthe basic concepts of game theory and then formulate the\nproblem.\nn\nA. Nash Bargaining Solution\nLet N = {I, ... , n} denote the set of players and let F\ndenote a closed and convex subset of Rn , representing the set\nof feasible payoff allocations that the players can get if they all\nwork together. Let d = (d1, ... , dn ) denote the disagreement\npayoff allocation that the players would expect if they did not\ncooperate, and suppose that {y E FIYi ~ di, 'Vi E N} is a\nnonempty bounded set. The pair (F, d) is called an n-person\nbargaining problem [14].\nDefinition 1: A solution to the bargaining problem (F, d),\n¢(F, d), is called a Nash bargaining solution (NBS), if the\nfollowing axioms are satisfied [14].\n1) Weak Pareto Efficiency: there is no other vector y E F\nsuch that Yi > ¢i(F, d) for every i in N.\n2) Individual Rationality: ¢(F, d) 2:: d.\n3) Scale Covariance: For any linear transformation 'ljJ of F,\n¢('ljJ(F), 'ljJ(d)) = 7jJ(¢(F, d)).\n4) Independence of Irrelevant Alternatives: For any closed\nconvex Q ~ F, if ¢(Q, d) E Q, then ¢(Q, d) = ¢(F, d).\n5) Symmetry: if F is invariant under all exchanges of\nagents, then ¢i(F,d) = ¢j(F,d), 'Vi,j EN.\nThere is exactly one bargaining solution that satisfies the\nabove axioms, which is the NBS, stated in the following\ntheorem [14].\nTheorem 1: There is a unique solution function ¢(F, d)\nthat satisfies all five axioms in Definition 1, and the solution\nsatisfies,\nn\n¢(F, d) E arg max II (Xi - di)Wi , (2)\nXEF,x~d i=l\nwhere Wi represents the weight of player i and L~l Wi = 1.\nDefinition 2: A dictatorial solution [15] of player i,\n¢P (F, d), represents the optimal utility of player i by as-\nsuming all other players adopting the strategies to achieve the\nutilities at the disagreement point d.\nDefinition 3: A solution to the bargaining problem (F, d),\n¢(F, d), is called a nonsymmetric Nash bargaining solution\n(NNBS) , if the following axioms are satisfied [15].\n1) Weak Pareto Efficiency.\n1 2 3 4 N-2 N-1 N\nOFDM Subcarrier Group\n(b) Subcarrier allocation at RS l(a) A relay-aided cell\nFig. 1. Relaying scenario\nAuthorized licensed use limited to: UNIVERSITY OF BRISTOL. Downloaded on January 13, 2009 at 10:53 from IEEE Xplore.  Restrictions apply.\n2Although it is more natural to define the players as the set of DEs U,\nallowing DEs to bargain among each other is not efficient for a power-limited\ndevice and the fairness is also hard to control with the possibility of malicious\nplayers.\nwhere ¢f (F, d) are dictatorial solutions.\nIt is proven in [15] that, when d = 0, a NNBS corresponds\nto the solution for weighted proportional fairness by solving\nmaxXEF E~lWi log Xi.\nc. Problem Formulation\nGiven the definitions and properties of various solutions to\na bargaining problem, we formally formulate our problem in\nthis section.\nWe define the set of players as the set of RSs n 2 with size\nL. The utility v(ri) of the RS i is defined as,\nmembers. If the assumption does not hold, each coalition will\nnot be able to purely consider other coalitions as a competitive\nrelationship because some of its member may also belong to\nother coalitions. This will further complicate the situation and\nis beyond the scope of this paper.\nIn the following sections, we will firstly discuss how the\nutility shall be shared within the coalition and how the power\nshall be allocated. We will then discuss both non-cooperative\nand cooperative solutions for resource sharing among coali-\ntions.\n3Note that although the RSs share the same spectrum, in Theorem 3 (intra-\ncoalition), we assume that the RSs do not interfere with each other by not\nallocating same subcarrier to different DEs. This is because they may have\nstrong interference to each other due to location proximity. For inter-coalition\nresource allocation in later sections, this assumption is relaxed.\nIV. RESOURCE ALLOCATION WITHIN COALITIONS\nThe resource allocation, especially the subcarrier allocation\nproblem, is a combinatorial matching problem that is known to\nbe NP-complete. Before we introduce the detailed distributed\nresource allocation algorithm, we firstly show several theorems\nwhich can greatly reduce the complexity of the resource\nallocation strategy.\nThe following theorem is cited from our previous work in\n[9].\nTheorem 2: In the general situation, for a relaying network\nwhere the number of subcarriers K is much larger than the\nnumber of RSs L, and L is a small integer value, there is\napproximately only one RS l among all L RSs that needs to\nrelay for any subcarrier transmitted from the BS. The optimal\npower allocation is water-filling by considering power from\nother RSs as noise.\nTheorem 2 states that, within a coalition S, for any subcar-\nrier from the BS to a UE that is dedicated to this coalition,\nthe optimal solution only requires one RS to relay for this\nsubcarrier among all the RSs in the coalition S. It is obvious\nthat the optimal power allocation scheme is actually a Nash\nequilibrium. However, the theorem has an assumption that the\nRSs strictly use different spectrum and that they strictly do\nnot interfere with each other.\nWe assume in this paper that RSs may share, and most likely\ndo share the same spectrum 3, especially for the RSs in the\nsame coalition (due to the possible location proximity). We\nextend the above theorem as follows.\nTheorem 3: The maximum capacity C2max under the as-\nsumption of RSs using the same frequency spectrum for\ntransmissions is equal to the maximum capacity Clmax under\nthe assumption of RSs using different frequency spectrum\nfor transmissions; and their power allocation schemes are the\nsame, provided that the subcarriers have already been allocated\nin the same way for both scenarios.\nProof: Due to space limitation, we only give a simple\nproof based on intuition. It is natural to see that in the\nassumption that RSs use different spectrum, maximal ratio\ncombining (MRC) can be applied at the UEs. Thus, given\nthe same subcarrier and power allocations, it will always be\n(3)\n(5)\n(4)v(ri) = L R(u),\nUEUi\nn\n¢(F, d) 2 L Wi¢P (F, d),\ni=l\n2) Individual Rationality.\n3) Independence of Irrelevant Alternatives.\n4) Disagreement Point Convexity (DPC): let x= ¢(F, d)\nbe a solution outcome, then the disagreement point d' =\n(1 - A)d + AX leads to the same outcome.\n5) Domination of Weighted Dictatorial Solution\n(DWD(w)): with weights E~=l Wi = 1, a solution\n¢(F, d) is domination of weighted dictatorial solution\nif,\nwhich is the summation of the information rates of the DEs\nthat it relays for. The information rate R(Uj) for UE Uj\nis defined by the summation of the Shannon capacity of\neach subcarrier by considering the BS-UE signal-to-noise ratio\n(SNR), as well as the RS-DE SNR, where the signals from\nother RSs that do not relay for this subcarrier are treated as\ninterference.\nWe define a coalition S to be the group of RSs that relay\nfor the same subset of DEs, i.e., if U' is any subset of U, then,\nThe utility is transferable within a coalition because of the\ncontinuous power allocation by water-filling (we will show\nin the next section that water-filling is still optimal under\na relaying scenario). For example, if a RS ri gives up a\nsubcarrier for RS r j (both are in the same coalition), then\nthe increase in SNR of one subcarrier r j will result in the\nincrease of the water level and thus benefits all the users that\nare relayed by rj.\nThere are potentially many coalitions in the network. We\nfurther simplify the problem by assuming if two RSs r i and r j\nrelay for the same UE, then they have exactly the same subset\nof UEs to relay for, i.e., Ui = Uj • Under this assumption, a\nRS only belongs to one coalition. Thus, each coalition treats\nitself as a \"larger\" player and competes with other coalitions\nfor resource. Any benefit to a coalition can be transferred to its\nAuthorized licensed use limited to: UNIVERSITY OF BRISTOL. Downloaded on January 13, 2009 at 10:53 from IEEE Xplore.  Restrictions apply.\nsuperior to the assumption that RSs use the same spectrum.\nHowever, since the optimal subcarrier allocation assumes\nonly one RS relays for a subcarrier, it is applicable to both\nscenarios. Therefore, their optimal subcarrier allocations are\nthe same. The power allocations are also the same. •\nSimilar to the case in [9], when considering user fairness,\nTheorem 3 still holds for any single DE. The RSs in a\ncoalition can simply adjust the power allocated to each user\nto control any fairness scheme such as max-min fairness [4]\nor proportional fairness [6].\nA simple and efficient greedy algorithm CoalitionAllocate()\nbased on the previous analysis is described as follows. The\naim of the algorithm is to allocate subcarriers and power at\neach RS to maximize the capacity of all the DEs serviced by\nthe RSs in a coalition; user fairness can also be optionally\nmaintained.\n1) Assume power is proportional to the number of subcar-\nriers allocated to the relay for a VE at each RS.\n2) For each subcarrier from the BS to the DE, find the best\nsubcarrier among all which have not been allocated for\nthe DE by comparing a combination of both water-level\nand channel condition at each subcarrier.\n3) According to the fairness scheme, iteratively adjust the\npower and recalculate the information rates for each VE,\nuntil the maximum step has been reached or the fairness\ncriteria is matched.\nThe algorithm has a running time proportional to the number\nof UEs serviced by the coalition, as well as the amount\nof adjustments for fairness maintenance. The key idea is to\ngreedily match the best RS to the DE using Theorem 3. Note\nthat this algorithm is a centralized algorithm. It requires a\nselected RS to act as a coalition decision maker. The RS needs\nto overhear the information feedback from DEs to other RSs\nin the same coalition before the algorithm can be executed.\nV. NON-COOPERATIVE RESOURCE ALLOCATION\nIn this section, we consider the non-cooperative resource\nallocation among coalitions. The algorithm can be run on\nthe selected coalition decision makers for distributed spectrum\nsharing of coalitions. As mentioned in previous sections, the\nrelationship among coalitions is competitive. The objective of\nthe non-cooperative scheme is to reach the Nash equilibrium in\na non-cooperative game where no RS can change its subcarrier\nallocation alone to benefit himself.\nFor a distributed system, such an equilibrium has to be\nreached in rounds. Due to space limitation, we only describe\nthe basic idea below. Each coalition distributively and locally\nallocates the resource (the algorithm described in Coalition-\nAllocate() using known channel conditions based on all other\ncoalitions' decision so far in tum. After a coalition makes\na decision, it informs other coalitions about its decision for\ninformation update. The process terminates when either a\npredefined maximum running round or an equilibrium has\nreached.\nDue to the discrete nature of the subcarrier allocation, this\nalgorithm may not reach the equilibrium. However, when the\ntotal bandwidth of the RSs is not smaller than the bandwidth\nof the BS, and when the channel conditions of all RSs are\ngood (which is most likely satisfied due to the multiuser\ndiversity), the algorithm converges fast. This is because if a\ncoalition 8 1 uses a frequency band, it is not likely for another\ncoalition 82 who makes decision later to make use of the\nsame frequency band, unless the interference caused by the\ncoalition 81 is negligible. In the later case, at the next round\nof execution of coalition 8 1, the decision from coalition 8 2\nin the previous round is unlikely to affect 81 's decision due\nto small levels of interference. Note that this algorithm may\ngive supreme priority to the coalition that starts first. Fairness\namong coalitions are hardly obtained.\nVI. COOPERATIVE RESOURCE ALLOCATION\nWe now discuss the cooperative resource allocation among\ncoalitions. The aim of cooperation is to maximize the total\ncapacity of VEs in each coalition while keeping fairness\namong the coalitions.\nWe choose to make use of the NNBS, which maximizes\nrr~=1 (Xi - di)Wi and maintains weighted proportional fairness\namong the coalitions. For each coalition i, we define its weight\nWi as the percentage of the subcarriers or DEs that it relays\nover the total number of subcarriers from the BS (or the total\nnumber of VEs). A coalition relaying for more DEs should\nhave higher priority. When the weighted fairness is maintained\namong the coalitions and each coalition also maintains its\nown fairness among DEs as shown in CoalitionAllocate(), the\nfairness among VEs is also maintained.\nObserve that, for a convex solution space F, the NNBS\nsatisfies the axiom DWD(111) in Definition 3. When d = 0, it\nis easy to see that,\n¢(F, d) 2: (w1¢f(F, d), ... ,wn¢~(F,d)) (6)\nThe dictatorial point is easy to obtain locally. Assuming\nd = 0, each coalition can simply apply CoalitionAllocate() by\nassuming there is no interference from other RSs to obtain its\ndictatorial point. Therefore, by applying Equation 6, the lower\nbound of the optimal solution can be obtained.\nWe propose the algorithm in Figure 2. The key idea of\nthis algorithm is to let the coalitions who lead the bound\nmost \"help\" those who lag the bound most by exchanging\nand reallocating their subcarriers. Each coalition shall locally\ncalculate its own dictatorial solution. The system initialization\ncan be either randomized, or using non-cooperative resource\nallocation shown in Section V.\nSince this is a distributed algorithm, the step that reallocates\nsubcarriers among the two sets of coalitions may require a\nlarge number of information exchanges. There are two ways\nto reduce such complexity. The coalitions that are not adjacent\nto each other (thus have negligible interference to each other)\nmay be neglected from such subcarrier reallocation. Alterna-\ntively, as proposed in [10], a solution based on the Hungarian\nmethod firstly finds a matching pair and then lets only the two\npairs exchange and reallocate subcarriers.\nAuthorized licensed use limited to: UNIVERSITY OF BRISTOL. Downloaded on January 13, 2009 at 10:53 from IEEE Xplore.  Restrictions apply.\nFig. 3. Simulation Results\nVIII. CONCLUSION\nIn this paper, resource allocation strategies have been inves-\ntigated in an OFDMA-based relaying network. The system was\nmodeled as a game where RSs with the same objectives are\ngrouped into coalitions. The detailed algorithms on how the\nsubcarriers and power can be allocated within each coalition,\nand how the subcarriers can be shared among the coalitions,\nwere discussed. Both non-cooperative and cooperative solu-\ntions based on the NNBS were proposed for inter-coalition\nresource allocation. Simulation results showed that the coop-\nerative algorithm had a 5% loss in system capacity comparing\nwith the non-cooperative algorithm with 3 RSs in each coali-\ntion, but achieved a significant gain by approximately 25% in\nterms of fairness performance.\nREFERENCES\n[1] A. Doufexi and S. Armour, \"Design Considerations and Physical Layer\nPerformance Results for 4G OFDMA System Employing Dynamic Sub-\ncarrier Allocation,\" in IEEE 16th International Symposium on Personal,\nIndoor and Mobile Radio Communications, Berlin, Germany, 2005.\n[2] J. Laneman, D. Tse, and G. W. Wornell, \"Cooperative Diversity in\nWireless Networks: Efficient Protocols and Outage Behavior,\" IEEE\nTransactions on Information Theory, vol. 50, pp. 3062-3080,2004.\n[3] C. Y. Wong, R. S. Cheng, K. B. Letaief, and R. D. Murch, \"Multiuser\nOFDM with Adaptive Subcarrier, Bit and Power Allocation,\" IEEE\nJournal on Selected Areas in Communications, vol. 17, pp. 1747-1758,\n1999.\n[4] W. Rhee and 1. M. Cioffi, \"Increase in Capacity of Multiuser OFDM\nSystem Using Dynamic Subchannel Allocation,\" in IEEE Vehicular\nTechnology Conference Proceedings, Tokyo, Japan, May 2000.\n[5] J. Jang and K. B. Lee, \"Transmit Power Adaptation for Multiuser OFDM\nSystems,\" IEEE Journal on Selected Areas in Communications, vol. 21,\npp. 171-178,2003.\n[6] Z. Shen, 1. G. Andrews, and B. L. Evans, \"Adaptive Resource Allocation\nin Multiuser OFDM Systems With Proportional Rate Constraints,\" IEEE\nTransactions on Wireless Communications, vol. 4, pp. 2726-2737, 2005.\n[7] Guoqing Li and Jui Liu, \"On the Capacity of the Broadband Relay\nNetworks,\" in Proceedings of the 38th Annual Asilomar Conference on\nSingals, Systems and Computers, CA, USA, 2004.\n[8] C. Bae and D.-H. Cho, \"Adaptive Resource Allocation Based on\nChannel Information in Multihop OFDM Systems,\" in IEEE Vehicular\nTechnology Conference Proceedings, Montreal, Canada, 2006.\n[9] Y. Pan, A. Nix, and M. Beach, \"Resource Allocation Techniques for\nOFDMA-Based Decode-and-Forward Relaying Networks,\" in IEEE 67th\nVehicular Technology Conference (VTC) , Singapore, May 2008.\n[10] Z. Han, Z. Ji, and K. J. R. Liu, \"Fair Multiuser Channel Allocation for\nOFDMA Networks Using Nash Bargaining Solutions and Coalitions,\"\nIEEE Transactions on Communications, vol. 53, pp. 1366-1376,2005.\n[11] J. E. Suris, L. A. DaSilva, Z. Han, and A. B. MacKenzie, \"Cooperative\nGame Theory for Distributed Spectrum Sharing,\" in IEEE International\nConference on Communications, Glasgow, Scotland, 2007.\n[12] T. Zhang, Z. Zeng, C. Feng, J. Zheng, and D. Ma, \"Utility Fair Resource\nAllocation Based on Game Theory in OFDM Systems,\" in Proceedings\nof 16th International Conference on Computer Communications and\nNetworks, Hawaii, USA, 2007.\n[13] J. Mitola and G. Maguire, \"Cognitive Radio: Making Software Radios\nMore Personal,\" IEEE Personal Communications, vol. 6, pp. 13-18,\n]999.\n[14] R. B. Myerson, Game Theory: Analysis of Conflict. Harvard University\nPress, ]99 ].\n[15] H. Boche, M. Schubert, N. Vucic, and S. Naik, \"Non-Symmetric Nash\nBargaining Solution for Resource Allocation in Wireless Networks and\nConnection to Interference Calculus,\" in Proc. 15th European Signal\nProcessing Conference, Poznan, Poland, Sep. 2007.\n[16] 3GPP, \"Technical Sepcification Group Radio Access Network: Physical\nlayer aspects for evolved UTRA (R7),\" June 2006, TR 25.8]4 V7.0.0.\n[] 7] R. Jain, D. M. Chiu, and W. Hawe, \"A Quantitative Measure of Fairness\nand Discrimination for Resource Allocation in Shared Systems. ,\" DEC\nResearch Report TR-30], Tech. Rep., ]984.\n1 2\nNumber of RSs\n0.6\no.\n)(\nCI)\n~ 0.8\n(Jl\n(Jl\nCI)\nE 0.7\n-iii\nu.\n1 2\nNumber of RSs\nFig. 2. Cooperative Resource Allocation Algorithm\nCooperative Resource Allocation\nfor each coalition Si\ncall CoalitionAllocateO\ncalculate the weighted dictatorial solution Wi¢p (F, d)\ninitialize the system\nwhile maximum adjustment step has not reached\nform a set of coalitions such that they are\nthe top n in exceeding their weighted dictatorial solution\nform a set of coalitions such that they are\nthe last m in exceeding their weighted dictatorial solution\ndo subcarrier reallocation between these two sets\nfor each coalition Si\ncall CoalitionAllocateO\n(a) System capacity (b) Fairness perfromance\nVII. SIMULATION\nWe set up the simulations as follows. In the relaying\ndownlink transmissions, the total bandwidth is chosen to be\n5MHz, and the number of data subcarriers is 300, which\nare parameters used in the LTE OFDMA downlink system\n[16]. The wireless channel is modeled as a frequency-selective\nchannel consisting of six independent Rayleigh fading paths.\nFor simplicity of simulation, we assume the total bandwidth\nof all the RS is the same as that of the BS. We evaluate both\nthe performance of non-cooperative and cooperative solutions\nby varying the size of the coalitions and the numbers of RSs\nand DEs inside each coalition.\nWe also assume that the BS does not apply any resource\nallocation to compare the effectiveness of the algorithms on\nthe RSs. The RSs (either in the same coalition or in different\ncoalitions) are set to have the same path fading to all DEs. The\nresource allocation inside each coalition has a fairness criteria\nsuch that each DE in the coalition has the same priority. Jain's\nfairness index [17] is adopted to measure fairness of difference\nresource allocation schemes.\nThe simulation results for 2 coalitions and 3 DEs for each\ncoalition are shown in Figure 3 (varying the number of RSs in\neach coalition). Although the RSs have individual power con-\nstraints, their total power is kept constant for fair comparison.\nIt can be seen that both non-cooperative and cooperative sub-\ncarrier allocation strategies outperform the random resource\nallocation strategy in system capacity. The non-cooperative\nalgorithm outperforms random allocation by approximately\n50% in system capacity with 3 RSs in each coalition. The\ncooperative algorithm has approximately 5% loss in system\ncapacity comparing with the non-cooperative algorithm, but\nachieves significant gain in fairness performance.\nAuthorized licensed use limited to: UNIVERSITY OF BRISTOL. Downloaded on January 13, 2009 at 10:53 from IEEE Xplore.  Restrictions apply.\n",
            "id": 17386830,
            "identifiers": [
                {
                    "identifier": "385683377",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "194023677",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2169151533",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "29025777",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:research-information.bris.ac.uk:openaire_cris_publications/aae15443-23b0-43b6-a1bc-7ecae22f4b95",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "oai:research-information.bris.ac.uk:publications/aae15443-23b0-43b6-a1bc-7ecae22f4b95",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "10.1109/pimrc.2008.4699870",
                    "type": "DOI"
                }
            ],
            "title": "A game theoretic approach to distributed resource allocation for OFDMA-based relaying networks",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2169151533",
            "oaiIds": [
                "oai:research-information.bris.ac.uk:openaire_cris_publications/aae15443-23b0-43b6-a1bc-7ecae22f4b95",
                "oai:research-information.bris.ac.uk:publications/aae15443-23b0-43b6-a1bc-7ecae22f4b95"
            ],
            "publishedDate": "2008-09-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 36628777,
                    "title": "A Quantitative Measure of Fairness and Discrimination for Resource Allocation in Shared Systems.",
                    "authors": [],
                    "date": null,
                    "doi": null,
                    "raw": "R. Jain, D. M. Chiu, and W. Hawe, &quot;A Quantitative Measure of Fairness and Discrimination for Resource Allocation in Shared Systems. ,&quot; DEC Research Report TR-30], Tech. Rep., ]984. 1 2 Number of RSs 0.6 o. )( CI) ~ 0.8 (Jl (Jl CI) E 0.7 -iii u. 1 2 Number of RSs",
                    "cites": null
                },
                {
                    "id": 36628759,
                    "title": "Adaptive Resource Allocation Based on Channel Information in Multihop OFDM Systems,&quot;",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/vtcf.2006.258",
                    "raw": "C. Bae and D.-H. Cho, &quot;Adaptive Resource Allocation Based on Channel Information in Multihop OFDM Systems,&quot; in IEEE Vehicular Technology Conference Proceedings, Montreal, Canada, 2006.",
                    "cites": null
                },
                {
                    "id": 36628757,
                    "title": "Adaptive Resource Allocation in Multiuser OFDM Systems With Proportional Rate Constraints,&quot;",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1109/twc.2005.858010",
                    "raw": "Z. Shen, 1. G. Andrews, and B. L. Evans, &quot;Adaptive Resource Allocation in Multiuser OFDM Systems With Proportional Rate Constraints,&quot; IEEE Transactions on Wireless Communications, vol. 4, pp. 2726-2737, 2005.",
                    "cites": null
                },
                {
                    "id": 36628764,
                    "title": "Cognitive Radio: Making Software Radios More Personal,&quot;",
                    "authors": [],
                    "date": null,
                    "doi": "10.1109/98.788210",
                    "raw": "J. Mitola and G. Maguire, &quot;Cognitive Radio: Making Software Radios More Personal,&quot; IEEE Personal Communications, vol. 6, pp. 13-18, ]999.",
                    "cites": null
                },
                {
                    "id": 36628753,
                    "title": "Cooperative Diversity in Wireless Networks: Efficient Protocols and Outage Behavior,&quot;",
                    "authors": [],
                    "date": null,
                    "doi": "10.1109/tit.2004.838089",
                    "raw": "J. Laneman, D. Tse, and G. W. Wornell, &quot;Cooperative Diversity in Wireless Networks: Efficient Protocols and Outage Behavior,&quot; IEEE Transactions on Information Theory, vol. 50, pp. 3062-3080,2004.",
                    "cites": null
                },
                {
                    "id": 36628762,
                    "title": "Cooperative Game Theory for Distributed Spectrum Sharing,&quot;",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1109/icc.2007.874",
                    "raw": "J. E. Suris, L. A. DaSilva, Z. Han, and A. B. MacKenzie, &quot;Cooperative Game Theory for Distributed Spectrum Sharing,&quot; in IEEE International Conference on Communications, Glasgow, Scotland, 2007.",
                    "cites": null
                },
                {
                    "id": 36628752,
                    "title": "Design Considerations and Physical Layer Performance Results for 4G OFDMA System Employing Dynamic Subcarrier Allocation,&quot;",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1109/pimrc.2005.1651458",
                    "raw": "A. Doufexi and S. Armour, &quot;Design Considerations and Physical Layer Performance Results for 4G OFDMA System Employing Dynamic Subcarrier Allocation,&quot; in IEEE 16th International Symposium on Personal, Indoor and Mobile Radio Communications, Berlin, Germany, 2005.",
                    "cites": null
                },
                {
                    "id": 36628761,
                    "title": "Fair Multiuser Channel Allocation for OFDMA Networks Using Nash Bargaining Solutions and Coalitions,&quot;",
                    "authors": [],
                    "date": null,
                    "doi": "10.1109/tcomm.2005.852826",
                    "raw": "Z. Han, Z. Ji, and K. J. R. Liu, &quot;Fair Multiuser Channel Allocation for OFDMA Networks Using Nash Bargaining Solutions and Coalitions,&quot; IEEE Transactions on Communications, vol. 53, pp. 1366-1376,2005.",
                    "cites": null
                },
                {
                    "id": 36628765,
                    "title": "Game Theory: Analysis ofConflict.",
                    "authors": [],
                    "date": null,
                    "doi": null,
                    "raw": "R. B. Myerson, Game Theory: Analysis ofConflict. Harvard University Press, ]99].",
                    "cites": null
                },
                {
                    "id": 36628755,
                    "title": "Increase in Capacity of Multiuser OFDM System Using Dynamic Subchannel Allocation,&quot;",
                    "authors": [],
                    "date": "2000",
                    "doi": "10.1109/vetecs.2000.851292",
                    "raw": "W. Rhee and 1. M. Cioffi, &quot;Increase in Capacity of Multiuser OFDM System Using Dynamic Subchannel Allocation,&quot; in IEEE Vehicular Technology Conference Proceedings, Tokyo, Japan, May 2000.",
                    "cites": null
                },
                {
                    "id": 36628754,
                    "title": "Multiuser OFDM with Adaptive Subcarrier, Bit and Power Allocation,&quot;",
                    "authors": [],
                    "date": "1999",
                    "doi": "10.1109/vetec.1999.778101",
                    "raw": "C. Y. Wong, R. S. Cheng, K. B. Letaief, and R. D. Murch, &quot;Multiuser OFDM with Adaptive Subcarrier, Bit and Power Allocation,&quot; IEEE Journal on Selected Areas in Communications, vol. 17, pp. 1747-1758, 1999.",
                    "cites": null
                },
                {
                    "id": 36628773,
                    "title": "Non-Symmetric Nash Bargaining Solution for Resource Allocation in Wireless Networks and Connection to Interference Calculus,&quot; in",
                    "authors": [],
                    "date": "2007",
                    "doi": null,
                    "raw": "H. Boche, M. Schubert, N. Vucic, and S. Naik, &quot;Non-Symmetric Nash Bargaining Solution for Resource Allocation in Wireless Networks and Connection to Interference Calculus,&quot; in Proc. 15th European Signal Processing Conference, Poznan, Poland, Sep. 2007.",
                    "cites": null
                },
                {
                    "id": 36628758,
                    "title": "On the Capacity of the Broadband Relay Networks,&quot;",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1109/acssc.2004.1399366",
                    "raw": "Guoqing Li and Jui Liu, &quot;On the Capacity of the Broadband Relay Networks,&quot; in Proceedings ofthe 38th Annual Asilomar Conference on Singals, Systems and Computers, CA, USA, 2004.",
                    "cites": null
                },
                {
                    "id": 36628760,
                    "title": "Resource Allocation Techniques for OFDMA-Based Decode-and-Forward Relaying Networks,&quot;",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1109/vetecs.2008.394",
                    "raw": "Y. Pan, A. Nix, and M. Beach, &quot;Resource Allocation Techniques for OFDMA-Based Decode-and-Forward Relaying Networks,&quot; in IEEE 67th Vehicular Technology Conference (VTC), Singapore, May 2008.",
                    "cites": null
                },
                {
                    "id": 36628775,
                    "title": "Technical Sepcification Group Radio Access Network: Physical layer aspects for evolved UTRA (R7),&quot;",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "3GPP, &quot;Technical Sepcification Group Radio Access Network: Physical layer aspects for evolved UTRA (R7),&quot; June 2006, TR 25.8]4 V7.0.0.",
                    "cites": null
                },
                {
                    "id": 36628756,
                    "title": "Transmit Power Adaptation for Multiuser OFDM Systems,&quot;",
                    "authors": [],
                    "date": null,
                    "doi": "10.1049/el:20021123",
                    "raw": "J. Jang and K. B. Lee, &quot;Transmit Power Adaptation for Multiuser OFDM Systems,&quot; IEEE Journal on Selected Areas in Communications, vol. 21, pp. 171-178,2003.",
                    "cites": null
                },
                {
                    "id": 36628763,
                    "title": "Utility Fair Resource Allocation Based on Game Theory in OFDM Systems,&quot;",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1109/icccn.2007.4317854",
                    "raw": "T. Zhang, Z. Zeng, C. Feng, J. Zheng, and D. Ma, &quot;Utility Fair Resource Allocation Based on Game Theory in OFDM Systems,&quot; in Proceedings of 16th International Conference on Computer Communications and Networks, Hawaii, USA, 2007.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "https://research-information.bris.ac.uk/files/3012084/Pan_IEEE_PIMRC_2008.pdf"
            ],
            "updatedDate": "2021-10-13T19:23:25",
            "yearPublished": 2008,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/29025777.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/29025777"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/29025777/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/29025777/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/17386830"
                }
            ]
        },
        {
            "acceptedDate": "2006-08-08T00:00:00",
            "arxivId": "math/0609173",
            "authors": [
                {
                    "name": "Jank, Wolfgang"
                },
                {
                    "name": "Shmueli, Galit"
                }
            ],
            "contributors": [
                "Wolfgang"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/206471356"
            ],
            "createdDate": "2012-04-13T14:24:00",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 145,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/145",
                    "logo": "https://api.core.ac.uk/data-providers/145/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2006-05-01T00:00:00",
            "abstract": "This paper describes opportunities and challenges of using functional data\nanalysis (FDA) for the exploration and analysis of data originating from\nelectronic commerce (eCommerce). We discuss the special data structures that\narise in the online environment and why FDA is a natural approach for\nrepresenting and analyzing such data. The paper reviews several FDA methods and\nmotivates their usefulness in eCommerce research by providing a glimpse into\nnew domain insights that they allow. We argue that the wedding of eCommerce\nwith FDA leads to innovations both in statistical methodology, due to the\nchallenges and complications that arise in eCommerce data, and in online\nresearch, by being able to ask (and subsequently answer) new research questions\nthat classical statistical methods are not able to address, and also by\nexpanding on research questions beyond the ones traditionally asked in the\noffline environment. We describe several applications originating from online\ntransactions which are new to the statistics literature, and point out\nstatistical challenges accompanied by some solutions. We also discuss some\npromising future directions for joint research efforts between researchers in\neCommerce and statistics.Comment: Published at http://dx.doi.org/10.1214/088342306000000132 in the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org",
            "documentType": "research",
            "doi": "10.1214/088342306000000132",
            "downloadUrl": "http://arxiv.org/abs/math/0609173",
            "fieldOfStudy": "computer science",
            "fullText": "ar\nX\niv\n:m\nat\nh/\n06\n09\n17\n3v\n1 \n [m\nath\n.ST\n]  \n6 S\nep\n 20\n06\nStatistical Science\n2006, Vol. 21, No. 2, 155–166\nDOI: 10.1214/088342306000000132\nc© Institute of Mathematical Statistics, 2006\nFunctional Data Analysis in Electronic\nCommerce Research\nWolfgang Jank and Galit Shmueli\nAbstract. This paper describes opportunities and challenges of using\nfunctional data analysis (FDA) for the exploration and analysis of data\noriginating from electronic commerce (eCommerce). We discuss the\nspecial data structures that arise in the online environment and why\nFDA is a natural approach for representing and analyzing such data.\nThe paper reviews several FDA methods and motivates their useful-\nness in eCommerce research by providing a glimpse into new domain\ninsights that they allow. We argue that the wedding of eCommerce\nwith FDA leads to innovations both in statistical methodology, due to\nthe challenges and complications that arise in eCommerce data, and in\nonline research, by being able to ask (and subsequently answer) new\nresearch questions that classical statistical methods are not able to ad-\ndress, and also by expanding on research questions beyond the ones\ntraditionally asked in the offline environment. We describe several ap-\nplications originating from online transactions which are new to the\nstatistics literature, and point out statistical challenges accompanied\nby some solutions. We also discuss some promising future directions\nfor joint research efforts between researchers in eCommerce and statis-\ntics.\nKey words and phrases: Process dynamics, special data structures,\nonline auctions.\nWolfgang Jank is Assistant Professor of Management\nScience and Statistics, Department of Decision and\nInformation Technologies, Robert H. Smith School of\nBusiness, University of Maryland, College Park,\nMaryland 20742, USA e-mail: wjank@rhsmith.umd.edu.\nGalit Shmueli is Assistant Professor of Management\nScience and Statistics, Department of Decision and\nInformation Technologies, Robert H. Smith School of\nBusiness, University of Maryland, College Park,\nMaryland 20742, USA e-mail:\ngshmueli@rhsmith.umd.edu.\nThis is an electronic reprint of the original article\npublished by the Institute of Mathematical Statistics in\nStatistical Science, 2006, Vol. 21, No. 2, 155–166. This\nreprint differs from the original in pagination and\ntypographic detail.\n1. INTRODUCTION\nFunctional data analysis (FDA) has been gain-\ning momentum in many fields. While much of the\nmethodological advances have been made within the\nstatistics literature, FDA has found many useful ap-\nplications in the agricultural sciences (Ogden et al.,\n2002), the behavioral sciences (Rossi, Wang and Ram-\nsay, 2002), in medical research (Pfeiffer et al., 2002)\nand many more. One reason for this momentum is\nthe technological advancement in computer storage\nand computing power. Today’s researchers gather\nmore and more data, often automatically, and store\nthem in large databases. However, these new capa-\nbilities for data generation and data storage have\nalso led to new data structures which do not nec-\nessarily fit into the classical statistical concept. Re-\nsearchers measure characteristics of customers over\ntime, store digitalized two- or three-dimensional im-\nages of the brain, and record three- or even four-\n1\n2 W. JANK AND G. SHMUELI\ndimensional movements of objects through space and\ntime. Many of these new data structures call for new\nstatistical methods in order to unveil the informa-\ntion that they carry. Data can contain trends that\nvary in longitudinal or spatial aspects, that vary\nacross different groups of customers or objects, or\nthat show different magnitudes of dynamics.\nFDA is a tool-set that, although based on the\nideas of classical statistics, differs from it (and, in a\nsense, generalizes it), especially with respect to the\ntype of data structures that it encompasses. While\nthe underlying ideas for FDA have been around for a\nlonger time, the surge in associated research can be\nattributed to the monograph of Ramsay and Silver-\nman (1997). In FDA, the object of interest is a set of\ncurves, shapes, objects, or, more generally, a set of\nfunctional observations. This is in contrast to clas-\nsical statistics where the interest centers around a\nset of data vectors. In recent years, a range of clas-\nsical statistical methods have been generalized to\nthe functional framework; James, Hastie and Sugar\n(2000) developed a principal components approach\nfor a set of sparsely sampled curves. Other exploratory\ntools include curve clustering (see Abraham et al.,\n2003; James and Sugar, 2003; Tarpey and Kinateder,\n2003) and curve classification (see Hall, Poskitt and\nPresnell, 2001; James and Hastie, 2001). Classical\nlinear models have also been generalized to func-\ntional ANOVA (Fan and Lin, 1998; Guo, 2002),\nfunctional regression (Faraway, 1997; Cuevas, Febrero\nand Fraiman, 2002; Ratcliffe, Leader and Heller, 2002)\nand the functional generalized linear model (Rat-\ncliffe, Heller and Leader, 2002; James, 2002). More-\nover, Ramsay (2000b) and Ramsay and Ramsay (2002)\nsuggest differential equations for data of functional\nform. While this list is far from complete, it shows\nsome of the current methodological efforts in this\nemerging field.\nElectronic commerce (eCommerce) is a growing\nfield of scholarly research especially in information\nsystems, economics and marketing, but it has re-\nceived little to no attention in statistics. This is sur-\nprising because it arrives with an enormous amount\nof data and data-related questions and problems.\nLike other web-based data, eCommerce data tend\nto be very rich, clean and structurally different from\noffline data. eCommerce research arrives with many\nnew data- and model-related challenges that promise\nnew ideas and motivation for further methodologi-\ncal advancements of FDA. One of the main char-\nacteristics of eCommerce data is the combination\nof longitudinal information (time-series data) with\ncross-sectional information (attribute data). A sam-\nple of n records typically comprises n time series,\neach linked with a set of n attributes. Take eBay ’s\nonline auctions as an example. There, each auction\nis characterized by a time series of bids placed over\ntime. This information is coupled with additional\nauction attributes such as a seller’s rating, the auc-\ntion duration and the currency used. Another ex-\nample is online product ratings on Amazon.com or\nmovie ratings on Yahoo! Movies (see Dellarocas and\nNarayan, 2006). Yahoo! Movies allows users to rate\nany movie according to different measures. This re-\nsults in a time series that describes the average daily\nrating or the number of daily postings (or both)\nfrom the date of the movie release until the time\nof data collection. This information is coupled with\nattribute data about the movie such as the movie\ngenre and critics’ rating. A third example, described\nin Stewart, Darcy and Daniel (2006), is the evolution\nof open-source software projects that is monitored\nby websites such as SourceForge.net. Here an obser-\nvation is a certain project, and it is characterized\nby a time series that describes project complexity\nfrom its first release until the time of data collection.\nEach project also has associated attributes such as\nthe number of developers, the operating system used\nand the programming language.\nThe combination of longitudinal and cross-sectional\ninformation is only one typical aspect of eCommerce\ndata. Another aspect is the uneven spacing between\nevents. In many cases, the observed time series is\ncomposed of events influenced by multiple users or\nagents who access the web at different points in time\n(and from different geographical locations). Conse-\nquently, the resulting times when new events arrive\nare extremely unevenly spaced. This is in contrast to\ntraditional time series, which are typically recorded\nat predefined and equidistant time-points, such as\ndaily, monthly or quarterly scales. Furthermore, be-\ncause of psychological, economic or other reasons,\neCommerce time series tend to feature very sparse\nareas at some times, followed by extremely dense\nareas at other times. For instance, bidding in eBay\nauctions tends to be concentrated at the end, re-\nsulting in very sparse bid-arrivals during most of\nthe auction except for its final moments, where the\nbidding volume can be extremely high.\neCommerce not only creates new data challenges,\nit also motivates the need for innovative models.\nFDA IN ECOMMERCE 3\nWhile the field of economics has created many the-\nories for understanding economic behavior at the\nindividual and market level, many of these theories\nwere developed before the emergence of the World\nWide Web. The existence of the web now allows re-\nsearchers, for the first time, to observe and record\ndata about economic behavior on a large-scale ba-\nsis. As it turns out, however, observed data often do\nnot support classical economic theories. As a result,\nempirical research is thriving. In fact, the empirical\nliterature has continuously shown that online be-\nhavior deviates in many ways from offline behavior\nand from what is expected by economic theory. This\ncalls for new economic models that can be validated\nempirically. In addition, the availability of eCom-\nmerce data allows researchers to ask new types of\nquestions. One major enhancement is the ability to\nstudy not only the evolution of a process, but also\nits dynamics: how fast it moves and how suddenly\nit changes, its rate of change and how this rate dif-\nfers at different time-points. Studying dynamics of\nprocesses can be very relevant in the online world,\nbecause it allows new approaches for characterizing\neCommerce processes (and thus distinguishing be-\ntween diverse processes), and even forecasting them\n(Wang, Jank and Shmueli, 2006). Changing dynam-\nics are inherent in a fast-moving environment like\nthe online world. Fast movements and change im-\nply nonstationarity which poses challenges to tra-\nditional time series modeling. And finally, it is im-\nportant to point out that for any one process that\nwe observe in the online world, there typically exist\nmany, many replicates of the same (or at least very\nsimilar) process. On eBay, for instance, if we think\nof the formation of price between the start and the\nend of an auction as a process of interest, then there\nexist several million similar processes of that form,\ntaking place at any given day on eBay. The replica-\ntion of processes, or time series, fits naturally within\nthe FDA framework and makes this an ideal ground\nfor the advancement of new functional methodology.\nFinally, eCommerce typically arrives with huge\ndatabases which can put a computational burden\non users’ storage and processing facilities. This bur-\nden is often increased by the complicated structure\nof eCommerce data. Taking a functional data ap-\nproach, one can relieve some of that burden. FDA\noperates on functional objects which can be more\ncompactly represented than the original data. Tak-\ning a functional approach may therefore be advan-\ntageous also from a resource point of view.\nThe process of studying a set of data via func-\ntional methods consists of two principal steps: First,\nthe functional object is “recovered,” typically by\nmeans of smoothing. There are multiple different\nways in which this smoothing step can be executed,\nand there are many challenges during that step. Sec-\nond, the resulting functional object is used for data\nexploration and analysis. Exploratory data analysis\n(including data visualization and summary) is per-\nformed in order to learn about general characteris-\ntics as well as unusual features and anomalies in the\ndata. Analysis includes explanatory and predictive\nmodeling and inference, just as in classical statis-\ntics. In the next sections we focus on the challenges\nand problems that arise during these steps within\nthe eCommerce context. We would like to note that\nour point of view of the functional approach and its\napplication to eCommerce has been forged during\nthe teaching of so-called Research Interaction Teams\n(www.amsc.umd.edu/Courses/RITDescrips/HowAndWhy.html)\nwhich are research classes that involve graduate stu-\ndents from the Statistics and the Applied Mathe-\nmatics and Scientific Computation programs at the\nUniversity of Maryland. Several of our studies per-\nformed during these classes have led to new method-\nological and practical insights.\n2. RECOVERING FUNCTIONAL OBJECTS\nThe first step in any functional data analysis con-\nsists of recovering, from the observed data, the un-\nderlying functional object. There exist a variety of\nmethods for recovering functional objects from a set\nof data, all of which are typically based on some kind\nof smoothing. As a result of the smoothing, and of\ncharacterizing the smooth object by its smoothing\nparameters only, we obtain a low-dimensional func-\ntional object. We focus here on objects, and in par-\nticular curves, that are based on unevenly spaced\ntime series and of which we have multiple replica-\ntions. An example is a set of bid histories from eBay\nauctions, as shown in Figure 1. The four panels cor-\nrespond to four separate seven-day auctions for a\nnew Palm PDA. Each consists of the bids (in $)\nplaced at different times during the auction.\n2.1 Challenges in Choosing the Right Smoother\nThe first step in recovering the functional object\nis to choose a family of basis functions. The choice of\nthe basis function depends on the nature of the data,\non the level of smoothness that the application war-\nrants, on what aspects of the data we want to study,\n4 W. JANK AND G. SHMUELI\non the size of the data and on the types of analy-\nses that we plan to perform. For example, to repre-\nsent the price path of an online auction for the pur-\npose of, say, studying price dynamics, one could use\nmonotone smoothing splines (Ramsay, 1998) since\nprices in auctions increase monotonically. Besides\nmaintaining the price monotonicity, this approach\nalso permits the computation of derivatives which\nlend themselves to price dynamics. However, fitting\nmonotone splines is computationally more intensive\nthan fitting ordinary polynomial smoothing splines.\nIn that sense, it may prove impractical to compute\nmonotone splines for very large databases if time\nand memory restrictions exist. In addition, polyno-\nmial smoothing splines can be represented as a linear\ncombination of basis functions. The practical mean-\ning of this is that if we use polynomial smoothing\nsplines and the intended analysis is based on a lin-\near operation (such as computing average curves,\nfitting functional linear regression models, or per-\nforming functional principal components analysis),\nwe can operate directly on the basis function coef-\nficients without any loss of information. The same\noperation using monotone splines would require an\napproximation step due to the need to first represent\nthe continuous curve in a finite-dimensional manner\nby evaluating it on a grid. Conversely, if the type\nof operation is nonlinear, then one would have to\nperform a grid-based computation for either type\nof spline and the choice would therefore not matter\nfrom this point of view. Thus, the way we recover the\nfunctional object is strongly influenced by a variety\nFig. 1. Scatterplots describing the bid history in each of four\neBay auctions, each lasting seven days.\nof different objectives all of which might compete\nwith one another.\nRecovering functional objects often involves more\nthan deciding on the appropriate type of smoother.\nThis can include a preprocessing step via interpola-\ntion, thereby creating a raw functional (e.g., Ram-\nsay and Silverman, 2002, page 21). This alleviates\nthe problem of the unevenly spaced series that are\ncommon in eCommerce. An important aspect in any\nfunctional data analysis is the robustness of analy-\nsis results with respect to the choice and level of\nsmoothing. A general study of this sort was carried\nout by our research interaction team, comparing the\neffects of smoothing splines versus monotone splines\non the conclusions derived from a functional regres-\nsion on the price path in online auctions (the func-\ntional object) as a function of explanatory variables\nsuch as the seller rating and the opening bid (both\nscalar) and current number of bids (a functional ex-\nplanatory variable). The study indicates that both\nsmoothers lead to similar conclusions (Alford and\nUrimi, 2004).\nAnother example of the challenges in choosing an\nappropriate smoothing method is the functional rep-\nresentation of online movie ratings. By that we mean\nthe series of user movie ratings on online services\nsuch as Yahoo.com. The volume of user postings is\nhighly periodic, with heavier activity on weekends\n(when people tend to watch movies in the theaters).\nFourier basis functions were found to be a better\nchoice among different alternatives for capturing the\ncyclical posting patterns (Wu, 2005).\n2.2 Additional Data Challenges\nOur different studies using eCommerce data have\nraised further challenges in the functional object\nrecovery stage that have previously not been ad-\ndressed in the literature. The first such challenge is\nhandling the extremely unevenly distributed mea-\nsurements in eCommerce data. That is, the num-\nber and location of events vary drastically from one\nfunctional object to another. One typical example\nis the bid arrival in eBay auctions. Returning to\nFigure 1, it can be seen that some bid histories\nare very dense at the auction end, while others are\nmuch sparser, and in addition the overall number of\nbids per auction can vary widely between none in\nsome auctions, and more than 100 in others. And\nyet, while the varying number of bids per auction\nmay suggest the use of a varying set of smooth-\ning methods, we prefer the use of a single family\nFDA IN ECOMMERCE 5\nof smoothers. The reason for this is that, in the end,\nthe choice of the smoother is merely a means to the\nend of arriving at a unifying functional object and it\nis not the direct object of our interest. Coming back\nto the example of online auctions with sometimes\nfew and sometimes many bids, this motivates the\nneed for new methodological advances in creating\nfunctional objects that naturally incorporate all ex-\ntremes under one hat. Some promising approaches\nin that direction can be found in James and Sugar\n(2003) and James, Hastie and Sugar (2000).\nThe extreme structure of eCommerce data is chal-\nlenging even for very basic visualization tasks: stan-\ndard time-series visualization tools typically require\nevenly spaced events! Because of this restrictive re-\nquirement, we collaborated with colleagues at the\nHuman–Computer Interaction Lab at the University\nof Maryland to develop new interactive visualiza-\ntion tools that can accommodate this special data\nstructure. We evaluated different approaches for rep-\nresenting eBay bid histories by an evenly spaced\nequivalent without losing important information with\nrespect to order, magnitude and distance between\nthe bids (see Aris et al., 2005). Interestingly, the\nfinal choice was to use a functional approach by\nfirst smoothing the bid history, and then feeding an\nevenly spaced grid of the smooth curves (and their\nderivatives) into a standard visualization tool.\nAnother challenge typical to eCommerce data is\ndefining meaningful start and end points of the func-\ntional objects in order to align the curves. The prob-\nlem of aligning functional objects is related to the\nproblem of registration (Ramsay and Silverman, 2005,\nChapter 7), but there are several additional compli-\ncations here: Many web-based events do not start\nand end at the same time. For instance, online prod-\nuct ratings over time have different starting points,\ndepending on when the product was first released\nto the market, when the first rating was placed,\netc. They also often have different ending points,\nfor instance, if one product is prematurely taken off\nthe market, if it is replaced by a product-upgrade,\nand so on. Another issue that complicates object\nalignment is that the data collection process itself\nmay act as a censoring mechanism. Therefore, it is\nnot obvious how methods such as landmark regis-\ntration, where curves are aligned according to one\nparticular feature of the curve such as its peak, can\nbe adapted to handle this situation. Finally, select-\ning the units for the time axis can be challenging.\nIn some applications calendar time (e.g., the date\nand time a transaction took place) is reasonable,\nwhereas in other applications the event index (i.e.,\nthe order of the event arrival) might make more\nsense. And yet in other cases an entirely different\n“clock” would be even more suitable. For instance,\nwe pointed out that eBay auctions typically exhibit\nvery low bidding activity during most of the auction\nand then extremely high activity near the end. For\nthis reason an auction might be better represented\nby a clock that “shrinks” the low-activity period and\n“stretches” the high-activity period, thereby putting\nmore emphasis on the part that matters more. All\nthese issues are illustrated and discussed further in\nthe paper by Stewart, Darcy and Daniel (2006).\n3. FUNCTIONAL EDA\nAfter the data are represented by functional ob-\njects, the analysis steps follow the same process as\nin classical statistics, with the first step being ex-\nploratory data analysis (EDA). EDA includes data\nsummaries, visualization, dimension reduction, out-\nlier detection, and more. The main difference be-\ntween FDA and classical statistics is the way in\nwhich the methods are applied and especially how\nthey are interpreted.\n3.1 Static vs. Interactive Visualization\nStarting with visualization, our goal is to:\n1. Visualize a sample of curves.\n2. Inspect summaries of these curves.\n3. Explore conditional curves, using various relevant\npredictor variables.\nTo that end, one solution is to create static graphs.\nFor instance, Figure 2 shows the price evolution in\n34 eBay auctions for various magazines. We can see\nlarge heterogeneity across the price formation pro-\ncess at different times of the auction. We refer to this\napproach as static, since once the graph is generated\nit can no longer be modified by the user without run-\nning the software code again. This static approach\nis useful for differentiating subsets of curves by at-\ntributes (e.g., by using color), or for spotting out-\nliers. However, a static approach does not allow for\nan interactive exploration of the data. By interac-\ntive we mean that the user can perform operations\nsuch as zooming in and out, filtering the data and\nobtaining details for the filtered data, and do all\nof this from within the graphical interface. Interac-\ntive visualizations for the special structure of eCom-\nmerce data are not straightforward, and solutions\n6 W. JANK AND G. SHMUELI\nhave only been proposed recently (Aris et al., 2005;\nShmueli et al., 2006). One such solution is Auction-\nExplorer (www.cs.umd.edu/hcil/timesearcher),\nwhich is tailored to handle the special structure of\nonline auction data. A snapshot of its user interface\nis shown in Figure 3. The interface includes several\npanels, which correspond to the price curves (top\nleft), their dynamics (not shown in this view) and\nthe corresponding attribute data (top right). The\ncurves can be filtered to display subsets according to\na selection of attribute values, according to a selec-\ntion of curves, and one can do pattern search. Sum-\nmarization is achieved through on-the-fly summary\nstatistics for attributes, and a “streaming boxplot”\ncalled a “river plot” of the curves (bottom panel of\nFigure 3). This is yet another attempt to general-\nize classical visualization methods to the functional\nenvironment.\n3.2 Data Reduction\nAnother goal of EDA is data reduction. Two of\nthe methods that are useful in this context are curve\nclustering and functional principal components anal-\nysis. Curve clustering partitions the set of curves\ninto a few clusters, thereby reducing the space of\nobservations, and attempts to derive insight from\nthe resulting clusters. The clustering can be applied\nto the curves themselves or to their derivatives. Jank\nand Shmueli (2005) apply curve clustering to bid his-\ntories of eBay auctions and find three main clusters.\nLinking the curve information with attribute infor-\nmation, they find that the different clusters corre-\nspond to three types of auctions: “greedy sellers,”\nFig. 2. Static plot of the price progression in 34 eBay auc-\ntions for magazines.\nFig. 3. Snapshot of user interface for AuctionExplorer\n(www. cs. umd. edu/ hcil/ timesearcher ).\n“bazaar auctions” and “experienced seller/buyer”\nauctions. Each of these types characterizes a differ-\nent auction profile, combining static and dynamic\ninformation. For instance, “greedy seller” auctions\nhave the highest average opening price and the low-\nest closing price. Unsurprisingly, they do not attract\nmuch competition, since unjustified high opening\nprices tend to deter users from bidding. Low compe-\ntition is also known to lead to lower prices. Sellers\nin these auctions are, on average, less experienced\nthan those in other clusters (as can be measured\nby their eBay rating), scheduling most auctions to\nend on a weekday. These auctions also attract ex-\nperienced winners who take advantage of the poorer\nauction design and resulting lower prices. The price\ndynamics of this cluster reflect this setting: price\nstarts accelerating late in the auction, not allowing\nit to achieve its full impact by the time the auction\ncloses. This mix of insight into static seller and bid-\nder characteristics coupled with the price dynamics\nis only available with a functional approach.\nFDA IN ECOMMERCE 7\nAnother popular method is functional principal\ncomponents analysis (f-PCA). The method uses stan-\ndard PCA to find principal sources of variability in\ncurves (or other functional objects). If we consider\ncurves that represent a process over time, then f-\nPCA can help us find “within-curve” (or more gen-\nerally, “within-process”) variation, thereby condens-\ning the time axis. This is done by selecting a dis-\ncrete grid of time-points and treating the points as\nthe variables in ordinary PCA. A preliminary study\nby Hyde, Moore and Hodge (2004) applied f-PCA\nto price curves and derivative curves of a sample\nof eBay auctions for premium wristwatches. They\nfound that two or three principal components cap-\ntured most of the within-curve variation: One source\nis price variation during the middle of the auction\nand the other distinguishes the price variability be-\ntween the beginning and end of the auction. Similar\nresults were obtained when using the price-dynamics\ncurves. Hyde, Moore and Hodge (2004) also used\nf-PCA to compare sources of “within-process” vari-\nation across different brands for the same product\ncategory as well as for different product categories.\nIt was found that price is most uncertain during\nmid-auction. As the auction approaches its end, though,\nthe price becomes more predictable, especially in\ncommon-value auctions (see also Wang, Jank and\nShmueli, 2006).\nAn alternative approach of principal components\nanalysis to functional data that, to the best of our\nknowledge, has not been explored would be to treat\nthe observations as the dimension to be transformed.\nThe idea is to find main sources of variation across\ncurves (instead of within curves), achieving a goal\nsimilar to curve clustering, where main features of\nthe curves are highlighted. The exact meaning and\ninterpretation of this variation deserve further at-\ntention.\n4. FUNCTIONAL MODELING, INFERENCE\nAND PREDICTION\nThere is quite a lot of ongoing research on gener-\nalizing classical regression models to the functional\nsetting. Examples include linear regression with func-\ntional predictors (Ratcliffe, Leader and Heller, 2002)\nor a functional response (Faraway, 1997), logistic re-\ngression (Ratcliffe, Heller and Leader, 2002), func-\ntional linear discriminant analysis (James and Hastie,\n2001) and general linear models with functional pre-\ndictors (James, 2002).\n4.1 Information in eCommerce Processes\nCurrent empirical research in eCommerce relies\non the use of very standard statistical tools such as\nleast-squares regression. These tools are used to in-\nvestigate how, say, the closing price in an online auc-\ntion relates to other auction-specific information. To\nthat end, one sets the closing price as the response\nvariable, and regresses it on potential explanatory\nvariables such as the opening bid, the auction du-\nration, the seller rating, etc. (see, e.g., Bajari and\nHortac¸su, 2003, 2004; Lucking-Reiley et al., 2000).\nWhile this approach is certainly useful for under-\nstanding some of the variation in closing prices, it\nalso leads to loss of a large amount of potentially\nuseful information about everything that happened\nbetween the start and end of the auction. More gen-\nerally, current research uses a response variable that\nis an aggregation of the process of interest: the maxi-\nmum bid in online auctions, the average product rat-\ning, etc. This (direct or indirect) choice is guided by\neconomic importance but is also likely done so that\nstandard models can be applied. Furthermore, the\nchoice of independent variables is limited to static\n“snapshot” information. The existence of more de-\ntailed data, however, can potentially shed more light\non the entire process rather than only its aggregated\nform.\nIn the online auction example, variables like the\nopening bid, the auction duration and the seller’s\nrating are determined before the auction start and\nthus do not capture any of the information that\narrives after that. However, it is well-known that\nevents that occur during the auction can also affect\nthe final price. For instance, the number of com-\npeting bidders, the bidders’ experience and the bid\ntimings can influence the final price. These three\nvariables are available only after the auction starts\nand in fact the information they carry changes as\nthe auction progresses.\nWhile it is possible to include time-varying ex-\nplanatory variables like the number of bidders into\na regression model, such a model would no longer be\nconsidered “standard” in the classical least-squares\nsense since it would have to account for time-depen-\ndence between the explanatory variable and the re-\nsponse, and also within the explanatory variable it-\nself.\nFurthermore, there is additional information revealed\nduring ongoing processes that cannot be captured\neasily by such models. An example is concurrency\n8 W. JANK AND G. SHMUELI\nand the effect that new events have on future events.\nIn the online auction context, incoming bids can in-\nfluence bidders in different ways: A new bid placed\nin an auction can result in an immediate response\nby other bidders or can be completely ignored. Bid-\nders also learn from each other: they adopt bid-\nding strategies of other bidders and they learn about\nan item’s value from bids that were placed. Many\nitems sold in online auctions do not have a com-\nmonly known value (such as collectibles, antiques,\nrare pieces of art, etc.), and therefore bidders of-\nten try to infer the item’s value from other people’s\nbids. In short, while the final price is certainly af-\nfected by directly observable phenomena (such as\nthe number of competing bidders), it is also depen-\ndent on indirect actions, reactions and interactions\namong bidders.\n4.2 Process Dynamics and FDA\nModeling the effects of user interactions with clas-\nsical regression models is challenging, to say the\nleast. An alternative approach is to capture some of\nthis dynamic information via evolution curves and\ntheir dynamics. In the auction context this would be\nthe price evolution, which is the progression of bids\nthroughout an auction. The evolution curve and its\ndynamics can reflect these bidder interactions: High\ncompetition in an auction will manifest itself as a\nsteep price curve with increasing dynamics. Price\nwill also increase, albeit at a slower rate, if bidders\nmerely use the new bid to update their own valua-\ntion about the product. The price increase will slow\ndown if bidders drop out of the auction due to a\nnewly placed bid or for some other reason. There-\nfore, the price-evolution curve, and in particular its\ndynamics, has the ability to capture much of the\nauction information that would otherwise not be\nintegrated into the model. By price dynamics we\nmean, for example, the price velocity and acceler-\nation which measure the change in price and the\nrate at which this change is occurring. The ability\nto measure dynamics is one of the most noteworthy\nfeatures of functional data analysis. FDA recovers\nthe price evolution via a smooth curve through the\nauction’s bid history and yields the price dynamics\nvia the derivatives of this curve. Examples of explor-\ning process dynamics via FDA in eCommerce are the\nprice dynamics in eBay auctions (Jank and Shmueli,\n2005) and bid dynamics in auctions for modern In-\ndian art by Reddy and Dass (2006). In these two\nexamples the price curves themselves are not very\nilluminating, but their dynamics reveal interesting\npatterns and sources of heterogeneity across records.\nOne can model the relationship between the pro-\ncess evolution (or its dynamics) and other predic-\ntors via functional regression analysis. For example,\nin a few studies of price formation in eBay auctions\n(Shmueli and Jank, 2006; Bapna, Jank and Shmueli,\n2004; Alford and Urimi, 2004) and other online auc-\ntions (Reddy and Dass, 2006) a functional regression\nmodel was fit to price-evolution curves from eBay\nauctions (the response) with static predictors (such\nas the seller rating) and functional predictors (such\nas the cumulative number of bids). One interesting\nfinding is that the impact of the opening bid on the\ncurrent price starts high, and slowly decreases as the\nauction progresses. This reflects the shift in informa-\ntion about the item’s value due to bidding: At first\nthere is not much information available and so the\nopening bid gives a sense of the item’s value. But as\nthe auction progresses new bids add more informa-\ntion about the value of the item, thereby reducing\nthe usefulness of the information contained in the\nopening bid.\nOne of the challenges in functional regression anal-\nysis is the interpretation of the results. Instead of\nscalar coefficient estimates, we obtain estimated co-\nefficient curves. Plotting these curves means that the\nx-axis is time, and not the ordinary predictor value.\nFor example, Figure 4 shows the estimated coeffi-\ncient (and a 95% confidence band) for a regression\nmodel with a functional response. In the top panel\nthe response is the price evolution. The coefficient\nis positive throughout the auction, signifying that\nthe current price is positively associated with the\nopening bid throughout the auction. However, this\nrelationship decreases in magnitude as the auction\nproceeds. This is reasonable, because bidders gain\nmore and more information as the auction proceeds\nand therefore derive less utility from the value of\nthe opening price. The middle and bottom panels in\nFigure 4 describe another useful information source:\nthe relationship between the opening price and the\nprice dynamics. If we are interested in relationships\nbetween various independent variables and the pro-\ncess dynamics, we can use the derivative curves as\nthe functional response. In this example we set the\nprice velocity (middle) and price acceleration (bot-\ntom) as the responses. We see that the price accel-\neration is positively associated with the opening bid\nat the auction start, but then this relationship loses\nFDA IN ECOMMERCE 9\nFig. 4. Estimated coefficient for opening price, in three regression models with a functional response: price evolution (top),\nprice velocity (middle) and price acceleration (bottom).\nmomentum and even becomes negative as the auc-\ntion comes to a close.\nThe capability of studying process dynamics could\nhave a huge impact on eCommerce research. Though\nthe concepts of dynamics are well grounded in physics\nand engineering, their exact economic impact re-\nquires more thought. However, there is an opportu-\nnity to create new economic measures with the help\nof FDA. For instance, we can develop concepts such\nas “auction energy” using the definition of kinetic\nenergy (energy =mass × velocity2/2) to arrive at\nauction energy\n10 W. JANK AND G. SHMUELI\n(1)\n= (current price)× (price velocity)2/2.\nA major challenge is to find a theoretical foundation\nin economics of such concepts. This is only one ex-\nample where collaboration could have a large impact\non the field.\n4.3 Other Functional Models for eCommerce\nAnother level of flexibility, but also complexity,\nis to incorporate interaction terms into functional\nregression models. Since interactions (in ordinary\nlinear regression models) are widely used in eCom-\nmerce studies, it could be useful to measure simi-\nlar effects in functional objects. The literature on\ninteractions in functional linear regression models\nappears to be scant, although this seems like an im-\nportant extension.\nAnother important direction for modeling the dy-\nnamic nature of web content in general, and eCom-\nmerce in particular, is the use of differential equa-\ntions. The use of differential equations in the func-\ntional literature is still in its infancy. Ramsay (2000a)\ngives an introduction to the use of differential equa-\ntions in statistics and several examples of functional\nestimation problems such as simultaneous estima-\ntion of a regression model and residual density, mono-\ntone smoothing, specification of a link function, dif-\nferential equation models of data, and smoothing\nover complicated multidimensional domains. Ram-\nsay calls this “principal differential analysis” (PDA)\nbecause of the similarities that it shares with prin-\ncipal components analysis.\nPDA is a natural formalization of the exploration\nof curve dynamics. Through a differential equation\nwe can relate, for instance, the price during an auction\nto its rate of increase and acceleration. It is possi-\nble that such relations exist in the dynamic, ever-\nchanging eCommerce world. These relationships need\nto be more formally integrated with economic theory\nto create a solid foundation for the empirical findings\nthat have been observed. For example, PDA was\nused to study the relationship between price curves\nin online auctions and their derivatives by Jank and\nShmueli (2005) and Wang (2005). The main finding\nis that relationships between the price curve and its\nacceleration are present in some types of auctions,\nbut not in others, suggesting that dynamics can vary\nbroadly in eCommerce processes.\n5. FUTURE TRENDS IN FUNCTIONAL\nMODELING\nIn the previous sections we have shown multi-\nple facets of FDA that make it a natural approach\nin eCommerce empirical research. Unlike currently\nused static models, FDA can capture processes and\ndynamic information which are inherent in the eCom-\nmerce environment. In the following we describe a\nfew important areas that are still undeveloped both\nin the eCommerce research world and in the FDA\ndomain, and in our opinion have the potential to\nmake a contribution to both.\nThe first area is related to concurrency of events.\nIn almost every eCommerce study, the events of in-\nterest occur concurrently or have at least some over-\nlap. This means that there is a dependence structure\nbetween records, with some events influencing oth-\ners. The most obvious example is the stock mar-\nket with stock prices influencing each other. Some\neCommerce examples are concurrent auctions on eBay\nfor the same item or even for competing items, and\nprices of a certain book at different online vendors\n(and perhaps even brick-and-mortar stores) over time.\nAlthough researchers acknowledge such relationships,\nnearly all studies make the simplifying assumption\nof independent observations. Ignoring the effects of\nconcurrency can lead to invalid results. A first step\nis therefore to find ways to evaluate the degree of\nconcurrency and its effect on the measure of inter-\nest. Shmueli and Jank (2005) introduce and evalu-\nate several data displays for exploring the effect of\nconcurrency in online auctions on final price. Hyde,\nJank and Shmueli (2006) expands this work to vi-\nsualize concurrency of the functional objects, using\ncurves to represent price evolution and its dynamics.\nIn addition to data displays, there is a need for defin-\ning measures of concurrency, and finally, for devel-\noping models that can incorporate and account for\nrelationships between processes that are represented\nas functional objects (see, e.g., Jank and Shmueli,\n2006).\nAnother important enhancement to FDA that would\ngreatly benefit eCommerce research is the incorpo-\nration of change into the functional objects over\ntime. As pointed out earlier, eCommerce experiences\nconstant change. Frequent technological advancement,\nnew website formats, changes in the global economy,\netc., can have a large influence on what we observe\nin the eCommerce world. If we use functional ob-\njects to represent observations which are themselves\nFDA IN ECOMMERCE 11\nlongitudinal, we need ways to incorporate an addi-\ntional temporal dimension that compares functional\nobjects over time.\nFDA research focuses more and more on p-dimensi-\nonal functional objects (e.g., Yushkevich et al., 2001).\nIn many eCommerce applications such representa-\ntions could be very useful. One example is com-\npetition in online auctions, where each auction is\nrepresented by its price curve coupled with the cu-\nmulative number of bidders, thus yielding a bivari-\nate functional representation (Wang and Wu, 2004).\nThere is also related work on symbolic data analysis\n(SDA) (see Bock and Diday, 2000), which provides\ntools for managing complex, aggregated, relational\nand higher-level data described by multivalued vari-\nables. This could be a new successful wedding with\nFDA methods.\nFinally, in many cases, and especially in economics,\nthe objects of interest are individual users. Economists\nare typically interested in how individuals strategize\nand react to others. The problem with formulating\nfunctional objects that represent individuals is spar-\nsity of data. That is, individuals typically do not\nleave many traces during one eCommerce transac-\ntion. For example, in eBay auctions if we treat an\nindividual bidder as our object of interest, then bid-\nders will leave very sparse data (1–2 bids per bid-\nder is the norm). In a similar setting, James, Hastie\nand Sugar (2000) and James and Sugar (2003) use a\nsemiparametric setting where information from data\naggregated across individuals is used to supplement\nthe information at the individual level. An alter-\nnative model would pool information from previous\nrecords that the individual was involved in and use it\nto supplement the current record. Approaches that\nenable the functional representation of sparse data\ncan prove very useful in tying economic theories to\nempirical results. This would further strengthen the\neCommerce research area.\n6. CONCLUSIONS\nThe emerging field of empirical eCommerce re-\nsearch is growing fast with many data-related chal-\nlenges. In light of the special data structures and\nthe types of research questions of interest, we be-\nlieve that functional data analysis can play a ma-\njor role in this field. On the one hand, this requires\nmore involvement by statisticians to further explore\nstatistical issues involved and to develop functional\nmethods and models that are called for in these ap-\nplications. On the other hand, collaborative work\nhas proven to be extremely fruitful for the multiple\ndisciplines involved. In that respect, more outreach\nshould be done to make these tools more popular.\nWider adoption of functional tools by nonstatisti-\ncians requires software accessibility. Currently\nFDA packages exist for Matlab, S-PLUS and R\n(ego.psych.mcgill.ca/misc/fda/software.html). Spe-\ncialized programs for particular applications\nare anticipated to grow, and we encourage researchers\nto make such code and data freely available.\nIn addition, making sample datasets freely\navailable will make this field more accessible\nand attractive to statisticians. Our website\n(www.smith.umd.edu/ceme/statistics/) contains\nsome eBay data and auction-specific FDA code.\nAnother important front in further developing this\nexciting new interdisciplinary field is the involve-\nment and training of graduate students. This in-\ncludes educating statistics students about both the\neCommerce domain and FDA. From our own experi-\nence through Interactive Research Teams, we found\nthis to be a very exciting ground for advancing sta-\ntistical research.\nACKNOWLEDGMENTS\nWe thank Professor Steve Marron from UNC for\nfruitful conversations, Professor Jim Ramsay from\nMcGill University for continuous advice and support\nwith FDA software, and the three referees for their\nconstructive comments.\nREFERENCES\nAbraham, C., Cornillon, P. A., Matzner-Løber, E. and\nMolinari, N. (2003). Unsupervised curve-clustering using\nB-splines. Scand. J. Statist. 30 581–595. MR2002229\nAlford, B. and Urimi, L. (2004). An analysis of various\nspline smoothing techniques for online auctions. Term pa-\nper, Research Interaction Team, VIGRE program, Univ.\nMaryland.\nAris, A., Shneiderman, B., Plaisant, C., Shmueli, G.\nand Jank, W. (2005). Representing unevenly-spaced time\nseries data for visualization and interactive exploration.\nHuman–Computer Interaction—INTERACT 2005 : IFIP\nTC13 International Conference. Lecture Notes in Comput.\nSci. 3585 835–846. Springer, Berlin.\nBajari, P. and Hortac¸su, A. (2003). The winner’s curse, re-\nserve prices and endogenous entry: Empirical insights from\neBay auctions. RAND J. Economics 34 329–355.\nBajari, P. and Hortac¸su, A. (2004). Economic insights\nfrom Internet auctions. J. Economic Literature 42 457–486.\nBapna, R., Jank, W. and Shmueli, G. (2004). Price forma-\ntion and its dynamics in online auctions. Working paper\nRHS-06-003, Smith School of Business, Univ. Maryland.\nAvailable at ssrn.com/abstract=902887.\n12 W. JANK AND G. SHMUELI\nBock, H. H. and Diday, E., eds. (2000). Analysis of Sym-\nbolic Data: Exploratory Methods for Extracting Statisti-\ncal Information from Complex Data. Springer, Heidelberg.\nMR1792132\nCuevas, A., Febrero, M. and Fraiman, R. (2002). Linear\nfunctional regression: The case of fixed design and func-\ntional response. Canad. J. Statist. 30 285–300. MR1926066\nDellarocas, C. and Narayan, R. (2006). A statistical\nmeasure of a population’s propensity to engage in post-\npurchase online word-of-mouth. Statist. Sci. 21 277–285.\nFan, J. and Lin, S.-K. (1998). Test of significance when\ndata are curves. J. Amer. Statist. Assoc. 93 1007–1021.\nMR1649196\nFaraway, J. J. (1997). Regression analysis for a functional\nresponse. Technometrics 39 254–261. MR1462586\nGuo, W. (2002). Inference in smoothing spline analysis of\nvariance. J. R. Stat. Soc. Ser. B Stat. Methodol. 64 887–\n898. MR1979393\nHall, P., Poskitt, D. S. and Presnell, B. (2001). A\nfunctional data-analytic approach to signal discrimination.\nTechnometrics 43 1–9. MR1847775\nHyde, V., Jank, W. and Shmueli, G. (2006). Investigat-\ning concurrency in online auctions through visualization.\nAmer. Statist. To appear.\nHyde, V., Moore, E. and Hodge, A. (2004). Functional\nPCA for exploring bidding activity times for online auc-\ntions. Term paper, Research Interaction Team, VIGRE\nprogram, Univ. Maryland.\nJames, G. M. (2002). Generalized linear models with func-\ntional predictors. J. R. Stat. Soc. Ser. B Stat. Methodol.\n64 411–432. MR1924298\nJames, G. M. and Hastie, T. J. (2001). Functional linear\ndiscriminant analysis for irregularly sampled curves. J. R.\nStat. Soc. Ser. B Stat. Methodol. 63 533–550. MR1858401\nJames, G. M., Hastie, T. J. and Sugar, C. A. (2000).\nPrincipal component models for sparse functional data.\nBiometrika 87 587–602. MR1789811\nJames, G. M. and Sugar, C. A. (2003). Clustering for\nsparsely sampled functional data. J. Amer. Statist. Assoc.\n98 397–408. MR1995716\nJank, W. and Shmueli, G. (2005). Profiling price dynamics\nin online auctions using curve clustering. Working paper\nRHS-06-004, Smith School of Business, Univ. Maryland.\nAvailable at ssrn.com/abstract=902893.\nJank, W. and Shmueli, G. (2006). Modeling concurrency\nof events in online auctions via spatio-temporal semipara-\nmetric models. Working paper, Smith School of Business,\nUniv. Maryland.\nLucking-Reiley, D., Bryan, D., Prasad, N. and\nReeves, D. (2000). Pennies from eBay: The determinants\nof price in online auctions. Technical report, Dept. Eco-\nnomics, Univ. Arizona.\nOgden, R. T., Miller, C. E., Takezawa, K. and\nNinomiya, S. (2002). Functional regression in crop lodg-\ning assessment with digital images. J. Agric. Biol. Environ.\nStat. 7 389–402.\nPfeiffer, R. M., Bura, E., Smith, A. and Rutter, J. L.\n(2002). Two approaches to mutation detection based on\nfunctional data. Stat. Med. 21 3447–3464.\nRamsay, J. O. (1998). Estimating smooth monotone func-\ntions. J. R. Stat. Soc. Ser. B Stat. Methodol. 60 365–375.\nMR1616049\nRamsay, J. O. (2000a). Differential equation models for\nstatistical functions. Canad. J. Statist. 28 225–240.\nMR1777224\nRamsay, J. O. (2000b). Functional components of variation\nin handwriting. J. Amer. Statist. Assoc. 95 9–15.\nRamsay, J. O. and Ramsey, J. B. (2002). Functional data\nanalysis of the dynamics of the monthly index of non-\ndurable goods production. J. Econometrics 107 327–344.\nMR1889966\nRamsay, J. O. and Silverman, B. W. (1997). Functional\nData Analysis. Springer, New York.\nRamsay, J. O. and Silverman, B. W. (2002). Applied Func-\ntional Data Analysis: Methods and Case Studies. Springer,\nNew York. MR1910407\nRamsay, J. O. and Silverman, B. W. (2005). Functional\nData Analysis, 2nd ed. Springer, New York. MR2168993\nRatcliffe, S. J., Heller, G. Z. and Leader, L. R. (2002).\nFunctional data analysis with application to periodically\nstimulated foetal heart rate data. II: Functional logistic\nregression. Stat. Med. 21 1115–1127.\nRatcliffe, S. J., Leader, L. R. and Heller, G. Z. (2002).\nFunctional data analysis with application to periodically\nstimulated foetal heart rate data. I: Functional regression.\nStat. Med. 21 1103–1114.\nReddy, S. K. and Dass, M. (2006). Modeling on-line art\nauction dynamics using functional data analysis. Statist.\nSci. 21 179–193.\nRossi, N., Wang, X. and Ramsay, J. O. (2002). Nonpara-\nmetric item response function estimates with the EM algo-\nrithm. J. Educational and Behavioral Statistics 27 291–317.\nShmueli, G. and Jank, W. (2005). Visualizing online auc-\ntions. J. Comput. Graph. Statist. 14 299–319. MR2160815\nShmueli, G. and Jank, W. (2006). Modeling the dynamics\nof online auctions: A modern statistical approach. In Eco-\nnomics, Information Systems and E-Commerce Research\nII : Advanced Empirical Methods 1 (R. Kauffman and P.\nTallon, eds.). Sharpe, Armonk, NY. To appear.\nShmueli, G., Jank, W., Aris, A., Plaisant, C. and Shnei-\nderman, B. (2006). Exploring auction databases through\ninteractive visualization. Decision Support Systems. To ap-\npear.\nStewart, K., Darcy, D. and Daniel, S. (2006). Opportu-\nnities and challenges applying functional data analysis to\nthe study of open source software evolution. Statist. Sci.\n21 167–178.\nTarpey, T. and Kinateder, K. K. J. (2003). Clustering\nfunctional data. J. Classification 20 93–114. MR1983123\nWang, S. (2005). Principal differential analysis of online auc-\ntions. Term paper, Research Interaction Team, VIGRE\nprogram, Univ. Maryland.\nWang, S., Jank, W. and Shmueli, G. (2006). Forecasting\neBay’s online auction prices using functional data analysis.\nJ. Bus. Econom. Statist. To appear.\nWang, S. andWu, O. (2004). Bivariate functional modelling\nof the bid amounts and number of bids in online auctions.\nTerm paper, Research Interaction Team, VIGRE program,\nUniv. Maryland.\nFDA IN ECOMMERCE 13\nWu, O. (2005). Dynamics of online movie ratings. Term pa-\nper, Research Interaction Team, VIGRE program, Univ.\nMaryland.\nYushkevich, P., Pizer, S., Joshi, S. and Marron, J. S.\n(2001). Intuitive, localized analysis of shape variability. In-\nformation Processing in Medical Imaging. Lecture Notes in\nComput. Sci. 2082 402–408. Springer, Berlin.\n",
            "id": 1091342,
            "identifiers": [
                {
                    "identifier": "oai:arxiv.org:math/0609173",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "2595953",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "206471356",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2010719772",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "21936025",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:citeseerx.psu:10.1.1.235.5177",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "math/0609173",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "10.1214/088342306000000132",
                    "type": "DOI"
                }
            ],
            "title": "Functional Data Analysis in Electronic Commerce Research",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2010719772",
            "oaiIds": [
                "oai:arxiv.org:math/0609173",
                "oai:citeseerx.psu:10.1.1.235.5177"
            ],
            "publishedDate": "2006-01-01T00:00:00",
            "publisher": "'Institute of Mathematical Statistics'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/math/0609173",
                "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.235.5177"
            ],
            "updatedDate": "2021-05-12T02:43:57",
            "yearPublished": 2006,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "0883-4237"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/math/0609173"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/1091342"
                }
            ]
        },
        {
            "acceptedDate": "2011-10-24T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Bradley, Elizabeth"
                },
                {
                    "name": "Gama, João"
                },
                {
                    "name": "Hollmén, Jaakko"
                },
                {
                    "name": "Kölling, Jan "
                },
                {
                    "name": "Langenkämper, Daniel "
                },
                {
                    "name": "Loyek, Christian"
                },
                {
                    "name": "Nattkemper, Tim Wilhelm "
                },
                {
                    "name": "Niehaus, Karsten"
                }
            ],
            "contributors": [],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/15959790"
            ],
            "createdDate": "2013-09-05T15:22:47",
            "dataProviders": [
                {
                    "id": 601,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/601",
                    "logo": "https://api.core.ac.uk/data-providers/601/logo"
                }
            ],
            "depositedDate": "2011-01-01T00:00:00",
            "abstract": "Loyek C, Kölling J, Langenkämper D, Niehaus K, Nattkemper TW. A Web2.0 Strategy for the Collaborative Analysis of Complex Bioimages. In: Gama J, Bradley E, Hollmén J, eds. Advances in Intelligent Data Analysis X: 10th International Symposium, IDA 2011, Porto, Portugal, October 29-31, 2011. Proceedings. Lecture Notes in Computer Science. Vol 7014. Berlin, Heidelberg: Springer;  2011: 258-269",
            "documentType": "research",
            "doi": "10.1007/978-3-642-24800-9_25",
            "downloadUrl": "https://core.ac.uk/download/15959790.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "A Web2.0 Strategy for the Collaborative Analysisof Complex BioimagesChristian Loyek1, Jan Kölling1, Daniel Langenkämper1, Karsten Niehaus2, andTim W. Nattkemper11Biodata Mining Group, Faculty of Technology, Bielefeld University, Germany2Genome Research and Systems Biology, Proteome and Metabolome Research,Faculty of Biology, Bielefeld University, GermanyAbstract. Life science research aims at understanding the relationshipsin genomics, proteomics and metabolomics on all levels of biological selforganization, dealing with data of increasing dimension and complexity.Bioimages represent a new data domain in this context, gaining growingattention since it closes important gaps left by the established moleculartechniques. We present a new, web-based strategy that allows a new wayof collaborative bioimage interpretaion through knowledge integration.We show, how this can be supported by combining data mining algo-rithms running on powerful compute servers and a next generation richinternet application (RIA) front-end offering database/project manage-ment and high-level tools for exploratory data analysis and annotation.We demonstrate our system BioIMAX using a bioimage dataset fromHigh-Content Screening experiments to study bacterial infection in cellcultures.Keywords: Life Science, Bioimage Informatics, Data Mining, ExploratoryData Analysis, Information Visualization, High-content screening, Web2.0,Rich Internet Application, Semantic Annotation1 IntroductionOne field of research which is of growing importance regarding the develop-ment and application of intelligent data analysis is life science research, com-bining a multitude of fields such as molecular biology (genomics, proteomics,metabolomics), biophysics, biotechnology, biochemistry, systems biology, bio-medicine, etc. The aim is to understand and model the building blocks of dy-namic living systems, which are built by entities from different scales (proteins,chemical compounds, cells) and relationships of different kinds and abstractionlevels (interacts-with, inhibition/excitation, co-localizes-with, ...). While most ofthe molecular data has been extracted for homogenized samples, i.e. withoutany spatial information for the molecular entities, spatial information has beenidentified recently as one of the last remaining open gaps in systems biologyand life sciences, which has to be closed if one wants to render a comprehen-sive picture of living systems on all levels of biological self-organization [1]. Aspreprint submitted to IDA 20112 Authors Suppressed Due to Excessive Lengtha consequence, new bioimaging techniques have been developed and proposedto close this gap, like MALDI imaging or High Content Screening [1]. This newdata promises to close many of the aforementioned gaps, but also trigger a newdemand for new technologies to analyze this data. For instance image data pro-duced by high-content screenings (HCS) is increasingly getting richer and morecomplex, since a growing number of variables is associated to each spatial ele-ment (i.e. pixel) of the sample. While this is an enormous gain in information(e.g. in pharmaceutical screenings each of the n variables encode a protein ofinterest or a cell compartment), it is impossible to access, quantify and extractall relevant image information in one session by one researcher. In fact, the im-ages need to be evaluated by researchers from different fields (biophysics, cellbiology, chemistry, computer science, statistics, ... ) regarding different aspects(image quality/noise, semantics, cell/function classification, staining specificity,statistical significance, ...) and the result of their studies need to be integratedmuch earlier in research as it is done now in many projects, where researchersfrom different institutes in different countries meet maybe once a year.To foster integration of results and views from different aspects of bioim-age analysis a new approach is needed, that covers a large variety of bioimageanalytics, ranging from manual annotation based on direct visual inspection tofull automatic data mining using unsupervised machine learning. Due to therecent developments of web technology, allowing rapid dynamic integration ofuser generated content into new user-shaped knowledge data bases (such trendsare sometimes referred to as Web2.0 or even Science 2.0 [2, 3]) we started thedevelopment of a purely web-based bioimage analysis platform which allows theuser to apply different analyses to the data, share data and results with otherresearchers without a complicated and time-consuming act or data modeling. Sothe aim is not to design a web-based LIMS (laboratory information managementsystem), but to provide a web-based work bench to interpret bioimages withina web-organized project together with a chosen group of other researchers, in-dependent from their whereabouts condition to an internet connection.Our fully web-based software approach to intelligent data analysis of bioim-age data is called BioIMAX (BioImage Mining, Analysis and eXploration) [4],developed to augment both, an easy initial exploratory access to complex high-content image data and the collaboration of geographically distributed scientists.BioIMAX was developed as a rich internet application (RIA), i.e. a web applica-tion whose performance and look-and-feel is comparable to a standard desktopapplication, but will mostly be executed in a web browser allowing for platformindependency and avoiding additional installation costs. With BioIMAX, severaltypes of high-content image data can be uploaded and organized in personalizedprojects through a simple web-based interface. This allows a rapid data searchand retrieval of own datasets and easily supports sharing of data with othercollaborating researchers by inviting them to own projects. With the BioIMAXLabeler, a graphical and textual annotation tool, the users have the possibility toannotate, discuss and comment specific image regions, e.g. by linking chat-likediscussions to image coordinates. In order to initially explore high-content imagepreprint submitted to IDA 2011A Web2.0 Strategy for the Collaborative Analysis of Complex Bioimages 3data, the BioIMAX VisToolBox provides general methods to get an initial visualaccess to the n-dimensional signal domain of high-content images. Higher leveldata mining applications (such as dimension reduction or clustering) which arecomputationally more expensive are triggered and evaluated in BioIMAX, butare computed on powerful external compute servers using specialized C/C++machine learning libraries.In recent years, several different toolboxes for bioimage informatics have beenproposed and we review them briefly here. General imaging analysis tools likeImageJ [5, 6] or ITK [7, 8] aim at providing a large variety of image processingmethods for tasks such as registration, filtering, thresholding or segmentation. Incontrast, single purpose tools such as CellProfiler [9, 10] focus on special biolog-ical or biomedical problems as well as on data from specific imaging techniques.Another group of approaches are meant as general technological platforms tostore and organize large amounts of image data in a central repository on aremote server architecture. In addition to the data management, analysis plat-forms can include selected methods for data visualization, annotation and anal-ysis. One of the first tools published in this context is OME (Open MicroscopyEnvironment) [11]. Bisque [12] is a recently introduced powerful tool, which pro-vides a platform with an automatic 3D nuclei detection or microtubule tracking.Although tools such as CellProfiler, OME or Bisque represent great steps to-wards improvements in bioimage data analysis, most of them are focussed onparticular well defined biological problems and provide specially adapted analy-sis methods to solve these problems. However, in many cases the analysis goal isvague and little a priori knowledge is available about the underlying data. Thus,it is not clear in advance, which analysis strategy should be applied. This is ageneral problem in the analysis of high-content bioimage data, which usuallyneeds to be discussed by collaborating researchers from different disciplines. Inthe context of HCS analysis, especially pharmaceutical HCS, analysis-related de-cisions increasingly take place associated to a particular region of interest (ROI).Thus, discussion needs to be linked to particular (x,y)-coordinates, which leadsto less trivial design issues in database and graphical user interface development.In addition to this, a successful (cross-domain) collaboration is often impeded,since the involved researchers are usually distributed across several research in-stitutes. In the future, we assume more impacts of web technology developmentsfor bioimage analysis. Especially the fact, that the web is getting more collabo-rative and user-shaped (effects referred to as Web2.0 ) and offers more and morepowerful graphics applications, will stimulate new developments such as ours.As an example, we demonstrate several aspects of BioIMAX, which couldsupport the study of bacterial infection of cells with Listeria monocytogenes.BioIMAX can be accessed at http://ani.cebitec.uni-bielefeld.de/BioIMAXwith the username “tuser” and the password “test1” for testing purposes.preprint submitted to IDA 20114 Authors Suppressed Due to Excessive LengthFig. 1. Example high-content fluorescence image showing infected cells: (a) Cell chan-nel: cytoplasm, (b) Nuclei channel, (c) Listeria channel: GFP stained Listeria and (d)RGB composition of the three channels (a)-(c)2 MaterialsListeria monocytogenes is an intracellular pathogenic bacterium that causes afood-borne disease called Listeriosis in both humans and animals. Listeriosis isa rare but serious disease with a high overall mortality rate of 30%, most com-mon in pregnant woman or immunocompromised individuals [13]. The bacte-ria is an important model organism for infection, intracellular proliferation andhost-pathogen interactions. Those intracellular bacteria are protected againstthe host immune system and are poorly accessible for treatment with antibi-otics. Therefore, the invasion of the host cells is an important and crucial stepin Listeria pathogenesis and virulence [14]. In order to study the grade of hostcell invasion with L. monocytogenes, a high-content screen has been set up usingautomated microscopy and L. monocytogenes expressing the green fluorescentprotein (GFP). Figure 1 shows an example high-content image, obtained withthe ScanR screening station (Olympus).3 ArchitectureAs previously mentioned, BioIMAX software was designed as a rich internetapplication. The usage of RIAs has several advantages, which meet the neces-preprint submitted to IDA 2011A Web2.0 Strategy for the Collaborative Analysis of Complex Bioimages 5sary requirements for the development of a system like BioIMAX. In contrast toconventional thin-client web applications, RIAs provide a richer and more com-plex graphical interface, resembling desktop applications’ interface interactivityand computation power. The RIA technology improves the efficiency of web ap-plications by moving part of the computation, interaction and presentation tothe client, thereby lowering the amount and frequency of client-server traffic con-siderably and permitting asynchronous client-server communication. As a result,the usability of web applications will be improved, annoying installation routineswill be avoided and the software will be accessible from any location.The BioIMAX client side was developed with Adobe Flex [15], which is anopen-source framework for building expressive web applications. RIAs developedwith Adobe Flex deploy consistently on all major browsers and operating sys-tems by leveraging the Adobe Flash Player. In order to efficiently and consistentlymanage the data collected, MySQL [16] is used as a relational database manage-ment system. The communication between the Flex client and the server-sidedatabase is realized by using AMFPHP [17], which is one of the fastest clientserver communication protocol available to Flash Player developers.Once a user has been authenticated by a username and password login pro-cedure, she/he is presented with the BioIMAX start page. The start page isdesigned in the style of a social media platform, creating a personalized environ-ment, which provides, e.g. access to the system-internal mail box or a navigationpanel for general data handling such as image upload, project management oraccess to the BioIMAX data browser. With the data browser the user can searchand browse the BioIMAX database. In addition to visualizing and managing thesearch results, the data browser serves as starting point for all data explorationtasks. In the following, we give a detailed description of the BioIMAX VisTool-Box for visual data exploration tasks and the BioIMAX Labeler for semanticimage annotation.4 Visual data explorationThe BioIMAX VisToolBox (illustrated in figure 2) provides a set of methodsto explore and analyze the signal domain of high-content images. The graphicaldisplay of the VisToolBox is divided into two panels. One panel contains an imageviewer, especially designed for high-content or multivariate images (see figure2(a)). The image viewer includes basic functions such as zooming or panning forimage navigation purposes and allows scrolling through a stack of high-contentimages, image by image. The other panel (see figure 2(b)) comprises severalmethods from the fields of visualization, co-location analysis and exploratorydata analysis (EDA), chosen by tabs, which will be described more detailed inthe following.Image comparison: This tool provides two different methods to compare up tothree single image channels of a high-content image simultaneously on a struc-tural/morphological level. The first method is called Alpha blending and aimspreprint submitted to IDA 20116 Authors Suppressed Due to Excessive LengthFig. 2. Screenshot of the VisToolBox. This tool provides several methods to exploreand analyze the signal domain of high-content images. It consists of an image viewer(a) and a panel (b) including methods from the fields of visualization, co-locationanalysis and exploratory data analysis (EDA), separated by tabs. In (c), three selectedimages can be visualized and compared simultaneously by adjusting the opacity of therespective images. By moving the mouse cursor over the red triangle, the opacity valueof each single image will be adapted in real-time depending on the distance to thecorners of the triangle, which represents the three selected images. Selection of imageswill be done consecutively per drag-and-drop from the image list (d) to one of the boxesdisplaying the letters A, B or C (e). The small figures below exemplary show furtherexploration displays: (f) Histogram and (g) Scatter plot. In (h) a co-location studyof two images with statistical measurements (Manders’ score and Pearson correlationcoefficient) is displayed in an bar chart.at comparing three images while superimposing them as layers and manuallyadjusting the opacity value of the respective layers by moving the mouse cursorover the opacity triangle (see figure 2(c)). This can be a useful tool, e.g. whileevaluating analysis methods such as segmentation methods regarding their accu-racy. Thus, the user can detect structural differences or similarities between theselected images. The purpose of the second method (RGB pseudo coloring) is togenerate a pseudo color fusion image from three selected images, by interpretingeach image as one color channel in a RGB image.Image Manipulation: This tab includes two histogram dialogs, which displayinformation about the statistical distribution of grey values in the currently se-preprint submitted to IDA 2011A Web2.0 Strategy for the Collaborative Analysis of Complex Bioimages 7lected image. Both histograms are interactive, i.e. the user can manipulate thedistribution and the visualization on the left is adapted in real-time. In thehistogram the user can filter out irrelevant / wrong signals or study variousthresholds needed for analysis tasks.Co-Fluorescence analysis: Here, the user can compare two selected imageson a statistical level by calculating (i) the Pearson correlation coefficient or (ii)the Manders’ score, which is a frequently used index for co-location studies influorescence microscopy [18]. The results are displayed in a bar chart (see figure2(h)).Gating, Link and Brush for in-depth visual exploration: The last partof the VisToolBox allows a more detailed exploration of specific image regions.Here, the user can focus the study of L. monocytogenes invasion on a single celllevel, e.g. to examine cell invasion in the nucleus (see figure 3). For this purpose,the user first has to select a region of interest (ROI) by drawing a rectangle onthe displayed image in the image viewer. In a next step the user chooses one ofthe three visualization techniques at the top of the Visualization window, whichopens a new plot dialog. Dependent on the chosen dialog, the user will be askedto drop one or more images from the image list to the dialog. After that, all pixelswithin the ROI will be displayed in the respective plot, i.e. a histogram, scatterplot or parallel coordinates. Selection of points in one plot triggers highlightingthe referring pixels in the image on the left (for detailed description see figure3). This process can also be referred to as “gating” or “link-and-brush” [19].Clustering and dimension reduction: Since with the growing number ofvariables (i.e. grey values) a visual inspection using the above techniques willonly give a limited view on the image data and the complex high-dimensionalmanifold given by its n-variate features, i.e. pixel values. As a consequence one isinterested in using methods from unsupervised learning to reduce the complexityof the data so it can be visualized, like clustering to reduce the number of patternsto be assigned to graphical parameters (such as colour) or dimension reductionto reduce the number of variables directly. In the design of BioIMAX we createdan interface that allows the integration of such algorithms so these can run onremote compute servers and write the results into the BioIMAX data base. Foreach algorithm at least one individual tool is integrated as well, so the user canstart the computationally expensive methods from the web interface, wait forthe results and can inspect these again through the web inside BioIMAX. Infigure 5 we show an example result obtained with the clustering tool TICAL(Toolbox for Image Clustering ALgorithms, currently in alpha release stage) forone of our images.preprint submitted to IDA 20118 Authors Suppressed Due to Excessive LengthFig. 3. Interactive exploration of bivariate data from a selected region of interest (ROI)in the image viewer (a). For the ROI, the user selects two image channels (here thenucleus channel (Bisbenzimid) and the GFP-marked L. monocytogenes channel) andone tool, e.g. a scatter plot (b) from the visualization tab (see figure 2). The pixelvalues corresponding to the same location within the ROI are displayed as points inthe scatterplot. Selection of points x in the plot (c) triggers highlighting the referringpixels in the image (displayed as red regions superimposing on the original image), withrespect to the following criterion: Γ =nx|tminb ≤ b(x) ≤ tmaxb ∧ tminl ≤ l(x) ≤ tmaxlo,with Γ describing the selection of points x in the scatter plot, tminb and tmaxb definesthe minimum and maximum of the selection range regarding Bisbenzimid values andb(x) is the Bisbenzimid value of point x. The same applies to the L. monocytogenesvalues, accordingly. This process is often referred to as “gating” or “link-and-brush”.5 Semantic image annotationThe BioIMAX Labeler tool allows one to graphically annotate image regions insingle image channels. The interface provides the image viewer described beforeon the left and an options toolbar on the right (see figure 4(a,b)). In the toolbarthe user can adjust several label properties like geometry, color or size beforelabeling (see figure 4(c)). Furthermore, the user can select specific semantic labeltypes, which will be textually associated to the label. The user can choose oneof the label types from the predefined semantic categories, e.g. Cell or Cellu-lar compartment or she/he can create own label types or categories (see figure4(d,e)). By clicking in the currently selected image, the annotations are placedas graphical objects on an invisible layer belonging to each single image, allow-ing for easy modification existing labels, e.g. reforming, recoloring or resizing. Aset of labels can be stored into the database by saving all parameters for eachsingle label, i.e. location, type, size, color and form, and will be linked to therespective image channel. In order to get and modify detailed information aboutsingle labels, the user can open the annotation/info window by selecting thepreprint submitted to IDA 2011A Web2.0 Strategy for the Collaborative Analysis of Complex Bioimages 9Fig. 4. Screenshot of the Labeler. It consists of the image viewer on the left (a), onwhich the user can place a number of graphical objects (called labels), to annotatespecific image regions. On the right, the options toolbar (b) provides options for ad-justing several label properties auch as geometry, color or size (c) and the possibilityto link specific semantic label types to single annotations, which can be predefinedtypes selected by (d) or newly generated types (e), depending on the current scientificcontext. In order to initiate a discussion about a specifc label on a higher semanticlevel, the user can invoke an annotation/info window by selecting the toggle button (f)at the top of the toolbar. The annotation/info window provides an option to start oropen an existent chat-like discussion about an label (g).toggle button showing the callout icon. With the Labeler tool we are aiming attwo goals. First, user shall be enabled to label interesting image regions, whichcan be important in quantification and evaluation tasks. In this study, the ex-perts have to annotate cells in a large number of images into different semanticcategories. With the Labeler the experts can define and insert new label typesrepresenting different infection grades (see figure 4(d)) and can start labelingcells, e.g. using circles with different colors, each color representing a specificinfection grade. Using the Labeler, the process of establishing a gold standardfrom several experts is speeded up and simplified, e.g. there is no need to trans-fer multiple copies of images to the experts. The users can easily login to theBioIMAX system and can immediately start labeling from any location and alllabel results will centrally be stored in the database and can be inspected by allcollaborating researchers at any time.Second, we want to link chat-like discussions to image regions to link high-level semantics to morphological features. Therefore, the Labeler provides a chatwindow (see figure 4(g)), which can be accessed from the annotation/info win-dow. Here, several users can communicate about the selected label and the con-preprint submitted to IDA 201110 Authors Suppressed Due to Excessive LengthFig. 5. A clustering based visualization of one three-dimensional HCS bioimage. Com-bining clustering with dimension reduction can be done by a) using a self-organizingmap or b) a combination of other vector quantization algorithms (k-means, neural gas)with dimension reduction techniques (PCA, LLE, t-sne). Both approaches allow themapping of cluster prototypes to colors which is used to colorize each pixel applyingthe best matching criterion to the pixel and all the cluster prototypes. In the middle ofthe display, one can the the result pseudo color image. On the bottom, a small numberof clusters has been chosen in the overview (right panel) and displayed. The lengths ofthe horizontal boxes display the average signal intensity in that cluster.versation will additionally be stored together with the label. This facilitatesWeb2.0 style collaborative work on one image, while the stored states of commu-nication content are directly linked to image coordinates/ROIs. While developingnew analysis strategies for high-content image data, researchers have to discussaspects about the original data, e.g. the trustworthiness of signals, and aboutanalysis methods, e.g. the quality of intermediate results such as registration orsegmentation. Figure 6 illustrates both scenarios.6 DiscussionIn this paper we proposed a Web2.0 approach for the collaborative explorationof high-content screening bioimages in life sciences and demonstrated its applica-tion with an example dataset from Listeria monocytogenes cell invasion analysis.Due to the complexity of high-content image data, the extraction and quantifi-cation of all image information and the generation of analysis strategies is apreprint submitted to IDA 2011A Web2.0 Strategy for the Collaborative Analysis of Complex Bioimages 11Fig. 6. Illustration of a chat-like discussion about image regions with the BioIMAXLabeler. The figure demonstrates two possible discussion scenarios based on the sameimage data: discussion about the raw image and discussion about analysis methods orresults.difficult task for researchers and different aspects need to be discussed by collab-orating researchers from different disciplines. Thus, we presented fully web-basedtools, which support both, exploratory analysis of high-content image data andimportant collaborative aspects in Listeria monocytogenes infection analysis.The BioIMAX VisToolBox provides methods from the field of exploratorydata analysis, in order to gain initial insights into the structural characteristicsof the underlying data, following Ben Shneidermans information visualizationmantra: Overview first, zoom in and filter, details on demand. The concept ofVisToolBox does not include predefined analysis pipelines regarding a specialbiological question in the form of a black box model, which gets an image asinput and the user is presented with the finalized result. With the VisToolBox,the user is directly involved in the knowledge discovery process, while exploringthe data space themselves with specific information visualization techniques.This is an important strategy in the field of visual data mining and exploration[20]. Using the clustering tool TICAL, even higher dimensional image data canbe visually explored without the need to install a machine learning toolbox onones desktop, since BioIMAX allows the application of clustering independentfrom the users whereabouts, condition to an internet connection.The BioIMAX Labeler provides tools to communicate and discuss aboutspecific image regions, which is of great value, since analysis-related decisionsare increasingly associated to particular regions of interest. The Labeler allowsto annotate image regions with graphical objects and to link chat-like discussionsrepresenting high-level semantics to morphological features.Since BioIMAX is designed as a rich internet application, one of the keyfeature is, that a user only needs a login and a password to get access to theBioIMAX platform provided an internet connection is available. Except for theinstallation of the Flash Player, which is available for most browsers, no ad-preprint submitted to IDA 201112 Authors Suppressed Due to Excessive Lengthditional software packages and libraries have to be installed. The fact that allcollaboration and exploration tasks will be performed within one web-basedplatform is of great value, since it simplifies and speeds up several aspects inthe analysis process, e.g. avoiding transfer of data between researchers, since allresearchers work on the same copy centrally stored in the BioIMAX database.We believe, that in the age of the ongoing development of web technologies,our Web2.0 approach is an important step forward, to support complex analysistasks regarding high-content data. Such an approach is of particular benefitto those scientific projects, where several scientists from different institutes atdifferent locations are involved and has to collaborate.References1. Megason, S.G., Fraser, S.E.: Imaging in Systems Biology. Cell. 130(5), 784–795(2007)2. Shneiderman, B.: Science 2.0. Science. 319, 1349–1350 (2008)3. Waldrop, M.M.: Science 2.0 - Great new tool, or great risk? Scientific American.(January 9, 2008)4. Loyek, C., et al.: BioIMAX: A Web 2.0 approach for easy exploratory and collabo-rative access to multivariate bioimage data. BMC Bioinformatics. 12, 297 (2011)5. Image Processing and Analysis in Java, http://rsbweb.nih.gov/ij/6. Abramoff, M.D., Magelhaes, P.J., Ram, S.J.: Image Processing with ImageJ. Bio-photo. Int. 11(7), 36–42 (2004)7. Insight Segmentation and Registration Toolkit (ITK), http://www.itk.org8. Yoo, T.S., et al.: Engineering and Algorithm Design for an Image Processing API:A Technical Report on ITK - the Insight Toolkit. In: Westwood,J (ed) Proceedingsof Medicine Meets Virtual Reality. IOS Press, Amsterdam, pp. 586–592 (2002)9. Carpenter, A.E., et al.: CellProfiler: image analysis software fpr identifying andquantifying cell phenotypes. Genome Biol. 7(10), R100 (2006)10. Lamprecht, M.R., Sabatini, D.M., Carpenter, A.E.: CellProfiler: free, versatile soft-ware for automated biological image analysis. Biotechniques. 42, 71–75 (2007)11. Swedlow J.R., et al.: Informatics and Quantitative Analysis in Biological Imaging.Science. 300, 100–102 (2003)12. Kvilekval, K., et.al.: Bisque: a platform for bioimage analysis and management.Bioinformatics. 26(4), 544–552 (2010)13. Ramaswamy, V., et al.: Listeria - review of epidemiology and pathogenesis. J.Microbiol. Immunol. Infect. 40(1), 4–13 (2007)14. Ireton K.: Entry of the bacterial pathogen Listeria monocytogenes into mammaliancells. Cell Microbioll. 9(6), 1365–1375 (2007)15. Adobe Flex. http://www.adobe.com/products/flex/16. MySQL. http://www.mysql.com/17. AMFPHP - Action Message Format PHP. http://amfphp.sourceforge.net/18. Manders, E., et al.: Dynamics of three-dimensional replication patterns duringthe S-phase, analysed by double labelling of DNA and confocal microscopy. J. CellScience. 103, 857–862 (1992)19. Ware, C.: Information Visualization - Perception for Design, Morgan KaufmannPublishers Inc. San Francisco (2004)20. Keim, D.A.: Information Visualization and Visual Data Mining. IEEE Transactionson Visualization and Computer Graphics. 7(1), 100–107 (2002)preprint submitted to IDA 2011",
            "id": 14037507,
            "identifiers": [
                {
                    "identifier": "oai:pub.uni-bielefeld.de:2300232",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "10.1007/978-3-642-24800-9_25",
                    "type": "DOI"
                },
                {
                    "identifier": "15959790",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "16282430",
                    "type": "MAG_ID"
                }
            ],
            "title": "A Web2.0 Strategy for the Collaborative Analysis of Complex Bioimages",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "16282430",
            "oaiIds": [
                "oai:pub.uni-bielefeld.de:2300232"
            ],
            "publishedDate": "2011-01-01T00:00:00",
            "publisher": "'Springer Science and Business Media LLC'",
            "pubmedId": null,
            "references": [
                {
                    "id": 10029619,
                    "title": "A.E.: CellProﬁler: free, versatile software for automated biological image analysis.",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.2144/000112257",
                    "raw": "Lamprecht, M.R., Sabatini, D.M., Carpenter, A.E.: CellProﬁler: free, versatile software for automated biological image analysis. Biotechniques. 42, 71–75 (2007)",
                    "cites": null
                },
                {
                    "id": 10029624,
                    "title": "Action Message Format PHP.",
                    "authors": [],
                    "date": null,
                    "doi": null,
                    "raw": "AMFPHP - Action Message Format PHP. http://amfphp.sourceforge.net/",
                    "cites": null
                },
                {
                    "id": 10029613,
                    "title": "BioIMAX: A Web 2.0 approach for easy exploratory and collaborative access to multivariate bioimage data.",
                    "authors": [],
                    "date": "2011",
                    "doi": "10.1186/1471-2105-12-297",
                    "raw": "Loyek, C., et al.: BioIMAX: A Web 2.0 approach for easy exploratory and collaborative access to multivariate bioimage data. BMC Bioinformatics. 12, 297 (2011)",
                    "cites": null
                },
                {
                    "id": 10029618,
                    "title": "CellProﬁler: image analysis software fpr identifying and quantifying cell phenotypes.",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "Carpenter, A.E., et al.: CellProﬁler: image analysis software fpr identifying and quantifying cell phenotypes. Genome Biol. 7(10), R100 (2006)",
                    "cites": null
                },
                {
                    "id": 10029625,
                    "title": "Dynamics of three-dimensional replication patterns during the S-phase, analysed by double labelling of DNA and confocal microscopy.",
                    "authors": [],
                    "date": "1992",
                    "doi": null,
                    "raw": "Manders, E., et al.: Dynamics of three-dimensional replication patterns during the S-phase, analysed by double labelling of DNA and confocal microscopy. J. Cell Science. 103, 857–862 (1992)",
                    "cites": null
                },
                {
                    "id": 10029617,
                    "title": "Engineering and Algorithm Design for an Image Processing API: A Technical Report on ITK - the Insight Toolkit. In: Westwood,J (ed)",
                    "authors": [],
                    "date": "2002",
                    "doi": null,
                    "raw": "Yoo, T.S., et al.: Engineering and Algorithm Design for an Image Processing API: A Technical Report on ITK - the Insight Toolkit. In: Westwood,J (ed) Proceedings of Medicine Meets Virtual Reality. IOS Press, Amsterdam, pp. 586–592 (2002)",
                    "cites": null
                },
                {
                    "id": 10029623,
                    "title": "Entry of the bacterial pathogen Listeria monocytogenes into mammalian cells.",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1111/j.1462-5822.2007.00933.x",
                    "raw": "Ireton K.: Entry of the bacterial pathogen Listeria monocytogenes into mammalian cells. Cell Microbioll. 9(6), 1365–1375 (2007)",
                    "cites": null
                },
                {
                    "id": 10029621,
                    "title": "et.al.: Bisque: a platform for bioimage analysis and management.",
                    "authors": [],
                    "date": "2010",
                    "doi": "10.1093/bioinformatics/btp699",
                    "raw": "Kvilekval, K., et.al.: Bisque: a platform for bioimage analysis and management. Bioinformatics. 26(4), 544–552 (2010)",
                    "cites": null
                },
                {
                    "id": 10029614,
                    "title": "Image Processing and Analysis",
                    "authors": [],
                    "date": null,
                    "doi": "10.1002/9780470918548.ch9",
                    "raw": "Image Processing and Analysis in Java, http://rsbweb.nih.gov/ij/",
                    "cites": null
                },
                {
                    "id": 10029620,
                    "title": "Informatics and Quantitative Analysis in",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1126/science.1082602",
                    "raw": "Swedlow J.R., et al.: Informatics and Quantitative Analysis in Biological Imaging. Science. 300, 100–102 (2003)",
                    "cites": null
                },
                {
                    "id": 10029626,
                    "title": "Information Visualization - Perception for Design,",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1016/b978-155860819-1/50011-x",
                    "raw": "Ware, C.: Information Visualization - Perception for Design, Morgan Kaufmann Publishers Inc. San Francisco (2004)",
                    "cites": null
                },
                {
                    "id": 10029627,
                    "title": "Information Visualization and Visual Data Mining.",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1109/2945.981847",
                    "raw": "Keim, D.A.: Information Visualization and Visual Data Mining. IEEE Transactions on Visualization and Computer Graphics. 7(1), 100–107 (2002) preprint submitted to IDA 2011",
                    "cites": null
                },
                {
                    "id": 10029616,
                    "title": "Insight Segmentation and Registration Toolkit (ITK),",
                    "authors": [],
                    "date": null,
                    "doi": "10.1109/iembs.2007.4352434",
                    "raw": "Insight Segmentation and Registration Toolkit (ITK), http://www.itk.org",
                    "cites": null
                },
                {
                    "id": 10029622,
                    "title": "Listeria - review of epidemiology and pathogenesis.",
                    "authors": [],
                    "date": "2007",
                    "doi": null,
                    "raw": "Ramaswamy, V., et al.: Listeria - review of epidemiology and pathogenesis. J. Microbiol. Immunol. Infect. 40(1), 4–13 (2007)",
                    "cites": null
                },
                {
                    "id": 10029611,
                    "title": "S.E.: Imaging in Systems Biology.",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1016/j.cell.2007.08.031",
                    "raw": "Megason, S.G., Fraser, S.E.: Imaging in Systems Biology. Cell. 130(5), 784–795 (2007)",
                    "cites": null
                },
                {
                    "id": 10029615,
                    "title": "S.J.: Image Processing with ImageJ.",
                    "authors": [],
                    "date": "2004",
                    "doi": null,
                    "raw": "Abramoﬀ, M.D., Magelhaes, P.J., Ram, S.J.: Image Processing with ImageJ. Biophoto. Int. 11(7), 36–42 (2004)",
                    "cites": null
                },
                {
                    "id": 10029612,
                    "title": "Science 2.0 - Great new tool, or great risk? Scientiﬁc American.",
                    "authors": [],
                    "date": "2008",
                    "doi": null,
                    "raw": "Waldrop, M.M.: Science 2.0 - Great new tool, or great risk? Scientiﬁc American. (January 9, 2008)",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "http://pub.uni-bielefeld.de/publication/2300232"
            ],
            "updatedDate": "2022-05-11T23:43:41",
            "yearPublished": 2011,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "0302-9743"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/15959790.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/15959790"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/15959790/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/15959790/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/14037507"
                }
            ]
        },
        {
            "acceptedDate": "2010-03-23T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Foulds, James Richard"
                },
                {
                    "name": "Frank, Eibe"
                }
            ],
            "contributors": [],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/58831609"
            ],
            "createdDate": "2015-06-05T10:29:28",
            "dataProviders": [
                {
                    "id": 2114,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/2114",
                    "logo": "https://api.core.ac.uk/data-providers/2114/logo"
                },
                {
                    "id": 677,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/677",
                    "logo": "https://api.core.ac.uk/data-providers/677/logo"
                }
            ],
            "depositedDate": "2010-03-01T00:00:00",
            "abstract": "Multi-instance (MI) learning is a variant of inductive machine learning, where each learning example contains a bag of instances instead of a single feature vector. The term commonly refers to the supervised setting, where each bag is associated with a label. This type of representation is a natural fit for a number of real-world learning scenarios, including drug activity prediction and image classification, hence many MI learning algorithms have been proposed. Any MI learning method must relate instances to bag-level class labels, but many types of relationships between instances and class labels are possible. Although all early work in MI learning assumes a specific MI concept class known to be appropriate for a drug activity prediction domain; this ‘standard MI assumption’ is not guaranteed to hold in other domains. Much of the recent work in MI learning has concentrated on a relaxed view of the MI problem, where the standard MI assumption is dropped, and alternative assumptions are considered instead. However, often it is not clearly stated what particular assumption is used and how it relates to other assumptions that have been proposed. In this paper, we aim to clarify the use of alternative MI assumptions by reviewing the work done in this area",
            "documentType": "research",
            "doi": "10.1017/s026988890999035x",
            "downloadUrl": "https://core.ac.uk/download/29197595.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "The Knowledge Engineering Review, Vol. 25:1, 1–25. & Cambridge University Press, 2010doi:10.1017/S026988890999035XA review of multi-instance learning assumptionsJ AME S FOULD S and E IBE FRANKDepartment of Computer Science, University of Waikato, Private Bag 3105, Hamilton, New Zealand;e-mail: jf47@cs.waikato.ac.nz, eibe@cs.waikato.ac.nzAbstractMulti-instance (MI) learning is a variant of inductive machine learning, where each learning examplecontains a bag of instances instead of a single feature vector. The term commonly refers to thesupervised setting, where each bag is associated with a label. This type of representation is a natural fitfor a number of real-world learning scenarios, including drug activity prediction and image classifi-cation, hence many MI learning algorithms have been proposed. Any MI learning method must relateinstances to bag-level class labels, but many types of relationships between instances and class labelsare possible. Although all early work in MI learning assumes a specific MI concept class known to beappropriate for a drug activity prediction domain; this ‘standard MI assumption’ is not guaranteed tohold in other domains. Much of the recent work in MI learning has concentrated on a relaxed view ofthe MI problem, where the standard MI assumption is dropped, and alternative assumptions areconsidered instead. However, often it is not clearly stated what particular assumption is used and howit relates to other assumptions that have been proposed. In this paper, we aim to clarify the use ofalternative MI assumptions by reviewing the work done in this area.1 IntroductionMulti-instance (MI) learning (Dietterich et al., 1997; also known as ‘multiple-instance learning’) isa variant of inductive machine learning that has received a considerable amount of attention dueto both its theoretical interest and its applicability to real-world problems such as drug activityprediction and image classification.MI learning, as it is commonly defined, belongs to the supervised learning paradigm, which aimsto solve classification and regression problems by using algorithms to build models from data basedon a set of labeled examples. The majority of the work in MI learning is concerned with binaryclassification problems, where each example has a classification label that assigns it into one of twocategories—‘positive’ or ‘negative’. The goal is to ‘learn’ a model based on the training examples thatis effective in predicting the classification labels of future examples. All training examples have been(often manually) assigned a class label, which is why the term supervised learning is used.Where MI learning differs from the traditional scenario is in the nature of the learning examples. Inthe traditional supervised learning scenario, each example is represented by a fixed-length vector offeatures. However, in MI learning each example is represented by a multi-set (or bag, as computerscientists often call it) of feature vectors. In other words, each example contains one or more featurevectors. The feature vectors are referred to as instances. Classification labels are only provided for entirebags, and the task is to learn a model that predicts the classification labels for unseen future bags.11 In many cases, the instances are assumed to have hidden class labels that are in some way related to thelabels for the bags. Depending on the problem domain, the prediction of the instance labels can also be animportant task in its own right.In early MI research, a strong assumption was made regarding the relationship between instancesinside the bags and the label of the bag. This assumption is generally referred to as the standard MIassumption. Under this assumption, each instance has a hidden class label that identifies it as either apositive or a negative instance, and a bag is considered to be positive if and only if it contains at leastone positive instance. This is generally believed to be true for the musk drug activity predictionproblem, where a molecule will have the desired drug effect if and only if one or more of itsconformations binds to the target binding site (Dietterich et al., 1997). However, in other problemdomains this assumption may not apply, and different or more general assumptions may be needed.A significant amount of the more recent research in MI is concerned with cases where the standardview of MI learning is relaxed, and alternative assumptions are used instead.Unfortunately, it is often not clear what particular assumptions are used and how they relate toother assumptions from the literature. This is perhaps at least partially due to the fact that the useof the term ‘MI learning’ has evolved from the original statement by Dietterich et al. (1997).Dietterich et al. included the standard MI assumption in their original definition of MI learning,but many authors now include alternative assumptions within the MI learning framework(see, e.g. Xu, 2003; Chen et al., 2006; Dong, 2006). To compound the issue, some authors usealternative MI assumptions without explicitly describing the assumptions used. In this paper, weaim to shed some light on existing MI assumptions and relationships by reviewing the MIassumptions that can be found in the literature. This paper is not intended as a review of algo-rithms for MI learning that implement the standard MI assumption.2 BackgroundThis section gives an overview of machine learning, with emphasis on supervised learning and theMI learning scenario. MI learning is defined, and the motivations for it are explained.2.1 Machine learningEvery day, we as humans discover new facts about our world. We interact with the environmentaround us, and receive feedback through our empirical faculties—our senses. We are able torecognize trends and can begin to anticipate the consequences of our actions. The process thatallows us to do this is called learning. It is ubiquitous and most of us take it for granted.Learning is a task that is normally associated with humans (and intelligent non-human ani-mals), hence the problem of creating machines that can learn falls within the umbrella of artificialintelligence. While the creation of truly ‘intelligent’ machines still seems to be a long way off,machine learning as a practical discipline is a success story of modern artificial intelligence. Manyalgorithms have been discovered that allow machines to make inferences from observed data,effectively ‘learning’ non-trivial facts and behaviors.Under the guise of data mining, these algorithms have many commercial applications.Machines are far more efficient and reliable than humans at processing large amounts of data. Forthis reason, learning algorithms can offer huge cost saving and efficiency benefits to businesses,and have successful applications in many domains from medicine to marketing.2.2 Supervised learningSupervised learning is the branch of machine learning that is concerned with algorithms that canlearn concepts from labeled examples. As an input, the algorithm requires a set of example cases,each of which has been given a label corresponding to some important property of the example.The task of the algorithm is to build a model that will generate accurate predictions of the labels offuture examples.Let us illustrate this with a simple example. Suppose that we are amateur botanists, and we wishto learn to distinguish between instances of the various species of the iris genus of flowering plants.2 J . F OULD S AND E . F RANKAn expert has given us a set of examples of some of the species of the genus. Once we have seen afew examples of each, we can attempt to infer the defining characteristics of each species. Once wehave discovered the pattern, we can become proficient at labeling arbitrary iris plants.Having introduced the subject and its terminology via a simple example, we may now formallydefine the standard supervised machine-learning scenario. An instance is a vector of N featuresconcatenated with a class label, of the form {x | g(x)}, where x5 {x1, x2,y, xN} is the featurevector and g(x) is the label of the instance. Features and class labels are typically either elements ofthe real numbers (numeric attributes) or domain-specific sets of names (nominal attributes).The task is to find g(x), based on a given labeled set of instances, where the labels have beenassigned based on g(x). When the class is a nominal attribute, this process is called classification.When the class is a numeric attribute, the process is called regression.The underlying classification process g(x) is known in machine learning terminology as aconcept. The g(x) may be either a function or a non-deterministic process. Given a set of trainingexamples to learn from, a supervised machine learning algorithm outputs a model that is intendedto be a best-guess approximation to g(x). Such a model is known as a concept description.This paper is about a variation of standard (single-instance) supervised learning called MIlearning.2.3 MI LearningMI learning, as defined by Dietterich et al. (1997), is a variation on the standard supervisedmachine learning scenario. In MI learning, each example consists of a multi-set (bag) of instances.Each bag has a class label, but the instances themselves are not explicitly labeled. The learningproblem is to build a model based on given example bags that can accurately predict the classlabels of future bags. The difference between standard supervised learning and MI learning isillustrated in Figure 1.An example will once again help to illuminate the concept. Chevaleyre and Zucker (2001) referto this example as the simple jailer problem. Imagine that there is a locked door, and we have N keychains, each containing a bunch of keys. If a keychain (i.e. bag) contains a key (i.e. instance) thatcan unlock the door, that key chain is considered to be useful. The learning problem is to build amodel that can predict whether a given key chain is useful or not.2.3.1 Definition of MI learningWe now present a formal definition of the MI problem. This formalization is a refinement of thoseused by Weidmann et al. (2003) and Ga¨rtner et al. (2002). In this paper, we follow the trendestablished by the majority of the work in this field (a notable exception being Zhou & Zhang,2006) and assume a binary class attribute V5 {1, 2}. Let x be the instance space. Then an MIObjectUnknownProcess ResultUnknownProcess3InstanceInstanceInstance...Instance12nResultObject(a)(b)Figure 1 (a) The traditional supervised machine-learning scenario. (b) Multi-instance learning. Figure basedon a similar diagram by Dietterich et al. (1997)A review of multi-instance learning assumptions 3concept is a function nMI : Nw ! O. The task in MI learning is to learn this function, based on anumber of example elements of the function.Here, Nw refers to the set of all functions from x toN, which is isomorphic to the set of all multi-subsets of x, viewing the output of f ðxÞ 2 Nw as the number of occurrences of x in the multi-set.Such functions are known as multiplicity functions, and are a direct generalization of indicatorfunctions for ordinary sets.Note that this differs slightly from the formulation used by Weidmann et al., who define an MIconcept as a function nMI : 2x-V. Here, 2x, the set of indicator functions over x, is isomorphic tothe power set of x, but this does not take into account the fact that duplicate instances are allowedin a bag. Our alternative definition of an MI concept explicitly defines the problem examples asmulti-sets rather than just sets. This is important for some generalized MI concepts.2.4 The standard MI assumptionA large percentage of the work on MI learning, including all early works and notably Dietterich etal. (1997) and Maron and Lozano-Pe´rez (1997), makes a particular assumption regarding therelationship between the instances within a bag and the class label of the bag. Dietterich et al.considered this assumption to be so fundamentally important that they included it as part of theirdefinition of MI learning. We will follow Weidmann et al. (2003), and refer to this assumption asthe standard MI assumption.The standard MI assumption states that each instance has a hidden class label cAV5 {1, 2}.Under this assumption, an example is positive if and only if one or more of its instances arepositive. Thus, the bag-level class label is determined by the disjunction of the instance-level classlabels.Formally, let X ¼ fX1;X2; . . . ;Xng 2 Nw be a bag containing n instances from feature space x.Each instance has a class label determined by some process g : x-V. Let nS : Nw ! O be astandard MI concept, and equate ‘1’ with the logical constant ‘True’, and ‘2’ with the logicalconstant ‘False’. Then:nSðXÞ3ðgðX1Þ _ gðX2Þ _ . . . _ gðXnÞÞ: ð1ÞIt should be noted that the standard MI assumption is asymmetric: if the positive and negativelabels are reversed, the assumption has a different meaning. Therefore, when we apply thisassumption, we need to be clear which label should be the positive one.The standard MI assumption was adopted because it is believed to be appropriate for themusk problem domain. In the musk problem, it is assumed that a molecule (represented by a bagof instances) will emit a musky smell if and only if one of its conformations (represented by anindividual instance) binds to a certain target site, hence the standard MI assumption applies(Dietterich et al., 1997).A number of learning algorithms for MI classification under the standard MI assumption havebeen proposed in the literature. Dietterich et al. (1997) presented several algorithms for learningaxis-parallel rectangles to identify the positive region of instance space. A bag is classified aspositive if it has at least one instance in this region. Maron and Lozano-Pe´rez (1997) defineddiverse density, a measure of the likelihood that a point in instance space is a positive targetconcept, and used a gradient search to find the point that is most likely to define the targetconcept. A refinement of this algorithm, expectation maximization (EM)-diverse density (DD),was proposed by Zhang and Goldman (2001).Several single instance learning methods have been ‘upgraded’ to the MI scenario under thestandard MI assumption, including support vector machines (Andrews et al., 2002), neural net-works (Ramon & De Raedt, 2000) decision trees (Blockeel et al., 2005), (Chevaleyre & Zucker,2001), decision rules (Chevaleyre & Zucker, 2001) and weak learners for boosting (Auer & Ortner,2004). Zhou and Xu (2007) showed that MI learning under the standard MI assumption could beviewed as a semi-supervised learning problem with the additional constraint that positive bags4 J . F OULD S AND E . F RANKmust contain at least one positive instance. They adapted semi-supervised support vector machinesto the standard MI scenario by encoding this ‘positive constraint’ in the objective function of thesupport vector machine (SVM).2.5 Alternative AssumptionsAs (at least in part) the inclusion of the standard MI assumption in Dietterich et al.’s (1997)definition of MI learning, it was initially adopted ubiquitously by the fledgeling MI learningcommunity. In more recent years, there has been a trend toward the relaxation of this strict view ofMI learning (Xu, 2003).When the standard MI assumption is relaxed, other interactions between instances and the classlabels of bags are possible. We refer to such interactions asMI assumptions, since we must assumethat such a relationship between bags and class labels occurs when we use a learning algorithm tobuild a predictive model. In order to make learning computationally feasible, it is generallynecessary to reduce the hypothesis space by making use of some MI assumption.Although many recent authors have (implicitly or explicitly) abandoned the standardassumption, it is often not precisely stated that new assumptions have been used (Xu, 2003).Moreover, the literature is not in agreement on whether the relaxed version of the MI problembelongs within the umbrella of MI learning, or is a separate problem. Some authors, notablyWeidmann et al. (2003) and Scott et al. (2005), refer to the relaxed MI scenario as generalized MI,while others, such as Xu (2003), Chen et al. (2006), Dong (2006) and Foulds (2008), includealternative MI assumptions within the MI framework. In particular, Xu explicitly extends thedefinition of MI learning to include other assumptions.We contend that the term ‘multi-instance learning’ should contrast directly with ‘single-instancelearning’, and connotes any type of learning where several instances can be included within a singlelearning example, regardless of the assumptions used. Hence, we follow Xu, and use the term torefer to the relaxed version of MI learning as well as the standard MI scenario. We shall reservethe term ‘generalized MI learning’ to refer MI assumptions that are strictly more general than thestandard assumption, such as those proposed by Weidmann et al. (2003) and Scott et al. (2005).2.6 Motivations for alternative MI assumptionsAlthough the standard MI assumption is widely believed to be appropriate for the musk drugactivity prediction problem, the MI representation can be applied to a number of other problemdomains where the standard MI assumption may not be directly applicable. In these domains,algorithms that rely upon alternative MI assumptions may be more appropriate.A prominent example of this is the task of learning visual concepts from databases of labeledimages. This learning problem arises in several computer vision tasks including object detection orrecognition, image categorization and content-based image retrieval. Although standard super-vised learning could be applied directly to learn from global features of images, the task of learningvisual concepts lends itself well to an MI representation because the target concepts typically onlyoccupy part of the space of an image. Therefore, it makes sense to split the image into smallerregions (segments; Burl et al., 1998).Based on this approach, an image can be represented as a bag of segments, which are representedby instances. Segments can simply be equal-sized blocks, or more sophisticated segmentationmethods can be used. Each instance in a bag contains features extracted from the correspondingsegment, such as color, texture and shape information. Features describing relative relationships toadjacent segments can also be used (Maron & Ratan, 1998). MI learning has been frequently appliedto visual concept learning tasks—see, for example, Maron and Ratan (1998), Andrews et al. (2002),Zhang et al. (2002), Chen and Wang (2004), Chen et al. (2006) and Qi et al. (2007).Methods using the standard MI assumption have been applied to visual concept learning taskswith some success. The standard assumption is a good heuristic for many such tasks, but not allvisual concepts can be represented under that assumption. First, for the purposes of comparisonA review of multi-instance learning assumptions 5let us briefly consider a task where the standard MI assumption may be applicable. Maron andRatan (1998) identified the task of identifying natural scenes of waterfalls as such a problem. Here,if image segments (instances) containing a waterfall can be identified, images containing thatinstance-level concept can be identified under the standard MI assumption: an image contains awaterfall if and only if it contains at least one waterfall segment.We will now describe a learning task where the standard MI assumption is not sufficient torepresent the desired concept. Consider the task of categorizing images of natural scenes of beaches,oceans and deserts. Since the standard MI assumption requires a binary classification task, onewould generally approach this by learning one against the rest models for each class. However, thereis no single item contained in a segment of a beach scene that defines it as belonging to themphbeach category, as opposed to the other two alternatives. Unlike the waterfall scenario, wherethe existence of at least one segment with a specific property is a necessary and sufficient conditionfor a positive class label, we cannot identify a part of a scene that directly corresponds to a beach ornon-beach scenario. Thus, the standard MI assumption cannot apply. We would still like to use theMI representation in order to capture localized information from the image, but we need to assumea non-standard relationship between instances and bag-level class labels.Let us now consider a generalized MI model that would allow us to represent this type ofconcept. For the sake of simplicity, we can define ocean scenes as images with water instances(segments), and no sand instances, desert scenes as images with sand instances and no waterinstances, and beach scenes as images with both sand and water segments. Then the beach conceptcan be defined to benbeachðXÞ3ð9x 2 X sandðxÞÞ ^ ð9x 2 X waterðxÞÞ:Such a concept can be represented under alternative MI assumptions such as presence-based MI(Weidmann et al., 2003) and the generalized multiple instance learning (GMIL) assumption (Scottet al., 2005; described in Sections 3.1.1 and 3.2, respectively).A similar scenario arises in text categorization, where the task is to assign semantic labels to textdocuments. A document can be represented as an MI bag: instances are obtained by splitting thedocument into smaller passages. Features such as word occurrence frequencies can be extractedfrom each passage to form instances. This MI approach to text categorization has been applied byAndrews et al. (2002) and Ray and Craven (2005).Like visual concept learning, text categorization can potentially benefit from an MI repre-sentation because it allows localized information to be used. In both of these problem scenarios,the MI representation is used to describe an object by a set of parts, each of which is a featurevector (instance). Chevaleyre and Zucker (2001) refer to this type of problem as a multiple partproblem (MPP). As they observe, although the MI representation is useful for describing MPPlearning examples, the standard MI assumption is not guaranteed to hold.Other problem domains where the relaxation of the standard MI assumption is appropriateinclude robot localization via landmark matching, activity prediction for drugs that bind atmultiple sites simultaneously, and identifying thioredoxin-fold proteins. For a thorough accountof these scenarios, the reader is referred to Scott et al. (2005).A further motivation for the investigation of MI approaches based on alternative assumptionsis the empirical success that such methods have enjoyed on benchmark problems, including themusk datasets where the standard MI assumption was originally claimed to be necessary (Diet-terich et al., 1997). A number of authors have reported very competitive results on these datasetsusing methods that do not strictly respect the standard MI assumption (deliberately or otherwise),including Ga¨rtner et al. (2002), Wang and Zucker (2000), Frank and Xu (2003), Chen et al. (2006)and Dong (2006).As we have seen, different problem domains require different MI assumptions. Although thestandard MI assumption is often an effective heuristic, the existence of a natural MI repre-sentation for learning examples in a given domain does not imply that this assumption will applyin that domain. Data mining practitioners need to take this into consideration, and select algorithms6 J . F OULD S AND E . F RANKthat are known to depend only on assumptions that are likely to be true for the problem at hand. Wetherefore consider the relaxed MI scenario to be worthy of continued research, with the caveat thatauthors make explicit their assumptions whenever the standard MI assumption is disregarded.2.7 MI assumptions versus MI concept classesEach MI assumption defines a relationship between instances in a bag and bag-level class labels. Ifwe know that a certain MI assumption is applicable for a certain problem domain (i.e. the relevantrelationship between instances and bag-level class labels does in fact hold in that domain), then wemay assert that the assumption is true for that domain. We would then consider using the MIlearning algorithms that make use of that assumption.Hence, the assumption view of MI learning (exemplified by Xu (2003)) is useful from a practicalmachine learning perspective. From a theoretical machine learning perspective, although it can beuseful to consider concepts instead of assumptions. For an MI assumption A, if we assert that A istrue for a given domain, we assert that the concept space for that domain is c(A), where c(A)denotes the set of MI concepts allowable under A. We say that c(A) is the corresponding MIconcept class of A, and A is the corresponding MI assumption of c(A).Thus, for example, if we assert that the standard MI assumption is true for the problem domainof detecting molecules that emit a musky odor, we assert that the concept space of that domain isthe set of concepts following the form specified by Equation 1. Clearly, the assumption andconcept views of MI learning are equivalent.3 Alternative MI assumptions, concepts and modelsIn this section we review the MI assumptions/concept classes that have been proposed, and brieflydiscuss the algorithms that have been developed to learn models of MI data under theseassumptions. The relationships between the various MI assumptions are shown in Figure 2.3.1 Weidmann’s concept hierarchy for instance-based generalized MI learningWeidmann et al. (2003) formulated a hierarchy of generalized instance-based assumptions for MIlearning. The hierarchy consists of the standard MI assumption and three types of generalized MIassumptions, each more general than the last.To illustrate the three types of generalized MI assumptions, we will follow Weidmann (2003) anduse an extended version of Chevaleyre and Zucker’s (2001) simple jailer problem (discussed earlier inSection 2.3). Recall that in the simple jailer problem, each bag is a keychain containing several keys,and a bag is considered to be useful (i.e. positive) if one or more of its keys can unlock a specific door.Standard MI assumption, but can approximate it arbitrarily closely.The Weighted Collective and Weighted Linear Threshold assumptions do not strictly subsume the*GMIL and Count−based GMIL assume instance−level concepts are axis−parallel hyper−rectangles.MIStandardThreshold−basedCount−basedDDSVM BARTMIPOtherMetadataNearestNeighbourWeightedCollective/ MILESBased on instance−level distances Based on bag−level distancesMetadata assumptionsWeidmann’s hierarchyCount−based GMIL*GMIL* MIGraphWeighted Linear Threshold/ Extended Weighted Collective*Collective*Presence−basedFigure 2 Relationships between multi-instance (MI) assumptions. Arrows indicate increasing generality.GMIL, generalized multiple instance learning; DD, diverse density; SVM, support vector machine; MILES,multiple-instance learning via embedded instance selectionA review of multi-instance learning assumptions 73.1.1 Presence-based MI assumptionIn presence-based MI learning, the assumption is that a bag is positive if and only if there exist oneor more instances in the bag that belong to a set of required instance-level concepts (i.e. have therequired hidden instance-level class labels). This can be visualized as a version of the jailer problemwhere there are multiple locks on the door. To unlock the door, we need at least one key that canopen each type of lock on the door.Formally, let vPB : Nw ! O be a presence-based MI concept, let C^ \u0002 C be the set of requiredinstance-level concepts, and let D : Nw \u0003 C ! N be the function that outputs the count of thenumber of occurrences of a concept in the bag. Then:vPBðXÞ38c 2 C^ : DðX ; cÞ \u0004 1:It should be noted that the standard MI assumption is a special case of presence-based MI,where jC^j ¼ 1, that is, there is just one required concept.3.1.2 Threshold-based MI assumptionThe threshold-based MI assumption states that a bag is positive if and only if there are at least acertain number of instances in the bag that belong to each of the required concepts. Each concept canhave a different threshold. In terms of the jailer problem, this is similar to the presence-based MIjailer problem except that multiple copies of each type of lock are allowed, and keys are consumedduring the unlocking process. If there are n copies of a certain lock, then we need at least n keys of theappropriate type to unlock it. This is reminiscent of the Microsoft puzzle game Chip’s Challenge.2To state the threshold-based assumption formally, let us use the same lexicon as before, and letvTB : Nw ! O be a threshold based MI concept. Then we have:vTBðXÞ38ci 2 C^ : DðX ; ciÞ \u0004 ti;where ti 2 N is the lower threshold for concept i.3.1.3 Count-based MI assumptionUnder the count-basedMI assumption, there is a maximum and a minimum number of instances fromeach of the required concepts that must be observed in order for a bag to be positive. Imagine this asthe threshold-based jailer problem, except that there is also a stingy jailer who despises wastefulness,and will not allow anybody to open the door if they have too many keys of any particular type.Formally, let vCB : Nw ! O be a count-based MI concept. ThenvTBðXÞ38ci 2 C^ : ti \u0005 DðX ; ciÞ \u0005 zi;where ti 2 N is a lower threshold for concept i, and zi 2 N is an upper threshold for concept i.3.1.4 The concept hierarchyWeidmann et al. (2003) showed that these assumptions form a hierarchy of generality, wherestandard MI \u0006 presence-based \u0006 threshold-based \u0006 count-based (see Figure 3 for an illustration).Therefore, in theory at least, a strong MI learner designed to work under a general assumptionshould still be able to solve an MI problem where one of the less general assumptions applies. Forinstance, a strong algorithm designed to use the count-based assumption should work well on adataset where the generative model is presence-based.3.1.5 Algorithms and modelsThe two-level classification (TLC) algorithm (Weidmann et al., 2003) is designed to learn the typeof MI concepts that are described in Weidmann’s concept hierarchy, where it is assumed that bag-level class labels are determined by the counts of each instance-level concept in a bag.TLC learns in a two-step process. The first step learns instance-level concepts using a decisiontree. The tree is built on all of the instances in all of the bags in the training data, with class labelsof the instances set to the labels of the parent bags. Each node in the tree is considered to represent2 Microsoft Game Studios (1990).8 J . F OULD S AND E . F RANKa candidate concept. Then each bag is converted into a single-instance representation, with anattribute for every node in the tree (i.e. each candidate concept), the value of which is set to thenumber of instances that reach that node in the decision tree.The second step learns bag-level concepts, based on the candidate instance-level concepts dis-covered in the first step. A single-instance learning algorithm is applied to the transformed data.The same mapping is performed at classification time, and the bag-level predictions are made bythe single-instance learner. A further (optional) refinement to the algorithm is to use attributeselection to try to eliminate the attributes that do not contribute to the instance-level classificationproblem learned by the decision tree.The constructive clustering ensemble (CCE) method (Zhou & Zhang, 2007) also uses a proposi-tionalization method that may be appropriate for some Weidmann type concepts. The algorithm usesa clustering method to cluster the instances in the training bags into d clusters. Bags are mapped into aboolean feature space where each attribute corresponds to a cluster, and the value of an attribute is setto 1 if and only if that bag has an instance in that cluster. A single-instance model is built on theresulting dataset. The algorithm is repeated for multiple values of d, and classification predictions aremade via a majority vote of the resulting ensemble of single-instance classifiers.Given that the feature space constructed by CCE represents the presence or absence of instanceswith certain properties, it appears that this algorithm may be best suited for learning presence-based MI concepts. However, the algorithm could be easily extended to learn count-based con-cepts if the transformed feature space was modified to include the number of occurrences of theinstance-level concepts, as in the earlier TLC algorithm. It should also be noted that Zhou andZhang’s (2007) results indicate that the algorithm is less accurate when learning on presence-basedMI data than TLC (with attribute selection enabled).3.2 The GMIL assumptionScott et al. (2005)3 introduced a new MI assumption based on theoretical results from geometricpattern recognition. We will refer to this assumption as the GMIL assumption. In this model, thereStandard MIPresence−based MIThreshold−based MICount−based MIFigure 3 Weidmann’s hierarchy of instance-based multi-instance (MI) concepts3 Originally published in 2003 as a technical report at the University of Nebraska, Lincoln.A review of multi-instance learning assumptions 9is a set of target points C5 {c1, c2,y, ck}. A bag is positive if and only if it contains instancessufficiently close to at least r points, out of the k target points.Scott et al. extend this model to also include a set of repulsion points \u0002C ¼ fc1; c2; . . . ; ck0 g. In theextended model, a positive bag may only contain instances that are close to at most s of therepulsion points.The model can be understood with reference to the ranked half-Hausdorff metric using theweighted infinity norm. The Hausdorff metric (see, e.g. Edgar (1990)) provides a measure ofdistance between two bags of points, and is commonly used in computer vision applications. Thesets of target points and repulsion points can be viewed as ‘ideal bags’, where positive bags arewithin a ranked half-Hausdorff distance of some threshold g from the ideal positive bag, and atleast a ranked half-Hausdorff distance of g 0 away from the ideal negative bag.The Hausdorff distance between bags P and Q is defined to be the largest distance fromeither a point in P to its closest point in Q, or from a point in Q to its closest point in P,whichever is larger, under some norm. However, this is not robust against noise, so theranked Hausdorff metric is used: instead of using the largest distance, the sth largest distance isused. Scott et al. compute the distance from the bag to the model (i.e. the half-Hausdorff metric),but not vice-versa, as it is assumed that the model is accurate and does not contain extraneouspoints.Scott et al. used the weighted infinity norm as the instance-level distance measure required tocompute the Hausdorff distance. The infinity norm defines the length of a vector as ||x||N5max(|x1|,|x2|,y,|xn|), the largest absolute value of its components. Thus the set of points with adistance of at most d from a point p under the infinity norm are the points within a hypercube ofwidth 2d and center at point p. The weighted infinity norm allows scaling of the vector compo-nents, such as for normalization. The hypercubes from the infinity norm are hyperrectangles or‘boxes’ when the weighted infinity norm is used.The ranked half-Hausdorff metric using the weighted infinity norm can be stated formally asmaxq2Qs minp2Pfjjp\u0007 qjj1g\u0002 \u0003;where maxs is the sth max, P is a bag, Q is the set of target points, and ‘2’ denotes standard vectorsubtraction. Let a bag P be positive if and only if the above equation evaluates to at most g. Thena target concept is a set of k5 |Q| axis-parallel target boxes, and a bag is positive if and only if itcontains points within at least r5 k2 s1 1 of the k target boxes.To also include a set \u0002Q of k0 axis-parallel repulsion boxes, we must also check that the followingformula evaluates to at least g 0, which is another constant:minq2 \u0002Qs 0 minp2Pfjjp\u0007 qjj1g\u0002 \u0003:Under this extended model, for a bag to be positive, it must also contain points within at mosts02 1 of the k0 repulsion boxes.In terms of Weidmann’s hierarchy, Scott et al.’s MI formulation, without repulsion points, isthe same as presence-based MI learning when boxes are viewed as instance-level concepts and theminimum threshold r is equal to the number of target points k. When r 6¼ k, Scott et al.’s model ismore general than the presence-based MI model. Weidmann’s threshold and count-based MIconcepts generalize presence-based MI concepts in a different fashion to Scott et al.’s model, andneither is strictly more general than the other.Count-based MI concepts can model repulsion points by setting the maximum count for someinstance-level concepts to zero. However, count-based and threshold-based concepts cannot modelthe case where only r out of k concepts must be present for a bag to be positive. The GMILassumption cannot represent problems where the number of instances belonging to specific con-cepts must be within a given range (as in threshold and count-based MI), as only concept presencerather than concept counts are included in the model.10 J . F OULD S AND E . F RANK3.2.1 Algorithms and modelsScott et al. (2005) proposed the GMIL-1 algorithm to learn GMIL concepts. The algorithmexplicitly enumerates all possible axis-parallel boxes. It creates a single-instance feature space withboolean attributes for each box, signifying whether a bag contains an instance within that box. Toreduce the dimensionality of this space, boxes that cover the same instances are grouped together,and only one representative box for each group is used. The training bags are mapped into thefeature space, and the single-instance algorithm Winnow (Littlestone, 1987) is trained on thetransformed dataset.The task of enumerating all axis-parallel boxes is exponential in the number of dimensions,which makes GMIL-1 very inefficient. GMIL-2 Tao and Scott (2004) is an attempt to improve thecomputational and memory efficiency of the algorithm. The algorithm is roughly the same asGMIL-1, but it selects groups of boxes in a different way. First, GMIL-2 reduces the number ofinstances to consider by selecting a subset of representative instances, C. Then it constructs groupsby considering the boxes represented by the bounding box of each possible subset ofC. A breadth-first search approach is used to attempt to efficiently find the sets of groups that are geometricallyvalid, that is, all instances within the bounding box of the group are contained within the group.Although GMIL-2 is far more efficient than GMIL-1, it still suffers from limited scalability(Tao et al., 2004a). In a further attempt to improve the algorithm’s computational complexity,Tao et al. (2004a) presented a kernel-based reformulation of the GMIL learning problem. Thekernel, k4, allows a support vector machine to be applied directly to the problem. As the com-putation of k4 belongs to the complexity class #P-complete and thus suffers from severe scalabilityissues that quickly make the problem intractable as the problem size increases, the authors pre-sented a fully polynomial randomized approximation scheme (FRAPS) for it.3.3 The count-based GMIL assumptionAs mentioned earlier, neither the GMIL assumption nor the count-based MI assumption is strictlymore general than the other—some MI concepts can be represented by one assumption and notthe other, and vice-versa. Tao et al. (2004b) proposed an MI assumption that is more general thanboth of the assumptions, which we will refer to as the count-based GMIL assumption. Under thisassumption, a bag is positive if and only if it satisfies at least r of a set of k concepts, and at most sof a set of k0 ‘repulsion’ concepts. A concept ci is satisfied by a bag B if the number of points in theregion of instance space associated with ci is between a certain specified minimum value, ti, and amaximum value, zi.3.3.1 Algorithms and modelsTao et al. proposed an extended version of the k4 kernel, called kmin, which allows a supportvector machine to solve the MI learning problem under the count-based GMIL assumption.Unlike the k4 kernel, the feature space associated with the kmin kernel includes information relatedto the number of instances within the box that describes the concept concerned.3.4 The DD-SVM/multiple-instance learning via embedded instance selection assumptionThe DD-SVM (Chen & Wang, 2004) algorithm and its successor multiple-instance learning viaembedded instance selection (MILES) (Chen et al., 2006) also use a generalized MI assumption, wherebag-level class labels are determined based on the distance from each of a set of target points.Although the authors of DD-SVM and MILES note that their algorithms do not follow the standardMI assumption, they do not explicitly describe their new assumptions independently from thedescriptions of the algorithms. This section attempts to isolate the common assumptions between thesealgorithms and thus describe the types of MI concepts that the algorithms attempt to learn.The DD-SVM/MILES assumption is related to Scott et al.’s (2005) GMIL assumption, in thatdistance from a set of target points is used to determine bag labels. However, ‘distance’ is definedA review of multi-instance learning assumptions 11differently, and the r-of-k threshold is not used. As in the GMIL assumption, the target points canbe related to either positive or negative concepts. The DD-SVM and MILES methods each includea distance-related measure of similarity between a target point and a bag, and it is assumed thatbag-level class labels are in some way determined by these similarity values. In DD-SVM, thesimilarity function issðx;BiÞ ¼ minjjjBij \u0007 xjjw;where Bij are the instances in the bag Bi, x is a target point and w is a weight vector determiningthe importance of each feature. In MILES, a Gaussian function is used instead:sðx;BiÞ ¼ maxjexp \u0007 jjxij \u0007 xjj2s2 !; ð2Þwhere s is a scaling factor, which is a parameter to the algorithm. The relationship between the‘similarities’ to target points and bag-level class labels is dependent on the single-instance baselearner that is applied to the data after transformation based on the similarity scores. In theoriginal statement of DD-SVM and MILES, a support vector machine is used. If a linear kernel isused for the SVM, class labels are determined by a weighted linear threshold defined on thesimilarity values, that is, DD-SVM/MILES concepts are of the formnD=MðXÞ3Xk2Twksðk;XÞ þ b \u0004 0;where T is the set of target points, wk is the weight associated with target point k and b is a biasparameter. If a target point has a positive weight, it can be viewed as being positive—bags withpoints close to that target point are more likely to be positive; and similarly for negative targetpoints, that is, points with negative weight.However, alternative single-instance base learners are possible for both DD-SVM and MILES.If we view the algorithms as ‘wrapper’ methods where arbitrary base learners are possible, theassumption is merely that bag-level labels can be determined in some way from the similarities tothe set of target points.3.4.1 Algorithms and modelsMILES (Chen et al., 2006) embeds bags into a single-instance feature space based on similarityscores obtained from Equation 2, and applies the 1-norm support vector machine algorithm to thetransformed dataset.MILES uses the instances in the training bags as candidates for target points. A feature-spacemapping is defined, where each attribute represents the closeness of an instance to a candidatetarget point (i.e. training instance). Each training bag is mapped into this space (with class labelsappended), and a single-instance base learner is built on the transformed dataset. At testing time,bags are similarly mapped into the instance-based feature space, and classification predictions aremade by the single-instance base learner.The diverse density support vector machine (DD-SVM) algorithm (Chen & Wang, 2004) is apredecessor to MILES that is conceptually very similar to the later method. Chen et al.’s (2006)experimental results show that MILES is much more efficient than DD-SVM in terms of com-putational complexity, while maintaining similar or better classification accuracy and increasedrobustness to label noise, hence we do not discuss DD-SVM in detail here.3.5 The bag-level representation transformation for multi-instance prediction assumptionThe BARTMIP algorithm (Zhang & Zhou, 2009) is closely related to MILES, and thus implicitlyrelies on a related MI assumption. While MILES assumes that bag labels are related to theinstance-level distances from a set of target points, the BARTMIP method assumes that bag labelsare related to distances from target bags.12 J . F OULD S AND E . F RANKDistances between bags of points can be computed via the Hausdorff distance (see Section 3.2).Zhang and Zhou use three different variants of the Hausdorff distance to define bag-level dis-tances: the maximal, minimal and average Hausdorff distances.The (maximal) Hausdorff distance between two sets of points (or bags) A and B is the largestEuclidean distance between a point in A and its closest point in B, or vice versa. Formally, theHausdorff distance is defined asHmaxðA;BÞ ¼ max fhðA;BÞ; hðB;AÞg;wherehðA;BÞ ¼ maxa2Aminb2Bjja\u0007 bjj:The minimal Hausdorff distance was proposed by Wang and Zucker (2000) in the context of asimple nearest-neighbor MI learning algorithm that is discussed in more detail in Section 3.11. Inthis variant, the h function is replaced by a function h1, whereh1ðA;BÞ ¼ mina2Aminb2Bjja\u0007 bjj:Note that the minimal Hausdorff distance is simply the shortest distance between a point in Aand a point in B. It can be stated asHminðA;BÞ ¼ mina2A;b2Bjja\u0007 bjj:Zhang and Zhou additionally proposed the average Hausdorff distance, which is defined to bethe average distance between a point in one bag and its closest point in the other bag:HavgðA;BÞ ¼Pa2Aminb2B jja\u0007 bjj þPb2Bmina2A jjb\u0007 ajjjAj þ jBj :The choice of distance measure determines the set of MI concepts that can be represented.Clearly, alternative bag-level distance measures could potentially be used if appropriate for aspecific problem domain.3.6 Algorithms and modelsThe BARTMIP algorithm performs an initial bag-level clustering step on the training bags usingthe k-medoids algorithm adapted to MI learning by using a Hausdorff distance variant for itsdistance function. As well as grouping the bags into k clusters, the clustering algorithm alsooutputs the medoid 4 of each cluster.The training bags are then mapped to a k-dimensional single-instance feature space, where theith attribute corresponds to the distance of the bag to the ith medoid, under the same bag-leveldistance measure that was used in the clustering step. The class labels of the original bags areappended to the transformed instances, and a single-instance base learner is applied to theresulting feature space. At classification time, the mapping is performed to the test bags andpredictions are made by the single-instance base learner. Note that this method is identical toMILES, except for the different feature-space transformation used.3.7 The collective assumptionUnder the standard MI assumption, only a few special instances (those with a ‘positive’ label) canhave any influence on the class label. In contrast, the collective assumption is an MI assumption,where all instances in a bag contribute equally to the bag’s label (Xu, 2003).4 The medoid of a cluster is the element whose average distance to the other elements is minimal. In ageometric space, this is equivalent to choosing the element that is closest to the center of the cluster.A review of multi-instance learning assumptions 13The collective assumption, designed as a general alternative to the standard MI assumption,was not precisely defined by Xu (2003). However, all algorithms in (Xu, 2003) that were designedto use this assumption actually depend on the same specific generative model. We will thereforeuse the term collective assumption to refer to this specific model.The collective assumption is motivated by a view of the nature of MI bags that is based onprobability theory. Under this view, a bag is not a finite collection of fixed elements (as is generallyassumed), but instead is a sample of an underlying population specific to that particular bag. Here,a bag can be modeled as a probability distribution Pr(X|b) over the instance space, where theobserved instances were generated by random sampling from that distribution.Instances are assumed to be assigned class labels according to some (typically unknown)probability function (or non-deterministic probabilistic process) g(x)5Pr(Y|x). Under the col-lective assumption, the bag-level class probability function is determined by the expected classvalue of the population of that bag. Let c be a class label AY5 {0,1}, and let b be a bag. ThenPrðcjbÞ ¼ EX ½PrðcjxÞjb\b ¼ZXPrðcjxÞPrðxjbÞ dx:To compute this exactly, we must know Pr(x|b), the probability distribution for the bag.However, this is generally not known in practice so the sample provided by the instances in the bagis used instead:PrðcjbÞ ¼ 1nbXnbi¼1PrðcjxiÞ;where nb is the number of instances in the bag. In the limit, as the sample size approaches infinity,the sample version of the equation will approach the population version.3.7.1 Algorithms and modelsXu (2003) developed statistical algorithms for learning this kind of probabilistic concept, the mostnotable of which are versions of logistic regression and boosting, upgraded to solve MI learningproblems under the collective MI assumption (see also (Xu & Frank, 2004)).Frank and Xu (2003) also investigated a simple heuristic algorithm calledMIWrapper for applyingsingle-instance learners under the collective assumption. The first step of the MI Wrapper algorithmis to collect all of the instances from all of the bags, and label each of them with the label of the bagthat they came from. This effectively creates a propositional (i.e. single-instance) dataset. The algo-rithm then weights all of the instances so that each bag has equal total weight. A single-instancelearner is applied to this propositional dataset. At classification time, the single-instance learnerpredicts class probabilities for all of the instances in the bag for which the classification is to bepredicted. The output is merely the average (arithmetic or geometric) of the predicted instance-levelclass probabilities. Using the arithmetic mean at prediction time, the method applies the ‘sample’version of the collective assumption formula when making predictions.3.8 MI assumptions using instance weightsIn the collective assumption, each instance receives equal weight when computing bag-level classprobabilities. Foulds (2008) introduced two MI assumptions based on the notion of instanceweights that determine the level of influence that instances have on bag-level class labels. Theweighted collective MI assumption is an extended version of the collective assumption thatincorporates a weight function over instance space as well as a probability function, while theweighted linear threshold MI assumption is based on linear classification models from single-instance learning. Although the two assumptions are quite different in form, and each facilitatesdifferent concept description models and algorithms, it can be shown that the weighted linearthreshold assumption is equivalent to an extended version of the weighted collective assumption interms of the MI concepts that can be represented (Foulds, 2008).14 J . F OULD S AND E . F RANK3.8.1 The weighted collective assumptionWhile under the collective assumption it is assumed that instances contribute equally and inde-pendently to bag-level class labels, the weighted collective assumption asserts that each instancecontributes independently but not necessarily equally to the class label of the bag. This is achievedby incorporating a weight function into the collective assumption:PrðcjbÞ ¼ 1Pnbi¼1 wðxiÞXnbi¼1wðxiÞprðcjxiÞ; ð3Þwhere wðxÞ : w ! Rþ is a weight function from instance space to the positive real numbers (notincluding zero) that determines the level of influence that an instance has on the bag-level class label.The weighted collective assumption, as stated in Equation 3, is a probabilistic model. Some-times, however, a deterministic classifier may be more appropriate. It is also possible to state adeterministic version of the assumption in the standard fashion, where bags are labeled with the‘most likely’ class according to the probability function described in Equation 3. In the case ofbinary classification, this corresponds to:ndwðBÞ3t \u0004 0; t ¼ 1Pnbi¼1 wðxiÞXnbi¼1wðxiÞprðþjxiÞ \u0007 0:5:Here, t is the decision variable, the sign of which determines the classification outcome. This MIassumption is more powerful than the collective assumption because it allows some instances to beignored when determining bag-level class labels. The collective assumption gives all instances in abag the same weight, which means that every instance must be taken into account, and irrelevantinstances may bias the class probability estimates in some problem domains. For instance, thecollective assumption cannot model the standard MI assumption, where only a few (positive)examples affect the class labels of bags. Under the weighted collective assumption, the standardMI assumption can be very closely approximated by giving positive instances a large weight andsetting all other weights to values close to zero.3.8.2 The weighted linear threshold MI assumptionThe weighted linear threshold MI assumption is so named because the accumulated (signed) weights fora bag are compared against a threshold to obtain a classification. A weight function wwltðxÞ : w ! Rþand a classification function cwlt (x) :x-{11,21} are defined over instance space. Instances belongingto the positive class (cwlt (x)511) influence their parent bag toward a positive class label, and instancesbelonging to the negative class (cwlt (x)521) influence their bag toward a negative class label. Theweight of an instance determines the strength of that instance’s influence on bag-level class labels.Formally, let nwlt : Nw ! O ¼ fþ;\u0007g be a weighted linear threshold MI concept. Then nwlt is ofthe formnwltðXÞ3t \u0004 0; t ¼XiwwltðxiÞcwltðxiÞ þ b:Here, b is a bias variable, which determines the location of the decision boundary. This for-mulation of weight-based MI learning is inspired by linear classification in single instance learning.Recall the classification equation for a linear classifier:nðmÞ3t \u0004 0; t ¼Xiwimi þ b:In the weighted linear threshold model, instances are treated analogously to attributes in thecase of linear classification. The class cwlt (x) of an instance corresponds to an attribute value mi.Instance weights wwlt (x) in the MI assumption correspond directly to attribute weights wi in alinear classifier. The bias parameter b performs an identical function to the parameter b in thelinear classification model.It can be shown that the weighted linear threshold assumption is at least as powerful as thedeterministic version of the weighted collective assumption (in terms of the set of representableA review of multi-instance learning assumptions 15concepts). An arbitrary deterministic weighted collective concept can be converted into a weightedlinear threshold concept using the following formula (Foulds, 2008):ndwðBÞ3t \u0004 0; t ¼XiwdwðxiÞcdwðxiÞ þ 0 !;wherecdwðxÞ ¼þ1 prðþjxÞ \u0007 0:50\u00071 otherwise; ; and wdwðxÞ ¼ jwðxÞðprðþjxÞ \u0007 0:5Þj:\u0002The converse is also true with one restriction: any weighted linear threshold concept where b5 0can be represented as a deterministic weighted collective concept. This restriction can be elimi-nated; by introducing a bias parameter bedw into the formulation, the extended deterministicweighted collective assumption becomes equivalent to the weighted linear threshold assumption:nedwðBÞ3t \u0004 0; t ¼ 1Pnbj¼1 wðxjÞXnbi¼1wðxiÞprðþjxiÞ \u0007 bedw:3.8.3 Algorithms and modelsImplementing the weighted collective assumption requires a method for learning instance weights.Foulds (2008) investigated an iterative framework for learning instance weights (IFLIW), a heuristicalgorithm for learning weighted collective assumption concepts. The algorithm is an extension ofthe MI Wrapper approach from Section 3.7.1. IFLIW uses MI Wrapper to learn the classprobability function pr(c|x) via the simple propositionalization method described in Section 3.7.1and a single-instance base learner. The challenge, however, is to estimate the weight function. Aniterative method is applied, where instance weights of the training data are updated according toan update function, and the MI Wrapper model is rebuilt using the new weights. The updatefunction that is used is:x:weight ¼ x:weight\u0003 expðinfogainðprðcjxÞ; prðcÞÞÞ;where x.weight is the weight of instance x, infogain is the information gain of pr (c|x), the classprobability distribution for the instance x predicted by the single-instance base classifier, relativeto pr (c), the prior class probabilities computed from the class frequencies in the training data. Theiteration continues until a stopping criterion is met. The weight function is then estimated using aregression model built on the training instance weights.It is also possible to learn a model based on the weighted linear threshold assumption. TheMILES method from Section 3.4 can be modified so that it learns weighted linear thresholdconcepts when a linear classifier is used as the base learner (Foulds, 2008). This is achieved byusing an alternative similarity measure between a bag and a target point.Recall that the similarity measure s(x,B) used in MILES (Equation 2) includes a max operator,which effectively selects only the closest instance in the bag B when determining the similarityvalue. This is based on Maron’s (1998) most likely cause estimator from the diverse densityframework. The models learnt by MILES (with a linear classifier as the base learner) can beunderstood as being similar to weighted linear threshold concepts, except that the use of the maxoperator in the similarity measure means that instance weights are bag-dependent, as only theclosest instance in the bag to each target point contributes to the bag-level classification. By simplyreplacing the max operator with a sum operator, the bag-dependence is removed, resulting in atrue weight function over instance space. The resulting similarity measure, called yet another radialdistance-based similarity measure (YARDS; Foulds, 2008), is defined as follows:syðx;BÞ ¼Xjexp \u0007 jjBj \u0007 xjj2s2 !:16 J . F OULD S AND E . F RANKHence, by replacing the similarity measure s(x,B) in the MILES algorithm with the YARDSsimilarity measure sy(x,B), MILES can be adapted to learn weighted linear threshold concepts.The YARDS method can represent weighted linear threshold concepts where the weight functionis the sum of a set of Gaussian-like influence functions, and makes the further assumption that thepeak (or trough) of each of the Gaussian-like functions is at the location of an instance from oneof the training bags.3.9 Metadata-based assumptionsA simple approach to MI learning is to perform propositionalization by replacing each bag with afeature vector consisting of metadata features derived in some way from the instances in that bag.A single-instance learning algorithm can then be applied directly to the transformed version of thedataset. At classification time, new bags are mapped into the metadata feature space, and pre-dictions are made by outputting the prediction of the single-instance learner for the transformedversion of the bag. Xu (2003) refers to methods of this kind as metadata approaches.When this type of method is used, the implicit assumption is merely that the classification labelsof the learning examples are directly related to the metadata. We will therefore refer to this type ofMI assumption as a metadata assumption.3.9.1 Algorithms and modelsThe MILES, YARDS, BARTMIP, TLC and CCE algorithms discussed above all use feature-space transformations, where bags are mapped to single-instance feature vectors, and single-instance algorithms are applied to the resulting datasets. These methods can therefore be viewedas metadata approaches. However, the feature spaces used by these methods are intendedto represent more sophisticated MI concepts, and are perhaps better understood with respect tothe underlying MI assumptions that the feature-space transformations are designed to encode.In contrast, we will now describe a method that uses simple summary statistics as metadata.Using this approach, MI learning problems are converted into single-instance problems byreplacing each bag with a feature vector consisting of summary statistics derived from theinstances in that bag. This method originates from a similar approach to propositionalization forrelational data known as relational aggregations (RELAGGS) (Krogel & Wrobel, 2002). We willfollow Dong (2006), and refer to the approach based on summary statistics as simple MI.Dong described three versions of simple MI, each of which differs only in the type of summarystatistics used for the single-instance feature space. The first two methods merely average thevalues of the instances in a bag for each dimension, using either the arithmetic or the geometricmean. Formally, the two methods can be defined as follows: if b is a bag with instances fromfeature space x5 (x1, x2,y, xn), then b is mapped to ð \u0002x1; \u0002x2; . . . ; \u0002xnÞ, where \u0002x is the arithmetic (orgeometric) mean of the instances in the bag.The third option is called the ‘minimax’ method. Here, the minimum and maximum values ofeach variable are recorded for each bag. This method is equivalent to Ga¨rtner et al.’s (2002)minimax kernel, used as a kernel in a standard support vector machine algorithm. Using the samenotation as before, each bag b is mapped to (minx1, min x2,y, min xn, max x1, max x2,y,max xn). The new feature space contains 2n dimensions.The main advantage of simple MI is that it is extremely fast. The computation of the feature-space transformation is trivial, and the single-instance base learner only has to learn from as manyinstances as there are bags in the training set, regardless of how many instances are containedinside the bags. Of course, this simple model is not able to represent some types of problems.However, Dong found that simple MI (with appropriate base learners) performs surprisinglywell on many datasets, even outperforming all of the special-purpose MI algorithms that wereinvestigated in some cases.A more sophisticated metadata approach is used by the multiple-instance learning using classconditional log likelihood ratio (MICCLLR) algorithm (El-Manzalawy & Honavar, 2007), whichA review of multi-instance learning assumptions 17performs propositionalization by replacing each bag by a feature vector containing statistics com-puted based on the class conditional log-likelihood ratios of the attribute values of the instances in thebags. These statistics are computed using the relative frequencies of attribute values and class labels ina flattened version of the MI dataset (using the same method as in the MI Wrapper algorithm fromSection 3.7), under the assumption that attribute values are conditionally independent given the classvalue. As the authors note, Ga¨rtner et al.’s kernel (or equivalently, the minimax simple MI method)may not be able to represent binary data well, unlike MICCLLR; however, the former method doesnot rely upon the conditional independence assumptions used by the latter.MI learning algorithms can also be said to rely on a metadata assumption if they are equivalentto a metadata approach for some feature space transformation, even when the algorithm does notexplicitly perform the transformation. Learning algorithms of this type include the Relic MIdecision tree learner (Ruffo, 2000) and Ga¨rtner et al.’s (2002) KMI MI kernel method. Relic is aninformation gain-based decision tree learner that has been upgraded to handle MI data by defininga test-selection criterion for MI bags. Although Relic does actually not perform propositionali-zation, Xu (2003) showed how, in the case of data with numeric attributes, Relic is equivalent tothe minimax version of simple MI with a decision tree base learner, and hence effectively reliesupon the same MI assumption.Ga¨rtner et al. (2002) presented MI kernels that can be used to apply a standard SVM algorithmdirectly to MI data. As well as the aforementioned minimax kernel, they also proposed the MIkernel KMI, a variant of the set kernel (Ga¨rtner, 2000). The kernel is defined askMIðX ;YÞ ¼Xx2X ; y2YkpI ðx; yÞ;where kpI is an arbitrary instance-level SVM kernel kI, raised to the pth power. As products ofkernels are kernels, kpI is also a kernel. Ga¨rtner et al. showed that for a sufficiently large p, anystandard MI concept is separable (and thus representable by an SVM using that kernel) assumingthat the underlying instance-level concept is separable. It follows from this result that MI conceptsthat respect the standard MI assumption (with separable instance-level concepts) can be learnt bythis method. However, this method does not actually make any use of the standard MIassumption, and can in fact be shown to use a metadata assumption.Using the fact that the dot product is distributive over scalar multiplication, it is not hard toshow that KMI can be rewritten askMI ðX ;YÞ ¼Xx2X ; y2YfI ðxÞ \t fI ðyÞ ¼Xx2XfI ðXÞ !\tXy2YfIðYÞ !;where fI (x) is the feature space transformation implicit in the kernel kpI . Thus, an SVM using theKMI kernel is equivalent to propositionalizing via mapping each bag X toPxAXfI (X), andapplying a standard SVM using a linear kernel to the resulting dataset.Later, Cheung and Kwok (2006) proposed a regularization framework for MI learning viaSVMs using a loss function that encodes a trade-off between Ga¨rtner et al.’s KMI model and anSVM algorithm based on the standard MI assumption that is due to Andrews et al. (2002). Thetrade-off is accomplished via a weight parameter l in the loss function. The implicit assumption ofthis method is that bag-level class labels are determined by some combination of the KMI metadataassumption and the standard MI assumption.3.10 The MI graph assumptionZhou et al. (2009) proposed algorithms that depend upon the assumption that the spatial rela-tionships between instances in bags are important contributors to bag labels. Consider the e-graphof a bag, which has nodes for each instance, and edges exist between nodes if and only if thedistance between their associated instances (under some metric) is less than a fixed threshold e. Theedges are weighted according to the affinity of the two nodes—Zhou et al. set the weights to bethe normalized reciprocal of the (non-zero) distance between them. The assumption, which we will18 J . F OULD S AND E . F RANKcall the MI graph assumption, is that bag labels are in some way determined by the properties ofthe e-graph.3.10.1 Algorithms and modelsThe MIGraph and miGraph algorithms (Zhou et al., 2009) apply support vector machines to MIdata by using graph kernels on the e-graphs of the bags. Although any graph kernel could be used,Zhou et al. define two new kernels based on Ga¨rtner et al.’s MI kernel. The MIGraph andmiGraph methods differ only in the kernels used. MIGraph uses the kernel kG, defined askGðX ;YÞ ¼Xx2X ; y2Yknodeðx; yÞ þXex2EðXÞ;ey2EðYÞkedgeðex; eyÞ;where E(I) is the edge set of bag I, and knode and kedge are positive semidefinite kernels defined onnodes and edges, respectively. Zhou et al. use the Gaussian radial basis function (RBF) kernel forknode. For the kedge kernel, they define a kernel with the property that edges are similar if theirending nodes have similar degree, taking the edge weights into account. Note that the nodeportion of the kernel kG is the same as Ga¨rtner et al.’s MI kernel.As the computational complexity of kG is dominated by the number of edges in X and Y if thegraphs are not sparse, it can be computationally expensive to compute the kernel function. Tocounter this, Zhou et al. introduce the miGraph algorithm, where the kg kernel is used:kgðX ;YÞ ¼Px2X ; y2Y WXxWYyknodeðx; yÞPx2X WXxPy2Y WYy;where WIi is the reciprocal of the number of instances from bag I in an e-ball around instance i(including itself). In (Zhou et al., 2009), the WIis are computed using the Gaussian distance,consistently with the Gaussian RBF kernel used for knode. The authors describe kg as a soft versionof a clique-based graph kernel; it behaves identically to a clique-based kernel when all instancesare clustered into cliques.3.11 Nearest neighbor assumptionsIn traditional single-instance learning, the k-nearest neighbor algorithm is a simple classificationmethod, where examples are labeled according to the majority class of the k-closest trainingexamples. Here, ‘closest’ is easily defined using a distance metric such as the Euclidean distance.In MI learning, it is not as immediately obvious how distances between bags should be com-puted. Wang and Zucker (2000) used the maximal and minimal Hausdorff distance for this pur-pose (see Section 3.5 for more information on this distance). Zhou et al. (2009) note that the graphedit distance (Neuhaus & Bunke, 2007), as computed on the e-graphs of the bags, could be used asa metric for k-nearest neighbors.In nearest-neighbor approaches a specific kind of relationship between bags and class labels is notdirectly assumed. Instead, the implicit assumption is that bags that are ‘similar’ according to thedistance measure used are likely to have the same class label. This is closely related to the BARTMIPassumption (Section 3.5). Note that there is no clear relationship between the nearest neighborassumption (at least when using variants of the Hausdorff distance) and the standard MI assumption.3.11.1 Algorithms and modelsWang and Zucker proposed two variants of the standard k-nearest neighbor algorithm. In thesemethods, neighbors are computed in the normal way via the (maximal or minimal) Hausdorffdistance; the difference is in the method for selecting the label of an example given a set ofneighbors.These methods were motivated by the authors’ observation that predicting the majority class ofthe neighbors does not always give the optimal classification result. Their Bayesian-K-nearestneighbor algorithm uses a Bayesian method for predicting the most likely class given a set ofA review of multi-instance learning assumptions 19neighbors, while the Citation-KNN algorithm is based on the notions of references and citers fromthe field of library and information science—when making a classification decision, not only arenearest neighbors (references) of an example considered, but also bags that consider the exampleto be a nearest neighbor (citers).The experimental results presented by Wang and Zucker indicate that these methods are verycompetitive with other algorithms on the musk benchmark datasets. However, a comparison withthe standard k-NN majority voting method is not provided. It should also be noted that theBayesian and Citation alternatives to majority voting are not at all dependent on the MI natureof the data, and are hence equally applicable in a single-instance scenario. Finally, Wang andZucker used parameter values selected on the test data when comparing their methods to otheralgorithms, so their comparative results may be optimistic.4 MI learning in other supervised settingsAlthough the majority of the research on MI learning has been devoted to classification problems,some work has been done on other supervised MI learning scenarios. The most notable of theseare MI multi-label learning (Zhou & Zhang, 2006) and MI regression (Ray & Page, 2001; Amaret al., 2001). Similarly to MI classification, any learning approach in these scenarios must dependupon an implicit assumption regarding the nature of the relationship between instances and baglabels; we therefore discuss these assumptions in this section.Other interesting learning scenarios using MI representations include MI clustering (Zhang &Zhou, 2009; Kriegel et al., 2006), learning instance-level classifiers from bags labeled with apercentage of positive instances (Ku¨ck & de Freitas, 2005), and predicting the salience of instancesin an MI regression setting (Wagstaff & Lane, 2007). None of these scenarios involves bag-levelpredictions, however, so we do not consider them in this paper.4.1 MI multi-label learningIn traditional supervised learning, multi-class learning problems contain more than two classifi-cation categories, but each learning example belongs to exactly one of these categories. Anextension to this is multi-label learning, where the categories are not mutually exclusive, so thateach example may belong to several class categories (Schapire & Singer, 2000).Zhou and Zhang (2006) formalized MI multi-label learning (MIML), where each MI bag maybe associated with multiple class labels. In their formulation, the task in MIML is to learn afunction of the form fMIML : 2x-2Y, where x is the instance space, and Y is the set of classcategories. Given that MI examples are really bags (multi-sets) rather than sets, we modify thisdefinition to be fMIML : Nw ! 2Y (see Section 2.3.1 for more information on this notation).As Zhou and Zhang observe, MI learning and multi-label learning are both natural general-izations of traditional single-instance learning, and MIML is a generalization of both of these. InMIML, it is clear that the standard MI assumption is not directly applicable, as that assumption isdependent on the learning task being a binary classification problem. Other assumptions regardingthe relationships between the instances and the bag-level labels are required for MIML.Zhou and Zhang proposed two solution frameworks for applying single-instance learners tosolve MIML problems. Although they did not discuss the types of concepts that these frameworksare appropriate for, we will attempt to unify them under a more general algorithm template, andthus expose the MIML assumptions used by both methods.Before discussing these solution frameworks, it is instructive to first consider a method forconverting multi-label problems to single-label problems, used by both of Zhou and Zhang’sMIML approaches. The method is referred to by Tsoumakas and Katakis (2007) as problemtransformation method PT4. In this method, the multi-label problem is converted into a set ofbinary classification problems—one for each of the labels. For each label, a dataset is createdwhere the multi-label training examples that are associated with that label are tagged as positive in20 J . F OULD S AND E . F RANKthe new dataset, and are otherwise tagged as negative. Multi-label predictions are made bybuilding a single-label classifier on each of the new datasets, and outputting the union of thepositive predictions made by these classifiers.The first solution framework proposed by Zhou and Zhang (Solution 1) is to use MI learning asa bridge between MIML and single-instance learning. The MIML problem is converted to a set ofMI problems using PT4. Zhou and Zhang were interested specifically in methods that use tra-ditional single-instance algorithms to solve MIML problems, and hence their formulation ofSolution 1 insists on the use of an MI method that can be solved using a single-instance algorithm.Thus an MI method that applies a single-instance algorithm, such as MI Boosting (Xu & Frank,2004), is then applied to the resulting MI problems.However, it is clear that any arbitrary MI learning algorithm could in fact be applied. We willrefer to this relaxed version of the Solution 1 framework as MIML_PT4. Here, the assumption isthat the MI concept corresponding to each label can be learned under the assumption used by theMI base learner. For example, when the MI base learner is Xu and Frank’s (2004) MI boostingalgorithm, the implicit assumption is that the concept associated with each of the labels is acollective assumption MI concept. We shall call this general MIML assumption the MIML_PT4assumption.Zhou and Zhang’s other solution framework (Solution 2) uses multi-label learning as the bridgebetween MIML and traditional single-instance learning. First, a propositionalization method isused to map the MI bags into a single-instance feature space, retaining the multiple labels,resulting in a single-instance multi-label dataset. This new learning problem is transformed into aset of traditional single-instance single-label datasets by applying PT4, and hence is solved bybuilding single-instance models on the resulting datasets.However, if we view the propositionalization step in Solution 2 as the application of a ‘wrapper’-type MI algorithm, Solution 2 can in fact also considered to be within the MIML_PT4 framework.This is because the order of the transformations is not important—the result is the same whether weapply PT4 and then use the wrapper method to propositionalize the data (MIML_PT4), or propo-sitionalize first and then apply PT4 (Solution 2). Hence, the MIML_PT4 assumption applies toSolution 2 algorithms—it is assumed that the MI assumption used by the propositionalizationalgorithm applies to each of the MI concepts associated with the MIML labels.4.1.1 Algorithms and modelsZhou and Zhang’s (2006) MIMLBOOST algorithm uses MI Boosting (Xu & Frank, 2004) as the MI baselearner for Solution 1, while their MIMLSVM algorithm uses the constructive clustering propositiona-lization method (Zhou & Zhang, 2007) and an SVM base learner to implement Solution 2.4.2 MI regressionAt the International Conference on Machine Learning in 2001, Ray and Page (2001) and Amaret al. (2001)5 independently formulated multiple instance regression, where bags are associatedwith real-valued labels instead of the usual binary class labels. The task is again to predict theselabels. Similarly to MI classification, MI regression is motivated by the drug activity predictionproblem. The authors of both papers observe that many drug developers prefer predictions ofactivity levels of drugs, instead of active/inactive classification predictions. Application areasidentified by later authors include aerosol optical depth prediction for climate research (Wanget al., 2008) and crop yield modeling (Wagstaff & Lane, 2007).Ray and Page assume that the data is generated by a linear model with Gaussian noise on thereal-valued labels. Critically, they further assume that for each bag, there is one instance (referredto as the primary instance) that is responsible for the label. Similarly to the standard MIassumption in classification problems, this further assumption is useful for modeling ambiguity,5 See also the later journal article (Dooly et al., 2002).A review of multi-instance learning assumptions 21where the instances in a bag represent different views or different states of an object, and it isunknown which of the instances is responsible for the class label. We thus refer to it as the standardMI regression assumption.Amar et al. proposed the direct application of the Citation-KNN algorithm (Wang & Zucker, 2000)and traditional k-NN (using the minimal Hausdorff distance) to data with real-valued labels. Thesemethods depend on the same assumption as the nearest neighbor MI classification methods; namelythat bags that are similar according to the bag-level distance measure will have similar labels.The later MI regression algorithms proposed by Wang et al. (2008) use the assumption thateach bag is generated by some random noise around a point in instance space, which they refer toas a prime instance.6 Bag labels are assumed to be generated from the prime instances via somefunction (with added noise).Zhang and Zhou (2009) observed that their BARTMIP algorithm (see Section 3.5), which mapsbags into a single-instance feature space, can be directly applied to MI regression when a single-instance regression base learner issued—the method works the same way regardless of whetherlabels are discrete or real-valued. This method could in fact be applied to any other metadataalgorithm, such as MILES or simple MI. Under this approach, the metadata assumption used bythe corresponding MI classification algorithm is applied in the regression setting.4.2.1 Algorithms and modelsUnder Ray and Page’s assumptions, an ideal MI regression model is a hyperplane Y5Xb such thatb ¼ argminbXni¼1Lðyi;Xip; bÞ;where n is the number of bags, yi is the real-valued label of bag i, Xip is the primary instance of the ithbag, and L is a loss function. Ray and Page use L(yi,Xij, b)5 (yi2Xij,b)2, similarly to traditionalmultiple regression. However, the primary instances Xip are not known at training time, so Ray andPage propose that the ‘best fit’ hyperplane be used instead:b ¼ argminbXni¼1minjLðyi;Xij ; bÞ; 1 \u0005 j \u0005 jXij:They state that the decision problem for the existence of such a hyperplane can be shown to beNP-complete via a reduction from the 3SAT problem. They therefore instead present anapproximation algorithm. Their algorithm is an EM approach, which iteratively improves aninitial guess at a hypothesis. In the expectation step, an instance is selected from each bag, namelythe one that has the least L-error with respect to the current guess at the hypothesis hyperplane. Inthe maximization step, ordinary multiple regression is performed to find a hyperplane that best fitsthe selected instances. These steps are repeated until convergence. As Ray and Page observe, thisalgorithm can easily be modified to incorporate alternative L-error functions and alternative(possibly non-linear) hypotheses. The algorithm is, however, dependent upon the standard MIregression assumption.Cheung and Kwok (2006) presented a support vector regression approach for MI regressionunder the standard MI regression assumption. To make computation feasible, their method relieson the simplifying assumption that the primary instance is the one with the highest output valueaccording to the SVM.Amar et al. (2001) adapted the diverse density algorithm (see Section 2.4) to MI regression databy using a real-valued version of Maron’s (1998) most likely cause model, and Zhang andGoldman (2001) used a similar method to apply EM-DD (see also Section 2.4) to real-valued data.Here, the assumption is that the closest instance to a certain ‘target point’ is responsible for a bag’slabel, which is compatible with the standard MI regression assumption. Bag labels are determined6 Not to be confused with Ray and Page’s (2001) primary instances, which are elements of a bag and are notassumed to ‘cause’ the other instances.22 J . F OULD S AND E . F RANKfrom the distance between the target point and the closest point in the bag by a Gaussian functionwith a peak at the target point.Wang et al. (2008) proposed MI regression algorithms that are similar to the simple MI and MIWrapper algorithms described in Sections 3.9 and 3.7, respectively. The Global Pruning-multipleinstance regression (MIR) and Balanced Pruning-MIR algorithms are expectation-maximizationversions of an MI Wrapper-like approach that discards those instances that are most likely to benoisy in each iteration. It should be noted that these methods are very similar to the IFLIWalgorithm from Section 3.8.3, except that sampling is used instead of weighting in the formermethods, in order to remove the effects of noisy or unimportant instances.5 ConclusionsThe MI representation is more expressive than the traditional feature-vector model, and is anatural way to describe learning examples in a diverse array of real-world scenarios. Learningfrom sets of MI examples is a difficult problem because there are many ways that instances caninteract with bag-level class labels, and consequently the hypothesis space is very large. Thisdifficulty can (and must) be mitigated by assuming that MI concepts are of some specific form.The standard MI assumption, namely that bags have a positive class label if and only if at leastone instance in the bag is positive, is widely believed to be applicable to drug activity predictionproblems such as identifying molecules that emit a musky odor. While this assumption is also agood heuristic for some other problem domains, this does not imply that it will always hold, thusalternative assumptions can be required.We have reviewed MI assumptions from the literature that have been used for the supervisedlearning scenarios of classification, regression and multi-label learning with MI data. We havediscussed alternatives to the standard MI assumption that have been explicitly introduced by theauthors, and also attempted to clarify MI assumptions implicitly used by algorithms whereauthors do not state them explicitly. We have found that some of the most popular and widelycited MI approaches actually disregard the standard MI assumption. This indicates that thisassumption may not be as crucial for the musk problem as was initially hypothesized by Dietterichet al. (1997)—several approaches that depart from the standard MI assumption have been shownempirically to be very competitive on the musk data.The expressivity of the MI representation is a strong motivation for continued work in thisarea. As MI learning continues to be applied to a wider selection of practical machine learningproblems, the use of appropriate MI assumptions for the problems at hand becomes increasinglyimportant. We therefore anticipate that future research into algorithms and assumptions for therelaxed MI learning problem will prove fruitful.We do, however, believe that it is important to explicitly state assumptions used by an algo-rithm whenever the standard MI assumption is not used. Researchers and practitioners need to beaware that different MI problem domains may require different MI assumptions, and the standardMI assumption is not always applicable. It is important to verify that the MI assumption used byan algorithm is at least plausible for the problem at hand.A task that is of particular interest is to find more effective and generally applicable algorithmsfor learning visual concepts, such as for image classification and content-based image retrieval.The MI representation allows for concept descriptions that are defined upon the interaction ofinstance-level concepts, which is a very natural way to describe visual concepts. Different visualconcepts are likely to require vastly different interactions between instances and bag-level classlabels—for example, the banana concept is likely to be very different to the beach concept—sothere is significant scope for work on alternative MI assumptions in this domain.ReferencesAmar, R., Dooly, D., Goldman, S. & Zhang, Q. 2001. Multiple-instance learning of real-valued data.Proceedings of the 18th International Conference on Machine Learning, 3–10. ACM.A review of multi-instance learning assumptions 23Andrews, S., Tsochantaridis, I. & Hofmann, T. 2002. Support vector machines for multiple-instancelearning. In Proceedings of the 16th Conference on Neural Information Processing Systems (Advances inNeural Information Processing Systems 15) 561–568. MIT Press.Auer, P. & Ortner, R. 2004. A boosting approach to multiple instance learning. In Proceedings of the 15thEuropean Conference on Machine Learning, 63–74. Springer.Blockeel, H., Page, D. & Srinivasan, A. 2005. Multi-instance tree learning. In Proceedings of the 22ndInternational Conference on Machine Learning, 57–64. ACM.Burl, M. C., Weber, M. & Perona, P. 1998. A probabilistic approach to object recognition using localphotometry and global geometry. In Proceedings of the 5th European Conference on Computer Vision,628–641. Springer.Chen, Y., Bi, J. & Wang, J. Z. 2006. MILES: Multiple-instance learning via embedded instance selection.IEEE Transactions on Pattern Analysis and Machine Intelligence 28(12), 1931–1947.Chen, Y. & Wang, J. Z. 2004. Image categorization by learning and reasoning with regions. Journal ofMachine Learning Research 5, 913–939.Cheung, P. & Kwok, J. 2006. A regularization framework for multiple-instance learning. In Proceedings of the23rd International Conference on Machine Learning, 193–200. ACM.Chevaleyre, Y. & Zucker, J.-D. 2001. Solving multiple-instance and multiple-part learning problems withdecision trees and rule sets. Application to the mutagenesis problem. In Proceedings of the 14th BiennialConference of the Canadian Society for Computational Studies of Intelligence, 204–214. Springer.Dietterich, T. G., Lathrop, R. H. & Lozano-Pe´rez, T. 1997. Solving the multiple instance problem with axis-parallel rectangles. Artificial Intelligence 89(1–2), 31–71.Dong, L. 2006. A comparison of multi-instance learning algorithms. Master’s thesis, University of Waikato.Dooly, D., Zhang, Q., Goldman, S. & Amar, R. 2002. Multiple-instance learning of real-valued data. Journalof Machine Learning Research 3, 651–678.Edgar, G. A. 1990. Measure, Topology, and Fractal Geometry, 2nd edn. Undergraduate Texts inMathematics. Springer.El-Manzalawy, Y. & Honavar, V. 2007. MICCLLR: A generalized multiple-instance learning algorithmusing class conditional log likelihood ratio. Technical report, Computer Science Department, Iowa StateUniversity.Foulds, J. 2008. Learning Instance Weights in Multi-Instance Learning. Master’s thesis, University ofWaikato.Frank, E. & Xu, X. 2003. Applying propositional learning algorithms to multi-instance data. Technical report06/03, Department of Computer Science, University of Waikato.Ga¨rtner, T. 2000. Kernel-based Feature Space Transformation in Inductive Logic Programming. Master’sthesis, University of Bristol.Ga¨rtner, T., Flach, P. A., Kowalczyk, A. & Smola, A. 2002. Multi-instance kernels. In Proceedings of the 19thInternational Conference on Machine Learning, 179–186. Morgan Kaufmann.Kriegel, H., Pryakhin, A. & Schubert, M. 2006. An EM-approach for clustering multi-instance objects. InProceedings of the 10th Pacific-Asia Conference on Knowledge Discovery and Data Mining, 139–148.Springer.Krogel, M.-A. & Wrobel, S. 2002. Feature selection for propositionalization. In Proceedings of the 5thInternational Conference on Discovery Science, 430–434. Springer.Ku¨ck, H. & de Freitas, N. 2005. Learning about individuals from group statistics. In Proceedings of the 21thAnnual Conference on Uncertainty in Artificial Intelligence, 332–339. AUAI Press.Littlestone, N. 1987. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm.Machine Learning 2(4), 285–318.Maron, O. 1998. Learning from ambiguity. Ph.D. thesis, Massachusetts Institute of Technology.Maron, O. & Lozano-Pe´rez, T. 1997. A framework for multiple-instance learning. In Proceedings of the 11thConference on Neural Information Processing Systems, 570–576. MIT Press.Maron, O. & Ratan, A. L. 1998. Multiple-instance learning for natural scene classification. In Proceedings ofthe 15th International Conference on Machine Learning, 341–349. Morgan Kaufmann.Neuhaus, M. & Bunke, H. 2007. A quadratic programming approach to the graph edit distance problem. InProceedings of the 6th IAPR-TC-15 International Workshop on Graph Based Representations in PatternRecognition, 92–102. Springer.Qi, G.-J., Hua, X.-S., Rui, Y., Mei, T., Tang, J. & Zhang, H.-J. 2007. Concurrent multiple instance learningfor image categorization. In Proceeding of the IEEE Conference on Computer Vision and PatternRecognition, 1–8. IEEE Computer Society.Ramon, J. & De Raedt, L. 2000. Multi instance neural networks. In Proceedings of the InternationalConference on Machine Learning 2000 Workshop on Attribute-Value and Relational Learning.Ray, S. & Craven, M. 2005. Supervised learning versus multiple instance learning: an empirical comparison.In Proceedings of the 22nd International Conference on Machine Learning, 697–704. ACM.24 J . F OULD S AND E . F RANKRay, S. & Page, D. 2001. Multiple instance regression. In Proceedings of the 18th International Conference onMachine Learning, 425–432. Morgan Kaufmann.Ruffo, G. 2000. Learning single and multiple instance decision trees for computer security applications. PhDthesis, Universida di Torino, Italy.Schapire, R. E. & Singer, Y. 2000. BoosTexter: A boosting-based system for text categorization. MachineLearning 39(2/3), 135–168.Scott, S., Zhang, J. & Brown, J. 2005. On generalized multiple-instance learning. International Journal ofComputational Intelligence and Applications 5(1), 21–35.Tao, Q. & Scott, S. 2004. A faster algorithm for generalized multiple-instance learning. In Proceedings of the17th International Florida Artificial Intelligence Research Society Conference, 550–555. AAAI Press.Tao, Q., Scott, S., Vinodchandran, N. V., Osugi, T. & Mueller, B. 2004a. An extended kernel for generalizedmultiple-instance learning. In Proceedings of the 16th IEEE International Conference on Tools withArtificial Intelligence, 272–277. IEEE Computer Society.Tao, Q., Scott, S., Vinodchandran, N. & Osugi, T. T. 2004b. SVM-based generalized multiple-instancelearning via approximate box counting. In Proceedings of the 21st International Conference on MachineLearning, 779–806. ACM.Tsoumakas, G. & Katakis, I. 2007. Multi-Label classification: An overview. International Journal of DataWarehousing and Mining 3(3), 1–13.Wagstaff, K. & Lane, T. 2007. Salience assignment for multiple-instance regression. In Proceedings of theInternational Conference on Machine Learning 2007 Workshop on Constrained Optimization and StructuredOutput Spaces.Wang, J. & Zucker, J.-D. 2000. Solving the multiple-instance problem: A lazy learning approach. InProceedings of the 17th International Conference on Machine Learning, 1119–1125. Morgan Kaufmann.Wang, Z., Radosavljevic, V., Han, B. & Obradovic, Z. 2008. Aerosol optical depth prediction from satelliteobservations by multiple instance regression. In Proceedings of the SIAM International Conference on DataMining, 165–176. SIAM.Weidmann, N. 2003. Two-level classification for generalized multi-instance data. Master’s thesis, AlbertLudwigs University of Freiburg.Weidmann, N., Frank, E. & Pfahringer, B. 2003. A two-level learning method for generalized multi-instanceproblems. In Proceedings of the 14th European Conference on Machine Learning, 468–479. Springer.Xu, X. 2003. Statistical Learning in Multiple Instance Problems. Master’s thesis, University of Waikato.Xu, X. & Frank, E. 2004. Logistic regression and boosting for labeled bags of instances. In Proceedings of the8th Pacific-Asia Conference on Knowledge Discovery and Data Mining, 272–281. Springer.Zhang, M-L. & Zhou, Z.-H. 2009. Multi-instance clustering with applications to multi-instance prediction.Applied Intelligence 31(1), 47–68.Zhang, Q. & Goldman, S. 2001. EM-DD: An improved multiple-instance learning technique. In Proceedingsof the 15th Conference on Neural Information Processing Systems, 1073–1080. MIT Press.Zhang, Q., Yu, W., Goldman, S. & Fritts, J. 2002. Content-based image retrieval using multiple-instancelearning. In Proceedings of the 19th International Conference on Machine Learning, 682–689. MorganKaufmann.Zhou, Z.-H., Sun, Y.-Y. & Li, Y.-F. 2009. Multi-instance learning by treating instances as non-I.I.D.samples. In Proceedings of the 26th International Conference on Machine Learning, 1249–1256. ACM.Zhou, Z.-H. & Xu, J.-M. 2007. On the relation between multi-instance learning and semi-supervised learning.In Proceedings of the 24th International Conference on Machine learning, 1167–1174. ACM.Zhou, Z.-H. & Zhang, M.-L. 2006. Multi-instance multi-label learning with application to scene classifica-tion. In Proceedings of the 20th Annual Conference on Neural Information Processing Systems, 1609–1616.MIT Press.Zhou, Z.-H. & Zhang, M.-L. 2007. Solving multi-instance problems with classifier ensemble based on con-structive clustering. Knowledge and Information Systems 11(2), 155–170.A review of multi-instance learning assumptions 25",
            "id": 15107808,
            "identifiers": [
                {
                    "identifier": "10.1017/s026988890999035x",
                    "type": "DOI"
                },
                {
                    "identifier": "29197595",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:researchcommons.waikato.ac.nz:10289/3870",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "58831609",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "2125479168",
                    "type": "MAG_ID"
                }
            ],
            "title": "A review of multi-instance learning assumptions",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "2125479168",
            "oaiIds": [
                "oai:researchcommons.waikato.ac.nz:10289/3870"
            ],
            "publishedDate": "2010-01-01T00:00:00",
            "publisher": "'Cambridge University Press (CUP)'",
            "pubmedId": null,
            "references": [
                {
                    "id": 37873034,
                    "title": "A boosting approach to multiple instance learning.",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1007/978-3-540-30115-8_9",
                    "raw": "Auer, P. & Ortner, R. 2004. A boosting approach to multiple instance learning. In Proceedings of the 15th European Conference on Machine Learning, 63–74. Springer.",
                    "cites": null
                },
                {
                    "id": 37873042,
                    "title": "A comparison of multi-instance learning algorithms. Master’s thesis,",
                    "authors": [],
                    "date": "2006",
                    "doi": null,
                    "raw": "Dong, L. 2006. A comparison of multi-instance learning algorithms. Master’s thesis, University of Waikato.",
                    "cites": null
                },
                {
                    "id": 37873068,
                    "title": "A faster algorithm for generalized multiple-instance learning.",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1109/ictai.2004.29",
                    "raw": "Tao, Q. & Scott, S. 2004. A faster algorithm for generalized multiple-instance learning. In Proceedings of the 17th International Florida Artificial Intelligence Research Society Conference, 550–555. AAAI Press.",
                    "cites": null
                },
                {
                    "id": 37873058,
                    "title": "A framework for multiple-instance learning.",
                    "authors": [],
                    "date": "1997",
                    "doi": null,
                    "raw": "Maron, O. & Lozano-Pe ´ rez, T. 1997. A framework for multiple-instance learning. In Proceedings of the 11th Conference on Neural Information Processing Systems, 570–576. MIT Press.",
                    "cites": null
                },
                {
                    "id": 37873036,
                    "title": "A probabilistic approach to object recognition using local photometry and global geometry.",
                    "authors": [],
                    "date": "1998",
                    "doi": "10.1007/bfb0054769",
                    "raw": "Burl, M. C., Weber, M. & Perona, P. 1998. A probabilistic approach to object recognition using local photometry and global geometry. In Proceedings of the 5th European Conference on Computer Vision, 628–641. Springer.",
                    "cites": null
                },
                {
                    "id": 37873060,
                    "title": "A quadratic programming approach to the graph edit distance problem.",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1007/978-3-540-72903-7_9",
                    "raw": "Neuhaus, M. & Bunke, H. 2007. A quadratic programming approach to the graph edit distance problem. In Proceedings of the 6th IAPR-TC-15 International Workshop on Graph Based Representations in Pattern Recognition, 92–102. Springer.",
                    "cites": null
                },
                {
                    "id": 37873039,
                    "title": "A regularization framework for multiple-instance learning.",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1145/1143844.1143869",
                    "raw": "Cheung, P. & Kwok, J. 2006. A regularization framework for multiple-instance learning. In Proceedings of the 23rd International Conference on Machine Learning, 193–200. ACM.",
                    "cites": null
                },
                {
                    "id": 37873033,
                    "title": "A review of multi-instance learning assumptions 23Andrews,",
                    "authors": [],
                    "date": "2002",
                    "doi": null,
                    "raw": "A review of multi-instance learning assumptions 23Andrews, S., Tsochantaridis, I. & Hofmann, T. 2002. Support vector machines for multiple-instance learning. In Proceedings of the 16th Conference on Neural Information Processing Systems (Advances in Neural Information Processing Systems 15) 561–568. MIT Press.",
                    "cites": null
                },
                {
                    "id": 37873076,
                    "title": "A two-level learning method for generalized multi-instance problems.",
                    "authors": [],
                    "date": "2003",
                    "doi": "10.1007/978-3-540-39857-8_42",
                    "raw": "Weidmann, N., Frank, E. & Pfahringer, B. 2003. A two-level learning method for generalized multi-instance problems. In Proceedings of the 14th European Conference on Machine Learning, 468–479. Springer.",
                    "cites": null
                },
                {
                    "id": 37873074,
                    "title": "Aerosol optical depth prediction from satellite observations by multiple instance regression.",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1137/1.9781611972788.15",
                    "raw": "Wang, Z., Radosavljevic, V., Han, B. & Obradovic, Z. 2008. Aerosol optical depth prediction from satellite observations by multiple instance regression. In Proceedings of the SIAM International Conference on Data Mining, 165–176. SIAM.",
                    "cites": null
                },
                {
                    "id": 37873051,
                    "title": "An EM-approach for clustering multi-instance objects.",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1007/11731139_18",
                    "raw": "Kriegel, H., Pryakhin, A. & Schubert, M. 2006. An EM-approach for clustering multi-instance objects. In Proceedings of the 10th Pacific-Asia Conference on Knowledge Discovery and Data Mining, 139–148. Springer.",
                    "cites": null
                },
                {
                    "id": 37873069,
                    "title": "An extended kernel for generalized multiple-instance learning.",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1109/ictai.2004.29",
                    "raw": "Tao, Q., Scott, S., Vinodchandran, N. V., Osugi, T. & Mueller, B. 2004a. An extended kernel for generalized multiple-instance learning. In Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence, 272–277. IEEE Computer Society.",
                    "cites": null
                },
                {
                    "id": 37873048,
                    "title": "Applying propositional learning algorithms to multi-instance data.",
                    "authors": [],
                    "date": "2003",
                    "doi": null,
                    "raw": "Frank, E. & Xu, X. 2003. Applying propositional learning algorithms to multi-instance data. Technical report 06/03, Department of Computer Science, University of Waikato.",
                    "cites": null
                },
                {
                    "id": 37873066,
                    "title": "BoosTexter: A boosting-based system for text categorization.",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "Schapire, R. E. & Singer, Y. 2000. BoosTexter: A boosting-based system for text categorization. Machine Learning 39(2/3), 135–168.",
                    "cites": null
                },
                {
                    "id": 37873061,
                    "title": "Concurrent multiple instance learning for image categorization.",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1109/cvpr.2007.383152",
                    "raw": "Qi, G.-J., Hua, X.-S., Rui, Y., Mei, T., Tang, J. & Zhang, H.-J. 2007. Concurrent multiple instance learning for image categorization. In Proceeding of the IEEE Conference on Computer Vision and Pattern Recognition, 1–8. IEEE Computer Society.",
                    "cites": null
                },
                {
                    "id": 37873081,
                    "title": "Content-based image retrieval using multiple-instance learning.",
                    "authors": [],
                    "date": "2002",
                    "doi": null,
                    "raw": "Zhang, Q., Yu, W., Goldman, S. & Fritts, J. 2002. Content-based image retrieval using multiple-instance learning. In Proceedings of the 19th International Conference on Machine Learning, 682–689. Morgan Kaufmann.",
                    "cites": null
                },
                {
                    "id": 37873080,
                    "title": "EM-DD: An improved multiple-instance learning technique.",
                    "authors": [],
                    "date": "2001",
                    "doi": null,
                    "raw": "Zhang, Q. & Goldman, S. 2001. EM-DD: An improved multiple-instance learning technique. In Proceedings of the 15th Conference on Neural Information Processing Systems, 1073–1080. MIT Press.",
                    "cites": null
                },
                {
                    "id": 37873052,
                    "title": "Feature selection for propositionalization.",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1007/3-540-36182-0_45",
                    "raw": "Krogel, M.-A. & Wrobel, S. 2002. Feature selection for propositionalization. In Proceedings of the 5th International Conference on Discovery Science, 430–434. Springer.",
                    "cites": null
                },
                {
                    "id": 37873038,
                    "title": "Image categorization by learning and reasoning with regions.",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1007/1-4020-8035-2_6",
                    "raw": "Chen, Y. & Wang, J. Z. 2004. Image categorization by learning and reasoning with regions. Journal of Machine Learning Research 5, 913–939.",
                    "cites": null
                },
                {
                    "id": 37873049,
                    "title": "Kernel-based Feature Space Transformation in Inductive Logic Programming. Master’s thesis,",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "Ga ¨ rtner, T. 2000. Kernel-based Feature Space Transformation in Inductive Logic Programming. Master’s thesis, University of Bristol.",
                    "cites": null
                },
                {
                    "id": 37873054,
                    "title": "Learning about individuals from group statistics.",
                    "authors": [],
                    "date": "2005",
                    "doi": null,
                    "raw": "Ku ¨ ck, H. & de Freitas, N. 2005. Learning about individuals from group statistics. In Proceedings of the 21th Annual Conference on Uncertainty in Artificial Intelligence, 332–339. AUAI Press.",
                    "cites": null
                },
                {
                    "id": 37873057,
                    "title": "Learning from ambiguity.",
                    "authors": [],
                    "date": "1998",
                    "doi": null,
                    "raw": "Maron, O. 1998. Learning from ambiguity. Ph.D. thesis, Massachusetts Institute of Technology.",
                    "cites": null
                },
                {
                    "id": 37873047,
                    "title": "Learning Instance Weights in Multi-Instance Learning. Master’s thesis,",
                    "authors": [],
                    "date": "2008",
                    "doi": "10.1007/springerreference_179281",
                    "raw": "Foulds, J. 2008. Learning Instance Weights in Multi-Instance Learning. Master’s thesis, University of Waikato.",
                    "cites": null
                },
                {
                    "id": 37873055,
                    "title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm.",
                    "authors": [],
                    "date": "1987",
                    "doi": "10.1109/sfcs.1987.37",
                    "raw": "Littlestone, N. 1987. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning 2(4), 285–318.",
                    "cites": null
                },
                {
                    "id": 37873065,
                    "title": "Learning single and multiple instance decision trees for computer security applications.",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "Ruffo, G. 2000. Learning single and multiple instance decision trees for computer security applications. PhD thesis, Universida di Torino, Italy.",
                    "cites": null
                },
                {
                    "id": 37873078,
                    "title": "Logistic regression and boosting for labeled bags of instances.",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1007/978-3-540-24775-3_35",
                    "raw": "Xu, X. & Frank, E. 2004. Logistic regression and boosting for labeled bags of instances. In Proceedings of the 8th Pacific-Asia Conference on Knowledge Discovery and Data Mining, 272–281. Springer.",
                    "cites": null
                },
                {
                    "id": 37873045,
                    "title": "Measure, Topology, and Fractal Geometry, 2nd edn. Undergraduate Texts in Mathematics.",
                    "authors": [],
                    "date": "1990",
                    "doi": "10.1007/978-1-4757-4134-6",
                    "raw": "Edgar, G. A. 1990. Measure, Topology, and Fractal Geometry, 2nd edn. Undergraduate Texts in Mathematics. Springer.",
                    "cites": null
                },
                {
                    "id": 37873046,
                    "title": "MICCLLR: A generalized multiple-instance learning algorithm using class conditional log likelihood ratio.",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1007/978-3-642-04747-3_9",
                    "raw": "El-Manzalawy, Y. & Honavar, V. 2007. MICCLLR: A generalized multiple-instance learning algorithm using class conditional log likelihood ratio. Technical report, Computer Science Department, Iowa State University.",
                    "cites": null
                },
                {
                    "id": 37873037,
                    "title": "MILES: Multiple-instance learning via embedded instance selection.",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/tpami.2006.248",
                    "raw": "Chen, Y., Bi, J. & Wang, J. Z. 2006. MILES: Multiple-instance learning via embedded instance selection.",
                    "cites": null
                },
                {
                    "id": 37873062,
                    "title": "Multi instance neural networks.",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "Ramon, J. & De Raedt, L. 2000. Multi instance neural networks. In Proceedings of the International Conference on Machine Learning 2000 Workshop on Attribute-Value and Relational Learning.",
                    "cites": null
                },
                {
                    "id": 37873079,
                    "title": "Multi-instance clustering with applications to multi-instance prediction.",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1007/s10489-007-0111-x",
                    "raw": "Zhang, M-L. & Zhou, Z.-H. 2009. Multi-instance clustering with applications to multi-instance prediction. Applied Intelligence 31(1), 47–68.",
                    "cites": null
                },
                {
                    "id": 37873050,
                    "title": "Multi-instance kernels.",
                    "authors": [],
                    "date": "2002",
                    "doi": null,
                    "raw": "Ga ¨ rtner, T., Flach, P. A., Kowalczyk, A. & Smola, A. 2002. Multi-instance kernels. In Proceedings of the 19th International Conference on Machine Learning, 179–186. Morgan Kaufmann.",
                    "cites": null
                },
                {
                    "id": 37873082,
                    "title": "Multi-instance learning by treating instances as non-I.I.D. samples.",
                    "authors": [],
                    "date": "2009",
                    "doi": "10.1145/1553374.1553534",
                    "raw": "Zhou, Z.-H., Sun, Y.-Y. & Li, Y.-F. 2009. Multi-instance learning by treating instances as non-I.I.D. samples. In Proceedings of the 26th International Conference on Machine Learning, 1249–1256. ACM.",
                    "cites": null
                },
                {
                    "id": 37873084,
                    "title": "Multi-instance multi-label learning with application to scene classification.",
                    "authors": [],
                    "date": "2006",
                    "doi": "10.1109/icig.2009.108",
                    "raw": "Zhou, Z.-H. & Zhang, M.-L. 2006. Multi-instance multi-label learning with application to scene classification. In Proceedings of the 20th Annual Conference on Neural Information Processing Systems, 1609–1616. MIT Press.",
                    "cites": null
                },
                {
                    "id": 37873035,
                    "title": "Multi-instance tree learning.",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1145/1102351.1102359",
                    "raw": "Blockeel, H., Page, D. & Srinivasan, A. 2005. Multi-instance tree learning. In Proceedings of the 22nd International Conference on Machine Learning, 57–64. ACM.",
                    "cites": null
                },
                {
                    "id": 37873071,
                    "title": "Multi-Label classification: An overview.",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.4018/jdwm.2007070101",
                    "raw": "Tsoumakas, G. & Katakis, I. 2007. Multi-Label classification: An overview. International Journal of Data Warehousing and Mining 3(3), 1–13.",
                    "cites": null
                },
                {
                    "id": 37873064,
                    "title": "Multiple instance regression.",
                    "authors": [],
                    "date": "2001",
                    "doi": null,
                    "raw": "24 J. FOULDS AND E. FRANKRay, S. & Page, D. 2001. Multiple instance regression. In Proceedings of the 18th International Conference on Machine Learning, 425–432. Morgan Kaufmann.",
                    "cites": null
                },
                {
                    "id": 37873059,
                    "title": "Multiple-instance learning for natural scene classification.",
                    "authors": [],
                    "date": "1998",
                    "doi": null,
                    "raw": "Maron, O. & Ratan, A. L. 1998. Multiple-instance learning for natural scene classification. In Proceedings of the 15th International Conference on Machine Learning, 341–349. Morgan Kaufmann.",
                    "cites": null
                },
                {
                    "id": 37873032,
                    "title": "Multiple-instance learning of real-valued data.",
                    "authors": [],
                    "date": "2002",
                    "doi": "10.1007/3-540-45583-3_14",
                    "raw": "Dooly, D., Zhang, Q., Goldman, S. & Amar, R. 2002. Multiple-instance learning of real-valued data. Journal of Machine Learning Research 3, 651–678.",
                    "cites": null
                },
                {
                    "id": 37873067,
                    "title": "On generalized multiple-instance learning.",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1142/s1469026805001453",
                    "raw": "Scott, S., Zhang, J. & Brown, J. 2005. On generalized multiple-instance learning. International Journal of Computational Intelligence and Applications 5(1), 21–35.",
                    "cites": null
                },
                {
                    "id": 37873083,
                    "title": "On the relation between multi-instance learning and semi-supervised learning.",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1145/1273496.1273643",
                    "raw": "Zhou, Z.-H. & Xu, J.-M. 2007. On the relation between multi-instance learning and semi-supervised learning.",
                    "cites": null
                },
                {
                    "id": 37873072,
                    "title": "Salience assignment for multiple-instance regression.",
                    "authors": [],
                    "date": "2007",
                    "doi": null,
                    "raw": "Wagstaff, K. & Lane, T. 2007. Salience assignment for multiple-instance regression. In Proceedings of the International Conference on Machine Learning 2007 Workshop on Constrained Optimization and Structured Output Spaces.",
                    "cites": null
                },
                {
                    "id": 37873085,
                    "title": "Solving multi-instance problems with classifier ensemble based on constructive clustering.",
                    "authors": [],
                    "date": "2007",
                    "doi": "10.1007/s10115-006-0029-3",
                    "raw": "Zhou, Z.-H. & Zhang, M.-L. 2007. Solving multi-instance problems with classifier ensemble based on constructive clustering. Knowledge and Information Systems 11(2), 155–170. A review of multi-instance learning assumptions 25",
                    "cites": null
                },
                {
                    "id": 37873040,
                    "title": "Solving multiple-instance and multiple-part learning problems with decision trees and rule sets. Application to the mutagenesis problem.",
                    "authors": [],
                    "date": "2001",
                    "doi": "10.1007/3-540-45153-6_20",
                    "raw": "Chevaleyre, Y. & Zucker, J.-D. 2001. Solving multiple-instance and multiple-part learning problems with decision trees and rule sets. Application to the mutagenesis problem. In Proceedings of the 14th Biennial Conference of the Canadian Society for Computational Studies of Intelligence, 204–214. Springer.",
                    "cites": null
                },
                {
                    "id": 37873041,
                    "title": "Solving the multiple instance problem with axisparallel rectangles.",
                    "authors": [],
                    "date": "1997",
                    "doi": "10.1016/s0004-3702(96)00034-3",
                    "raw": "Dietterich, T. G., Lathrop, R. H. & Lozano-Pe ´ rez, T. 1997. Solving the multiple instance problem with axisparallel rectangles. Artificial Intelligence 89(1–2), 31–71.",
                    "cites": null
                },
                {
                    "id": 37873073,
                    "title": "Solving the multiple-instance problem: A lazy learning approach.",
                    "authors": [],
                    "date": "2000",
                    "doi": null,
                    "raw": "Wang, J. & Zucker, J.-D. 2000. Solving the multiple-instance problem: A lazy learning approach. In Proceedings of the 17th International Conference on Machine Learning, 1119–1125. Morgan Kaufmann.",
                    "cites": null
                },
                {
                    "id": 37873077,
                    "title": "Statistical Learning in Multiple Instance Problems. Master’s thesis,",
                    "authors": [],
                    "date": "2003",
                    "doi": null,
                    "raw": "Xu, X. 2003. Statistical Learning in Multiple Instance Problems. Master’s thesis, University of Waikato.",
                    "cites": null
                },
                {
                    "id": 37873063,
                    "title": "Supervised learning versus multiple instance learning: an empirical comparison.",
                    "authors": [],
                    "date": "2005",
                    "doi": "10.1145/1102351.1102439",
                    "raw": "Ray, S. & Craven, M. 2005. Supervised learning versus multiple instance learning: an empirical comparison. In Proceedings of the 22nd International Conference on Machine Learning, 697–704. ACM.",
                    "cites": null
                },
                {
                    "id": 37873070,
                    "title": "SVM-based generalized multiple-instance learning via approximate box counting.",
                    "authors": [],
                    "date": "2004",
                    "doi": "10.1145/1015330.1015405",
                    "raw": "Tao, Q., Scott, S., Vinodchandran, N. & Osugi, T. T. 2004b. SVM-based generalized multiple-instance learning via approximate box counting. In Proceedings of the 21st International Conference on Machine Learning, 779–806. ACM.",
                    "cites": null
                },
                {
                    "id": 37873075,
                    "title": "Two-level classification for generalized multi-instance data. Master’s thesis,",
                    "authors": [],
                    "date": "2003",
                    "doi": null,
                    "raw": "Weidmann, N. 2003. Two-level classification for generalized multi-instance data. Master’s thesis, Albert Ludwigs University of Freiburg.",
                    "cites": null
                }
            ],
            "sourceFulltextUrls": [
                "http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=7415676"
            ],
            "updatedDate": "2022-05-12T04:34:31",
            "yearPublished": 2010,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "0269-8889"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/29197595.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/29197595"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/29197595/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/29197595/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/15107808"
                }
            ]
        },
        {
            "acceptedDate": "2016-03-28T00:00:00",
            "arxivId": "1507.02049",
            "authors": [
                {
                    "name": "Ng, Cong Jie"
                },
                {
                    "name": "Teoh, Andrew Beng Jin"
                }
            ],
            "contributors": [
                "Cong Jie"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/192176389"
            ],
            "createdDate": "2015-09-24T00:59:50",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2015-12-01T00:00:00",
            "abstract": "PCANet was proposed as a lightweight deep learning network that mainly\nleverages Principal Component Analysis (PCA) to learn multistage filter banks\nfollowed by binarization and block-wise histograming. PCANet was shown worked\nsurprisingly well in various image classification tasks. However, PCANet is\ndata-dependence hence inflexible. In this paper, we proposed a\ndata-independence network, dubbed DCTNet for face recognition in which we adopt\nDiscrete Cosine Transform (DCT) as filter banks in place of PCA. This is\nmotivated by the fact that 2D DCT basis is indeed a good approximation for high\nranked eigenvectors of PCA. Both 2D DCT and PCA resemble a kind of modulated\nsine-wave patterns, which can be perceived as a bandpass filter bank. DCTNet is\nfree from learning as 2D DCT bases can be computed in advance. Besides that, we\nalso proposed an effective method to regulate the block-wise histogram feature\nvector of DCTNet for robustness. It is shown to provide surprising performance\nboost when the probe image is considerably different in appearance from the\ngallery image. We evaluate the performance of DCTNet extensively on a number of\nbenchmark face databases and being able to achieve on par with or often better\naccuracy performance than PCANet.Comment: APSIPA ASC 201",
            "documentType": "research",
            "doi": "10.1109/apsipa.2015.7415375",
            "downloadUrl": "http://arxiv.org/abs/1507.02049",
            "fieldOfStudy": "computer science",
            "fullText": "DCTNet : A Simple Learning-free Approach for Face \nRecognition \nCong Jie Ng\n*\n and Andrew Beng Jin Teoh\n†\n \nYonsei University, Seoul, South Korea \nE-mail: congjie@yonsei.ac.kr\n*\n, bjteoh@yonsei.ac.kr\n†\n \n \n \n \n \nAbstract— PCANet was proposed as a lightweight deep learning \nnetwork that mainly leverages Principal Component Analysis \n(PCA) to learn multistage filter banks followed by binarization \nand block-wise histograming. PCANet was shown worked \nsurprisingly well in various image classification tasks. However, \nPCANet is data-dependence hence inflexible. In this paper, we \nproposed a data-independence network, dubbed DCTNet for \nface recognition in which we adopt Discrete Cosine Transform \n(DCT) as filter banks in place of PCA. This is motivated by the \nfact that 2D DCT basis is indeed a good approximation for high \nranked eigenvectors of PCA. Both 2D DCT and PCA resemble a \nkind of modulated sine-wave patterns, which can be perceived as \na bandpass filter bank. DCTNet is free from learning as 2D DCT \nbases can be computed in advance. Besides that, we also \nproposed an effective method to regulate the block-wise \nhistogram feature vector of DCTNet for robustness. It is shown \nto provide surprising performance boost when the probe image \nis considerably different in appearance from the gallery image. \nWe evaluate the performance of DCTNet extensively on a \nnumber of benchmark face databases and being able to achieve \non par with or often better accuracy performance than PCANet. \nI. INTRODUCTION \nDeep convolutional network shows its success in various \nimage classification tasks has drawn significant attention in \nrecent years [1][2][3]. The key ingredient of the success is the \nability to automatically discover and learn abstract \nrepresentation of the data build up in multiple stages where \neach stage represents intermediate level representation \ndeveloped from the previous stage. Nonetheless, besides filter \nlearning, one of the key challenges is designing the proper \nnetwork architecture and choosing the right configuration and \nparameters such as number of layers, filter size, choice of \npooling function and etc. AlexNet [1], which outperformed \nthe runner up by 10% error gap in 2012 ILSVRC challenge \n[4] adopts similar architecture as early convolution network, \nie. LeNet [5], but with deeper and bigger network structure. \nGoogLeNet [2] adopts Inception module inspired by Network \nin Network [6] won ILSVRC 2014 [4].  \nDespite the successes, the feature learning mechanism and \noptimal network configurations of deep networks are not well \nunderstood [7]. Scattering Convolution Network (ScatNet) [7] \nthat is based on scattering theory addresses these open \nproblems partially. With prefixed filters generated from \nmathematical functions, ScatNet demonstrates state-of-the-art \nperformance over ConvNet [8] in handwritten recognition and \ntexture discrimination tasks.  \nRecently, a lightweight unsupervised deep learning \nnetwork proposed by Chan et al. called PCANet (Principal \nComponent Analysis Network) [9] works unexpectedly well \nin most of the image classification tasks despite very simple \narchitecture. PCANet processes an input image via a layer-\nwise convolution with PCA filters and followed by \nbinarization, block-wise histograming and eventually yield a \nlong histogram feature vector. The histogram vector can be \nfurther compressed via dimension reduction technique such as \nwhitening PCA. Prior to PCANet, a similar filter called \nBinarized Statistical Image Features (BSIF) [10] is proposed. \nBSIF binarizes the filter responses obtained from the \nconvolution of an image with Independent Component \nAnalysis (ICA) learned filters. However, BSIF is merely \ntreated as an image descriptor in [14] but not expanded to a \nnetwork form.  \nIn this paper, we propose a much simpler learning-free \nalternative of PCANet via 2D DCT filters dubbed DCTNet, \nspecifically tailored for face recognition. The choice of 2D \nDCT basis as filter bank is inspired by the Karhunen Loève \nTransform (KLT) in transform coding literature, which is also \nknown as Principal Component Analysis (PCA) in \nmultivariate statistics community. KLT is an optimal \northogonal transform that can decorrelate any signal \ncompletely and condense the signal energy maximally. \nHowever, despite these attractive properties, 2D DCT is \nchosen as the baseline JPEG image compression standard \ninstead due to the reasons that KLT is data dependence and \nthere is no fast algorithm available for KLT, which requires \n𝑂(𝑁3) to solve eigenvalue problem of the 𝑁 × 𝑁 dimension \ncovariance matrix, whereas 2D DCT can be computed with \n𝑂(𝑁 log2 𝑁) operations [11]. Apart from low complexity, 2D \nDCT computation is independent from data, which implies \nlearning-free. Both 2D DCT and PCA filters are indeed \nequivalence when an image is assumed to be the first order \nMarkov process subject to the condition when the local \ncorrelation between neighborhood pixels is high. We will \nelaborate this interesting fact in section III. \nOn the other hand, block-wise histograming of PCANet \nthat is capable of implicitly encoding spatial information of \nimage regions is useful for classification task like face \nrecognition [12]. Block-wise histograming is essentially used \nto estimate the probability distribution function (pdf) of the \nimage features in block-wise manner. Bigger block size \nimplies better pdf estimation due to larger number of feature \nsamples but poorer spatial precision. However, small block \nsize introduces another problem in which the number of \nhistogram bins would be more than the number of samples, \nand hence the resulting histogram becomes very sparse. \nSparse histogram may render poor pdf estimation for that \nparticular block. \nIn order to mitigate this trade-off, we propose an effective \nmethod called Tied Rank Normalization (TR Normalization) \nto regulate the histogram of DCTNet for robustness despite \nunder sampling. Our proposed technique is based on tied rank \nprinciple inspired by Spearman’s rank correlation [13] that \ncomputes the Pearson correlation between ranked variables. \nBy adopting the ranking idea that is well tolerated to outliers, \nthe appearance disparity between probe and gallery samples \ndue to pose variation and occlusion, can be eliminated and \nhence to provide better robustness. In addition to that, we also \nadopt the intra-normalization proposed by [14] to spread the \nconcentrated component energy of histogram vector more \nevenly, which was shown to be beneficial for accuracy \nperformance improvement. \nIn a nutshell, the contribution of this paper is three-fold: \n We propose a much simple learning-free DCTNet by \nadopting 2D DCT bases as filter bank, which was shown \nequivalence to PCANet subject to certain condition. \n We also propose a histogram normalization technique \ncalled Tied Rank Normalization (TR Normalization) to \neliminate the disparity of histogram vector of DCTNet \nbased on the tied rank principle used by robust statistic \n(Spearman’s rank correlation) and intra-normalization for \nfeature-evenization.  \n Lastly, we provide extensive experiment results with a \nnumber of benchmark face datasets for the proposed \nlearning-free DCTNet. The datasets considered, ie. AR, \nFERET-I (‘b’ subset) and FERET-II (‘fa’, ‘fb’, ‘fc’, ‘dup \nI’, ‘dup II’ subset), covers various undesirable scenarios \nin face recognition such as variations in poses, lighting, \nexpression, occlusion and time span. \nII. PRELIMINARY \nPCANet [9] is designed to be a lightweight convolutional \nneural network (CNN) in which the filters in the convolution \nlayers are learned by PCA, an unsupervised learning method \nas opposed to supervised learning approach via \nBackpropagation adopted in CNN. Unlike typical CNN, this \nsimplistic CNN has no nonlinear operation in between layers \ninstead the operation is only performed at the output layer. \nThe nonlinear operation in PCANet refers to the binary \nthresholding operation that converts the filter responses into a \nbinary map. Then, block-wise histograming is carried out to \nencode the spatial relation between blocks [12]. The detail of \nbinarization and block-wise histograming is given in Section \nIV(B). Finally, the output feature vector is formed by \nconcatenating all block-wise histograms.  \nDespite unsupervised, Chan et al. [9] also shows that by \nreplacing the PCA filters with the linear discriminant analysis \n(LDA) learned filters, which exploits the class labels does not \noffer significant advantage over PCANet. \nIII. METHODOLOGY \nA. Relationship Between DCT and PCA \nIn this section, we present a theoretical and empirical \njustification on the equivalence of PCA and DCT [11]. In \nessence, the local correlation between neighborhood pixels of \nan image makes it convenient to be regarded as a stochastic \nprocess, which can be modeled by a two dimensional \nstationary first order Markov process.  \nWithout loss of generality, given a 1D signal with samples \n{𝑥𝑖 | i=1,…,N }, the correlation between any two samples 𝑥𝑖 \nand 𝑥𝑗 is defined as 𝑟\n|𝑗−𝑖| where 0 ≤ r ≤ 1. 𝑟|𝑗−𝑖| indicates the \ncorrelation between two samples that declines exponentially \nas they get further apart. With the definition, the correlation \nmatrix of this Markov chain is defined with a Toeplitz matrix \nas follows with all diagonal elements being the same as 1 has \nthe highest correlation value [11]. \n \n \n𝑅 =\n[\n \n \n \n \n  \n1  𝑟 \n𝑟  1 \n𝑟2     𝑟   \n𝑟2 … 𝑟𝑁−1\n𝑟  … 𝑟𝑁−2\n  1     … 𝑟𝑁−3\n⋮ ⋮\n𝑟𝑁−1 𝑟𝑁−2\n⋮ ⋱    ⋮     \n𝑟𝑁−3 …    1     ]\n \n \n \n \n (1) \n \nIt is shown by [15] that the principal components \n(eigenvectors) and its associated variances (eigenvalues) in eq. \n(1) can be obtained by performing eigen-decomposition on R. \nHence, the 𝑛𝑡ℎeigenvalue is given as \n \n \n𝜆𝑛 =\n1 − 𝑟\n1 − 2𝑟 𝑐𝑜𝑠𝜔𝑛 + 𝑟2\n \n \n(2) \nand the 𝑚𝑡ℎ element of 𝑛𝑡ℎ eigenvector is given as \n \n \n𝜙𝑚𝑛 = (\n2\n𝑁 + 𝜆𝑛\n)\n1\n2\nsin (𝜔𝑛 (𝑚 −\n𝑁 − 1\n2\n) +\n(𝑛 + 1)𝜋\n2\n)  \n \n(3) \nwhere 0 ≤ 𝑚, 𝑛 ≤ 𝑁 − 1  and 𝜔𝑛  is the N real roots of the \nfollowing equation \n \n \ntan(𝑁𝜔) = −\n(1 − 𝑟2)𝑠𝑖𝑛𝜔\n(1 + 𝑟2) cos𝜔 − 2𝑟\n \n \n(4) \nThe relationship of PCA and DCT is unveiled when 𝑟 \napproaches 1 [11]. The following equation shows that the \neigenvector of the resulting eigendecomposition of R is \nindeed identical to DCT bases for 𝑛 > 0  has 𝜆𝑛 = 0  and \n𝜔𝑛 = 𝑛𝜋/𝑁 when r →1: \n \n \n𝜙𝑚𝑛 = √\n2\n𝑁\ncos (\n𝑛𝜋\n2𝑁\n(2𝑚 + 1)) (5) \nFor the case when 𝑛 = 0, it takes the following form which \nhas 𝜆0 = 𝑛 and 𝜔0 = 0 \n \n𝜙𝑚0 = √\n1\n𝑁\n \n \n(6) \nwhere 0 ≤ 𝑚 < 𝑁 − 1, 1 ≤ 𝑛 ≤ 𝑁 − 1 \n \nIn summary, given 𝑀 overlapping blocks of a signal with \nsize N of stride 1, if the correlation between blocks is very \nhigh, the PCA eigenvector of the blocks covariance matrix \nwill approach DCT basis. It is also worth noting that when \n𝑟 = 1 , the eigenvectors are no longer unique as all the \nelements of the correlation matrix become unity, which imply \nsingularity.  \nTo gain further insight into the relationship of DCT and \nPCA, eq. (2) is plotted with 𝑟 = 0.9, 𝑁 = 100 and 𝜔𝑛 =\n𝑛𝜋\n𝑁\n \nas shown in Fig. 1. The plot suggests that large eigenvalue of \nPCA corresponds to low frequency in DCT and vice versa. \nThis property is vital for DCT basis selection for DCTNet in \nsection V, which follows the PCA by ranking the importance \nof eigenvector based on the respective eigenvalue. This \nproperty also explains the reason why zig-zag scanning is \nadopted in Baseline JPEG.  \nFig. 1 Plot of (2) shows the inverse exponential relationship \nbetween eigenvalue and frequency \n \nB. 2D DCT and PCA \nAlbeit the derived equations show the equivalence of 1D DCT \nand PCA when 𝑟  approaches 1 in stationary first order \nMarkov process model, it is shown to be applicable to 2D \nDCT on image too. Without resort to the rigorous proof that \nrequires decomposing a much more complicated Toeplitz \nMatrix than (1), the similarity of 2D DCT and PCA \neigenvectors is shown pictorially. To generate PCA bases, we \nuse gray-scale frontal faces with expression and illumination \nof FERET ‘b’ subset dataset (‘ba’, ‘bj’ and ‘bk’) [16], which \ncomposed of 600 images of size 64 × 64 each. Each image is \nfirst segmented into overlapping patches of size 5 × 5 with \nstride 1. Each extracted patch is then vectorized into a 25 \ndimension vector. Lastly, eigen-decomposition is performed \non the vectorized patches covariance to obtain the \neigenvectors. To show the similarity between 2D DCT and \nPCA, the eigenvectors are reordered manually to be alike with \n2D DCT basis ordering for better visualization as shown in \nFig. 2.  \nNote that the eigenvectors may look quite different from \nthe corresponding 2D DCT basis due to negation in the \nnumeric sign. Besides sign inversion, both 2D DCT basis and \nPCA learned eigenvector from FERET ‘b’ subset are shown \nto have very similar structure. These bases can be essentially \nperceived as a filter bank with different cutoff frequencies at \nhorizontal direction, vertical direction and their products.  \n \n  \n(a) (b) \nFig. 2 (a) shows the DCT bases, (b) shows the PCA learned \neigenvectors on FERET ba, bj and bk dataset with manual reordering \nfor illustration purposes. \nIV. DISCRETE COSINE TRANSFORM NETWORK (DCTNET) \nARCHITECTURE \nDCTNet adopts a similar structure to PCANet except there is \nan extra layer at the histogram output for histogram \nnormalization as shown in Fig. 3. The detail of each \ncomponent is described below.  \nA. Convolution Layer \nAssume that filter size of all stages have the same size 𝑘 × 𝑘. \nGiven an input image 𝐼𝑑  of size 𝑚 × 𝑛  with 𝐷  channels \n(multiple channel image or input from previous layer), \nboundary of each channel 𝑑  is zero padded with pad size \n(𝑘 − 1)/2  before convolution to keep the size of output 𝑂𝑑\n𝑝\n \nsame as 𝐼𝑑. With a set of 2D DCT bases selected as described \nin section V denoted by 𝑊𝑝\n𝑙 ∈  ℝ𝑘×𝑘, 𝑝 = 1,2, … , 𝑃𝑙  where 𝑃𝑙  \nis the number of filters at layer 𝑙 , convolving each with 𝐼𝑑 \nyields  \n 𝑂𝑑\n𝑝 = {𝐼𝑑 ∗ 𝑊𝑝\n𝑙}\n𝑝=1\n𝑃𝑙\n (7) \n   \nThe number of output of each layer is 𝑑. 𝑃𝑙 . Cascading this \nlayer can form a deeper network. Since, there is no nonlinear \noperation in between the previous convolution layer and the \nnext layer, DCT bases of each layer can be combined to form \na flat single layer network. The number of bases formed is \n∏ 𝑃𝑖\n𝐿\n𝑖=1  where 𝐿 represents the number of convolution layers. \nFor the sake of convenience without storing large number of \ncombined filters and to ease the binarization process, the flat \nsingle layer architecture is not considered in this paper. \n0 20 40 60 80 100\n0\n5\n10\nComponent, n\nE\nig\ne\nn\nv\na\nlu\ne\n0 0.2 0.4 0.6 0.8 1\nNormalized Frequency\nB. Binarization and Block-wise Histograming \nThe last convolution layer of DCTNet forms 𝐷 sets of real \nvalued outputs. Each set has a total of 𝑃𝐿  outputs where the \noutputs are the response of DCT filters. Binarization is \nperformed on each set separately by first binarizing the \nresponses with threshold at zero (value one for positive \nresponse, zero otherwise) denoted by 𝐵𝐼𝑁(. ). Followed by \nbinarization, each binary string is encoded as a single integer \nnumber ∑ 2𝑝−1𝐵𝐼𝑁(𝑂𝑑\n𝑝)\n𝑃𝐿\n𝑝  and forming an “image” for each \nset of 𝑑𝑡ℎ  output where each pixel has an integer range of \n[0, 2𝑃𝐿−1] . Then, each of these 𝐷  binarized “image” is \npartitioned into 𝐵 non-overlapping blocks. Histogram of each \nblock denotes by 𝐻𝑏\n𝑑 , 𝑏 = 1,2, … , 𝐵;  𝑑 = 1,2, … , 𝐷 with bin \n[0,2𝑃𝐿−1] is obtained as the input for histogram normalization \nlayer that will be described in next section.  \nIt is also worth to mention that block-wise histogram not \nonly encodes spatial information [12], it also provides local \ntranslation invariance in the extracted features within each \nblocks. The combination of binarization and block-wise \nhistograming is expected to be able to extract discriminative \nfeatures.  \nC. Histogram Tied Rank Normalization (TR Normalization) \nThe first stage of TR normalization uses tied rank principle \nthat computes rank of a given vector 𝒙  which produces a \nvector 𝒙 that has a range from 1 to the length of 𝒙 where each \nelement  ?̅?𝑖  corresponds to the ascending order rank of 𝑥𝒊. In \ncase of ties, their average rank is assigned to all ties which \nmay produce non-integer values. Given 𝑯  as the extracted \nblock-wise histogram of a given face data, where 𝑯 =\n{𝐻𝑏\n𝑑}\n𝑏=1,𝑑=1\n𝐵,𝐷\n. Each 𝐻𝑏\n𝑑  is ranked with tied ranking without \nconsidering the bin with zero occurrence denoted by 𝐻𝑏\n𝑑. This \nis because bin with zero occurrences is not a sample in \nhistogram, it should be ignored in the ranking process. In \norder to make ?̅?𝑏\n𝑑  to be more evenly distributed, we first \napply square root on ?̅?𝑏\n𝑑  forming 𝑣𝑏\n𝑑 = √𝐻𝑏\n𝑑 . Follow by L2 \nnorm normalization which follows the idea of intra-\nnormalization uses by [14] we obtain ?̂?𝑏\n𝑑 . The final TR \nnormalized histogram feature vector is constructed by \nconcatenating all ?̂?𝑏\n𝑑  \n 𝒗 = [?̂?1\n1, ?̂?2\n1, … , ?̂?𝐵\n1 , ?̂?1\n2, … , ?̂?𝐵\n𝐷 ] ∈ ℝ(2\n𝑃𝐿)𝐵 𝐷 (8) \n   \nAlgorithm 1 : Histogram TR Normalization \nInput: \nExtracted block-wise histogram of an image : 𝑯 \nOutput: \nTR normalized histogram feature vector : 𝒗 \nStart: \n1. For each 𝐻𝑑\n𝑏  compute tied rank without bin with zero \noccurrence yields 𝐻𝑑\n𝑏 \n2. 𝑣𝑏\n𝑑 = √𝐻𝑏\n𝑑 \n3. Normalize 𝑣𝑏\n𝑑  with L2 norm to obtain ?̂?𝑏\n𝑑 \n4. Repeat step 1 to step 3 for 𝑏 = 1,2, … , 𝐵; 𝑑 = 1,2, … , 𝐷 \n5. Concatenate all 𝑣𝑏\n𝑑  to obtain the final output 𝒗  \n \nThe pseudo code of histogram TR normalization is shown \nin Algorithm 1. The TR normalized block-wise histogram is \nshown in Fig. 4. The disparity of the original block-wise \nhistogram is shown to be eliminated and it is also shown to be \nmore evenly distributed. Finally, the dimension of the \nresulting TR normalized block-wise histogram vector is \noptionally compressed with whitening PCA (WPCA) to \nobtain the final feature vector where the projection matrix is \nlearned from Gallery set. \nFig. 3 The block diagram of the proposed DCTNet \nFilters Convolution \nInput \nBinary Hashing Block-wise  \nHistograming \nTR-Norm. \n& \nConcatenation \nDimension  \nReduction \n(Optional) \nConvolution Layers \nOutput Vector \n  \nFig. 4 Top shows a part of the original block-wise histogram feature \nvector; bottom shows the resulting TR normalized block-wise \nhistogram feature vector. Note that, scale difference between the \ninput and the output is due to normalization process  \nV. SELECTION OF DCT BASES AS FILTER BANK \nOne essential issue to address when adopting 2D DCT basis \ninto the network as filter bank is the basis selection. Unlike \nPCANet, eigenvectors are ranked by their respective \neigenvalue strength. The first 𝑃 eigenvectors with the highest \neigenvalue are selected as the network filter for each level. To \naddress the issue one can refer to the derived eigenvalue \nequation (2) and Fig. 1 as discussed in previous section which \nshows that eigenvalue has inverse exponential relationship. \nLow frequency DCT basis corresponds to high ranked \neigenvector. Although (2) corresponds to 1D DCT, 2D DCT \nis just a product of vertical basis and horizontal basis of 1D \nDCT. Without lengthy mathematical proof for simplicity one \ncan assume that the eigenvalue of the horizontal basis, vertical \nbasis and the diagonal bases of the same frequency have the \nsame value. That is to say, bases in the same antidiagonal row \nas shown in Fig. 5 are assumed to have the same eigenvalue \nhence they are ranked equally. \nTo further rank the equally ranked DCT bases, the prior-\nknowledge of human face characteristic is taken into account. \nSince human face distinct features are composed of more high \nfrequency horizontal components (eyes, eyebrows and lips) \nthan low frequency vertical component, it is natural to rank \nthe 2D DCT bases by horizontal-frequency major order. As \nillustrated in Fig. 5, zig-zag scanning used by Baseline JPEG \nalternates the frequency direction importance at each turn; the \nDCTNet keeps the importance of horizontal frequency \ndirection at each turn to extract a more representative face \nfeatures.  \nLastly, DC component is not considered as a filter in \nDCTNet as reported by PCANet removing mean of each \npatch yields better performance. The basis selection is \ntherefore starting from 2 to 𝑃 + 1 in the horizontal-frequency \nmajor scanning order. Omitting the DC component which \nextracts the lowest frequency component or mean of the patch \ncan improve the robustness of the extracted feature against \nglobal illumination changes.  \n \nFig. 5 Left shows the zig-zag scanning order; right shows the \nproposed scanning order with horizontal-frequency major direction. \nVI. EXPERIMENT AND DISCUSSION \nIn this section, the effectiveness of the proposed DCTNet and \nPCANet is evaluated on a number of benchmark face datasets \nnamely AR [17], FERET-I (‘b’ subset) and FERET-II (‘fa’, \n‘fb’, ‘fc’, ‘dup I’, ‘dup II’ subset)[16].  \nTo have a fair comparison, the PCANet filter learned from \nMulti-PIE dataset [18] consists of 337 subjects with around \n100,000 images (shared by the PCANet’s author) is used in \nthe experiment, denoted as PCANet-A. The filter is learned \nfor a 2 layers PCANet with filter size 𝑘 = 5 × 5  for each \nlayer and the number of filter for each layer is 𝑃1 = 𝑃2 = 8. \nWe also learn our own PCA filters with the same parameter \nfrom gallery for each dataset separately and it is denoted as \nPCANet-B. Lastly, DCTNet with the same parameter is also \nused (ie, 2D DCT basis of size 5 × 5 with 8 bases for each \nlayer). In other words, the experiment is conducted with 3 \ntypes of filters – filter learned from external dataset, filter \nlearned from gallery and precomputed filters obtained from \n2D DCT. All networks examined are restricted to two layers \nas we find that the network with more than two layers does \nnot offer significant performance gain whereas incurs higher \ncomputation load. \nIn addition, to evaluate the effectiveness of the proposed \nhistogram TR normalization technique, each experiment is \nconducted with the presence and absence of the proposed \nmethod. Finally, Nearest Neighbor classifier with cosine \ndistance is used for all experiments. \nA. Evaluation on AR Dataset \nTABLE I \nAR DATASET RECOGNITION RATES (%) \nTR Norm. Method Expres. Illum. Occlus. Avg \nNo \nPCANet-A 95.960 100 98.232 98.064 \nPCANet-B 94.276 100 97.896 97.391 \nDCTNet 94.108 100 97.643 97.250 \nYes \nPCANet-A 98.148 100 99.074 99.074 \nPCANet-B 97.811 100 99.158 98.990 \nDCTNet 97.811 100 99.242 99.018 \n \nAR dataset [17] contains 126 subjects with over 4000 \nimages. It is composed of frontal faces with different facial \nexpression, illumination variations and occlusions (sun-\nglasses and scarf). In the experiment, subset of 50 male \nsubjects and 50 female subjects are used. Each image is \n1 \n2 \n3 \n4 \n7 \n11 \n6 \n5 \n8 \n12 \n16 \n13 \n9 \n10 15 \n14 19 \n17 \n20 \n21 \n18 \n23 \n22 \n24 \n25 \n1 \n3 \n2 \n4 \n10 \n11 \n6 \n5 \n9 \n12 \n19 \n13 \n8 \n7 15 \n14 16 \n18 \n20 \n21 \n17 \n24 \n22 \n23 \n25 \nconverted to gray scale and cropped to 165 × 120 . For \ngallery, 2 frontal faces with neutral facial expression of each \nsubject are selected and the rest are used as probes which are \ndivided into 3 groups (ie, expression, illumination and \nocclusion). For all networks, the size of block-wise histogram \nis set to 20 × 20 and the dimension of the final feature vector \nis reduced to 150 with WPCA.  \nTable I reports the performance of each method. It is \nobserved that all methods are insensitive to illumination \nvariations and robust against facial expression variation and \nocclusions. DCTNet filter without DC component and \nPCANet mean removal for each patch make them robust \nagainst various lighting conditions. \nBig block-wise histogram block size covers bigger area of \neach face region make it robust against various local \ndeformation such as facial expression variation as reported in \n[9]. The block-wise histogram that encodes pdf of each face \nregion could be the reason that makes it robust against \nocclusions. In other words, occluded region yields very \ndifferent block-wise histogram from all subjects in the gallery \nat the same region yields low score and is somehow ignored \nduring the match. Another explanation as discussed in [9]  \ncould be that the selection of frequency band of PCA and 2D \nDCT basis as filters, which is based on human facial \ncharacteristic as described in section V, leads to low response \nof the occluded region that does not fall within the frequency \nbands.  \nApart from that, the presence of the proposed TR \nnormalization is observed to boost the performance for both \nexpression and occlusions probe sets. As the gallery set only \ncontains frontal faces with neutral facial expression, the \nencoded pdf of each block-wise histogram may fit the neutral \nfacial expression well that does not cater expression changes. \nIt shows its advantage over probe set that has different \nprobability distribution from gallery set. \nB. Evaluation on FERET-I \nFig. 6 Samples of FERET ‘b’ subset \n \nThis dataset is the ‘b’ subset of FERET dataset [16]. It \ncontains 200 subjects with total of 1800 images. Each image \nis aligned with eyes and mouth coordinate and cropped to \n64 × 64 . The protocol used by [19] is adopted in the \nexperiment, which uses frontal faces with expression and \nillumination variations (ie, ‘ba’, ‘bj’ and  ‘bk’) as gallery set, \nand non-frontal viewing subset (ie, ‘bc’, ‘bd’, ‘be’, ‘bf’, ‘bg’, \nand ‘bh’) with pose angle range from +40 to -40 degree are \nused as probe set. The size block-wise histogram is set to \n16 × 16  and the final feature vector is used without \ndimension reduction.  \nTable II shows that with the absence of histogram TR \nnormalization, the proposed DCTNet has the best \nperformance. Big performance difference between DCTNet \nand PCANet is observed on probe set with pose angle +40 \nand -40 (Bc and Bh respectively). Big pose angle in the probe \nset leads to very different pdf from the training data used by \nPCANets which only contains frontal face. The learning-free \nDCTNet that does not rely on training data may be the reason \nthat makes it extracts more generic feature rather than feature \nthat is bound to a specific feature pdf learned from training \ndata.  \nFurthermore, a surprising huge performance boost is \nobserved on learning based PCANet when TR normalization \nis applied. The robustness against outliers contributed from \nthe tied-rank as used in Spearman’s rank correlation may be \none of the reasons to the gain. Moreover, the idea of evenly \ndistributed feature seems contribute to the performance boost \ntoo. The square-root operation that compresses large value \nmore and intra-normalization on block-wise histogram that \nmake the resulting histogram more evenly distributed. Here \nwe see that, the advantage of both tied-rank and evenly \ndistributed features make the resulting block-wise histogram \nbe robust when gallery set and probe set have very different \npdf.  \nC. Evaluation on FERET-II \nTABLE III \nFERET-II RECOGNITION RATES (%) \nTR Norm. Method Fb Fc Dup-I Dup-II Avg \nNo \nPCANet-A 99.25 100 94.46 93.16 96.72 \nPCANet-B 99.25 100 93.49 91.45 96.05 \nDCTNet 99.08 100 93.35 91.45 95.97 \nYes \nPCANet-A 99.33 100 94.88 94.44 97.16 \nPCANet-B 99.58 100 95.15 93.59 97.08 \nDCTNet 99.67 100 95.57 94.02 97.32 \n \nLastly, with the same FERET dataset [16] but different \nprotocol, subset ‘fa’, ‘fb’, ‘fc’, ‘dup-I’ and ‘dup-II’ are used in \nthis experiment. Where ‘fa’ is regular facial expression, ‘fb’ is \ndifferent facial expression, ‘fc’ is face with illumination \nvariation, ‘dup-I’ probe images were taken between 0 to 1031 \ndays after the gallery match and ‘dup-II’ probe images were \ntaken at least 18 months after the gallery match which is also \na subset of ‘dup-I’. In this experiment, we use gray scale \nimages with each cropped to 128×128. Finally ‘fa’ is used as \nba bj bk bc bd be bf bg bh\nTABLE II \nFERET-I RECOGNITION RATES (%) \nTR Norm. Method Bc Bd Be Bf Bg Bh Avg \nNo \nPCANet-A 51.5 91.0 99.0 99.5 93.0 51.5 80.92 \nPCANet-B 62.0 92.5 100 100 95.5 55.5 84.25 \nDCTNet 70.5 97.0 99.5 100 96.0 73.0 89.33 \nYes \nPCANet-A 82.0 97.0 100 100 98.5 76.0 92.25 \nPCANet-B 88.5 99.5 100 100 99.5 86.0 95.58 \nDCTNet 85.5 98.5 100 100 99.5 85.0 94.75 \n \ngallery set and the rest are used as probe sets. For this dataset, \nthe block-wise histogram size is set to 16×16 and the final \nfeature vector is reduced to 1000 dimension with WPCA. \nThe experiment results as given in Table III shows that \nDCTNet without TR normalization has the worst performance \namong other methods. However, with the presence of TR \nnormalization, DCTNet has the overall best recognition rates. \nOnce again the histogram normalization technique \nconsistently boosts the performances of all methods.  \nD. Comparison with other methods \nTo compare the performance of the proposed method with \nother state-of-the-arts we compile the result of FERET-II in \nTable IV. The learning free DCTNet achieves the state-of-the-\nart accuracy with average of 97.32%. Note that, PCANet-2 [9] \nand PCANet-A use the PCA filter shared by the author which \nis learned from Multi-PIE dataset. PCANet-2 uses cropped \nFERET-II image of size 150 × 90 pixels and 15 × 15 block-\nwise histogram while PCANet-A uses cropped image of size \n128 × 128 and 16 × 16 block-wise histogram. With the same \ndataset used by PCANet-2 we expect some performance gain \nin DCTNet.  \n \nTABLE IV \nFERET-II  RECOGNITION RATES (%) WITH OTHER METHODS \nMethod Fb Fc Dup-I Dup-II Avg \nLBP [12] 93.00 51.00 61.00 50.00 63.75 \nDMMA [20] 98.10 98.50 81.60 83.20 90.35 \nG-LBP [21] 98.00 98.00 90.00 85.00 92.75 \nWPCA-POEM [22] 99.60 99.50 88.80 85.00 93.23 \nG-LQP [23] 99.90 100 93.20 91.00 96.03 \nLGBP-LGXP [24] 99.00 99.00 94.00 93.00 96.25 \nsPOEM+POD [25] 99.70 100 94.90 94.00 97.15 \nGOM [26] 99.90 100 95.70 93.10 97.18 \nPCANet-2 [9] 99.58 100 95.43 94.02 97.26 \nPCANet-A 99.25 100 94.46 93.16 96.72 \nDCTNet 99.67 100 95.57 94.02 97.32 \nVII. DISCUSSION AND CONCLUSIONS \nIn this paper, the proposed learning free DCTNet gives us a \ndifferent perspective of the filters learned by PCANet. The \nnature of image local correlation characteristic that can be \nmodeled with stationary first order Markov process with the \nassumption that the neighboring pixels are highly correlated \nleading us to a much simple learning-free convolutional \nnetwork. The relationship of frequency and variance of PCA \nand 2D DCT leads us to rank the 2D DCT basis importance \nfrom the lowest frequency as filter selection and it is \ndemonstrated on various face datasets to work very well. On \nthe down side, DCTNet may not work well if the nature of \ninput image does not follow the high local correlation \nassumption such as image that contains high spectral activity \nand fine details like texture images. Such image data may \nneed different DCT basis selection schemes.  \nOn the bright side, as long as the input image meets the \nmodel assumption which happened to be the nature of most \nnatural images, makes the learning-free DCTNet stand out. \nPCANet on the other hand that relies on training data to learn \nthe filters may over fit especially if the probe set distribution \nis far deviated from the training set as observed in FERET ‘b’ \nsubset experiment without histogram TR normalization.  \nIn conjunction with the proposed histogram TR \nnormalization technique, DCTNet contributes a huge \nperformance gain as observed in FERET ‘b’ subset \nexperiment where the frontal face training data and probe \nwith large pose angle may have very different distribution. \nAR ‘expression’ subset and FERET aging (dup-I and dup-II) \nsubset that have local facial deformations are shown to have \nsome gain in performance too. The proposed histogram TR \nnormalization method can also be seen as a post-processing \nmethod to regulate the extracted block-wise histogram from \nrepresenting the subject with the gallery specific distribution.  \nTo conclude, despite learning free, the remarkable \nperformance from extensive face recognition experiments, \nwhich comprise of illumination variation, facial expression \nvariation, occlusions, pose and time span endorses the \ncapability of DCTNet. Indeed, each component of the \nnetwork which play different roles in extracting invariant and \ndiscriminative feature is important for DCTNet to achieve \ngood performance. \nACKNOWLEDGMENT \nThis work was supported by Basic Science Research \nProgram through the National Research Foundation of Korea \n(NRF) funded by Ministry of Science, ICT and Future \nPlanning (2013006574) and Institute of BioMed-IT, Energy-\nIT and SmartIT Technology (BEST), a Brain Korea 21 Plus \nProgram, Yonsei University. \nREFERENCES \n[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \n“ImageNet Classification with Deep Convolutional Neural \nNetworks,” presented at the Advances in Neural Information \nProcessing Systems, 2012, pp. 1106–1114. \n[2] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. \nAnguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \n“Going Deeper with Convolutions,” ArXiv14094842 Cs, Sep. \n2014. \n[3] M. D. Zeiler and R. Fergus, “Visualizing and \nUnderstanding Convolutional Networks,” in Computer Vision \n– ECCV 2014, D. Fleet, T. Pajdla, B. Schiele, and T. \nTuytelaars, Eds. Springer International Publishing, 2014, pp. \n818–833. \n[4] O. Russakovsky, J. Deng, H. Su, J. Krause, S. \nSatheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. \nBernstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large Scale \nVisual Recognition Challenge,” ArXiv14090575 Cs, Sep. \n2014. \n[5] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, \n“Gradient-based learning applied to document recognition,” \nProc. IEEE, vol. 86, no. 11, pp. 2278–2324, Nov. 1998. \n[6] M. Lin, Q. Chen, and S. Yan, “Network In Network,” \nArXiv13124400 Cs, Dec. 2013. \n[7] J. Bruna and S. Mallat, “Invariant Scattering \nConvolution Networks,” IEEE Trans. Pattern Anal. Mach. \nIntell., vol. 35, no. 8, pp. 1872–1886, Aug. 2013. \n[8] H. Larochelle, Y. Bengio, J. Louradour, and P. \nLamblin, “Exploring Strategies for Training Deep Neural \nNetworks,” J Mach Learn Res, vol. 10, pp. 1–40, Jun. 2009. \n[9] T.-H. Chan, K. Jia, S. Gao, J. Lu, Z. Zeng, and Y. \nMa, “PCANet: A Simple Deep Learning Baseline for Image \nClassification?,” IEEE Trans. Image Process. Publ. IEEE \nSignal Process. Soc., Sep. 2015. \n[10] J. Kannala and E. Rahtu, “BSIF: Binarized statistical \nimage features,” in Pattern Recognition (ICPR), 2012 21st \nInternational Conference on, 2012, pp. 1363–1366. \n[11] R. Wang, “Karhunen-Loève Transform and Principal \nComponent Analysis,” in Introduction to Orthogonal \nTransforms : With Applications in Data Processing and \nAnalysis, Cambridge University Press, 2012. \n[12] T. Ahonen, A. Hadid, and M. Pietikainen, “Face \nDescription with Local Binary Patterns: Application to Face \nRecognition,” IEEE Trans. Pattern Anal. Mach. Intell., vol. \n28, no. 12, pp. 2037–2041, Dec. 2006. \n[13] C. Spearman, “The Proof and Measurement of \nAssociation between Two Things,” Am. J. Psychol., vol. 15, \nno. 1, pp. 72–101, Jan. 1904. \n[14] R. Arandjelovic and A. Zisserman, “All About \nVLAD,” in 2013 IEEE Conference on Computer Vision and \nPattern Recognition (CVPR), 2013, pp. 1578–1585. \n[15] W. Ray and R. Driver, “Further decomposition of the \nKarhunen-Loève series representation of a stationary random \nprocess,” IEEE Trans. Inf. Theory, vol. 16, no. 6, pp. 663–668, \nNov. 1970. \n[16] P. J. Phillips, H. Wechsler, J. Huang, and P. J. Rauss, \n“The FERET database and evaluation procedure for face-\nrecognition algorithms,” Image Vis. Comput., vol. 16, no. 5, \npp. 295–306, Apr. 1998. \n[17] A. M. MARTINEZ, “The AR face database,” CVC \nTech. Rep., vol. 24, 1998. \n[18] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. \nBaker, “Multi-PIE,” in 8th IEEE International Conference on \nAutomatic Face Gesture Recognition, 2008. FG ’08, 2008, pp. \n1–8. \n[19] M. Harandi, M. Salzmann, and R. Hartley, “From \nManifold to Manifold: Geometry-Aware Dimensionality \nReduction for SPD Matrices,” in Computer Vision – ECCV \n2014, vol. 8690, D. Fleet, T. Pajdla, B. Schiele, and T. \nTuytelaars, Eds. Springer International Publishing, 2014, pp. \n17–32. \n[20] J. Lu, Y.-P. Tan, and G. Wang, “Discriminative \nMultimanifold Analysis for Face Recognition from a Single \nTraining Sample per Person,” IEEE Trans. Pattern Anal. \nMach. Intell., vol. 35, no. 1, pp. 39–51, Jan. 2013. \n[21] X. Tan and B. Triggs, “Fusing Gabor and LBP \nFeature Sets for Kernel-Based Face Recognition,” in Analysis \nand Modeling of Faces and Gestures, S. K. Zhou, W. Zhao, X. \nTang, and S. Gong, Eds. Springer Berlin Heidelberg, 2007, pp. \n235–249. \n[22] N.-S. Vu and A. Caplier, “Enhanced Patterns of \nOriented Edge Magnitudes for Face Recognition and Image \nMatching,” IEEE Trans. Image Process., vol. 21, no. 3, pp. \n1352–1365, Mar. 2012. \n[23] S. U. Hussain, T. Napoléon, and F. Jurie, Face \nRecognition using Local Quantized Patterns. BMVC, 2012. \n[24] S. Xie, S. Shan, X. Chen, and J. Chen, “Fusing Local \nPatterns of Gabor Magnitude and Phase for Face Recognition,” \nIEEE Trans. Image Process., vol. 19, no. 5, pp. 1349–1361, \nMay 2010. \n[25] N.-S. Vu, “Exploring Patterns of Gradient \nOrientations and Magnitudes for Face Recognition,” IEEE \nTrans. Inf. Forensics Secur., vol. 8, no. 2, pp. 295–304, Feb. \n2013. \n[26] Z. Chai, Z. Sun, H. Mendez-Vazquez, R. He, and T. \nTan, “Gabor Ordinal Measures for Face Recognition,” IEEE \nTrans. Inf. Forensics Secur., vol. 9, no. 1, pp. 14–26, Jan. \n2014. \n  \n",
            "id": 18078517,
            "identifiers": [
                {
                    "identifier": "29551946",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "10.1109/apsipa.2015.7415375",
                    "type": "DOI"
                },
                {
                    "identifier": "192176389",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:arxiv.org:1507.02049",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "1507.02049",
                    "type": "ARXIV_ID"
                }
            ],
            "title": "DCTNet : A Simple Learning-free Approach for Face Recognition",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:arxiv.org:1507.02049"
            ],
            "publishedDate": "2015-09-29T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1507.02049"
            ],
            "updatedDate": "2021-08-03T16:53:39",
            "yearPublished": 2015,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1507.02049"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/18078517"
                }
            ]
        },
        {
            "acceptedDate": "2015-07-30T00:00:00",
            "arxivId": "1507.03468",
            "authors": [
                {
                    "name": "Kuznetsov, N. V."
                },
                {
                    "name": "Kuznetsova, O. A."
                },
                {
                    "name": "Leonov, G. A."
                },
                {
                    "name": "Neittaanmaki, P."
                },
                {
                    "name": "Yuldashev, M. V."
                },
                {
                    "name": "Yuldashev, R. V."
                }
            ],
            "contributors": [
                "N. V."
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/284321815"
            ],
            "createdDate": "2015-09-24T01:04:18",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2015-05-01T00:00:00",
            "abstract": "Nonlinear analysis of the classical phase-locked loop (PLL) is a challenging\ntask. In classical engineering literature simplified mathematical models and\nsimulation are widely used for its study. In this work the limitations of\nclassical engineering phase-locked loop analysis are demonstrated, e.g., hidden\noscillations, which can not be found by simulation, are discussed. It is shown\nthat the use of simplified dynamical models and the application of simulation\nmay lead to wrong conclusions concerning the operability of PLL-based circuits",
            "documentType": "research",
            "doi": "10.1109/iscas.2015.7168688",
            "downloadUrl": "http://arxiv.org/abs/1507.03468",
            "fieldOfStudy": "computer science",
            "fullText": "Limitations of the classical phase-locked loop analysis\nKuznetsov N. V., Kuznetsova O. A., Leonov G. A., Neittaanma¨ki P., Yuldashev M. V., Yuldashev R. V.\nAbstract— Nonlinear analysis of the classical phase-locked\nloop (PLL) is a challenging task. In classical engineering\nliterature simplified mathematical models and simulation are\nwidely used for its study. In this work the limitations of classical\nengineering phase-locked loop analysis are demonstrated, e.g.,\nhidden oscillations, which can not be found by simulation, are\ndiscussed. It is shown that the use of simplified dynamical\nmodels and the application of simulation may lead to wrong\nconclusions concerning the operability of PLL-based circuits.\nI. INTRODUCTION\nThe Phase locked-loop (PLL) circuits were invented in\nthe first half of the twentieth century and nowadays are\nwidely used in modern telecommunications and computers.\nPLL is essentially a nonlinear control system and its real\nmodel is described by a nonlinear nonautonomous system\nof differential equations (mathematical model in the signal\nspace). In practice, simulation and simplified mathematical\nmodels are widely used for the analysis of PLL-based circuits\n[1]–[3].\nIn the following it will be shown that 1) the use of\nsimplified mathematical models and 2) the application of non\nrigorous methods of analysis (e.g., simulation) may lead to\nwrong conclusions concerning the operability of real model\nof classical PLL.\nII. SIMULATION OF THE CLASSICAL PHASE-LOCKED\nLOOP IN MATLAB SIMULINK\nConsider the classical PLL nonlinear models in the signal\nand signal’s phase spaces [4]–[9].\n• Real model of the classical PLL in the signal space\n(Fig. 1) or its nonlinear mathematical model in the sig-\nnal space (corresponds to the SPICE-level simulation):\nx˙ = Ax+bϕ(t), ϕ(t) = sin(θ1(t))cos(θ2(t))\nθ˙1 ≡ ω1, θ˙2 = ωfree2 +L(c∗x)+Lhϕ(t).\n(1)\n• Model of the classical PLL in signal’s phase space\n(Fig. 2); system (1) with averaged ϕ(t)≈ ϕ(θ∆(t)) gives\nthe nonlinear mathematical model in signal’s phase\nspace:\nx˙ = Ax+bϕ(θ∆),\nθ˙∆ = ω∆−L(c∗x)−Lhϕ(θ∆),\nθ∆(t) = θ1(t)−θ2(t), ω∆ ≡ ω1−ωfree2 .\n(2)\nKuznetsov N.V., Kuznetsova O.A., Leonov G.A., Neittaanmaki P., Yulda-\nshev M.V., Yuldashev R.V., Limitations of the classical phase-locked loop\nanalysis, Proceedings of International Symposium on Circuits and Systems\n(ISCAS), IEEE, 2015, pp. 533-536\nFaculty of Mathematics and Mechanics, Saint-Petersburg State Univer-\nsity, Russia; Dept. of Mathematical Information Technology, University of\nJyva¨skyla¨, Finland; email: nkuznetsov239@gmail.com\nFig. 1. Real model of the classical PLL in the signal space\nFig. 2. Simplified model of the classical PLL in signal’s phase space\nLet us construct MatLab Simulink model, which corre-\nsponds to the model in the signal space (see Fig. 3).\nFig. 3. Simulink realization of the real model in the signal space\nHere all elements are standard blocks from Simulink Library\nexcept for the VCO. The VCO subsystem is shown in Fig. 4.\nFig. 4. Simulink realization of the VCO for the real model\nThe VCO subsystem consists of one input, which is amplified\nby L (Gain block). The integration of the sum of amplified\ninput signal and the VCO free-running frequency omega free\nforms the phase of the VCO output. The VCO output\ncorresponds to cos(·).\nNow consider MatLab Simulink model, which corresponds\nto the model in signal’s phase space (see Fig. 5).\nar\nX\niv\n:1\n50\n7.\n03\n46\n8v\n1 \n [m\nath\n.D\nS]\n  1\n3 J\nul \n20\n15\nFig. 5. Simulink realization of the model in signal’s phase space\nThe PD subsystem is shown in Fig. 6.\nFig. 6. Simulink realization of the PD in signal’s phase space\nThe VCO subsystem in signal’s phase space is shown in\nFig. 7. The VCO output in signal’s phase space corresponds\nto θ2(t).\nFig. 7. Simulink realization of the VCO in signal’s phase space\nA. Simulation parameters and examples\nConsider a passive lead-lag loop filter with the transfer\nfunction F(s) = 1+sτ21+s(τ1+τ2) , τ1 = 0.0448, τ2 = 0.0185 and the\ncorresponding parameters A = − 1τ1+τ2 , b = 1−\nτ2\nτ1+τ2\n, c =\n1\nτ1+τ2\n, h = τ2τ1+τ2 . The input signal frequency is ω1 = 100000,\ninitial phase is zero: θ1(0) = 0, and the VCO input gain\nL = 250.\nExample 1: This example shows the importance of initial\nstate of filter (see Fig. 8): while the real model (see Fig. 1)\nwith nonzero initial state of loop filter x0 = 0.18 does not\nacquire lock (black color), the same real model with zero\ninitial state of loop filter x(0) = 0 acquires lock (red color).\nHere the VCO free-running frequency ωfree2 = 100000−95.\nExample 2: This example shows that the initial phase\ndifference θ1(0)−θ2(0) between the VCO signal and input\nsignal may affect stability of the classical PLL. In Fig. 9\nthe real model (see Fig. 1) with zero initial phase difference\nacquire lock (red color), the same real model with nonzero\ninitial phase difference θ∆(0) = pi is out of lock (black color).\nHere the VCO free-running frequency ωfree2 = 100000− 95\nand the initial state of loop filter is x0 = 0.01.\nExamples 1 and 2 shows that while the term “initial\nfrequency” (without an explanation) is sometimes used in-\nstead of the the term “free-running frequency” in engineering\ndefinitions of various stability ranges, it may lead to a mis-\nunderstanding (see corresponding discussion in [10], [11]).\nExample 3: This example shows that the PLL model in\nsignal’s phase space may not be equivalent to the PLL real\nFig. 8. Loop filter output g(t) for real model with nonzero initial state of\nloop filter (red), real model with zero initial state of loop filter (black).\nFig. 9. Loop filter output g(t) for real model with nonzero initial phase\ndifference (black), real model with zero initial phase difference (red).\nmodel in the signal space. In Fig. 10 the real model (see\nFig. 1) does not acquire lock (red color), the equivalent\nsignal’s phase space model acquires lock (black color). Here\nthe VCO free-running frequency ωfree2 = 100000− 95, the\ninitial state of loop filter is x0 = 0.017, and the initial phase\ndifference θ∆(0) = 2.276.\nExample 4: These examples shows the importance of\nanalytic methods for investigation of PLL stability. More\nprecisely, it is shown that the simulation may lead to wrong\nresults. In Fig. 11 the PLL model in signal’s phase space\nsimulated with relative tolerance “1e-3” does not acquire\nlock (black color), but the PLL model in signal’s phase\nspace simulated with standard parameters (relative tolerance\nset to “auto”) acquires lock (red color)1. Here the input\nsignal frequency is 10000, the VCO free-running frequency\nωfree2 = 10000− 178.9, the VCO input gain is L = 500, the\ninitial state of loop filter is x0 = 0.1318, and the initial\nphase difference is θ∆(0) = 0. Consider now a phase portrait\n(the loop filter state x versus the phase difference θ∆)\n1 See, e.g., the corresponding internal time step parameter in PSpice\n[http://www.stuffle.net/references/PSpice help/tran.html]. In [12] the SIMet-\nrics SPICE model of the two-phase PLL with lead-lag filter gives two es-\nsentially different results with default sampling step and minimum sampling\nstep set to 1m.\nFig. 10. Loop filter output g(t) for signal’s phase model (black), real\nmodel (red).\nFig. 11. Loop filter output g(t) for signal’s phase space model with\nstandard integration parameters (red), signal’s phase space model with\nrelative tolerance set to “1e-3”(black).\ncorresponding to signal’s phase model (see Fig. 12). The\nsolid blue line in Fig. 12 corresponds to the trajectory with\nthe loop filter initial state x(0) = 0.2206 and the VCO phase\nshift −6.808 rad. This line tends to the periodic trajectory,\ntherefore it will not acquire lock.\nThe solid red line corresponds to the trajectory with the\nloop filter initial state x(0) = 0.187386698333130 and the\nVCO initial phase 12.938118990628919. This trajectory lies\njust under the unstable periodic trajectory and tends to a\nstable equilibrium. In this case PLL acquires lock.\nAll trajectories between stable and unstable periodic tra-\njectories tend to the stable one (see, e.g., a solid green line).\nTherefore, if the gap between stable and unstable periodic\ntrajectories is smaller than the discretization step, the nu-\nmerical procedure may slip through the stable trajectory. In\nother words, the simulation will show that the PLL acquires\nlock, but in reality it is not the case. The considered case\ncorresponds to the coexisting attractors (one of which is\nso-called hidden oscillation) and the bifurcation of birth of\nsemistable trajectory [13], [14].\nAn oscillation in a dynamical system can be easily lo-\ncalized numerically if the initial conditions from its open\nneighborhood lead to long-time behavior that approaches the\noscillation. Thus, from a computational point of view, it is\nFig. 12. Phase portrait of the classical PLL with stable and unstable\nperiodic trajectories\nnatural to suggest the following classification of attractors,\nbased on the simplicity of finding the basin of attraction\nin the phase space [13], [15]–[17]: An attractor is called a\nhidden attractor if its basin of attraction does not intersect\nwith small neighborhoods of equilibria, otherwise it is called\na self-excited attractor.\nFor a self-excited attractor its basin of attraction is con-\nnected with an unstable equilibrium and, therefore, self-\nexcited attractors can be localized numerically by the stan-\ndard computational procedure, in which after a transient\nprocess a trajectory, started from a point of an unstable\nmanifold in a neighborhood of an unstable equilibrium, is\nattracted to the state of oscillation and traces it. Thus self-\nexcited attractors can be easily visualized.\nIn contrast, for a hidden attractor its basin of attraction is\nnot connected with unstable equilibria. For example, hidden\nattractors are attractors in the systems with no equilibria\nor with only one stable equilibrium (a special case of\nmultistable systems and coexistence of attractors).\nIII. CONCLUSION\nThe derivation of mathematical model in signal’s phase\nspace and the use of the results of its analysis to draw\nconclusions about the behavior of real model in the signal\nspace have need for a rigorous foundation. But the attempts\nto justify analytically the reliability of conclusions, based\non such engineering approaches, and to study the nonlinear\nmodels of PLL-based circuits are quite rare in the modern\nengendering literature [18]. One of the reasons is that “non-\nlinear analysis techniques are well beyond the scope of most\nundergraduate courses in communication theory” [3].\nThe examples considered in the paper are the motivation to\nuse rigorous analytical methods for the analysis of nonlinear\nPLL models (1)-(2). Some analytical tools can be found in\n[8], [19]–[24].\nNote once more that various simplifications and the anal-\nysis of linearized models of control systems may result in\nincorrect conclusions (see, e.g., the counterexamples to the\nfilter hypothesis, Aizerman’s and Kalman’s conjectures on\nthe absolute stability of nonlinear control systems [13], [25],\nand the Perron effects of the largest Lyapunov exponent sign\nreversals [26], etc.).\nIn the work it is shown that 1) the consideration of\nsimplified models, constructed intuitively by engineers and\n2) the application of non-rigorous methods of analysis (e.g.,\nsimulation and linearization) can lead to wrong conclusions\nconcerning the operability of the classical phase-locked loop.\nSimilar examples for nonlinear Costas loop models can be\nfound in [27]–[31].\nACKNOWLEDGEMENTS\nAuthors were supported by Saint-Petersburg State Univer-\nsity and Russian Scientific Foundation. The authors would\nlike to thank Roland E. Best (the founder of Best Engineering\ncompany, Switzerland; the author of the bestseller on PLL-\nbased circuits [1]) for valuable discussions.\nREFERENCES\n[1] R. Best, Phase-Lock Loops: Design, Simulation and Application,\n6th ed. McGraw-Hill, 2007.\n[2] D. Pederson and K. Mayaram, Analog Integrated Circuits for Com-\nmunication: Principles, Simulation and Design. Springer, 2008.\n[3] W. Tranter, T. Bose, and R. Thamvichai, Basic Simulation Models of\nPhase Tracking Devices Using MATLAB, ser. Synthesis lectures on\ncommunications. Morgan & Claypool, 2010.\n[4] A. Viterbi, Principles of coherent communications. New York:\nMcGraw-Hill, 1966.\n[5] F. Gardner, Phase-lock techniques. New York: John Wiley & Sons,\n1966.\n[6] G. A. Leonov, N. V. Kuznetsov, M. V. Yuldahsev, and R. V. Yuldashev,\n“Analytical method for computation of phase-detector characteristic,”\nIEEE Transactions on Circuits and Systems - II: Express Briefs,\nvol. 59, no. 10, pp. 633–647, 2012.\n[7] G. A. Leonov, N. V. Kuznetsov, M. V. Yuldashev, and R. V. Yuldashev,\n“Nonlinear dynamical model of Costas loop and an approach to the\nanalysis of its stability in the large,” Signal processing, vol. 108, pp.\n124–135, 2015.\n[8] G. A. Leonov and N. V. Kuznetsov, Nonlinear Mathematical Models\nOf Phase-Locked Loops. Stability and Oscillations. Cambridge\nScientific Press, 2014.\n[9] R. E. Best, N. V. Kuznetsov, G. A. Leonov, M. V. Yuldashev, and R. V.\nYuldashev, “Simulation of analog Costas loop circuits,” International\nJournal of Automation and Computing, vol. 11, no. 6, pp. 571–579,\n2014, 10.1007/s11633-014-0846-x.\n[10] N. Kuznetsov, G. Leonov, M. Yuldashev, and R. Yuldashev, “Rigorous\nmathematical definitions of the hold-in and pull-in ranges for phase-\nlocked loops,” in 1st IFAC Conference on Modelling, Identification and\nControl of Nonlinear Systems. IFAC Proceedings Volumes (IFAC-\nPapersOnline), 2015, pp. 720–723.\n[11] N. V. Kuznetsov, G. A. Leonov, M. V. Yuldashev, and R. V. Yuldashev,\n“Hold-in, pull-in, and lock-in ranges of PLL-based circuits: rigorous\nmathematical definitions and limitations of classical theory,” ArXiv\ne-prints, 2015.\n[12] G. Bianchi, N. Kuznetsov, G. Leonov, M. Yuldashev, and R. Yuldashev,\n“Limitations of PLL simulation: hidden oscillations in SPICE analy-\nsis,” arXiv:1506.02484, 2015, http://arxiv.org/pdf/1506.02484.pdf.\n[13] G. A. Leonov and N. V. Kuznetsov, “Hidden attractors in dynamical\nsystems. From hidden oscillations in Hilbert-Kolmogorov, Aizerman,\nand Kalman problems to hidden chaotic attractors in Chua circuits,”\nInternational Journal of Bifurcation and Chaos, vol. 23, no. 1, 2013,\nart. no. 1330002.\n[14] N. Kuznetsov, G. Leonov, M. Yuldashev, and R. Yuldashev, “Nonlinear\nanalysis of classical phase-locked loops in signal’s phase space,” IFAC\nProceedings Volumes (IFAC-PapersOnline), vol. 19, pp. 8253–8258,\n2014.\n[15] N. V. Kuznetsov, G. A. Leonov, and V. I. Vagaitsev, “Analytical-\nnumerical method for attractor localization of generalized Chua’s\nsystem,” IFAC Proceedings Volumes (IFAC-PapersOnline), vol. 4,\nno. 1, pp. 29–33, 2010.\n[16] G. A. Leonov, N. V. Kuznetsov, and V. I. Vagaitsev, “Localization\nof hidden Chua’s attractors,” Physics Letters A, vol. 375, no. 23, pp.\n2230–2233, 2011.\n[17] ——, “Hidden attractor in smooth Chua systems,” Physica D: Non-\nlinear Phenomena, vol. 241, no. 18, pp. 1482–1486, 2012.\n[18] D. Abramovitch, “Phase-locked loops: A control centric tutorial,” in\nAmerican Control Conf. Proc., vol. 1, 2002, pp. 1–15.\n[19] A. Gelig, G. Leonov, and V. Yakubovich, Stability of Nonlinear\nSystems with Nonunique Equilibrium (in Russian). Nauka, 1978,\n(English transl. 2004, World Scientific).\n[20] G. A. Leonov, V. Reitmann, and V. B. Smirnova, Nonlocal Methods\nfor Pendulum-like Feedback Systems. Stuttgart-Leipzig: Teubner\nVerlagsgesselschaft, 1992.\n[21] J. Stensby, Phase-Locked Loops: Theory and Applications, ser. Phase-\nlocked Loops: Theory and Applications. Taylor & Francis, 1997.\n[22] A. Suarez and R. Quere, Stability Analysis of Nonlinear Microwave\nCircuits. Artech House, 2003.\n[23] N. Margaris, Theory of the Non-Linear Analog Phase Locked Loop.\nNew Jersey: Springer Verlag, 2004.\n[24] J. Kudrewicz and S. Wasowicz, Equations of phase-locked loop.\nDynamics on circle, torus and cylinder. World Scientific, 2007.\n[25] V. O. Bragin, V. I. Vagaitsev, N. V. Kuznetsov, and G. A. Leonov,\n“Algorithms for finding hidden oscillations in nonlinear systems. The\nAizerman and Kalman conjectures and Chua’s circuits,” Journal of\nComputer and Systems Sciences International, vol. 50, no. 4, pp. 511–\n543, 2011.\n[26] N. V. Kuznetsov and G. A. Leonov, “On stability by the first approx-\nimation for discrete systems,” in 2005 International Conference on\nPhysics and Control, PhysCon 2005, vol. Proceedings Volume 2005.\nIEEE, 2005, pp. 596–599.\n[27] N. Kuznetsov, O. Kuznetsova, G. Leonov, P. Neittaanmaki, M. Yul-\ndashev, and R. Yuldashev, “Simulation of nonlinear models of QPSK\nCostas loop in Matlab Simulink,” in 2014 6th International Congress\non Ultra Modern Telecommunications and Control Systems and Work-\nshops (ICUMT), vol. 2015-January. IEEE, 2014, pp. 66–71.\n[28] N. Kuznetsov, O. Kuznetsova, G. Leonov, S. Seledzhi, M. Yuldashev,\nand R. Yuldashev, “BPSK Costas loop: Simulation of nonlinear models\nin Matlab Simulink,” in 2014 6th International Congress on Ultra\nModern Telecommunications and Control Systems and Workshops\n(ICUMT), vol. 2015-January. IEEE, 2014, pp. 83–87.\n[29] N. Kuznetsov and G. Leonov, “Hidden attractors in dynamical sys-\ntems: systems with no equilibria, multistability and coexisting attrac-\ntors,” IFAC Proceedings Volumes (IFAC-PapersOnline), vol. 19, pp.\n5445–5454, 2014.\n[30] E. V. Kudryasoha, O. A. Kuznetsova, N. V. Kuznetsov, G. A. Leonov,\nS. M. Seledzhi, M. V. Yuldashev, and R. V. Yuldashev, “Nonlinear\nmodels of BPSK Costas loop,” ICINCO 2014 - Proceedings of the\n11th International Conference on Informatics in Control, Automation\nand Robotics, vol. 1, pp. 704–710, 2014.\n[31] R. Best, N. Kuznetsov, O. Kuznetsova, G. Leonov, M. Yuldashev,\nand R. Yuldashev, “A short survey on nonlinear models of the classic\nCostas loop: rigorous derivation and limitations of the classic analysis,”\nin American Control Conference (ACC). IEEE, 2015, pp. 1296–1302,\nhttp://arxiv.org/pdf/1505.04288v1.pdf.\n",
            "id": 18079100,
            "identifiers": [
                {
                    "identifier": "1890716699",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "29553365",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "284321815",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:arxiv.org:1507.03468",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "1507.03468",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "10.1109/iscas.2015.7168688",
                    "type": "DOI"
                }
            ],
            "title": "Limitations of the classical phase-locked loop analysis",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "1890716699",
            "oaiIds": [
                "oai:arxiv.org:1507.03468"
            ],
            "publishedDate": "2015-07-13T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1507.03468"
            ],
            "updatedDate": "2021-08-03T14:08:51",
            "yearPublished": 2015,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1507.03468"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/18079100"
                }
            ]
        },
        {
            "acceptedDate": "2015-09-03T00:00:00",
            "arxivId": "1507.05268",
            "authors": [
                {
                    "name": "Dalal, Gal"
                },
                {
                    "name": "Mannor, Shie"
                }
            ],
            "contributors": [
                "Gal"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/193018037"
            ],
            "createdDate": "2015-09-24T01:10:15",
            "dataProviders": [
                {
                    "id": 144,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/144",
                    "logo": "https://api.core.ac.uk/data-providers/144/logo"
                },
                {
                    "id": 4786,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/4786",
                    "logo": "https://api.core.ac.uk/data-providers/4786/logo"
                }
            ],
            "depositedDate": "2015-06-01T00:00:00",
            "abstract": "In this work we solve the day-ahead unit commitment (UC) problem, by\nformulating it as a Markov decision process (MDP) and finding a low-cost policy\nfor generation scheduling. We present two reinforcement learning algorithms,\nand devise a third one. We compare our results to previous work that uses\nsimulated annealing (SA), and show a 27% improvement in operation costs, with\nrunning time of 2.5 minutes (compared to 2.5 hours of existing\nstate-of-the-art).Comment: Accepted and presented in IEEE PES PowerTech, Eindhoven 2015, paper\n  ID 46273",
            "documentType": "research",
            "doi": "10.1109/ptc.2015.7232646",
            "downloadUrl": "http://arxiv.org/abs/1507.05268",
            "fieldOfStudy": "computer science",
            "fullText": "Reinforcement Learning for\nthe Unit Commitment Problem\nGal Dalal\nShie Mannor\nDepartment of Electrical Engineering\nTechnion\nHaifa, Israel\ngald@tx.technion.ac.il\nshie@ee.technion.ac.il\nAbstract—In this work we solve the day-ahead unit commit-\nment (UC) problem, by formulating it as a Markov decision\nprocess (MDP) and finding a low-cost policy for generation\nscheduling. We present two reinforcement learning algorithms,\nand devise a third one. We compare our results to previous\nwork that uses simulated annealing (SA), and show a 27%\nimprovement in operation costs, with running time of 2.5 minutes\n(compared to 2.5 hours of existing state-of-the-art).\nIndex Terms—Power generation dispatch, Learning (artificial\nintelligence), Optimal scheduling, Optimization methods.\nI. INTRODUCTION\nUnit commitment (UC) is the process of determining the\nmost cost-effective combination of generating units and their\ngeneration levels within a power system to meet forecasted\nload and reserve requirements, while adhering to genera-\ntor and transmission constraints [2]. This is a non-linear,\nmixed-integer combinatorial optimization problem [3]. Low-\ncost solutions to this problem will directly translate into low\nproduction costs for power utilities. As the size of the problem\nincreases, it becomes a very complex, hard to solve problem\n[4].\nMultiple optimization approaches have been applied over the\npast years, such as the priority ordering methods [9], [10],\ndynamic programming [11], Lagrangian relaxation [13], the\nbranch-and-bound method [15], and the integer and mixed-\ninteger programming [16], [17]. Other, more recent methods\nare from the field of artificial intelligence, such as the expert\nsystems [18], neural networks [19], fuzzy logic [20], genetic\nalgorithms [21], and simulated annealing [1].\nMany of these approaches are either purely heuristic (e.g.\npriority ordering) or semi-heuristic (e.g. simulated annealing)\n, thus are often very sensitive to choice of architecture,\nmanual parameter tuning, and different cost functions. On\nthe other hand, analytical methods can also introduce critical\nshortcomings. The branch-and-bound algorithm, for instance,\nsuffers from an exponential growth in execution time with\nthe size of the UC problem [14], [15]. In addition, using\napproximations for making it tractable for large scale systems\ncauses solutions to be highly sub-optimal.\nTherefore in our work, we take an analytical approach to\nthe problem, while assuring it will not become intractable\nnor highly suboptimal in large scale systems. We use a\nMarkov Decision Process (MDP) framework. MDPs are used\nto describe numerous phenomena in many fields of science [7].\nSuch a model is aimed to describe a decision making process,\nwhere outcomes of the process are partly random and partly\nunder the control of the decision maker.\nIn this work we assume that generation cost functions of\nthe different generators are known to the decision maker.\nWe note that this is often not the case with European sys-\ntem operators, since in a European competitive electricity\nmarket, cost information is not available. However, in many\nother cases this information is indeed available, such as in\nsome north American TSOs, and generation companies with\nmultiple generation units (such a company would not know\nthe characteristics of the power system, nevertheless it is not\nproblematic since they do not play a role in our formulation).\nIn addition, the UC problem can easily be extended to generate\nproduction schedules in a competitive market environment [5].\nAnother paper shows the framework in which a traditional\ncost-based unit commitment tool can be used to assist bidding\nstrategy decisions to a day-ahead electricity pool market [6].\nIn general, European TSOs can approximate generation costs\nbased on historical data (that include past and present bids\nthey receive from generators) and market simulation. Also, in\nfuture work, the uncertainty in these approximations can be\nnaturally expressed in our MDP model.\nThe rest of the paper is organized as follows. Section II\nformulates the unit commitment problem. We then present\nour MDP model for the UC problem in section III , and give\nan introduction to reinforcement learning in section IV. The\nalgorithms we use are presented in section V. Then, in section\nVI we show numerical tests of our methods. Lastly, in section\nVII we summarize our work.\nII. UNIT COMMITMENT PROBLEM FORMULATION\nThe problem is formulated as the following constrained\noptimization program.\nar\nX\niv\n:1\n50\n7.\n05\n26\n8v\n1 \n [c\ns.A\nI] \n 19\n Ju\nl 2\n01\n5\nA. Objective\nThe objective is to find a feasible plan with minimal cost\nfor operating generators to meet client demands –\nmin\nαi(t),Pi(t),∀i,t\nT∑\nt=1\nN∑\ni=1\n[αi(t)Ci(Pi(t)) + (1)\nαi(t)[1− αi(t− 1)]SCi(toffi)].\nWhere:\nai(t) = 1 when unit i is turned on at time ti, and αi(t) = 0\notherwise.\nPi(t) is the injected power [MW ] in unit i at time t.\nCi(P ) is the cost [$] of injecting power P in unit i .\nSCi(toffi) is the start-up cost [$] of unit i after it has been\noff for a time period of toffi .\nB. Constraints\nAny feasible solution is subject to the following constraints:\n• Load balance –\n∀t :\nN∑\ni=1\n(αi(t)Pi(t)) = D(t). (2)\n• Generation limits –\n∀i, t : αi(t)Pmini ≤ Pi(t) ≤ αi(t)Pmaxi . (3)\n• Set generation limits –\n∀t : (\nN∑\ni=1\nαi(t)Pmini) ≤ D(t), (4)\n(\nN∑\ni=1\nαi(t)Pmaxi) ≥ D(t) +R(t).\n• Minimum up/down time –\n∀i : toffi ≥ tdowni , (5)\ntoni ≥ tupi .\nWhere:\nD(t) is the demand at time t.\nR(t) is the needed power reserve at time t.\nPmini , Pmaxi are the minimal and maximal power injections\nin unit i.\ntoffi , toni are the minimal time periods before turning unit i\non/off.\nC. Costs\n• Generation Cost – quadratic function of the power gen-\nerated by that unit:\nCi(Pi) = aiP\n2\ni + biPi + ci. (6)\n• Start-up cost – an exponentially dependent function of\nthe number of hours a unit has been down:\nSCi(toffi) = ei exp(−gitoffi) + fi exp(−hitoffi). (7)\nA graphical example of generation (A) and start-up (B) costs\nof a specific generator (with the parameters used in the\nexperiments section) is displayed in Figure 1.\nFig. 1. (A) shows the generation cost of a specific generator, and (B) shows\nthe start-up cost of that generator, as a function of the time it was off\nIII. MARKOV DECISION PROCESS APPROACH\nFinding a global optimum is intractable for this non\nconvex, mixed integer-quadratic problem. Therefore, unlike\nin [1], where a direct search in the solution space was\nperformed, we suggest an alternative approach: decomposing\nthe objective into a sequential decision making process. We\nuse a Markov Decision Process 4-tuple (S,A, P,R) [7] to\nmodel the system’s dynamics. Briefly, in this model at each\ntime step, the process is in some state s, an action a is\ntaken, the system transitions to a next state s′ according to\na transition kernel P (s, a, s′), and a reward R(s, a, s′) is\ngranted. Thus, the next state s′ depends only on the current\nstate s and the decision maker’s action a.\nA. State-Space\nThe system’s state can be fully described by the on/off time\nof each of the N generators, and the time of the day (negative\nvalues indicate off time):\nS = {−24,−23, . . . ,−1, 1, 2, . . . , 24}N × {1, . . . , 24}.\nB. Action Space\nEach unit can be turned/kept on, or turned/kept off:\nA = {0, 1}N .\nC. Reward\nAt each time step, the reward is (minus) the cost of operation\nof the N machines:\nR(s, a, s′) = −\nN∑\ni=1\n[I[s′\ni\n>0]Ci(Pi) + I[s′\ni\n>0]I[si<0]SCi(si)].\nThe power injections Pi are chosen by solving the appropriate\nconstrained quadratic program (generation cost is quadratic).\nBy maximizing the undiscounted cumulative reward of the\nMDP, we minimize the original objective.\nD. Transition Kernel\nTransition is deterministic: f(s, a) = s′. The transition\nfunction restricts the process to satisfy the constraints of the\noptimization problem.\nA transition example is presented in Figure 2.\nFig. 2. Example for state transition - f(s, a) = s′. Generators are turned/kept\non or off when an action of 1 or 0 is taken. Time is also represented in the\nstate.\nIV. REINFORCEMENT LEARNING\nA policy is a mapping between a state-space S and an\naction-space A. Given a policy, we know what action to\nperform at each state of the system.\nFor the defined MDP, our goal is the following: Find an\noptimal policy pi∗ : S → A s.t:\npi∗ = arg max pi∈Π\nT∑\nt=1\nR(st, pi(st), f(st, pi(st))). (8)\nWhere Π is the space of all possible policies.\nReinforcement Learning (RL) [8] is a field in Machine Learn-\ning that studies algorithms that learn by interacting with the\nenvironment. The learning is done for states and actions. For\neach state s, given a policy pi, a state value vpi(s) is defined\nas:\nvpi(s) =\nT∑\nt=1\nR(st, pi(st), f(st, pi(st))) for s1 = s. (9)\nV. REINFORCEMENT LEARNING ALGORITHMIC\nSOLUTIONS\nIn this section we present three different reinforcement\nlearning algorithms for solving 8.\nA. Algorithm 1 – Approximate Policy Iteration (API) using\nClassification\nAn extension to the state value vpi(s) defined above, is the\nstate-action value function Qpi(s, a), which denotes the value\nof performing action a (regardless of the policy pi), and only\nafter that – following policy pi. In our first algorithm we use\nthe state-action value function. This function is defined the\nset of all (s, a) pairs, which is of size |S| · |A|.\n1) Approximation: Our state-space grows exponentially\nwith N : |S| = 24·48N , as well as the action-space: |A| = 2N .\nAlready for N ≥ 4, it is impossible to find the exact value\nfor each state-action pair. We therefore use an approximation\nmethod for evaluating the state-action value function Qpi(s, a).\nWe use feature-based regression, which significantly lowers\nthe dimension of Qpi(·, ·) from |S| · |A| to dim(φ(s, a)), the\ndimension of the feature vector φ(s, a). We use 4 binary fea-\ntures for each generator i, for each of the possible ’interesting’\nzones it can be in:\nsi < −toffi ,\n−toffi ≤ si < 0,\n0 < si ≤ toni ,\ntoni < si .\nThe features are then duplicated N times and zeroed out\nat indices where the action vector is 0. The result is again\nduplicated into two – for distinguishing between (s, a) pairs\nthat will lead to a catastrophe (transition to infeasible states).\nWe end up with a feature vector φ(s, a) of dimension that is\nonly quadratic in N : dim(φ(s, a)) = 2 · 4 ·N2.\n2) Policy Iteration Algorithm: The basic algorithm is policy\niteration [8]. This well-known algorithm iterates between two\nstages: evaluation of the states’ values under a fixed policy, and\nthe improvement of the policy using the learned values. We\nperform the evaluation using the SARSA [8] algorithm (with\nepsilon-greedy exploration), and the improvement is simply\ndone using the following maximization (for step k):\npik(s) = arg max\na∈A\nQk(s, a) = arg max\na∈A\nφ(s, a)Twk. (10)\n3) Policy Representation: The biggest challenge in en-\nabling policy iteration for our problem is the choice of policy\nrepresentation. On the one hand, the policy should be defined\nfor all states s ∈ S. On the other hand, it can practically only\nbe trained using a very small fraction of this huge state-space.\nAlso, its output is a selection from the enormous space of\nactions.\nTo handle the above difficulties, we chose the policy to be a\nclassifier, that classifies states into actions. This gives rise to a\nlarge-scale multi-class classification task (2N optional classes),\nwhich is considered to be a difficult problem on its own. We\ntackle that by using a hierarchical classifier with a tree-based\nstructure: Each node classifies an action bit and splits into\ntwo nodes for the next bit. Classification is done in the feature-\nspace. The perceptron algorithm [24] is used a the basic binary\nclassifier (online updates can be made to save memory). A\ndifferent tree is stored per each time-step.\nNote that this is not a decision-tree classifier, but multiple\nbinary hyper-plane based classifiers that are being traversed\nthrough in a sequential manner. The leafs determine the final\naction prediction (encapsulate the path).\nAlgorithm 1 Approximate Policy Iteration using Classification\nInitialize:\nα - SARSA step size\n\u000f - exploration parameter\nNpi - iteration count\npi0 - intial policy\nφ - basis functions\n1: for k = 1 to k = Npi do\n2: wk = SARSA(pik, α, \u000f)\n3: for all s ∈ S do\n4: a∗ = arg maxa∈A φ(s, a)\nTwk\n5: pik = updateClassifier(a\n∗, pik)\n6: end for\n7: end for\n8: return piNpi\nB. Algorithm 2 – Tree Search\nAn MDP can be represented as a tree, where each node\nis a state and each edge corresponds to an action. In our\nproblem, we can theoretically express the tree explicitly, where\nthe edges include the exact reward of the transition since\ntransitions and rewards are deterministic. Let us also denote\nstj so be the j-th state at time-step t. Under this representation,\nFig. 3. Visualisation of algorithm 2 – tree search. Nodes are states, edges\nare transitions with the corresponding rewards.\nfinding an optimal policy pi∗ corresponds to finding the largest\naggregated reward path from the root (initial state s00).\nHowever, since the number of possible paths in the tree is in\nthe order of |A|T = 2N ·T , naively searching the tree for an\noptimal path is intractable for a problem of our size. Therefore\nin our tree search algorithm we limit the time horizon in which\nwe search to be H (H < T ). I.e, tree search seeks a lookahead\npolicy by iterating through all of the possible outcomes in a\nlimited lookahead horizon H . Our algorithm contains the two\nfollowing main components:\n1) Algorithm 2.1: The first part of the algorithm,\nfindBestAction(st, H), recursively searches for the next\nbest action that can be taken from state st, by iterating on\nall possible actions from that state. ”Best” in this case, is\nconsidered with regard to all possible paths in a lookahead\nhorizon of H time-steps ahead. That is, it finds at, the first\naction in the vector a = (at, at+1, . . . , at+H), where\na = arg max\na′∈AH\nt+H∑\nt′=t\nR(st\n′\n, at\n′\n, f(st\n′\n, at\n′\n)).\nAlgorithm 2.1 (am, vm) = findBestAction(st, H)\n1: if H == 0 or t == T − 1 then\n2: return (0, 0)\n3: end if\n4: vm = −∞, am = 0¯\n5: for all a ∈ A do\n6: st+1 = f(st, a)\n7: (at+1, vt+1) = findBestAction(st+1, H − 1)\n8: vt = R(st, a, st+1) + vt+1\n9: if vt ≥ vm then\n10: am = a, vm = v\n11: end if\n12: end for\n13: return (am, vm)\n2) Algorithm 2.2: The second component of our tree search\nalgorithm finds the optimal lookahead policy by initiating\nfindBestPolicy per each time step from t = 0 to t = T −1.\nAlgorithm 2.2 pi = treeSearch(st, H)\n1: for t = 0 to t = T − 1 do\n2: (am, vm) = findBestAction(s\nt, H)\n3: pi(st) = am\n4: st+1 = f(st, am)\n5: end for\n6: return pi\n3) Improve by Sub-sampling: We can take advantage of\na certain property of this problem: it is very unlikely that\nin ”good” (highly rewarding) paths, subsequent actions will\ndiffer significantly from each other. This is both because of the\nhigh start-up costs of generators, and because of the minimal\nup/down time limitation (rapidly switching different machines\non/off can lead to infeasible states, where there aren’t enough\navailable generators to satisfy demand).\nExploiting this property, we added an improvement for our\nalgorithm. Instead of iterating throughout all actions when\nsearching for the best one at time t+1, we only sample small\ndeviations from last best action at time t (denoted as at∗). For\nthe sampling we use a probability density over the action at+1,\nwith an inverse relation to ‖at∗ − at+1‖2. This improvement\nsignificantly reduces the runtime, and can also enable setting\na larger value for H . In the experiments section, we test the\nusability of our improvement and compare it to the original\napproach.\nC. Algorithm 3 – Back Sweep\nOur ”Back Sweep” algorithm is a novel algorithm, inspired\nby the concept in dynamic programming of backtracking from\nthe terminal time and going backwards. That way, we have a\nreliable estimation of the value of future states, and can base\ndecisions correctly based on that knowledge of future values.\nThe main novelty is in sampling ’interesting’ (potentially\nbeneficial) areas of the state-space, and use a nearest neighbour\n(NN) approximation of them in the Bellman update step\n(defined below).\n1) Algorithm 3.1: First of two parts of the algorithm is\nevaluation the optimal value of each sampled state, v∗(s). The\noptimal value is the value of states when using the optimal\npolicy as defined in 8:\nv∗(s) = sup\npi∈Π\nvpi(s). (11)\nIt is found using Bellman’s update step, that lies in the heart\nof the algorithm:\nv∗(s) = max\na∈A\n[R(s, a, s′) + v∗(s′)]. (12)\nAlgorithm 3.1 D = evaluateStates(Ns, s0)\n1: Initialize D = ∅, s˜ = sT\n2: for t = T − 1 to 0 do\n3: St = sampleEnvironment(s˜, Ns)\n4: for i = 1 to Ns do\n5: vˆ∗t(sti) = maxa∈A[R(s\nt\ni, a, f(s\nt\ni, a)) +\nvˆ∗t+1(NN(f(sti, a),D))]\n6: end for\n7: D = D ∪ {(St, Vˆ∗t)}\n8: s˜ = arg maxs Vˆ\n∗t\n(s)\n9: end for\n10: return D\n• sampleEnvironment(s˜, Ns) returns Ns samples of\nstates that are ’close’ to s˜. Closeness is quantified using\na metric we defined.\n• NN(s,D) returns the nearest-neighbor state from all\nstates that are in the (s, v) pairs in D.\n2) Algorithm 3.2: The second part of the algorithm will\nproduce a greedy policy via one quick sweep forward, begin-\nning from the initial state s0. This policy is greedy since at\neach step we choose the best possible action, and it is proven\nthat for exact v∗ values, it will also be the optimal policy [8].\nAlgorithm 3.2 pi = findGreedyPolicy(D)\n1: for t = 0 to T − 1 do\n2: at = arg maxa∈A[R(st, a, f(st, a)) +\nvˆ∗t+1(NN(f(sti, a),D))]\n3: pi(st) = at\n4: st+1 = f(st, at)\n5: end for\n6: return pi\nVI. EXPERIMENTS\nIn order to test the performance of the three proposed\nalgorithms, we used Matlab [22] to implement and run them on\na problem setting with N = 12 generators, a 24-hour schedule\n(T = 24), with parameters taken from [1]. In [1], an Adaptive\nSimulated Annealing (SA) technique is used, and a minimal\nobjective of $644,951 is achieved.\nA. Algorithm 1\nAlgorithm 1 only performed well on a smaller setting of\nthe problem (N = 8, T = 12) and was not included in Table\nI. In spite of that, we chose to present algorithm 1 in this\npaper as a baseline. API is a very commonly used algorithm\nin the reinforcement learning literature. On top of that, the\npolicy structure we devised enabled the algorithm the leap\nfrom performing only on a N = 4, T = 8 setting, to the\nN = 8, T = 12 setting.\nWe also find value in understanding its weaknesses - it could\nnot handle a larger scheme due to its forward-looking mecha-\nnism. Since it starts with a random policy, state evaluation is\nvery poor at the beginning (compared to their optimal value),\nand the improvement becomes slow and inefficient throughout\niterations. Magnification of this problem is taking place since\nunlike in infinite horizon formulations, different policies are\nused for different time-steps.\nB. Algorithm 2\nAlgorithm 2 was tested with two different lookahead hori-\nzons: H = 1 and H = 3. The extremely low run-time for\nH = 1 make it the most preferable algorithm for this problem,\nin spite of the small increase in objective cost.\nThe large difference in run times for the two cases is due to\nthe exponential complexity in H .\nThe improved version of algorithm 2, which includes sub-\nsampling of actions, enables a reduction in run-time, while\ncompromising negligible value in the overall cost.\nC. Algorithm 3\nThe terminal state of algorithm 2’s solution is fed as an\ninitial state to algorithm 3, Sampling count used was Ns = 50.\nD. Result Comparison\nTABLE I\nEXPERIMENT RESULTS OF THE DIFFERENT ALGORITHMS\nAlgorithm Objective cost [$] Run-time [min]\nSA in [1] 702,379 N/A\nAdaptive SA in [1] 644,951 145\nTree Search, H=1 512,850 2.5\nTree Search, H=3 512,217 240\nSub-sampled Tree Search, H=3 512,850 85\nBack Sweep 511,500 60\nTable I summarizes the experiment’s results. The solutions\nwe obtain are very similar to each other, all around $512,000\nfor operation cost.\nAlgorithm 2 produces a 27% improvement in objective value\ncompared to the state-of-the-art algorithm presented in [1],\nwhich achieved a minimal objective of $644,951, with only\n2.5 minutes of running time, compared to 2.5 hours in [1].\nVII. SUMMARY\nIn this paper we introduced three algorithms from the field\nof reinforcement learning, one of them novel. We formulated\nthe unit commitment problem as a Markov Decision Process\nand solved it using the three algorithms (successfully with\ntwo).\nThe superior results in the experiments section lead us to\nbelieve that modelling the UC problem as an MDP is highly\nadvantageous over other existing methods, which were men-\ntioned in the introduction section.\nAn additional significant improvement is the option of an im-\nmediate extension for a stochastic environment, which include\nconsideration of uncertainties. Demand, generation capacity,\nand generation costs can be easily modelled as stochastic\nby setting the appropriate probabilistic transition kernel and\nreward function in our existing MDP model. The algorithms\npresented in the paper need not change for obtaining a solution\nfor such a probabilistic version. This transition to an uncertain\nformulation might be very challenging [25], [26], or even\nimpossible when using other optimization methods.\nWe intend to test our algorithms under such uncertainty\nconditions, and possibly to change the formulation in order\nto obtain a risk-averse strategy for the unit commitment. This\ncan be done by including a risk criterion in the objective, that\nwill take into account contingencies, shut-downs, and load\nshedding costs while considering the probabilities of those\nevents to happen.\nREFERENCES\n[1] Dudek, Grzegorz. ”Adaptive simulated annealing schedule to the unit\ncommitment problem.” Electric Power Systems Research 80.4 (2010):\n465-472.\n[2] Padhy, Narayana Prasad. ”Unit commitment-a bibliographical survey.”\nPower Systems, IEEE Transactions on 19.2 (2004): 1196-1205. APA\n[3] Sheble, Gerald B., and George N. Fahd. ”Unit commitment literature\nsynopsis.” Power Systems, IEEE Transactions on 9.1 (1994): 128-135.\n[4] Orero, S. O., and M. R. Irving. ”Large scale unit commitment using a\nhybrid genetic algorithm.” International Journal of Electrical Power &\nEnergy Systems 19.1 (1997): 45-55.\n[5] Arroyo, Jos M., and Antonio J. Conejo. ”Optimal response of a thermal\nunit to an electricity spot market.” Power Systems, IEEE Transactions\non 15.3 (2000): 1098-1104.\n[6] Borghetti, A., et al. ”Using of a cost-based unit commitment algorithm to\nassist bidding strategy decisions.” Power Tech Conference Proceedings,\n2003 IEEE Bologna. Vol. 2. IEEE, 2003.\n[7] Puterman, Martin L. Markov decision processes: discrete stochastic\ndynamic programming. Vol. 414. John Wiley & Sons, 2009.\n[8] Sutton, Richard S., and Andrew G. Barto. Introduction to reinforcement\nlearning. MIT Press, 1998.\n[9] Lee, Fred N. ”Short-term thermal unit commitment-a new method.”\nPower Systems, IEEE Transactions on 3.2 (1988): 421-428.\n[10] Senjyu, Tomonobu, et al. ”A technique for unit commitment with energy\nstorage system.” International Journal of Electrical Power & Energy\nSystems 29.1 (2007): 91-98.\n[11] Snyder, Walter L., H. David Powell, and John C. Rayburn. ”Dynamic\nprogramming approach to unit commitment.” Power Systems, IEEE\nTransactions on 2.2 (1987): 339-348.\n[12] Hargreaves, Jeremy J., and Benjamin F. Hobbs. ”Commitment and\ndispatch with uncertain wind generation by dynamic programming.”\nSustainable Energy, IEEE Transactions on 3.4 (2012): 724-734.\n[13] Virmani, Sudhir, et al. ”Implementation of a Lagrangian relaxation based\nunit commitment problem.” Power Systems, IEEE Transactions on 4.4\n(1989): 1373-1380.\n[14] Cheng, Chuan-Ping, Chih-Wen Liu, and Chun-Chang Liu. ”Unit com-\nmitment by Lagrangian relaxation and genetic algorithms.” Power Sys-\ntems, IEEE Transactions on 15.2 (2000): 707-714.\n[15] Cohen, Arthur I., and Miki Yoshimura. ”A branch-and-bound algorithm\nfor unit commitment.” Power Apparatus and Systems, IEEE Transactions\non 2 (1983): 444-451.\n[16] Carrin, Miguel, and Jos M. Arroyo. ”A computationally efficient mixed-\ninteger linear formulation for the thermal unit commitment problem.”\nPower Systems, IEEE Transactions on 21.3 (2006): 1371-1378.\n[17] Ostrowski, James, Miguel F. Anjos, and Anthony Vannelli. ”Tight\nmixed integer linear programming formulations for the unit commitment\nproblem.” Power Systems, IEEE Transactions on 27.1 (2012): 39-46.\n[18] Ouyang, Z., and S. M. Shahidehpour. ”Short-term unit commitment\nexpert system.” Electric power systems research 20.1 (1990): 1-13.\n[19] Kumar, S. Senthil, and V. Palanisamy. ”A dynamic programming based\nfast computation Hopfield neural network for unit commitment and\neconomic dispatch.” Electric power systems research 77.8 (2007): 917-\n925.\n[20] Saber, Ahmed Yousuf, et al. ”Fuzzy unit commitment solutiona novel\ntwofold simulated annealing approach.” Electric Power Systems Re-\nsearch 77.12 (2007): 1699-1712.\n[21] Dudek, Grzegorz. ”Unit commitment by genetic algorithm with special-\nized search operators.” Electric Power Systems Research 72.3 (2004):\n299-308.\n[22] www.matlab.com\n[23] Bertsekas, Dimitri P., and John N. Tsitsiklis. ”Neuro-dynamic program-\nming: an overview.” Decision and Control, 1995., Proceedings of the\n34th IEEE Conference on. Vol. 1. IEEE, 1995.\n[24] Freund, Yoav, and Robert E. Schapire. ”Large margin classification using\nthe perceptron algorithm.” Machine learning 37.3 (1999): 277-296.\n[25] Jiang, Ruiwei, Jianhui Wang, and Yongpei Guan. ”Robust unit com-\nmitment with wind power and pumped storage hydro.” Power Systems,\nIEEE Transactions on 27.2 (2012): 800-810.\n[26] Aminifar, Farrokh, Mahmud Fotuhi-Firuzabad, and Mohammad\nShahidehpour. ”Unit commitment with probabilistic spinning reserve and\ninterruptible load considerations.” Power Systems, IEEE Transactions on\n24.1 (2009): 388-397.\n",
            "id": 18080802,
            "identifiers": [
                {
                    "identifier": "10.1109/ptc.2015.7232646",
                    "type": "DOI"
                },
                {
                    "identifier": "1507.05268",
                    "type": "ARXIV_ID"
                },
                {
                    "identifier": "1526108348",
                    "type": "MAG_ID"
                },
                {
                    "identifier": "29555165",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "193018037",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:arxiv.org:1507.05268",
                    "type": "OAI_ID"
                }
            ],
            "title": "Reinforcement Learning for the Unit Commitment Problem",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": "1526108348",
            "oaiIds": [
                "oai:arxiv.org:1507.05268"
            ],
            "publishedDate": "2015-07-19T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://arxiv.org/abs/1507.05268"
            ],
            "updatedDate": "2021-08-03T14:28:32",
            "yearPublished": 2015,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "http://arxiv.org/abs/1507.05268"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/18080802"
                }
            ]
        },
        {
            "acceptedDate": "2007-08-22T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Hazas, Michael"
                },
                {
                    "name": "Marsden, Rebecca"
                }
            ],
            "contributors": [],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/70107"
            ],
            "createdDate": "2012-07-02T06:00:27",
            "dataProviders": [
                {
                    "id": 59,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/59",
                    "logo": "https://api.core.ac.uk/data-providers/59/logo"
                }
            ],
            "depositedDate": "2007-07-01T00:00:00",
            "abstract": "Declining enrollments in computer science and related fields are a global concern. This issue's column, by Mike Hazas and Rebecca Marsden of Lancaster University in the UK describes the novel Lancaster Headstart program that uses the excitement of pervasive computing to attract students into the computer science",
            "documentType": "research",
            "doi": "10.1109/mprv.2007.66",
            "downloadUrl": "https://core.ac.uk/download/70107.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "© 2007 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes orfor creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must beobtained from the IEEE.For more information, please see www.ieee.org/web/publications/rights/index.html.MOBILE AND UBIQUITOUS SYSTEMSwww.computer.org/pervasivePervasive Computingand Tomorrow’s Computer ScientistsMike Hazas and Rebecca MarsdenVol. 6, No. 3July–September 2007This material is presented to ensure timely dissemination of scholarly and technicalwork. Copyright and all rights therein are retained by authors or by other copyrightholders. All persons copying this information are expected to adhere to the termsand constraints invoked by each author's copyright. In most cases, these worksmay not be reposted without the explicit permission of the copyright holder.Starting in the first half of thisdecade, the number of students en-rolling in undergraduate computer sci-ence degree programs in the UK has sig-nificantly declined.1 Similar trends havebeen observed in the US and othercountries.2Although it’s difficult to pre-dict future industry needs for computerscientists in terms of either quantity orexpertise, some computing educationexperts argue that the number of com-puting employment opportunities in theUK will outstrip the number of nationalcomputer science graduates by the endof the decade.3Certainly, we could characterize therecent sharp drop in enrollment as a sideeffect of the dot-com bust, but small sur-veys and focus group interviews haveindicated that the limited take-up is alsodue to the perceptions that computerscience is nerdy, is too narrowly focusedto prepare students for multidisciplinaryjobs, and involves repetitive or mundanetasks lacking in creativity.4Practitioners and educators in perva-sive computing would beg to differ: thestudy of modern computing is a multi-disciplinary endeavor that includes engi-neering (software, hardware, and me-chanical), interaction methods, creativedesign, ethnography, and sociology.With technology becoming evermoreembedded, wireless, and ubiquitous, itfundamentally impacts our everydaylives. How, then, should we communi-cate computing’s increasing importanceto young people making decisions abouttheir undergraduate study?BACKGROUND AND AIMSOne thing higher education institutionscan do is host residential summer pro-grams that give in-depth, hands-on expe-rience to students nearing the end of sec-ondary school. This gives them a feel forundergraduate life and the chance to in-teract with peers and academics with sim-ilar interests and to deepen and broadentheir knowledge in a field they might beconsidering for undergraduate study. Inthe US, such programs typically spanmultiple weeks and involve an integra-tive design project (for example, Rose-Hulman Institute of Technology’s “Op-eration Catapult,” www.rose-hulman.edu/catapult). In the UK, Headstart (www.head-startcourses.org.uk) is a national schemefor talented students entering their finalyear of secondary school. Headstart ap-plicants can choose from courses at a number of hosting universities—in2007, 28 institutions offered Headstartcourses. Courses typically last aboutfour days, but their composition varies.Some have a broad focus on science andengineering, whereas others might focuson a particular field or be tailored specif-ically for female or minority students.Lancaster University’s Headstart is afocused course held in July, centered onthe theme of “ubiquitous computing.”The course aims to• expose students to the diversity of top-ics and interdisciplinary approachesin the field and• give students hands-on experiencewith developing mobile and perva-sive technologies. The course should balance these twoaims while also provisioning for the over-Reinvigorating the Discipline:Pervasive Computingand Tomorrow’s Computer ScientistsMike Hazas and Rebecca Marsden90 PERVASIVEcomputing Published by the IEEE Computer Society ■ 1536-1268/07/$25.00 © 2007 IEEEEditor: Scott F. Midkiff   ■ Virginia Tech   ■ midkiff@vt.edu Education & TrainingDeclining enrollments in computer science and related fields are a global concern. This issue’scolumn, by Mike Hazas and Rebecca Marsden of Lancaster University in the UK, describes thenovel Lancaster Headstart program that uses the excitement of pervasive computing to attractstudents into computer science. Your comments and suggestions for this column are welcome.Please contact me at midkiff@vt.edu.  —Scott F. MidkiffEDITOR’S INTROCourse: Headstart ProgramUnit: Computing DepartmentInstitution: University of LancasterCourse Directors: Rebecca Marsden, Mike Hazas Level: Secondary schoolURL: www.comp.lancs.ac.uk/headstartQUICK FACTSarching aims of the Headstart program,which include • visiting a local company,• sampling undergraduate life,• meeting academics and recent grad-uates, and• receiving career advice.Fitting all this into a three-and-a-half-day program is quite a challenge. COURSE STRUCTURE ANDOPERATIONWe use several course components tosatisfy our focus subject aims as well asthose of the Headstart program (see the“Course Highlights” sidebar). First, a setof multidisciplinary workshops give stu-dents a snapshot of the breadth of mod-ern computing. Past examples include aseminar on computer science innovationfrom a business standpoint, a workshopon developing multiplayer networkedgames on mobile phones using Python,and an interactive tutorial on installationart incorporating embedded sensing.Second, a central component of thecourse is the design project (discussedfurther in the next section). At thecourse’s beginning, we divide the stu-dents into predetermined teams. (Wefound that not allowing students toform their own teams is crucial—oth-erwise, students who happen to be fromthe same school might stick togetherand dominate their team, rather thangetting to know students from otherschools to create balanced teams.) Onthe first day, we present the teams withthe project goals and give them aboutthree hours per day to work on theprojects. An academic staff memberand three graduate students supervisethe project time. On the course’s finalday, each team creates a 15-minutepresentation and demonstration, givenin front of all attendees and a panel ofthree judges (see figure 1). Each mem-ber of the winning team receives a prize;in previous years these have been iPods. The third component of the LancasterHeadstart program focuses on informa-tion about university courses and careersin technology. We offer short sessionsthat discuss the procedure of applyingto university and what technology-basedcourses at university entail. The Head-start attendees particularly enjoy thequestion-and-answer session with recentcomputing graduates; they seem to iden-tify closely with our graduates and con-sider the graduates’ firsthand opinionsto be honest and relevant to their ownimpending career decisions.Computing in industry is a fourthcourse component. This involves an af-ternoon trip to Coniston in the LakeDistrict. Coniston Launch is a companyoperating solar-electric passenger boatson Coniston Water; the company haslinks with several departments at Lan-caster University. We also like to en-courage our Headstart course’s indus-trial sponsors to come and talk to thestudents. For example, in 2005, IntelResearch in Cambridge provided par-tial sponsorship, and James Scott, oneof their senior researchers visited Lan-caster and spoke about ubiquitouscomputing-themed research at Intel. Social and recreational activitiesmake up the course’s final component,which is meant to give students a tasteof undergraduate life. In addition tostaying in the residence halls on cam-pus, students can spend an evening atthe sports center, participate in a pubquiz (sans alcoholic drinks, of course),and watch (preapproved) DVD filmson a projector system in a lecture hall.And, as you might expect of under-graduates-to-be, the students seem toenjoy just hanging out in the residencehalls in the evenings and chatting. Even though we host relatively smallgroups of students, the course’s opera-tion is complex and requires many sup-port staff. These include two residentJULY–SEPTEMBER 2007 PERVASIVEcomputing 91• Tutorial on prototyping ubiquitous systems• Development of networked interactive applications on mobile phones using Python• Workshop on innovation in computing• Interactive and/or live performance art using embedded sensing• Group design project• Informational sessions on applying to university and studying for technology-based degrees• Onsite company visit: Coniston Launch in the English Lake District• “Grapevine” session: question-and-answer with a panel of recent computer science graduates• Social events: sports center evening, quiz night, film evenings• Seminar on how to use “bad ideas” in brainstorming activities• Overview of pervasive computing research by an industrial sponsorCOURSE HIGHLIGHTSFigure 1. In the final presentation, each team argued the rationale for their solutionwith respect to the design goals and then demoed the performance of their solution.The right projector displays a live video feed of their robot’s efforts to remove cansfrom the arena.EDUCATION & TRAININGEDUCATION & TRAINING92 PERVASIVEcomputing www.computer.org/pervasivegraduate students who stay with theattendees in the residence halls, threegraduate supervisors to assist and adviseduring the workshops and project ses-sions, a record keeper who takes pho-tos and video and documents the courseusing a blog, a resident Headstart super-visor (normally a teacher from a sec-ondary school appointed by HeadstartUK), the local course coordinator (aComputing Department staff member),and several Lancaster University pro-fessors, lecturers, and senior researcherswho conduct the workshops and judgethe design project presentations. Com-pounding the complexity is the closelevel of supervision of the students,which is legally required because mostHeadstart participants are under 18years of age. All of this essentially meansthat running the course is a 24-hour job,which requires ample people on hand todeal with unexpected situations and toallow adequate downtime for staff.DESIGN PROJECT The design project aims to demon-strate pervasive computing’s approachof detecting real-world phenomenon(via sensor data), processing it (oftenusing embedded computers instead ofconventional ones), and then using actu-ators (sound, displays, or motors) toachieve a desired effect. In this way, stu-dents learn that modern, practical, andrelevant computing is really about a lotof small devices that aid people in someway and with which people can interactin various environments. Often we usecomputers without giving them a spe-cific, cognitive focus as we undertakeour daily activities (for example, com-municating via text message or listeningto a pocket MP3 player). Students arealready familiar with this type of casualinteraction with nonconventional com-puting devices—perhaps even to a morepersonal degree than pervasive com-puting researchers are. So, students tendto find pervasive computing conceptsand ideas convincing and appealing.However, deciding on the design pro-ject’s technical content and requirementsis rather tricky. We can’t assume that stu-dents have any computing background,much less experience with any particularprogramming language. When we firstran the course in 2005, we had timetabled7.5 hours of project time. We opted foran open-ended design project centered onpaper-based conceptual prototyping. Not-withstanding the project’s abstract nature,we encouraged students to follow thesteps of the engineering design process.The teams came up with innovative per-vasive application scenarios that they il-lustrated using Wizard of Oz demonstra-tions. Despite this success, some of thestudents felt they hadn’t grappled enoughwith technical content. They commentedthat deeper subject matter and buildingsomething “real” in their design projectwould have been more rewarding. Aseducators who normally adopt a practicalapproach to teaching computer science,we too felt that something was missing.For Headstart 2006 (www.comp.lancs.ac.uk/headstart/2006), we increased theallotted project time to 9.25 hours andadopted the Lego Mindstorms RoboticsInvention System as a teaching tool. (Inmid-2006, Lego released MindstormsNXT, which has new sensor, actuator,and connectivity capabilities and uses aLabView-based programming environ-ment. See http://en.wikipedia.org/wiki/Lego_Mindstorms_NXT for more infor-mation.) Mindstorms was specificallydesigned for educational purposes.5Although the three design challenges(see the related sidebar) might not rep-resent typical pervasive computing ap-plications (particularly the more ad-versarial yet undeniably fun thirdchallenge), we used Mindstorms tostress the recursive “input/processing/output” aspect of pervasive computing.The Mindstorms controller unit (thething that gets programmed) is a goodexample of an embedded device—com-plete with the power, computational,and peripheral limitations common inpervasive computing. Each group rantheir batteries flat at least once, andgroups observed the limits of the sen-sors, the processor, and the actuators.The students also became acutely awareof the challenges of overcoming the dif-ficulties human programmers have try-Challenge 1• Build and program a robot that will remove empty aluminum cans from an arena (forexample, see figure A). The cans will be arbitrarily placed (standing up) in the arena.• The aluminum cans will be empty.• The arena will be marked out with white masking tape on black carpet or flooring.• The geometric center of your robot must not go outside the arena.Challenge 2• Your robot should remove the aluminum cans while navigating around obstacles.• Obstacles can include other robots or stationary, heavy, and rigid things.Challenge 3 • Your robot will periodically send over in-frared a unique code that identifies it. Youwill be given the program blocks to do this.• Of the six teams, there will be one that youare hunting (your prey) and one that ishunting you (your predator). • Your robot should try to force its prey outof the arena; likewise, your robot shouldresist being forced out by its predator. MINDSTORM DESIGN PROJECT CHALLENGESFigure A. Example of a final design: The wide bracket in front eases thecollection of cans.ing to express complex algorithms ascomputer programs that run on an em-bedded microprocessor.Pervasive computing device require-ments stand in contrast to the device re-quirements in robotics, where size, cost,and power consumption are typically notas much a concern. As such, after beinggiven the design challenge, each team hadan hour to plan their solution’s approachand come up with a list of resourcesneeded—sensors and actuators were inlimited supply. The teams didn’t have tomeet all three design challenges. How-ever, they could address the differentchallenges by reusing design compo-nents, and we made it clear to the stu-dents that judges would look more favor-ably on projects that addressed all threechallenges. To make the programmingtasks accessible to as many students aspossible, we opted for RCX Code, theGUI-based programming environmentthat came with consumer versions of theMindstorms sets.The projects were wildly successful;many group members worked extrahours in the evenings to revise their me-chanical designs (see figure 2), improvetheir firmware algorithms, and polishtheir presentations and demos. Severalstudents commented that they found theRCX Code programming environmenttoo limiting. This year we’re consideringusing community-developed Mind-storms programming languages such as NQC (Not Quite C, http://bricxcc.sourceforge.net/nqc) for students whowish to use a more powerful language.HISTORY AND CURRENTDEVELOPMENTS Student attendance in the course hasvaried from year to year. In 2005 (the firstyear we ran the course), we had 14 stu-dents, and in 2006, the number went upto 26. Survey respondents (14 in 2005,20 in 2006) in both years were generallypositive, with 56 percent and 35 percentrating the projects as “excellent’’ and“good,” respectively. All responding stu-dents reported that our Headstart pro-gram influenced their choices aboutundergraduate study and careers; 76 per-cent would consider computer or soft-ware engineering as their first choice. Formany (66 percent), Headstart helpedthem make a firm decision regarding thefield within computing or engineeringthey wanted to focus on. For a small pro-portion of the respondents (10 percent),their experience in the program helpedconfirm that they didn’t want to study anengineering-related subject.We seem to have achieved our coursegoals in that none of the students charac-terized computer science as nerdy or repet-itively boring. Some cited simply beingmore interested in other subjects (such asfilm). But interestingly, a small numberdidn’t feel confident they could grapplewith the “technical aspects” of computerscience and engineering. By this, we as-sume they were referring to computer,electrical, or mechanical system design.However, in pervasive computing, weknow that technical approaches varywidely in their nature, and the greatestleaps forward come from a synthesis ofapproaches. This year we aim to repre-sent more of these approaches by in-cluding sessions on human-computer in-teraction or sociology. By familiarizingthem with the concept of a user studyor ethnomethodological approaches tounderstanding people and technology,we hope to give students a better idea ofthe breadth of technical approachesneeded to understand computing in thepresent day.ACKNOWLEDGMENTS Over 25 members of Lancaster University (staff andstudents) work to make our Headstart program asuccess, and we can’t name them all here. However,we especially thank Matt Faulkner and Lorna Mc-Knight (resident graduate students); Mark Lowton(record keeper); and Ben Green, Martyn Welch, andAndrew Stone (project supervisors). Paul Coultonconducts challenging yet accessible hands-on ses-sions on programming mobile phones; these havebeen very well received by the students. We alsothank our industrial sponsors who provide financialsupport and a good collaborative spirit: Intel Re-search, Cambridge (in 2005 and 2006), and Mi-crosoft Research, Cambridge (in 2007).REFERENCES1. P. Ghosh, “Computer Industry ‘Faces Crisis’,”  BBC News, 17 Nov. 2006;http://news.bbc.co.uk/1/hi/technology/6155998.stm.2. K. Hafner, “Computing’s Lost Allure,”New York Times, 22 May 2003. 3. K. Mander, “Demise of Computer ScienceExaggerated,” Future of Computing, 17Nov. 2006; www.bcs.org/server.php?show=ConWebDoc.10138. 4. “Investigation into the Decline in BSc Com-puting/IT Applications to British Universi-ties,” commissioned research report, Coun-cil of Professors and Heads of Computing,July 2005; www.cphc.ac.uk/publications.php. 5. M. Resnick et al., “Programmable Bricks:Toys to Think With,” IBM Systems J., vol.35, nos. 3–4, 1996, pp. 443–452.EDUCATION & TRAININGJULY–SEPTEMBER 2007 PERVASIVEcomputing 93Figure 2. Team members collaborate on an iteration of their design. They’reaiming to engineer a stable, speedy robot using a low center of mass and a relatively low gearing ratio.Mike Hazas is an academicfellow and lecturer in theComputing Department atLancaster University.  He hasbeen responsible for the aca-demic content and designprojects of Lancaster’s Headstart and is coordi-nating the program in 2007. Contact him athazas@comp.lancs.ac.uk.Rebecca Marsden was a teaching fellow in theComputing Department at Lancaster Universityand coordinated Lancaster’s Headstart in 2005and 2006.",
            "id": 18613747,
            "identifiers": [
                {
                    "identifier": "70107",
                    "type": "CORE_ID"
                },
                {
                    "identifier": "oai:eprints.lancs.ac.uk:13075",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "10.1109/mprv.2007.66",
                    "type": "DOI"
                }
            ],
            "title": "Reinvigorating the discipline:pervasive computing and tomorrow's computer scientists",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:eprints.lancs.ac.uk:13075"
            ],
            "publishedDate": "2007-01-01T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://eprints.lancs.ac.uk/13075/1/Hazas07_TomorrowsComputerScientists.pdf"
            ],
            "updatedDate": "2022-05-16T06:43:36",
            "yearPublished": 2007,
            "journals": [
                {
                    "title": null,
                    "identifiers": [
                        "1536-1268"
                    ]
                }
            ],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/70107.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/70107"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/70107/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/70107/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/18613747"
                }
            ]
        },
        {
            "acceptedDate": "2015-11-30T00:00:00",
            "arxivId": null,
            "authors": [
                {
                    "name": "Adeel"
                },
                {
                    "name": "McCann"
                },
                {
                    "name": "Tahir"
                },
                {
                    "name": "Yang, Shusen"
                }
            ],
            "contributors": [
                "Intel Corporation (UK) Ltd",
                "NEC Corporation"
            ],
            "outputs": [
                "https://api.core.ac.uk/v3/outputs/77002128"
            ],
            "createdDate": "2017-02-17T18:40:28",
            "dataProviders": [
                {
                    "id": 105,
                    "name": "",
                    "url": "https://api.core.ac.uk/v3/data-providers/105",
                    "logo": "https://api.core.ac.uk/data-providers/105/logo"
                }
            ],
            "depositedDate": "2015-08-01T00:00:00",
            "abstract": "© 2015 IEEE.In this paper, we study the problem of multi-resource fairness in multi-user sensor networks with heterogeneous and time-varying resources. Particularly we focus on data gathering applications run on Wireless Sensor Networks (WSNs) or Internet of Things (IoT) in which users require to run a serious of sensing operations with various resource requirements. We consider both the resource demands of sensing tasks, and data forwarding tasks needed to establish multi-hop relay communications. By exploiting graph theory, queueing theory and the notion of dominant resource shares, we develop Symbiot, a light-weight, distributed algorithm that ensures multi-resource fairness between these users. With Symbiot, nodes can independently schedule its resources while maintaining network-level resource fairness through observing traffic congestion levels. Large-scale simulations based Contiki OS and Cooja network emulator show the effectiveness of Symbiot in adaptively utilizing available resources and reducing average completion times",
            "documentType": "research",
            "doi": "10.1109/hpcc-css-icess.2015.23",
            "downloadUrl": "https://core.ac.uk/download/77002128.pdf",
            "fieldOfStudy": "computer science",
            "fullText": "Symbiot: Congestion-driven Multi-resource Fairnessfor Multi-User Sensor NetworksYad Tahir∗, Shusen Yang†, Usman Adeel∗, Julie McCann∗∗Imperial College London, †University of Liverpool{yst11, u.adeel09, j.mccann}@imperial.ac.uk, shusen.yang@liverpool.ac.ukAbstract—In this paper, we study the problem of multi-resourcefairness in multi-user sensor networks with heterogeneous andtime-varying resources. Particularly we focus on data gatheringapplications run on Wireless Sensor Networks (WSNs) or Internetof Things (IoTs) in which users require to run a serious of sensingtasks with various resource requirements. By exploiting graphtheory, queueing theory and the notion of dominant resourceshares, we develop Symbiot, a light-weight, distributed algorithmthat ensures multi-resource fairness between these users. WithSymbiot, nodes can independently schedule its resources whilemaintaining network-level resource fairness through observingtraffic congestion levels. Large-scale simulations based Contiki OSand Cooja network emulator show the effectiveness of Symbiotin utilizing resources and reducing average completion times.I. INTRODUCTIONWireless Sensor Networks (WSNs) and Internet of Things(IoTs) [1] are evolving towards interconnected, sensing andprocessing infrastructures that are expected to provide servicesfor multiple concurrent users. One of the main reasons ofhaving such shared enterprise deployments is to maximize thereturn on investment while minimizing operating costs.Such multi-user networks are almost invariably heteroge-neous in terms of their hardware components, offered resourcecapacities as well as user demands. Various types of sensorscan be attached to nodes. The assorted capacities of someresource types, such as bandwidth, can be highly dynamicand time-varying [2], [3]. Diversity is also found when usersdemand specific resources. For instance, a user may require theexecution of a memory-heavy task on nodes with temperaturesensors, while another user may need to compute a bandwidth-hungry task on nodes having humidity and light sensors.Resource sharing in any multi-user sensor network is akey issue for one primary reason. The rapid growth of trafficand computation requirements are often not matching thegrowth and expansion of overall network capacity. Hence, thelimited network resources should be distributed fairly betweenusers [4], [5]. Accounting for diversity across node resourcesand user requirements presents an increased challenge toschedulers for fair provisioning of network resources. Gettingthis right is fundamental to next-generation WSN and IoTsystems.Both resource and demand heterogeneity have an impact onthe efficient and fair allocation of resources to users. Currentmax-min-based resource allocation schemes such as [6] do notdeal well with both heterogeneous resources and user demandsin multi-resource systems. Utilizing multi-resource fairness!\"#$%\"&'SELECT light FROM sensor u2SELECT temperatureFROM sensor u1!\"#$%\"&'SELECT MAX(humidity), AVG(humidity)FROM sensorPER 10000 SAMPLESu3humidity  sensor'temperature sensor'light sensor'sensing requirements'other node type'(')'Fig. 1. A WSN with two gateways and three users. u1 and u2 are interestedin nodes with temperature and light sensors, respectively. u3 needs to executea query with higher computation complexity on nodes with humidity sensors.schemes [4], [5] such as Dominant Resource Fairness (DR-F) [4] have become a hot topic in both computation economicscommunities and cloud computing. It is proven that givenusers with heterogeneous demands for different resource types,these approaches achieve more efficient resource allocationthan single-resource fairness schemes.However, these approaches are limited when it comes toproviding distributed scheduling that can adapt to time-varyingresources. They assume that the system has a single, cen-tralized scheduler that is aware about the resource capacitiesof all the system components. This means in a networkwith dynamic resources, nodes have to repeatedly report theiravailable resource capacities to the scheduler. This is neitherscalable nor resilient, thus impractical for large-scale networks.Furthermore, these approaches do not address multi-hop natureon which the network relies. When a user executes a task ona particular node, the output has to be forwarded to a gatewaydevice using multi-hop communications. This creates indirectresource consumptions from the user. To ensure having fairmulti-resource scheduling, it is crucial to consider both directand indirect resource demands in multi-hop networks.The key issue we address in this paper concerns how theavailable network resources, including those that are time-varying, should be allocated between competing users giventhe fact that each user has heterogeneous resource require-ments. In particular, we consider the problem of fair resourceallocation for data gathering applications in WSNs and IoTs.To understand the complexity of the problem, consider atypical sensing application shown in Fig. 1. The network hasthree users u1, u2, and u3 with different sensing require-ments expressed as SQL-like queries for convenience. Twocases should be analyzed here: first, based on the sensingrequirements, u1 and u2 do not compete on resources. Eachof them requires a different set of nodes. However given thefact that multi-hop communications are required to rely data,it is clear that u1 and u2 are now indirectly related to eachother, and they compete for the shared resources of relay nodes(nodes A and B). The second case is that if u1 changed itsrequirements and asked for all the resources available in allnodes with temperature or light sensors, surely this would beconsidered unfair according to the max-min fairness model asu1 would consume considerably more network resources thanu2. From these two cases there are two key questions that haveto be carefully addressed: First, how can we formally definethe relationship between users? u1 and u2 are dependent interms of resource requirements, but u3 certainly is not. Second,given each user submit sensing tasks to the network, what isthe maximum number of tasks that each user can execute whilethe resource consumptions between related users remain fair?We present the study of the fair resource allocation problemin network with multiple, time-varying resource types. Thecontributions of this paper are summarized as follows:1. We establish a set of system models to formally describethe components of multi-user WSNs and IoT. By exploitinggraph theory, we formally define the relationship betweenusers in terms of their resource requirements. (Section II)2. We propose a new extension to max-min fairness thatexploits graph theory and dominant resource sharing to ensuremulti-resource fairness between users running a series ofsensing tasks. We develop a new algorithm, named Symbiot,that uses iterative linear programming and queueing theory tocompute the proposed fairness extension. Symbiot is not onlylightweight, but also it is fully distributed eliminating the needfor centralized scheduling. Through observing local resourcecapacities and current traffic congestion levels, nodes can in-dependently allocate resources for its users while maintainingmulti-resource fairness between related users in the wholenetwork. To the best of our knowledge, this work is not onlythe first work to address the indirect requirement problem andutilize queueing theory in multi-resource scheduling, but alsothere has been no prior work that has proposed a distributedsolution to achieve multi-resource fairness. (Section III)3. We implemented Symbiot on Contiki OS [7], an opensource operating system for wireless sensor networks andIoT. We integrated Symbiot with the IPv6 stack provided byContiki. Through large-scale simulations with the Cooja net-work emulator [7], we demonstrate the practical performanceof Symbiot algorithm. Our experiments show that Symbiotoutperforms the naive version of distributed DRF in terms ofreducing the average of job completion times. (Section IV)II. SYSTEM MODELSWe consider a WSN or IoT that consists of a set of wirelessnodes N = S ∪ H communicating in a multi-hop fashion,where S represents the set of all nodes that can sense, generateand relay data packets; while H is the set of all gateways thatcollect the data traffic produced by the network. Let the setNx holds all one-hop neighbors that node x ∈ N that cancommunicate with. The network has a set of users U and weassume that the network operates in discrete time with a unittime slot (e.g. a second) t ∈ {1, 2, ...}.A. Node Operations and Data QueuesNode n ∈ S can run two types of tasks: sensing and dataforwarding.Each user u ∈ U can execute sensing tasks periodicallyon nodes. We assume that a sensing task requires no morethan one time slot to finish. We assume that the sensing tasksare delay-tolerant, which is realistic as many environmentalmonitoring applications are reasonably delay tolerant.Performing sensing tasks will generate data packets to becollected by a gateway h ∈ H. To achieve this, nodes executedata forwarding tasks. The data can be transmitted directly ifh ∈ Nn, or indirectly in a multi-hop fashion if h ̸∈ Nn. Theunderlinying routing layer determines the forwarding policy.In this paper, we only consider networks running in a fixedrouting scenario, i.e., the route of each data flow is pre-determined and fixed during the time of interest. We alsoassume both sensing and forwarding tasks are indivisible.In the network, each node maintains data packets throughqueues (data buffers). Let Qun denote the queue existing innode n to hold the data packets generated by user u. Let Qun(t)be the queue backlog (or queue length) of the queue Qun attime slot t. The queue dynamics of Qun are defined as follows:0 ≤ Qun(t) ≤ MaxQunQun(t+ 1) = |Qun(t)− foutu,n (t)|+ + ru,n(t) + f inu,n(t) (1)Where the operator | . |+ represents max(0, .) and MaxQun >0 is the maximum queue length (i.e. allocated data buffersize) of Qun. ru,n(t) is the output of performing sensing tasksin node n for user u at time slot t. f inu,n(t) and foutu,n (t)are the incoming and outgoing traffic of node n for user u,respectively. It is worth noting that the queue length of anygateway always equals to zero, i.e.Quh(t) = 0, h ∈ H, ∀ u ∈ U , t ≥ 1B. Node ResourcesEach node n ∈ N offers K types of resources forusers such as computing (i.e. MCU), memory, bandwidth,and light sensors. Let a K-dimensional vector cn(t) =(cn,1(t), ..., cn,K(t)) represents node’s resource capacities attime slot t, where each entry cn,k(t) 1 ≤ k ≤ K representsthe resource capacity of kth resource of n at t. Let C(t) be a|N |×K-dimensional matrix representing the resource capacityfor the whole network at t.In the rest of this paper, we will use a tuple (n, k), n ∈N , 1 ≤ k ≤ K to refer to a specific resource.C. User Resource RequirementsLet !Ru be a |N | × K-dimensional matrix in which theentry !Run,k represents the requirement from (n, k) to performa sensing task by user u. Let \"Ru be a |N | ×K-dimensionalmatrix in which the entry \"Run,k is the amount of resourceneeds from (n, k) to perform a data forwarding task on Qun.The gross resource requirements of user u is represented asa |N |×K matrix Ru, where:Run,k = !Run,k + \"Run,k (2)Consider Du to represent the set of strictly positive resourcedemands of user u:Du := {(n, k) : Run,k > 0, n ∈ N , 1 ≤ k ≤ K} (3)D. Topological User DependencyThis section presents a graph model to characterize thedependency of all users in U , with respect to resource re-quirements.Definition 1. [User Dependency Graph]. We can use anundirected graph G(U ,V) to represent the dependency of allusers, whereV := {(u, v) : u, v ∈ U , Du ∩Dv ̸= ∅}is the set of the links in the graph. Each link (u, v) ∈ Lindicates that two users u and v share some resources.Definition 2. [User Dependent Cluster (UDC)]. For a givenuser dependence graph G(U ,V), a User Dependent Cluster(UDC) C ⊆ U is defined as the set of users in a connectedcomponent1 of G(U ,V).Define Un,k as the set of users that have a strictly positivedemand of resource (n, k), n ∈ N , 1 ≤ k ≤ K, i.e.Un,k := {u : Run,k > 0, u ∈ U} (4)It can be seen that two users u1 and u2 in a UDC C may notshare common resources, meaning that they are not directlydependent on each other. However the resource requirementsof u1 would affect that of all neighbors v ∈ Un,k, (n, k) ∈Du1 . By repeating the above process, the requirement of u1would affect all users in C, including u2. This implies that allusers in a UDC are directly or indirectly dependent in termsof resource requirements.E. ObjectiveThe objective of this paper is to develop a distributed, fairscheduling algorithm to allocate resources for sensing and dataforwarding tasks for the users of the network. In particular forevery time slot t, it is required to distributedly compute two|N | × |U|–dimensional matrices !X(t) and \"X(t), where theentries !Xu,n(t) and \"Xu,n(t) are the number of sensing andforwarding tasks performed by user u in node n, respectively.The network resources allocated to user u at time slot tis defined as a |N |×K–dimensional matrix Au(!X(t), \"X(t))where each entry Aun,k(t) represents the amount of resource(n, k) allocated to user u at time slot t:1In graph theory, a connected component is subgraph in which any twovertices are connected to each other by paths, and which is connected to noadditional vertices in the supergraph.Aun,k(t) = !Xu,n(t)!Run,k + \"Xu,n(t)\"Run,k (5)Definition 3. [Feasible Resource Allocation]. A resourceallocation matrix Au(!X(t), \"X(t)) is feasible if the followingcondition is satisfied:#u∈UAu(!X(t), \"X(t)) ≼ C(t) (6)where ≼ means entry-wise smaller than or equal to.III. SYMBIOTLet du,n,k(t) = Run,k/cn,k(t) be the resource demand shareof user u for resource (n, k) at slot t. Let dmaxu,n (t) denote thenode-wise dominant demand share of user u in node n.dmaxu,n (t) = max1≤k≤Kdu,n,k(t) (7)We define du,k(t) as the normalized resource demand shareof a user u for resource (n, k):du,k(t) = du,n,k(t)/dmaxu,n (t) (8)For each user u in a UDC C, define UDC-wise dominantdemand share asdmaxu (t) = max(n,k)∈D(C)dmaxu,n (t), u ∈ C (9)whereD(C) :=$u∈CDudenotes the set of all resources used by all users in C. For agiven dmaxu (t), we donate the bottleneck node n∗ as the nodewhich provides dmaxu (t), i.e.:n∗ = argmaxndu,n,k(t), (n, k) ∈ D(C), u ∈ C (10)We define the UDC-wise dominant share for user u in C as:dmaxu (t)(!Xu,n∗(t) + \"Xu,n∗(t)) (11)A. AlgorithmThe basic idea of Symbiot is to implement a distributed,light-weight algorithm to approximate the max-min fairnessfor the UDC-wise dominant shares for each UDC.Because the algorithm is distributed, every node n ∈ Ncomputes resource scheduling locally. However, we utilizenetwork traffic congestion to collect feedback from the bot-tleneck nodes existing in UDCs. The pseudo code of Symbiotis summarized in Fig. 2 and Fig. 3.In node n at the beginning of time slot t, all users thatuse n are considered to be unsaturated (line 01 in Fig. 2)and stored in the unsaturated user set Úus. Symbiot allocatesresources for the unsaturated users through multiple iterations(lines 02-21 in Fig. 2). In each iteration Symbiot finds a budgetb that maximizes the equalized node-wise dominant demandVariables:t: current time slot for the noden: the current nodeÚus: the set of unsaturated users.Úpt: the set of users that performed a task in an iteration.B: a |U|×K matrix holding resource budgets for an iterationÁ: a |U|×K matrix holding allocated resources for an iteration.Input:!Rn := {!Run,k, ∀u ∈ U , 1 ≤ k ≤ K}\"Rn := {\"Run,k, ∀u ∈ U , 1 ≤ k ≤ K}Output:!Xu,n(t), \"Xu,n(t) ∀u ∈ UMain Algorithm:01: Úus ← {u : Un,k, ∀k}; // Un,k defined in Eq. 402: while Úus(t) ̸= ∅ do03: Á← 0,Upt ← ∅; //reset the paramaters for this iteration04: compute du,k(t), ∀k, u ∈ Uus; //based on Eq. 805: b← 1/max∀k#u∈Úusdu,k(t);//node-wise dominant demand share maximization06: for all u ∈ Úus(t)07: Bu,k ← du,n(t)cn,k(t)b, ∀k; //assign resource budgets08: if containLocalPacket(Qun(t)) = FALSE09: [ r Á Upt ]← sensing(u, Á,Upt, !Rn,B);10: !Xu,n(t)← !Xu,n(t) + r;12: end if13: r ← 1;14: while r = 115: [ r Á Upt ]← dataForwarding(u, Á,Upt, \"Rn,B);16: \"Xu,n(t)← \"Xu,n(t) + r;17: end while18: end for19: cn,k(t)← cn,k(t)−#u∈ÚusÁu,k, ∀k;//update available resources based on actual consumptions20: Úus ← Úus ∩ Úpt; // update unsaturated users21: end whileFig. 2. Symbiot - Main Algorithmshares of these users (lines 04-05 in Fig. 2) by computing b =1/%u∈Úusdu,n,k(t)dmaxu,n (t). This also ensures that Symbiot adheresto the capacity constraint introduced in the Definition 3.Then Symbiot assigns each unsaturated user a resourcebudget for every resource type k (line 07 in Fig. 2). Thegiven resource budgets can be utilized for sensing tasks,data forwarding tasks or both. For sensing tasks, Symbiotfirst checks whether the data queue of the user contains alocal packet (i.e. generated by n). If no local packet found,Symbiot executes the sensing function. It is worth noting thatin the pseudo code of Symbiot, we assume the data queuesof the users follow the First-In-First-Out (FIFO) scheduling.Symbiot utilizes the remaining budget for data forwardingtasks by calling the dataForwarding function. As shownin Fig. 3, both sensing and dataForwarding update theresource consumptions and active users set Úpt for the currentiteration. These functions returns one if the sensing or datasensing(u, Á,Upt, !Rn,B)01: if Qun(t) ̸= MaxQun ∧ ∃k !Run,k ̸= 0 ∧∀k Áu,k + !Run,k ≤ Bu,k//if Qun is not full, user requires sensing, and sufficient budget02: Áu,k ← Áu,k + !Run,k, ∀k; //allocate the resources03: executeSensingTask(u); //sensing and storing data in Qun04: Úpt(t)← Úpt(t) ∪ {u};05: return [ 1 Á Upt ];06: end if07: return [ 0 Á Upt ];dataForwarding(u, Á,Upt, \"Rn,B)08: p← getNextHop(); //node to where packets should be sent09: if Qun(t) ̸= 0∧Qup(t) ̸= MaxQup ∧∀k Áu,k + \"Run,k ≤ Bu,k//check Qun is not empty, Qpn is not full, and sufficient budget10: Áu,k ← Áu,k + \"Run,k, ∀k; //allocate the resources11: sendDataPacket(u, p); //sends a packet from Qun to p12: Úpt ← Úpt ∪ {u};13: return [ 1 Á Upt ];14: end if15: return [ 0 Á Upt ];Fig. 3. Symbiot - Sensing and Data Forwarding functionsforwarding tasks is actually executed, otherwise zero. Basedon the feedback given in the lines 09 and 15 in Fig. 2, Symbiotupdates the !Xu,n(t) and \"Xu,n(t) outputs for all unsaturatedusers. Then the remaining resource capacities are adjustedaccording to the resource consumptions of this iteration. Inline 20 in Fig. 2, Symbiot removes saturated users from Úus.We define a saturated user as a user who did not perform anytask, whether sensing or data forwarding, in an iteration. Thisprocess continues until all users become saturated.Remark 1. Both sensing and data forwarding operationsare traffic-aware. Even if a user has the required budget toperform a task, Symbiot will not allocate resources if trafficcongestion is observed for that user. In line 01 in Fig. 3,sensing tasks check whether the queue of the current nodeis not full. Line 09 in Fig. 3 ensures data forwarding tasks arenot performed if the queue of the next-hop node is not full.Remark 2. In a bottleneck node, the data queues of someusers will be full. A bottleneck node n∗ in a UDC will limitthe budget of one or more users as they are seen to be locallyunfair with respect to other users. This means in the noden∗, the foutu,n∗ < f inu,n∗ for some users. Based on the queuedynamics defined in Eq. 1, the queue levels of some Qun∗ willgradually increase until it reaches the corresponding MaxQun∗ .Remark 3. Bottleneck nodes in a UDC cause traffic conges-tion for nodes sending data packets in. According to Remark2, when Qun∗ = MaxQun∗ , n∗ also limits data forwardingoperations for nodes that send data directly to n∗ as describedin Remark 1. Eventually when their data queues become full,this results in affecting the sensing operations performed onthese nodes. This process repeats itself throughout the UDCand will affect all nodes that generating and sending datapacket for user u through the n∗.Fig. 4. The network topology of the experiments.IV. EVALUATIONThe performance of Symbiot was evaluated through con-ducting a set of simulation on Cooja. We implemented Sym-biot on top of Contiki OS [7] and its IPv6 stack. Theimplementation provides a set of public APIs to allow usersto express their sensing and data forwarding requirements.Internally it uses UDP messages and IPv6 to transfer datapackets from nodes to gateways. We use ICMPv6 to periodi-cally broadcast the queue lengths of the users between nodes.We established a 100-node network with 5 gateways and95 nodes. The network topology of our experiments is shownin Fig. 4. The network uses CSMA and RPL with thedefault parameter settings for the MAC and routing layers,respectively. We set RPL to use the ETX Objective Function.The nodes employ IEEE 802.15.4 transceiver, CC2420. Basedon the experimental studies in [8], the bandwidth capacityof the nodes is set to be 160 40-bytes packets per second.The nodes in our network uses MSP430F1611 microcontrollerwith 10KB RAM. The network offers two types of sensors:temperature and humidity with the resource capacities of 1000and 2000 readings per second, respectively. We randomlychose 40 nodes to offer temperature sensing and another 40nodes for humidity sensing. In all simulations the duration ofa time slot was set as one second and MaxQun for all usersin all nodes were equal to 70.In the reset of the paper, we will use the vector ⟨r1 packets,r2 bytes of RAM, r3 temperature readings, r4 humidityreadings⟩ to represent the resource requirements of offeredresource types in the experiments.A. Dynamic Resource AllocationIn our first experiment, we show how Symbiot dynamicallyallocates resources between users in a system with time-varying resource requirements and capacities. Fig. 5 showsthe average allocated network resources and UDC-wise sharesfor each users as a function of time.In the beginning, the network has two users u1 and u2.u1 has the resource requirements of ⟨0, 1, 2, 0⟩ for sensingand ⟨1, 20, 0, 0⟩ for data forwarding. Similarly, u2 needsTime300 600 900 1200 1500 1800Average UDC-wise Domaint Share00.10.20.30.40.50.60.7u1u2u3u2 changed demandsu2 leftu3 enteredbandwidth capacityreducedTime300 600 900 1200 1500 1800Average Bandwidth Usage00.050.10.150.20.250.30.350.4u1u2u3Time300 600 900 1200 1500 1800Average RAM Usage00.050.10.150.20.250.30.350.4u1u2u3Time (second)300 600 900 1200 1500 1800Average Temperature Sensor Usage00.010.020.030.040.050.060.070.080.09u1u2u3Time (second)0 300 600 900 1200 1500 1800Average Humidity Sensor Usage00.020.040.060.080.10.12u1u2u3Fig. 5. Symbiot resource allocation in a dynamic system.⟨0, 10, 0, 20⟩ for sensing and ⟨2, 50, 0, 0⟩ for data forwardingoperations. As seen in Fig. 5, the UDC-wise dominant sharesfor both users are equal to 50% as bandwidth is the mostdemanded resource for both users in bottleneck nodes. Atslot 300, u2 changes his/her resource requirements making itmore memory demanding. u2 now demands ⟨0, 2048, 200, 0⟩for sensing and ⟨9, 512, 0, 0⟩ for data forwarding. Symbiotdynamically adjusts the resource allocation for both users.The UDC-wise dominant shares increased to around 60% asthe most required resource for u1 is bandwidth while foru2 now it becomes RAM. At time slot 600, u3 enters thenetwork with the resource requirements of ⟨0, 200, 20, 20⟩ forsensing and ⟨4, 200, 0, 0⟩ for data forwarding. Symbiot lowersthe resource allocation for both u1 and u2 to accommodate theresource demands for the new users. As u3 enters the network,the nodes in beginning tries to perform as much sensingas possible until bottleneck nodes are found causing trafficcongestion. This will result in gradually lowering the sensingrate as shown between slot 600 and 700. At slot 900 when u2leaves the network, Symbiot allocates the released resourcesfrom u2 to u1 and u3. At time slot 1900, we intentionallyreduce the bandwidth capacities of all nodes by 50%. Symbiotautomatically adjusts the allocations for both u1 and u3.B. Symbiot vs Alternative Multi-Resource AllocationTo compare the performance of Symbiot with another multi-resource allocation algorithm, we implemented a naive versionof distributed DRF. Here, each node runs the progressivefilling of DRF [4]. However, the sensing and data forwardingtasks are not traffic-aware. If any node receives data packetsmore than MaxQun, the node will drop the packets. In theseexperiments, we measured the completion times required toperform sensing tasks on all the nodes required by the users.Fig. 6 presents the reductions of average completion timesin networks with different numbers of gateways. Here, thenetwork has two users with sensing requirements of ⟨0, 1, 2, 0⟩and ⟨0, 10, 0, 20⟩ and forwarding requirements of ⟨1, 20, 0, 0⟩and ⟨2, 50, 0, 0⟩. Fig. 7 shows Symbiot outperforms the imple-mented distributed DRF in networks with different numbers ofusers. The used topology had 5 gateways and the sensing andforwarding requirements were randomly generated.Number of Gateways1 2 3 4 5Completion Time Reduction (%)05101520253035404550%31%36%33%41%40Fig. 6. Average job completion time saving that Symbiot achieved againstnaive, distributed DRF in networks with different numbers of gateways.Number of Users2 5 10 15 20Completion Time Reduction (%)05101520253035404550%28%16%22%27%33Fig. 7. Average job completion time saving that Symbiot achieved againstthe implemented version of distributed DRF in networks with different users.V. RELATED WORKOne way to ensure fairness in multi-resource systems isto employ single-resource abstraction models where systemresources are divided into fixed partitions, often referred asslots. As pointed out in [4], employing such simple abstractionmodels are considered to be inefficient when users have het-erogeneous demands for resources. Ghodsi et al. [4] proposeDominant Resource Fairness (DRF) as an alternative approachto ensure fairness in multi-resource systems. DRF satisfiesseveral highly desirable fairness properties, and it quicklyreceived significant attention from the research community.Parkes et al. [9] extend the DRF paradigm to provide acompelling solution for weighted and zero demands users. Infact, the node-wise dominant share computations in SymbIoTis greatly inspired by [9] work. Wang et al. [10] suggest tothe use of the notion of global dominant shares to addressthe heterogeneity of server capabilities in cloud computing.All the approaches above focus on systems with centralizedschedulers. As discussed in Section , deploying centralizedschedulers can be very expensive and not feasible for net-works with time-varying resources. We purpose a solution todistributedly allocating resources for users. We also show howindirect resource requirements coming from multi-hop com-munications should be handled. This is crucial for ensuringmulti-resource fairness accross the whole network.VI. CONCLUSIONThis paper addresses the problem of multi-resource fair-ness between users in data gathering applications in WSNsand IoT. Here, nodes can offer multiple types of resourceswith heterogeneous, time-varying capacities. Users can havedifferent requirements on various resource types. We explainwhy the current state-of-the-art falls short when it comes toachieve multi-resource fairness in such networks. By utilizinggraph theory, queueing theory and dominant resource sharing,we propose Symbiot, a lightweight, distributed algorithm asa solution to this problem. Our simulations based on Coojanetwork emulator shows the practical performance of Symbiot.REFERENCES[1] Z. Sheng, S. Yang, Y. Yu, A. Vasilakos, J. Mccann, and K. Leung, “Asurvey on the ietf protocol suite for the internet of things: Standards,challenges, and opportunities,” IEEE Wireless Commun., vol. 20, no. 6,pp. 91–98, 2013.[2] S. Yang, U. Adeel, and J. McCann, “Selfish mules: Social profitmaximization in sparse sensornets using rationally-selfish human relays,”IEEE JSAC, vol. 31, no. 6, pp. 1124–1134, 2013.[3] S. Yang, X. Yang, J. A. McCann, T. Zhang, G. Liu, and Z. Liu,“Distributed networking in autonomic solar powered wireless sensornetworks,” IEEE J. Sel. Areas Commun., vol. 31, no. 12, pp. 750–761,2013.[4] A. Ghodsi, M. Zaharia, B. Hindman, A. Konwinski, S. Shenker, andI. Stoica, “Dominant Resource Fairness: Fair allocation of multipleresource types,” in Proc. USENIX NSDI, 2011.[5] L. Popa, G. Kumar, M. Chowdhury, A. Krishnamurthy, S. Ratnasamy,and I. Stoica, “Faircloud: Sharing the network in cloud computing,” inProc. ACM SIGCOMM, 2012.[6] S. Yang and J. A. McCann, “Distributed optimal lexicographic max-minrate allocation in solar-powered wireless sensor networks,” ACM Trans.Sensor Networks, vol. 11, no. 1, p. 9, 2014.[7] “Contiki: The Open Source OS for the Internet of Things.” [Online].Available: http://www.contiki-os.org[8] A. Sridharan and B. Krishnamachari, “Explicit and precise rate controlfor wireless sensor networks,” in Proc. ACM SenSys, 2009, pp. 29–42.[9] D. Parkes, A. Procaccia, and N. Shah, “Beyond Dominant ResourceFairness: Extensions, limitations, and indivisibilities,” in Proc. ACM EC,2012.[10] W. Wang, B. Li, and B. Liang, “Dominant Resource Fairness incloud computing systems with heterogeneous servers,” in Proc. IEEEINFOCOM, 2014.",
            "id": 22966623,
            "identifiers": [
                {
                    "identifier": "10.1109/hpcc-css-icess.2015.23",
                    "type": "DOI"
                },
                {
                    "identifier": "oai:spiral.imperial.ac.uk:10044/1/23872",
                    "type": "OAI_ID"
                },
                {
                    "identifier": "77002128",
                    "type": "CORE_ID"
                }
            ],
            "title": "Symbiot: Congestion-driven Multi-resource Fairness for Multi-User Sensor Networks",
            "language": {
                "code": "en",
                "name": "English"
            },
            "magId": null,
            "oaiIds": [
                "oai:spiral.imperial.ac.uk:10044/1/23872"
            ],
            "publishedDate": "2015-06-15T00:00:00",
            "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
            "pubmedId": null,
            "references": [],
            "sourceFulltextUrls": [
                "http://spiral.imperial.ac.uk/bitstream/10044/1/23872/6/symbiot.pdf"
            ],
            "updatedDate": "2022-05-16T08:18:32",
            "yearPublished": 2015,
            "journals": [],
            "links": [
                {
                    "type": "download",
                    "url": "https://core.ac.uk/download/77002128.pdf"
                },
                {
                    "type": "reader",
                    "url": "https://core.ac.uk/reader/77002128"
                },
                {
                    "type": "thumbnail_m",
                    "url": "https://core.ac.uk/image/77002128/large"
                },
                {
                    "type": "thumbnail_l",
                    "url": "https://core.ac.uk/image/77002128/large"
                },
                {
                    "type": "display",
                    "url": "https://core.ac.uk/works/22966623"
                }
            ]
        }
]
